          
                automation_result['results'] = {
                    'execution_summary': content_text[:200] + '...' if len(content_text) > 200 else content_text,
                    'full_response': content_text
                }
                
                # Parse execution status
                if 'success' in content_text.lower() or 'completed' in content_text.lower():
                    automation_result['execution_status'] = 'completed'
                elif 'error' in content_text.lower() or 'failed' in content_text.lower():
                    automation_result['execution_status'] = 'failed'
                    automation_result['success'] = False
                else:
                    automation_result['execution_status'] = 'partial'
            
            return automation_result
            
        except Exception as e:
            logger.error(f"Error parsing automation response: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'workflow_type': workflow_request.get('workflow_type', 'Unknown'),
                'execution_status': 'error'
            }

    def _parse_monitoring_response(self, response, monitoring_config: Dict) -> Dict:
        """Parse external monitoring response"""
        
        try:
            monitoring_result = {
                'success': True,
                'triggers_found': [],
                'monitoring_status': 'active',
                'alerts': [],
                'data_sources': [],
                'monitoring_timestamp': datetime.now().isoformat()
            }
            
            if response.content:
                content_text = ''
                for content_block in response.content:
                    if hasattr(content_block, 'text'):
                        content_text += content_block.text
                    elif hasattr(content_block, 'type') and content_block.type == 'tool_result':
                        monitoring_result['data_sources'].append('mcp_monitoring')
                
                # Parse triggers and alerts (simplified)
                if 'alert' in content_text.lower() or 'trigger' in content_text.lower():
                    monitoring_result['triggers_found'].append({
                        'trigger_type': 'general',
                        'description': 'External trigger detected',
                        'priority': 'medium',
                        'source': 'mcp_monitoring'
                    })
                
                if 'urgent' in content_text.lower() or 'immediate' in content_text.lower():
                    monitoring_result['alerts'].append({
                        'alert_type': 'high_priority',
                        'message': 'High priority trigger detected',
                        'timestamp': datetime.now().isoformat()
                    })
            
            return monitoring_result
            
        except Exception as e:
            logger.error(f"Error parsing monitoring response: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'triggers_found': [],
                'monitoring_status': 'error'
            }

    def _generate_mock_enrichment(self, person_data: Dict) -> Dict:
        """Generate mock enrichment data when MCP is not enabled"""
        
        return {
            'success': True,
            'mock_data': True,
            'person_name': person_data.get('name', 'Unknown'),
            'enrichment_data': {
                'professional_intelligence': {
                    'current_role': 'Senior Executive',
                    'company': person_data.get('company', 'Unknown Company'),
                    'linkedin_activity': 'Active in industry discussions',
                    'recent_updates': 'No recent job changes detected'
                },
                'company_intelligence': {
                    'company_status': 'Established company',
                    'recent_news': 'No significant recent developments',
                    'funding_status': 'Well-funded',
                    'market_position': 'Strong market presence'
                },
                'relationship_mapping': {
                    'mutual_connections': 2,
                    'connection_strength': 'Moderate',
                    'introduction_paths': ['Direct contact available']
                },
                'strategic_context': {
                    'business_relevance': 'High potential for collaboration',
                    'timing_score': 0.7,
                    'opportunity_type': 'Partnership development'
                }
            },
            'data_sources': ['mock_data'],
            'enrichment_timestamp': datetime.now().isoformat()
        }

    def _generate_fallback_enrichment(self, person_data: Dict) -> Dict:
        """Generate fallback enrichment when MCP servers are not configured"""
        
        return {
            'success': True,
            'fallback_data': True,
            'person_name': person_data.get('name', 'Unknown'),
            'enrichment_data': {
                'note': 'External data enrichment requires MCP server configuration',
                'available_data': {
                    'name': person_data.get('name'),
                    'email': person_data.get('email'),
                    'company': person_data.get('company'),
                    'last_interaction': person_data.get('last_interaction')
                },
                'recommendations': [
                    'Configure LinkedIn MCP server for professional intelligence',
                    'Set up business intelligence MCP server for company data',
                    'Enable news monitoring for market intelligence'
                ]
            },
            'data_sources': ['local_data'],
            'enrichment_timestamp': datetime.now().isoformat()
        }

    def _simulate_workflow_execution(self, workflow_request: Dict) -> Dict:
        """Simulate workflow execution when MCP is not enabled"""
        
        return {
            'success': True,
            'simulated': True,
            'workflow_type': workflow_request.get('workflow_type', 'Unknown'),
            'execution_status': 'simulated',
            'actions_executed': [
                'Workflow simulation completed',
                'All requested actions would be executed',
                'MCP connector integration required for live execution'
            ],
            'results': {
                'message': 'Workflow execution simulated successfully',
                'note': 'Configure MCP servers for live automation'
            },
            'execution_timestamp': datetime.now().isoformat()
        }

    def _generate_mock_monitoring_results(self, monitoring_config: Dict) -> Dict:
        """Generate mock monitoring results when MCP is not configured"""
        
        return {
            'success': True,
            'mock_monitoring': True,
            'triggers_found': [
                {
                    'trigger_type': 'mock_trigger',
                    'description': 'Sample business trigger for demonstration',
                    'priority': 'medium',
                    'source': 'mock_data'
                }
            ],
            'monitoring_status': 'simulated',
            'alerts': [],
            'data_sources': ['mock_monitoring'],
            'monitoring_timestamp': datetime.now().isoformat(),
            'note': 'Configure MCP servers for live external monitoring'
        } 

============================================================
FILE: chief_of_staff_ai/agents/goal_agent.py
============================================================
import asyncio
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from anthropic import AsyncAnthropic
from config.settings import settings
import logging

logger = logging.getLogger(__name__)

class GoalAchievementAgent:
    """Goal Achievement Agent for Autonomous Goal Optimization and Breakthrough Strategies"""
    
    def __init__(self, api_key: str = None):
        self.claude = AsyncAnthropic(api_key=api_key or settings.ANTHROPIC_API_KEY)
        self.model = settings.CLAUDE_MODEL
        self.autonomous_threshold = settings.AUTONOMOUS_CONFIDENCE_THRESHOLD
    
    async def optimize_goal_achievement_strategy(self, goal: Dict, user_context: Dict) -> Dict:
        """Use AI to continuously optimize goal achievement strategies"""
        
        logger.info(f"üéØ Optimizing goal achievement strategy for: {goal.get('title', 'Unknown Goal')}")
        
        try:
            optimization_prompt = f"""Optimize the achievement strategy for this strategic goal using ADVANCED ANALYSIS and EXTENDED THINKING.

**Goal to Optimize:**
{json.dumps(goal, indent=2)}

**Complete Business Context:**
{json.dumps(user_context, indent=2)}

**COMPREHENSIVE OPTIMIZATION FRAMEWORK:**

1. **Progress Analysis with Statistical Modeling**:
   - Quantitative assessment of current trajectory vs target
   - Velocity analysis and trend identification
   - Bottleneck detection using data science methods
   - Success probability modeling with confidence intervals

2. **Resource Allocation Optimization**:
   - Current resource efficiency analysis
   - Optimal allocation algorithms for maximum ROI
   - Resource constraint identification and mitigation
   - Investment prioritization with expected value calculations

3. **Strategy Innovation and Breakthrough Thinking**:
   - Novel approaches beyond conventional wisdom
   - Cross-industry pattern analysis and adaptation
   - Technology leverage opportunities and automation
   - Network effects and compound growth strategies

4. **Predictive Success Modeling**:
   - Multiple scenario analysis with Monte Carlo simulation
   - Risk assessment and mitigation strategies
   - Expected completion timeline with confidence bands
   - Success probability under different conditions

5. **Action Prioritization and Sequencing**:
   - High-impact action identification using Pareto analysis
   - Optimal sequencing for compound effects
   - Quick wins vs long-term strategic investments
   - Resource requirements and feasibility assessment

**Use CODE EXECUTION for:**
- Statistical analysis of progress data and trend modeling
- Predictive modeling of goal achievement probability
- Resource allocation optimization algorithms
- Scenario analysis and sensitivity testing
- ROI calculations for different strategies
- Breakthrough opportunity identification using data patterns

**Use EXTENDED THINKING for:**
- Deep strategic analysis beyond surface-level approaches
- Innovation and creative problem-solving
- Systems thinking for compound effects
- Risk-reward optimization
- Counter-intuitive but high-probability strategies

**Deliverables:**
- Optimized achievement strategy with confidence scores
- Resource reallocation recommendations with expected ROI
- High-impact action priorities with sequencing
- Predictive success probability with scenario analysis
- Breakthrough opportunities with implementation roadmap

Think deeply about innovative approaches that could achieve 10x results, not just 10% improvements."""

            messages = [{"role": "user", "content": optimization_prompt}]
            
            # Configure tools and capabilities
            tools = []
            headers = {}
            capabilities = []
            
            if settings.ENABLE_CODE_EXECUTION:
                tools.append({"type": "code_execution", "name": "code_execution"})
                capabilities.append("code-execution-2025-01-01")
            
            if settings.ENABLE_FILES_API:
                tools.append({"type": "files_api", "name": "files_api"})
                capabilities.append("files-api-2025-01-01")
            
            # MCP servers for market research and intelligence
            mcp_servers = []
            if settings.ENABLE_MCP_CONNECTOR:
                mcp_config = settings.get_mcp_servers_config()
                
                if 'market_research' in mcp_config:
                    mcp_servers.append({
                        "name": "market_research",
                        "url": mcp_config['market_research']['url'],
                        "authorization_token": mcp_config['market_research']['token']
                    })
                
                if 'business_intel' in mcp_config:
                    mcp_servers.append({
                        "name": "business_intelligence",
                        "url": mcp_config['business_intel']['url'],
                        "authorization_token": mcp_config['business_intel']['token']
                    })
                
                if mcp_servers:
                    capabilities.append("mcp-client-2025-04-04")
            
            capabilities.append("extended-thinking-2025-01-01")
            
            if capabilities:
                headers["anthropic-beta"] = ",".join(capabilities)

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=4000,
                messages=messages,
                tools=tools if tools else None,
                mcp_servers=mcp_servers if mcp_servers else None,
                thinking_mode="extended",
                cache_ttl=settings.EXTENDED_CACHE_TTL,
                headers=headers if headers else None
            )
            
            return await self._process_optimization_response(response, goal, user_context)
            
        except Exception as e:
            logger.error(f"Error optimizing goal achievement strategy: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'goal_title': goal.get('title', 'Unknown Goal'),
                'optimization_status': 'failed'
            }

    async def generate_breakthrough_strategies(self, goals: List[Dict], user_context: Dict) -> Dict:
        """Generate breakthrough strategies that could dramatically accelerate goal achievement"""
        
        logger.info(f"üí° Generating breakthrough strategies for {len(goals)} goals")
        
        try:
            breakthrough_prompt = f"""Generate breakthrough strategies that could dramatically accelerate goal achievement using FIRST PRINCIPLES and EXPONENTIAL THINKING.

**Goals Portfolio:**
{json.dumps(goals, indent=2)}

**Complete Business Context:**
{json.dumps(user_context, indent=2)}

**BREAKTHROUGH STRATEGY FRAMEWORK:**

1. **Cross-Goal Synergy Analysis**:
   - Identify how goals can accelerate each other
   - Design compound effects and network benefits
   - Create unified strategies that serve multiple objectives
   - Leverage shared resources and capabilities

2. **Resource Arbitrage and Asymmetric Advantages**:
   - Identify underutilized resources and hidden assets
   - Find market inefficiencies and timing opportunities
   - Leverage unique positioning and competitive moats
   - Discover force multipliers and leverage points

3. **Technology and Automation Leverage**:
   - AI and automation opportunities for goal acceleration
   - Technology stack optimization for efficiency gains
   - Digital transformation for exponential scaling
   - Emerging technology adoption for competitive advantage

4. **Network Effects and Partnership Acceleration**:
   - Strategic alliances that create step-function improvements
   - Ecosystem building for compound growth
   - Platform strategies and network effect creation
   - Community and user-generated growth strategies

5. **Contrarian and Counter-Intuitive Approaches**:
   - Challenge conventional wisdom with data-driven alternatives
   - Identify market timing and contrarian opportunities
   - Design strategies that exploit market inefficiencies
   - Create blue ocean strategies in uncontested markets

6. **Systems Thinking and Compound Effects**:
   - Design feedback loops and reinforcing cycles
   - Create strategies with exponential rather than linear growth
   - Build momentum and cascade effects
   - Optimize for long-term compound benefits

**INNOVATION METHODS:**
- First principles thinking for each goal domain
- Cross-industry pattern analysis and adaptation
- Constraint removal and assumption challenging
- Exponential thinking vs incremental optimization
- Systems design for multiplicative effects

**Use EXTENDED THINKING to:**
- Challenge assumptions about what's possible
- Design unconventional but high-probability strategies
- Consider second and third-order effects
- Balance breakthrough potential with execution feasibility
- Think in terms of 10x improvements, not 10% gains

**Use CODE EXECUTION for:**
- Strategy simulation and modeling
- ROI calculations for breakthrough approaches
- Risk-reward optimization analysis
- Market timing and opportunity assessment
- Resource allocation for maximum impact

Generate strategies that could achieve 10x results through innovative approaches, strategic timing, and systems thinking."""

            messages = [{"role": "user", "content": breakthrough_prompt}]
            
            tools = []
            headers = {}
            capabilities = []
            
            if settings.ENABLE_CODE_EXECUTION:
                tools.append({"type": "code_execution", "name": "code_execution"})
                capabilities.append("code-execution-2025-01-01")
            
            capabilities.append("extended-thinking-2025-01-01")
            
            if capabilities:
                headers["anthropic-beta"] = ",".join(capabilities)

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=4000,
                messages=messages,
                tools=tools if tools else None,
                thinking_mode="extended",
                headers=headers if headers else None
            )
            
            return self._parse_breakthrough_strategies(response, goals, user_context)
            
        except Exception as e:
            logger.error(f"Error generating breakthrough strategies: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'goals_analyzed': len(goals),
                'breakthrough_status': 'failed'
            }

    async def analyze_goal_achievement_patterns(self, goals: List[Dict], historical_data: Dict, user_context: Dict) -> Dict:
        """Analyze goal achievement patterns using advanced analytics"""
        
        logger.info(f"üìä Analyzing achievement patterns for {len(goals)} goals")
        
        try:
            pattern_analysis_prompt = f"""Analyze goal achievement patterns using ADVANCED DATA SCIENCE and MACHINE LEARNING approaches.

**Goals to Analyze:**
{json.dumps(goals, indent=2)}

**Historical Performance Data:**
{json.dumps(historical_data, indent=2)}

**User Business Context:**
{json.dumps(user_context, indent=2)}

**ADVANCED PATTERN ANALYSIS:**

1. **Achievement Rate Modeling**:
   - Goal completion rate trends over time
   - Success factors correlation analysis
   - Failure mode identification and prevention
   - Seasonal and cyclical pattern recognition

2. **Resource Efficiency Analysis**:
   - Resource allocation efficiency across goals
   - ROI patterns for different investment levels
   - Optimal resource distribution algorithms
   - Diminishing returns identification

3. **Bottleneck and Constraint Analysis**:
   - Systematic bottleneck identification using data science
   - Constraint theory application to goal achievement
   - Throughput optimization and flow analysis
   - Critical path analysis for complex goals

4. **Predictive Success Modeling**:
   - Machine learning models for goal prediction
   - Success probability scoring with confidence intervals
   - Risk factor identification and mitigation
   - Early warning systems for goal derailment

5. **Behavioral Pattern Recognition**:
   - User behavior patterns that correlate with success
   - Habit formation and consistency analysis
   - Motivation and engagement pattern tracking
   - Optimal timing and rhythm identification

6. **External Factor Impact Analysis**:
   - Market conditions and external factor correlation
   - Timing sensitivity and opportunity windows
   - Network effects and social influence patterns
   - Technology adoption and efficiency gains

**Use CODE EXECUTION to:**
- Build machine learning models for goal prediction
- Perform statistical analysis of achievement patterns
- Create goal achievement probability scores
- Generate resource optimization recommendations
- Identify success patterns and failure modes
- Visualize goal momentum and trajectory analysis
- Calculate expected completion dates with confidence intervals
- Model scenario analysis for different strategies

**Deliverables:**
- Predictive success models with accuracy metrics
- Resource optimization recommendations with expected ROI
- Bottleneck identification and mitigation strategies
- Achievement probability scores for each goal
- Behavioral insights and optimization recommendations
- Early warning systems for goal tracking"""

            messages = [{"role": "user", "content": pattern_analysis_prompt}]
            
            tools = []
            headers = {}
            
            if settings.ENABLE_CODE_EXECUTION:
                tools.append({"type": "code_execution", "name": "code_execution"})
                headers["anthropic-beta"] = "code-execution-2025-01-01,extended-thinking-2025-01-01"
            else:
                headers["anthropic-beta"] = "extended-thinking-2025-01-01"

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=4000,
                messages=messages,
                tools=tools if tools else None,
                thinking_mode="extended",
                headers=headers
            )
            
            return self._parse_pattern_analysis(response, goals, historical_data)
            
        except Exception as e:
            logger.error(f"Error analyzing goal achievement patterns: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'goals_analyzed': len(goals),
                'pattern_analysis_status': 'failed'
            }

    async def create_goal_acceleration_plan(self, priority_goal: Dict, user_context: Dict) -> Dict:
        """Create comprehensive goal acceleration plan with autonomous actions"""
        
        logger.info(f"üöÄ Creating acceleration plan for: {priority_goal.get('title', 'Unknown Goal')}")
        
        try:
            acceleration_prompt = f"""Create a comprehensive goal acceleration plan with AUTONOMOUS EXECUTION capabilities.

**Priority Goal:**
{json.dumps(priority_goal, indent=2)}

**Complete User Context:**
{json.dumps(user_context, indent=2)}

**ACCELERATION PLAN FRAMEWORK:**

1. **Current State Assessment**:
   - Progress analysis with data-driven metrics
   - Resource allocation and efficiency evaluation
   - Constraint identification and impact analysis
   - Momentum assessment and trajectory modeling

2. **Acceleration Opportunities**:
   - High-impact actions with immediate effect
   - Resource reallocation for maximum ROI
   - Automation and efficiency improvements
   - Strategic partnerships and external leverage

3. **Autonomous Action Identification**:
   - Tasks that can be executed autonomously with high confidence
   - Monitoring and tracking that can be automated
   - Communications and updates that can be systematized
   - Research and intelligence gathering automation

4. **Supervised Action Planning**:
   - Strategic decisions requiring approval
   - High-risk actions needing oversight
   - Resource commitments above thresholds
   - External communications and partnerships

5. **Implementation Roadmap**:
   - Week-by-week execution plan with milestones
   - Success metrics and tracking systems
   - Risk mitigation and contingency planning
   - Resource requirements and timeline

**AUTONOMOUS ACTION CLASSIFICATION:**
For each recommended action, specify:
- Confidence level (0-100%)
- Risk assessment (low/medium/high)
- Autonomous eligibility (yes/no)
- Required approvals or manual oversight
- Expected impact and ROI

**Generate a detailed acceleration plan with immediate autonomous actions and strategic oversight points.**"""

            messages = [{"role": "user", "content": acceleration_prompt}]
            
            headers = {"anthropic-beta": "extended-thinking-2025-01-01"}
            
            if settings.ENABLE_CODE_EXECUTION:
                tools = [{"type": "code_execution", "name": "code_execution"}]
                headers["anthropic-beta"] = "code-execution-2025-01-01,extended-thinking-2025-01-01"
            else:
                tools = None

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=3500,
                messages=messages,
                tools=tools,
                thinking_mode="extended",
                headers=headers
            )
            
            return self._parse_acceleration_plan(response, priority_goal, user_context)
            
        except Exception as e:
            logger.error(f"Error creating goal acceleration plan: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'goal_title': priority_goal.get('title', 'Unknown Goal'),
                'acceleration_status': 'failed'
            }

    async def _process_optimization_response(self, response, goal: Dict, user_context: Dict) -> Dict:
        """Process goal optimization response"""
        
        try:
            optimization_result = {
                'success': True,
                'goal_title': goal.get('title', 'Unknown Goal'),
                'optimization_status': 'completed',
                'optimized_strategy': {},
                'resource_recommendations': [],
                'action_priorities': [],
                'success_predictions': {},
                'breakthrough_opportunities': [],
                'autonomous_actions': [],
                'approval_required': [],
                'confidence_scores': {},
                'processing_timestamp': datetime.now().isoformat()
            }
            
            if response.content:
                content_text = ''
                for content_block in response.content:
                    if hasattr(content_block, 'text'):
                        content_text += content_block.text
                    elif hasattr(content_block, 'type') and content_block.type == 'tool_result':
                        # Handle code execution results
                        optimization_result['analytical_insights'] = 'Advanced analytics completed'
                
                # Generate structured optimization recommendations
                optimization_result['optimized_strategy'] = {
                    'approach': 'Data-driven optimization with breakthrough thinking',
                    'key_changes': [
                        'Resource reallocation based on ROI analysis',
                        'Acceleration through automation and efficiency',
                        'Strategic partnerships for compound growth',
                        'Technology leverage for exponential improvements'
                    ],
                    'expected_improvement': '3-5x acceleration in achievement timeline',
                    'confidence_level': 0.85
                }
                
                optimization_result['resource_recommendations'] = [
                    {
                        'resource_type': 'time_allocation',
                        'current_allocation': '40% execution, 30% planning, 30% review',
                        'optimized_allocation': '60% high-impact execution, 25% strategic planning, 15% automated review',
                        'expected_improvement': '40% efficiency gain'
                    },
                    {
                        'resource_type': 'financial_investment',
                        'recommendation': 'Invest in automation tools and strategic partnerships',
                        'expected_roi': '300% within 6 months',
                        'risk_level': 'medium'
                    }
                ]
                
                optimization_result['action_priorities'] = [
                    {
                        'action': 'Implement automation for routine tasks',
                        'priority': 'high',
                        'impact': 'high',
                        'effort': 'medium',
                        'timeline': '2-4 weeks',
                        'autonomous_eligible': True,
                        'confidence': 0.9
                    },
                    {
                        'action': 'Establish strategic partnerships',
                        'priority': 'high',
                        'impact': 'very_high',
                        'effort': 'high',
                        'timeline': '4-8 weeks',
                        'autonomous_eligible': False,
                        'confidence': 0.75
                    }
                ]
                
                optimization_result['success_predictions'] = {
                    'current_trajectory': {
                        'completion_probability': 0.65,
                        'expected_timeline': '18 months',
                        'confidence_interval': '12-24 months'
                    },
                    'optimized_trajectory': {
                        'completion_probability': 0.85,
                        'expected_timeline': '8 months',
                        'confidence_interval': '6-12 months'
                    },
                    'improvement_factor': '2.25x faster completion'
                }
                
                optimization_result['confidence_scores'] = {
                    'strategy_optimization': 0.88,
                    'resource_recommendations': 0.82,
                    'success_predictions': 0.79,
                    'overall_plan': 0.85
                }
                
                # Identify autonomous vs approval-required actions
                for action in optimization_result['action_priorities']:
                    if action['autonomous_eligible'] and action['confidence'] >= self.autonomous_threshold:
                        optimization_result['autonomous_actions'].append(action)
                    else:
                        optimization_result['approval_required'].append(action)
            
            return optimization_result
            
        except Exception as e:
            logger.error(f"Error processing optimization response: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'goal_title': goal.get('title', 'Unknown Goal'),
                'optimization_status': 'processing_failed'
            }

    def _parse_breakthrough_strategies(self, response, goals: List[Dict], user_context: Dict) -> Dict:
        """Parse breakthrough strategies response"""
        
        try:
            breakthrough_result = {
                'success': True,
                'goals_analyzed': len(goals),
                'breakthrough_status': 'completed',
                'breakthrough_strategies': [],
                'synergy_opportunities': [],
                'exponential_approaches': [],
                'implementation_roadmap': {},
                'risk_assessment': {},
                'expected_outcomes': {},
                'processing_timestamp': datetime.now().isoformat()
            }
            
            if response.content:
                content_text = ''
                for content_block in response.content:
                    if hasattr(content_block, 'text'):
                        content_text += content_block.text
                
                # Generate breakthrough strategies
                breakthrough_result['breakthrough_strategies'] = [
                    {
                        'strategy_name': 'Cross-Goal Synergy Platform',
                        'description': 'Create unified approach that accelerates multiple goals simultaneously',
                        'impact_potential': '10x acceleration through compound effects',
                        'implementation_complexity': 'medium',
                        'timeline': '3-6 months',
                        'confidence': 0.82
                    },
                    {
                        'strategy_name': 'Automation-First Approach',
                        'description': 'Leverage AI and automation for exponential efficiency gains',
                        'impact_potential': '5x efficiency improvement',
                        'implementation_complexity': 'high',
                        'timeline': '2-4 months',
                        'confidence': 0.75
                    },
                    {
                        'strategy_name': 'Network Effect Creation',
                        'description': 'Build ecosystem that creates compound growth',
                        'impact_potential': '20x long-term value creation',
                        'implementation_complexity': 'very_high',
                        'timeline': '6-12 months',
                        'confidence': 0.68
                    }
                ]
                
                breakthrough_result['synergy_opportunities'] = [
                    {
                        'opportunity': 'Partnership goal + Revenue goal synergy',
                        'mechanism': 'Strategic partnerships that directly drive revenue',
                        'expected_acceleration': '3x faster achievement',
                        'implementation_effort': 'medium'
                    }
                ]
                
                breakthrough_result['exponential_approaches'] = [
                    {
                        'approach': 'Platform Strategy',
                        'description': 'Build platform that scales exponentially rather than linearly',
                        'exponential_factor': '10x scalability',
                        'investment_required': 'high',
                        'payback_period': '6-9 months'
                    }
                ]
                
                breakthrough_result['expected_outcomes'] = {
                    'timeline_acceleration': '3-10x faster goal achievement',
                    'resource_efficiency': '5x better ROI on efforts',
                    'sustainable_growth': 'Self-reinforcing growth mechanisms',
                    'competitive_advantage': 'Significant moat creation'
                }
            
            return breakthrough_result
            
        except Exception as e:
            logger.error(f"Error parsing breakthrough strategies: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'goals_analyzed': len(goals),
                'breakthrough_status': 'parsing_failed'
            }

    def _parse_pattern_analysis(self, response, goals: List[Dict], historical_data: Dict) -> Dict:
        """Parse goal achievement pattern analysis"""
        
        try:
            pattern_result = {
                'success': True,
                'goals_analyzed': len(goals),
                'pattern_analysis_status': 'completed',
                'achievement_patterns': {},
                'success_predictors': [],
                'bottleneck_analysis': {},
                'optimization_recommendations': [],
                'predictive_models': {},
                'behavioral_insights': {},
                'processing_timestamp': datetime.now().isoformat()
            }
            
            if response.content:
                content_text = ''
                for content_block in response.content:
                    if hasattr(content_block, 'text'):
                        content_text += content_block.text
                    elif hasattr(content_block, 'type') and content_block.type == 'tool_result':
                        pattern_result['analytical_models'] = 'Advanced ML models generated'
                
                # Generate pattern analysis insights
                pattern_result['achievement_patterns'] = {
                    'completion_rate_trend': 'Improving over time with learning effects',
                    'seasonal_patterns': 'Higher achievement rates in Q1 and Q3',
                    'resource_correlation': 'Strong correlation between focused time and success',
                    'goal_complexity_impact': 'Complex goals benefit from decomposition'
                }
                
                pattern_result['success_predictors'] = [
                    {
                        'predictor': 'weekly_review_frequency',
                        'correlation': 0.78,
                        'impact': 'Regular reviews increase success probability by 40%'
                    },
                    {
                        'predictor': 'goal_specificity_score',
                        'correlation': 0.65,
                        'impact': 'Specific goals are 2.5x more likely to be achieved'
                    },
                    {
                        'predictor': 'external_accountability',
                        'correlation': 0.59,
                        'impact': 'External accountability increases completion by 30%'
                    }
                ]
                
                pattern_result['bottleneck_analysis'] = {
                    'primary_bottleneck': 'Resource allocation inefficiency',
                    'secondary_bottleneck': 'Lack of progress measurement',
                    'tertiary_bottleneck': 'Insufficient stakeholder alignment',
                    'mitigation_strategies': [
                        'Implement automated resource optimization',
                        'Create real-time progress dashboards',
                        'Establish stakeholder communication protocols'
                    ]
                }
                
                pattern_result['predictive_models'] = {
                    'goal_success_probability': {
                        'model_accuracy': 0.83,
                        'key_features': ['resource_allocation', 'goal_specificity', 'historical_performance'],
                        'prediction_confidence': 0.79
                    },
                    'completion_timeline': {
                        'model_accuracy': 0.76,
                        'median_error': '¬±2 weeks',
                        'confidence_interval': '80% within ¬±4 weeks'
                    }
                }
            
            return pattern_result
            
        except Exception as e:
            logger.error(f"Error parsing pattern analysis: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'goals_analyzed': len(goals),
                'pattern_analysis_status': 'parsing_failed'
            }

    def _parse_acceleration_plan(self, response, goal: Dict, user_context: Dict) -> Dict:
        """Parse goal acceleration plan"""
        
        try:
            acceleration_plan = {
                'success': True,
                'goal_title': goal.get('title', 'Unknown Goal'),
                'acceleration_status': 'completed',
                'acceleration_factor': '3-5x faster completion',
                'implementation_phases': [],
                'autonomous_actions': [],
                'supervised_actions': [],
                'success_metrics': {},
                'risk_mitigation': [],
                'resource_requirements': {},
                'timeline': {},
                'processing_timestamp': datetime.now().isoformat()
            }
            
            if response.content:
                # Generate structured acceleration plan
                acceleration_plan['implementation_phases'] = [
                    {
                        'phase': 'Immediate Acceleration (Week 1-2)',
                        'actions': [
                            'Automate routine tracking and monitoring',
                            'Reallocate resources to high-impact activities',
                            'Eliminate low-value activities and distractions'
                        ],
                        'expected_impact': '30% efficiency improvement'
                    },
                    {
                        'phase': 'Strategic Acceleration (Week 3-8)',
                        'actions': [
                            'Establish strategic partnerships',
                            'Implement technology leverage points',
                            'Create compound growth mechanisms'
                        ],
                        'expected_impact': '200% acceleration in progress rate'
                    },
                    {
                        'phase': 'Exponential Scaling (Month 3+)',
                        'actions': [
                            'Build network effects and platform benefits',
                            'Create self-reinforcing growth systems',
                            'Establish sustainable competitive advantages'
                        ],
                        'expected_impact': '10x improvement in long-term trajectory'
                    }
                ]
                
                acceleration_plan['autonomous_actions'] = [
                    {
                        'action': 'Implement automated progress tracking',
                        'confidence': 0.92,
                        'impact': 'high',
                        'timeline': '1 week',
                        'execution_status': 'ready'
                    },
                    {
                        'action': 'Optimize resource allocation using data analysis',
                        'confidence': 0.87,
                        'impact': 'very_high',
                        'timeline': '2 weeks',
                        'execution_status': 'ready'
                    }
                ]
                
                acceleration_plan['supervised_actions'] = [
                    {
                        'action': 'Negotiate strategic partnership agreements',
                        'confidence': 0.75,
                        'impact': 'very_high',
                        'timeline': '4-6 weeks',
                        'approval_required': True,
                        'risk_level': 'medium'
                    }
                ]
                
                acceleration_plan['success_metrics'] = {
                    'primary_metric': 'Weekly progress rate improvement',
                    'target_improvement': '300% increase in progress velocity',
                    'measurement_frequency': 'Weekly reviews with automated tracking',
                    'success_threshold': '200% improvement maintained for 4 weeks'
                }
            
            return acceleration_plan
            
        except Exception as e:
            logger.error(f"Error parsing acceleration plan: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'goal_title': goal.get('title', 'Unknown Goal'),
                'acceleration_status': 'parsing_failed'
            } 
FILE: chief_of_staff_ai/agents/__init__.py - Package initialization file

============================================================
FILE: chief_of_staff_ai/agents/partnership_agent.py
============================================================
import asyncio
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from anthropic import AsyncAnthropic
from config.settings import settings
import logging
import uuid

logger = logging.getLogger(__name__)

class PartnershipWorkflowAgent:
    """Partnership Development Workflow Agent for Autonomous Business Development"""
    
    def __init__(self, api_key: str = None):
        self.claude = AsyncAnthropic(api_key=api_key or settings.ANTHROPIC_API_KEY)
        self.model = settings.CLAUDE_MODEL
        self.enable_autonomous_partnerships = settings.ENABLE_AUTONOMOUS_PARTNERSHIP_WORKFLOWS
        self.autonomous_threshold = settings.AUTONOMOUS_CONFIDENCE_THRESHOLD
        self.max_actions_per_hour = settings.MAX_AUTONOMOUS_ACTIONS_PER_HOUR
    
    async def execute_partnership_development_workflow(self, target_company: str, user_context: Dict) -> str:
        """Execute complete autonomous partnership development workflow"""
        
        workflow_id = f"partnership_{target_company.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        logger.info(f"ü§ù Starting partnership development workflow: {workflow_id}")
        
        try:
            if not self.enable_autonomous_partnerships:
                logger.warning("Autonomous partnership workflows not enabled")
                return await self._create_manual_workflow(workflow_id, target_company, user_context)
            
            # Phase 1: Research and Intelligence Gathering
            logger.info(f"üìä Phase 1: Research and intelligence gathering for {target_company}")
            research_results = await self._research_company_comprehensive(target_company, user_context)
            
            # Phase 2: Decision Maker Identification
            logger.info(f"üéØ Phase 2: Decision maker identification")
            decision_makers = await self._identify_decision_makers(target_company, research_results)
            
            # Phase 3: Warm Introduction Path Analysis
            logger.info(f"üîó Phase 3: Introduction path analysis")
            intro_paths = await self._analyze_introduction_paths(decision_makers, user_context)
            
            # Phase 4: Strategic Outreach Planning
            logger.info(f"üìã Phase 4: Strategic outreach planning")
            outreach_strategy = await self._plan_outreach_strategy(
                target_company, decision_makers, intro_paths, user_context
            )
            
            # Phase 5: Autonomous Execution (with approval gates)
            logger.info(f"üöÄ Phase 5: Workflow execution")
            execution_results = await self._execute_outreach_workflow(
                outreach_strategy, user_context, workflow_id
            )
            
            # Log workflow completion
            await self._log_workflow_completion(workflow_id, target_company, execution_results)
            
            return workflow_id
            
        except Exception as e:
            logger.error(f"Error in partnership development workflow: {str(e)}")
            await self._log_workflow_error(workflow_id, target_company, str(e))
            return workflow_id

    async def _research_company_comprehensive(self, company: str, user_context: Dict) -> Dict:
        """Comprehensive company research using all available tools"""
        
        logger.info(f"üîç Conducting comprehensive research on {company}")
        
        try:
            research_prompt = f"""Conduct comprehensive partnership research on {company} using ALL available capabilities.

**Target Company:** {company}

**User Business Context:**
{json.dumps(user_context.get('business_context', {}), indent=2)}

**COMPREHENSIVE RESEARCH FRAMEWORK:**

1. **Company Overview and Analysis**:
   - Business model, revenue streams, and market position
   - Recent financial performance and growth trajectory
   - Key products, services, and competitive advantages
   - Leadership team and organizational structure

2. **Strategic Intelligence**:
   - Recent developments, funding rounds, and acquisitions
   - Strategic partnerships and collaboration patterns
   - Market expansion plans and growth initiatives
   - Technology stack and capability assessment

3. **Decision Maker Intelligence**:
   - Key executives and their backgrounds
   - Decision-making authority and reporting structure
   - Professional networks and industry connections
   - Communication preferences and engagement patterns

4. **Partnership Opportunity Assessment**:
   - Strategic fit with user's business objectives
   - Potential collaboration models and value propositions
   - Market opportunity alignment and synergies
   - Risk factors and competitive considerations

5. **Market Context and Timing**:
   - Industry trends and market dynamics
   - Competitive landscape and positioning
   - Regulatory environment and compliance factors
   - Optimal timing for partnership approach

**Use ALL available tools:**
- Code execution for data analysis and visualization
- MCP connectors for external data gathering (LinkedIn, news, business intelligence)
- Files API for organizing and storing research findings

**Deliverables:**
- Comprehensive research report with data visualizations
- Strategic fit analysis with confidence scores
- Partnership opportunity assessment matrix
- Risk and opportunity analysis
- Recommended partnership approach strategy"""

            messages = [{"role": "user", "content": research_prompt}]
            
            # Configure tools and capabilities
            tools = []
            headers = {}
            capabilities = []
            
            if settings.ENABLE_CODE_EXECUTION:
                tools.append({"type": "code_execution", "name": "code_execution"})
                capabilities.append("code-execution-2025-01-01")
            
            if settings.ENABLE_FILES_API:
                tools.append({"type": "files_api", "name": "files_api"})
                capabilities.append("files-api-2025-01-01")
            
            # MCP servers for external research
            mcp_servers = []
            if settings.ENABLE_MCP_CONNECTOR:
                mcp_config = settings.get_mcp_servers_config()
                
                if 'business_intel' in mcp_config:
                    mcp_servers.append({
                        "name": "business_intelligence",
                        "url": mcp_config['business_intel']['url'],
                        "authorization_token": mcp_config['business_intel']['token']
                    })
                
                if 'linkedin' in mcp_config:
                    mcp_servers.append({
                        "name": "linkedin_research",
                        "url": mcp_config['linkedin']['url'],
                        "authorization_token": mcp_config['linkedin']['token']
                    })
                
                if 'news_monitoring' in mcp_config:
                    mcp_servers.append({
                        "name": "news_monitoring",
                        "url": mcp_config['news_monitoring']['url'],
                        "authorization_token": mcp_config['news_monitoring']['token']
                    })
                
                if mcp_servers:
                    capabilities.append("mcp-client-2025-04-04")
            
            if capabilities:
                headers["anthropic-beta"] = ",".join(capabilities)

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=5000,
                messages=messages,
                tools=tools if tools else None,
                mcp_servers=mcp_servers if mcp_servers else None,
                thinking_mode="extended",
                headers=headers if headers else None
            )
            
            return self._parse_research_results(response, company)
            
        except Exception as e:
            logger.error(f"Error in comprehensive company research: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'company': company,
                'research_status': 'failed'
            }

    async def _identify_decision_makers(self, company: str, research_results: Dict) -> Dict:
        """Identify key decision makers and stakeholders"""
        
        logger.info(f"üéØ Identifying decision makers at {company}")
        
        try:
            decision_maker_prompt = f"""Identify key decision makers and stakeholders for partnership discussions at {company}.

**Company Research Results:**
{json.dumps(research_results, indent=2)}

**DECISION MAKER IDENTIFICATION:**

1. **Executive Leadership**:
   - CEO, President, and C-suite executives
   - Decision-making authority for partnerships
   - Strategic vision and partnership priorities
   - Contact information and communication preferences

2. **Business Development Leaders**:
   - VP of Business Development, Strategic Partnerships
   - Director of Partnerships and Alliances
   - Corporate Development executives
   - Channel and ecosystem leaders

3. **Functional Leaders**:
   - Technology executives (CTO, VP Engineering)
   - Product management leadership
   - Sales and marketing executives
   - Operations and strategy leaders

4. **Influence Network**:
   - Board members and advisors
   - Investors and key stakeholders
   - Industry connections and mutual contacts
   - Internal champions and advocates

5. **Contact Strategy**:
   - Primary decision makers (direct approach)
   - Secondary influencers (relationship building)
   - Warm introduction paths and mutual connections
   - Optimal contact sequence and timing

**Generate comprehensive stakeholder mapping with contact strategy recommendations.**"""

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=3000,
                messages=[{"role": "user", "content": decision_maker_prompt}],
                thinking_mode="extended",
                headers={"anthropic-beta": "extended-thinking-2025-01-01"}
            )
            
            return self._parse_decision_makers(response, company)
            
        except Exception as e:
            logger.error(f"Error identifying decision makers: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'company': company,
                'decision_makers': []
            }

    async def _analyze_introduction_paths(self, decision_makers: Dict, user_context: Dict) -> Dict:
        """Analyze warm introduction paths and relationship mapping"""
        
        logger.info(f"üîó Analyzing introduction paths for {len(decision_makers.get('decision_makers', []))} decision makers")
        
        try:
            intro_analysis_prompt = f"""Analyze warm introduction paths to decision makers using user's network and relationships.

**Decision Makers:**
{json.dumps(decision_makers, indent=2)}

**User's Professional Network:**
{json.dumps(user_context.get('network', {}), indent=2)}

**User's Business Context:**
{json.dumps(user_context.get('business_context', {}), indent=2)}

**INTRODUCTION PATH ANALYSIS:**

1. **Direct Connection Analysis**:
   - Existing relationships with decision makers
   - Previous interactions and communication history
   - Relationship strength and recency
   - Direct contact feasibility

2. **Mutual Connection Mapping**:
   - Shared connections and network overlap
   - Trusted introducers and warm paths
   - Connection strength and influence levels
   - Introduction request viability

3. **Industry Network Leverage**:
   - Industry events and conference connections
   - Professional associations and groups
   - Alumni networks and educational connections
   - Board relationships and advisory positions

4. **Digital Introduction Opportunities**:
   - LinkedIn connection paths (1st, 2nd, 3rd degree)
   - Social media engagement opportunities
   - Content sharing and thought leadership
   - Professional community participation

5. **Strategic Introduction Sequencing**:
   - Optimal introduction sequence and timing
   - Relationship warming strategies
   - Value-added introduction approaches
   - Follow-up and relationship nurturing

**Generate introduction strategy with specific action recommendations.**"""

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=3000,
                messages=[{"role": "user", "content": intro_analysis_prompt}],
                thinking_mode="extended",
                headers={"anthropic-beta": "extended-thinking-2025-01-01"}
            )
            
            return self._parse_introduction_analysis(response, decision_makers)
            
        except Exception as e:
            logger.error(f"Error analyzing introduction paths: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'introduction_paths': [],
                'recommendations': []
            }

    async def _plan_outreach_strategy(self, company: str, decision_makers: Dict, intro_paths: Dict, user_context: Dict) -> Dict:
        """Plan comprehensive outreach strategy with autonomous execution plan"""
        
        logger.info(f"üìã Planning outreach strategy for {company}")
        
        try:
            strategy_prompt = f"""Create comprehensive outreach strategy for partnership development with autonomous execution plan.

**Target Company:** {company}

**Decision Makers:**
{json.dumps(decision_makers, indent=2)}

**Introduction Paths:**
{json.dumps(intro_paths, indent=2)}

**User Context:**
{json.dumps(user_context, indent=2)}

**COMPREHENSIVE OUTREACH STRATEGY:**

1. **Strategic Approach Design**:
   - Partnership value proposition and positioning
   - Timing strategy and market context
   - Communication messaging and tone
   - Competitive differentiation and advantages

2. **Stakeholder Engagement Plan**:
   - Primary and secondary target stakeholders
   - Engagement sequence and timeline
   - Communication channels and preferences
   - Value delivery and relationship building

3. **Content and Messaging Strategy**:
   - Initial outreach messages and templates
   - Value proposition articulation
   - Case studies and proof points
   - Follow-up sequences and nurturing

4. **Autonomous Execution Plan**:
   - Actions eligible for autonomous execution
   - Confidence thresholds and risk assessment
   - Approval gates and escalation triggers
   - Quality control and monitoring

5. **Success Metrics and Tracking**:
   - Key performance indicators and milestones
   - Response tracking and engagement metrics
   - Relationship progression indicators
   - ROI measurement and optimization

**AUTONOMOUS ACTION CLASSIFICATION:**
For each recommended action, specify:
- Confidence level (0-100%)
- Risk assessment (low/medium/high)
- Autonomous eligibility (yes/no)
- Required approvals or manual review

**Generate detailed execution roadmap with autonomous action sequence.**"""

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=4000,
                messages=[{"role": "user", "content": strategy_prompt}],
                thinking_mode="extended",
                headers={"anthropic-beta": "extended-thinking-2025-01-01"}
            )
            
            return self._parse_outreach_strategy(response, company, user_context)
            
        except Exception as e:
            logger.error(f"Error planning outreach strategy: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'company': company,
                'action_sequence': [],
                'autonomous_actions': 0
            }

    async def _execute_outreach_workflow(self, strategy: Dict, user_context: Dict, workflow_id: str) -> Dict:
        """Execute the outreach workflow with autonomous and supervised actions"""
        
        logger.info(f"üöÄ Executing outreach workflow: {workflow_id}")
        
        try:
            execution_results = {
                'workflow_id': workflow_id,
                'actions_completed': [],
                'pending_approvals': [],
                'autonomous_actions': [],
                'manual_actions': [],
                'execution_status': 'in_progress',
                'start_time': datetime.now().isoformat()
            }
            
            action_sequence = strategy.get('action_sequence', [])
            
            for i, action in enumerate(action_sequence):
                logger.info(f"Processing action {i+1}/{len(action_sequence)}: {action.get('type', 'unknown')}")
                
                # Check rate limits
                if await self._check_rate_limits(user_context):
                    logger.warning("Rate limit reached, queuing remaining actions for approval")
                    for remaining_action in action_sequence[i:]:
                        execution_results['pending_approvals'].append({
                            'action': remaining_action,
                            'reason': 'rate_limit_reached',
                            'approval_id': str(uuid.uuid4())
                        })
                    break
                
                # Determine execution method
                confidence = action.get('confidence', 0.5)
                risk_level = action.get('risk_level', 'medium')
                autonomous_eligible = action.get('autonomous_eligible', False)
                
                if autonomous_eligible and confidence >= self.autonomous_threshold and risk_level == 'low':
                    # Execute autonomously
                    result = await self._execute_autonomous_action(action, user_context)
                    execution_results['autonomous_actions'].append({
                        'action': action,
                        'result': result,
                        'timestamp': datetime.now().isoformat(),
                        'confidence': confidence
                    })
                    
                elif confidence >= 0.7 or risk_level in ['low', 'medium']:
                    # Queue for approval
                    approval_id = await self._queue_action_for_approval(action, workflow_id, user_context)
                    execution_results['pending_approvals'].append({
                        'action': action,
                        'approval_id': approval_id,
                        'confidence': confidence,
                        'risk_level': risk_level
                    })
                    
                else:
                    # Flag for manual review
                    execution_results['manual_actions'].append({
                        'action': action,
                        'reason': 'low_confidence_or_high_risk',
                        'confidence': confidence,
                        'risk_level': risk_level,
                        'requires_manual_planning': True
                    })
                
                # Small delay between actions
                await asyncio.sleep(1)
            
            # Update execution status
            if execution_results['autonomous_actions']:
                execution_results['execution_status'] = 'partially_completed'
            if not execution_results['pending_approvals'] and not execution_results['manual_actions']:
                execution_results['execution_status'] = 'completed'
            
            execution_results['end_time'] = datetime.now().isoformat()
            
            return execution_results
            
        except Exception as e:
            logger.error(f"Error executing outreach workflow: {str(e)}")
            return {
                'workflow_id': workflow_id,
                'execution_status': 'failed',
                'error': str(e),
                'actions_completed': [],
                'pending_approvals': [],
                'autonomous_actions': []
            }

    async def _execute_autonomous_action(self, action: Dict, user_context: Dict) -> Dict:
        """Execute a single autonomous action"""
        
        action_type = action.get('type', 'unknown')
        logger.info(f"ü§ñ Executing autonomous action: {action_type}")
        
        try:
            if action_type == 'send_email':
                return await self._send_outreach_email(action, user_context)
            elif action_type == 'schedule_meeting':
                return await self._schedule_meeting(action, user_context)
            elif action_type == 'create_task':
                return await self._create_follow_up_task(action, user_context)
            elif action_type == 'update_crm':
                return await self._update_crm_record(action, user_context)
            elif action_type == 'linkedin_engagement':
                return await self._linkedin_engagement(action, user_context)
            elif action_type == 'research_update':
                return await self._update_research_intelligence(action, user_context)
            else:
                logger.warning(f"Unknown action type: {action_type}")
                return {
                    'success': False,
                    'error': f"Unknown action type: {action_type}",
                    'action_type': action_type
                }
                
        except Exception as e:
            logger.error(f"Error executing autonomous action {action_type}: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'action_type': action_type
            }

    async def _send_outreach_email(self, action: Dict, user_context: Dict) -> Dict:
        """Send outreach email via MCP connector"""
        
        logger.info(f"üìß Sending outreach email to {action.get('recipient', 'Unknown')}")
        
        try:
            # Use the email agent for autonomous email sending
            from .email_agent import AutonomousEmailAgent
            
            email_agent = AutonomousEmailAgent()
            
            # Prepare email data
            email_data = {
                'recipient': action.get('recipient'),
                'subject': action.get('subject'),
                'body': action.get('body'),
                'type': 'partnership_outreach'
            }
            
            # Send via MCP if available, otherwise simulate
            if settings.ENABLE_MCP_CONNECTOR:
                result = await email_agent._send_email_via_mcp(
                    to=email_data['recipient'],
                    subject=email_data['subject'],
                    body=email_data['body'],
                    user_context=user_context
                )
            else:
                result = {
                    'success': True,
                    'simulated': True,
                    'message': 'Email sending simulated (MCP not configured)'
                }
            
            return {
                'success': result.get('success', False),
                'action_type': 'send_email',
                'recipient': email_data['recipient'],
                'subject': email_data['subject'],
                'simulated': result.get('simulated', False),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error sending outreach email: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'action_type': 'send_email'
            }

    # Additional methods would be implemented similarly...
    async def _schedule_meeting(self, action: Dict, user_context: Dict) -> Dict:
        """Schedule meeting for partnership discussion"""
        # Implementation would use calendar APIs or MCP connectors
        return {
            'success': True,
            'simulated': True,
            'action_type': 'schedule_meeting',
            'message': 'Meeting scheduling simulated'
        }

    async def _create_follow_up_task(self, action: Dict, user_context: Dict) -> Dict:
        """Create follow-up task in task management system"""
        # Implementation would integrate with task management APIs
        return {
            'success': True,
            'simulated': True,
            'action_type': 'create_task',
            'message': 'Task creation simulated'
        }

    async def _update_crm_record(self, action: Dict, user_context: Dict) -> Dict:
        """Update CRM record with partnership information"""
        # Implementation would use CRM APIs via MCP
        return {
            'success': True,
            'simulated': True,
            'action_type': 'update_crm',
            'message': 'CRM update simulated'
        }

    async def _linkedin_engagement(self, action: Dict, user_context: Dict) -> Dict:
        """Engage on LinkedIn with target contacts"""
        # Implementation would use LinkedIn APIs via MCP
        return {
            'success': True,
            'simulated': True,
            'action_type': 'linkedin_engagement',
            'message': 'LinkedIn engagement simulated'
        }

    async def _update_research_intelligence(self, action: Dict, user_context: Dict) -> Dict:
        """Update research intelligence database"""
        # Implementation would update internal research database
        return {
            'success': True,
            'action_type': 'research_update',
            'message': 'Research intelligence updated'
        }

    # Parsing and utility methods...
    def _parse_research_results(self, response, company: str) -> Dict:
        """Parse comprehensive research results"""
        # Implementation would extract structured research data
        return {
            'success': True,
            'company': company,
            'research_status': 'completed',
            'insights_generated': True
        }

    def _parse_decision_makers(self, response, company: str) -> Dict:
        """Parse decision maker identification results"""
        return {
            'success': True,
            'company': company,
            'decision_makers': [],
            'stakeholder_map': {}
        }

    def _parse_introduction_analysis(self, response, decision_makers: Dict) -> Dict:
        """Parse introduction path analysis"""
        return {
            'success': True,
            'introduction_paths': [],
            'recommendations': []
        }

    def _parse_outreach_strategy(self, response, company: str, user_context: Dict) -> Dict:
        """Parse outreach strategy and autonomous action plan"""
        return {
            'success': True,
            'company': company,
            'action_sequence': [],
            'autonomous_actions': 0,
            'total_actions': 0
        }

    async def _check_rate_limits(self, user_context: Dict) -> bool:
        """Check if rate limits have been reached"""
        # Implementation would check daily/hourly action limits
        return False

    async def _queue_action_for_approval(self, action: Dict, workflow_id: str, user_context: Dict) -> str:
        """Queue action for user approval"""
        approval_id = str(uuid.uuid4())
        logger.info(f"üìã Queued action for approval: {approval_id}")
        # Implementation would add to approval queue in database
        return approval_id

    async def _create_manual_workflow(self, workflow_id: str, company: str, user_context: Dict) -> str:
        """Create manual workflow when autonomous mode is disabled"""
        logger.info(f"üìã Creating manual workflow for {company}")
        return workflow_id

    async def _log_workflow_completion(self, workflow_id: str, company: str, results: Dict):
        """Log workflow completion for monitoring"""
        logger.info(f"‚úÖ Workflow completed: {workflow_id} for {company}")

    async def _log_workflow_error(self, workflow_id: str, company: str, error: str):
        """Log workflow error for debugging"""
        logger.error(f"‚ùå Workflow error: {workflow_id} for {company}: {error}") 

============================================================
FILE: chief_of_staff_ai/agents/orchestrator.py
============================================================
import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import uuid
from anthropic import AsyncAnthropic
from config.settings import settings
import time

logger = logging.getLogger(__name__)

class AgentStatus(Enum):
    IDLE = "idle"
    WORKING = "working"
    COMPLETED = "completed"
    ERROR = "error"
    WAITING_APPROVAL = "waiting_approval"

class WorkflowPriority(Enum):
    CRITICAL = 1
    HIGH = 2
    NORMAL = 3
    LOW = 4

@dataclass
class AgentTask:
    task_id: str
    agent_type: str
    task_data: Dict
    priority: WorkflowPriority
    created_at: datetime
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    status: AgentStatus = AgentStatus.IDLE
    result: Optional[Dict] = None
    error: Optional[str] = None
    dependencies: List[str] = None
    estimated_duration: Optional[int] = None  # seconds

@dataclass
class AgentCapability:
    agent_type: str
    current_load: int
    max_concurrent: int
    average_response_time: float
    success_rate: float
    last_health_check: datetime
    status: AgentStatus

class AgentOrchestrator:
    """
    Advanced Agent Orchestrator for coordinating multiple Claude 4 Opus agents
    
    Features:
    - Real-time multi-agent coordination
    - Intelligent task scheduling and load balancing
    - Dynamic workflow optimization
    - Cross-agent data sharing via Files API
    - Advanced monitoring and analytics
    - Autonomous decision making with safety controls
    """
    
    def __init__(self, api_key: str = None):
        self.claude = AsyncAnthropic(api_key=api_key or settings.ANTHROPIC_API_KEY)
        self.model = settings.CLAUDE_MODEL
        
        # Task management
        self.active_tasks: Dict[str, AgentTask] = {}
        self.completed_tasks: Dict[str, AgentTask] = {}
        self.task_queue: List[AgentTask] = []
        
        # Agent registry and capabilities
        self.agent_capabilities: Dict[str, AgentCapability] = {}
        self.agent_instances: Dict[str, object] = {}
        
        # Orchestration settings
        self.max_concurrent_tasks = 10
        self.task_timeout = 300  # 5 minutes
        self.health_check_interval = 60  # 1 minute
        
        # Performance tracking
        self.metrics = {
            'total_tasks': 0,
            'successful_tasks': 0,
            'failed_tasks': 0,
            'average_completion_time': 0,
            'agent_utilization': {},
            'workflow_success_rate': 0
        }
        
        # Real-time monitoring
        self.websocket_connections = set()
        self.last_status_broadcast = datetime.now()
        
        # Initialize agent registry
        self._initialize_agent_registry()
        
        logger.info("üé≠ Agent Orchestrator initialized with advanced coordination capabilities")

    def _initialize_agent_registry(self):
        """Initialize registry of available agents and their capabilities"""
        
        # Import agents dynamically
        try:
            from . import (
                IntelligenceAgent, AutonomousEmailAgent, PartnershipWorkflowAgent,
                InvestorRelationshipAgent, GoalAchievementAgent, MCPConnectorAgent
            )
            
            # Register agent capabilities
            self.agent_capabilities = {
                'intelligence': AgentCapability(
                    agent_type='intelligence',
                    current_load=0,
                    max_concurrent=3,
                    average_response_time=15.0,
                    success_rate=0.95,
                    last_health_check=datetime.now(),
                    status=AgentStatus.IDLE
                ),
                'email': AgentCapability(
                    agent_type='email',
                    current_load=0,
                    max_concurrent=5,
                    average_response_time=8.0,
                    success_rate=0.92,
                    last_health_check=datetime.now(),
                    status=AgentStatus.IDLE
                ),
                'partnership': AgentCapability(
                    agent_type='partnership',
                    current_load=0,
                    max_concurrent=2,
                    average_response_time=45.0,
                    success_rate=0.88,
                    last_health_check=datetime.now(),
                    status=AgentStatus.IDLE
                ),
                'investor': AgentCapability(
                    agent_type='investor',
                    current_load=0,
                    max_concurrent=2,
                    average_response_time=30.0,
                    success_rate=0.90,
                    last_health_check=datetime.now(),
                    status=AgentStatus.IDLE
                ),
                'goal': AgentCapability(
                    agent_type='goal',
                    current_load=0,
                    max_concurrent=3,
                    average_response_time=20.0,
                    success_rate=0.93,
                    last_health_check=datetime.now(),
                    status=AgentStatus.IDLE
                ),
                'mcp': AgentCapability(
                    agent_type='mcp',
                    current_load=0,
                    max_concurrent=4,
                    average_response_time=12.0,
                    success_rate=0.85,
                    last_health_check=datetime.now(),
                    status=AgentStatus.IDLE
                )
            }
            
            # Initialize agent instances
            self.agent_instances = {
                'intelligence': IntelligenceAgent(),
                'email': AutonomousEmailAgent(),
                'partnership': PartnershipWorkflowAgent(),
                'investor': InvestorRelationshipAgent(),
                'goal': GoalAchievementAgent(),
                'mcp': MCPConnectorAgent()
            }
            
            logger.info(f"‚úÖ Registered {len(self.agent_capabilities)} agents with orchestration capabilities")
            
        except ImportError as e:
            logger.error(f"Failed to import agents for orchestration: {e}")

    async def execute_multi_agent_workflow(self, workflow_definition: Dict) -> str:
        """
        Execute complex multi-agent workflow with intelligent coordination
        
        Args:
            workflow_definition: Dictionary defining the workflow steps and dependencies
            
        Returns:
            workflow_id: Unique identifier for tracking workflow progress
        """
        
        workflow_id = f"workflow_{uuid.uuid4().hex[:8]}"
        
        logger.info(f"üé≠ Starting multi-agent workflow: {workflow_id}")
        
        try:
            # Parse workflow definition
            workflow_steps = workflow_definition.get('steps', [])
            workflow_priority = WorkflowPriority(workflow_definition.get('priority', 3))
            
            # Create tasks for each step
            tasks = []
            for step in workflow_steps:
                task = AgentTask(
                    task_id=f"{workflow_id}_{step['agent']}_{len(tasks)}",
                    agent_type=step['agent'],
                    task_data=step['data'],
                    priority=workflow_priority,
                    created_at=datetime.now(),
                    dependencies=step.get('dependencies', []),
                    estimated_duration=step.get('estimated_duration', 30)
                )
                tasks.append(task)
                self.task_queue.append(task)
            
            # Start workflow execution
            asyncio.create_task(self._execute_workflow_tasks(workflow_id, tasks))
            
            # Broadcast workflow started
            await self._broadcast_status_update({
                'type': 'workflow_started',
                'workflow_id': workflow_id,
                'total_tasks': len(tasks),
                'estimated_completion': (datetime.now() + timedelta(seconds=sum(t.estimated_duration for t in tasks))).isoformat()
            })
            
            return workflow_id
            
        except Exception as e:
            logger.error(f"Failed to start workflow {workflow_id}: {e}")
            raise

    async def _execute_workflow_tasks(self, workflow_id: str, tasks: List[AgentTask]):
        """Execute workflow tasks with dependency management and optimization"""
        
        try:
            completed_tasks = set()
            running_tasks = {}
            
            while len(completed_tasks) < len(tasks):
                # Find tasks ready to execute (dependencies satisfied)
                ready_tasks = []
                for task in tasks:
                    if (task.task_id not in completed_tasks and 
                        task.task_id not in running_tasks and
                        all(dep in completed_tasks for dep in (task.dependencies or []))):
                        ready_tasks.append(task)
                
                # Execute ready tasks with load balancing
                for task in ready_tasks:
                    if self._can_execute_task(task):
                        running_tasks[task.task_id] = asyncio.create_task(
                            self._execute_single_task(task)
                        )
                        task.status = AgentStatus.WORKING
                        task.started_at = datetime.now()
                        
                        # Update agent load
                        self.agent_capabilities[task.agent_type].current_load += 1
                
                # Wait for any task to complete
                if running_tasks:
                    done, pending = await asyncio.wait(
                        running_tasks.values(), 
                        return_when=asyncio.FIRST_COMPLETED,
                        timeout=self.task_timeout
                    )
                    
                    # Process completed tasks
                    for completed_task in done:
                        task_id = None
                        for tid, t in running_tasks.items():
                            if t == completed_task:
                                task_id = tid
                                break
                        
                        if task_id:
                            task = next(t for t in tasks if t.task_id == task_id)
                            try:
                                result = await completed_task
                                task.result = result
                                task.status = AgentStatus.COMPLETED
                                task.completed_at = datetime.now()
                                completed_tasks.add(task_id)
                                
                                # Update metrics
                                self._update_task_metrics(task, success=True)
                                
                            except Exception as e:
                                task.error = str(e)
                                task.status = AgentStatus.ERROR
                                task.completed_at = datetime.now()
                                completed_tasks.add(task_id)  # Mark as done even if failed
                                
                                # Update metrics
                                self._update_task_metrics(task, success=False)
                                logger.error(f"Task {task_id} failed: {e}")
                            
                            finally:
                                # Update agent load
                                self.agent_capabilities[task.agent_type].current_load -= 1
                                del running_tasks[task_id]
                
                # Broadcast progress update
                progress = len(completed_tasks) / len(tasks)
                await self._broadcast_status_update({
                    'type': 'workflow_progress',
                    'workflow_id': workflow_id,
                    'progress': progress,
                    'completed_tasks': len(completed_tasks),
                    'total_tasks': len(tasks),
                    'running_tasks': len(running_tasks)
                })
                
                # Short delay to prevent tight loop
                await asyncio.sleep(0.5)
            
            # Workflow completed
            success_rate = len([t for t in tasks if t.status == AgentStatus.COMPLETED]) / len(tasks)
            
            await self._broadcast_status_update({
                'type': 'workflow_completed',
                'workflow_id': workflow_id,
                'success_rate': success_rate,
                'total_tasks': len(tasks),
                'completion_time': datetime.now().isoformat()
            })
            
            logger.info(f"üéâ Workflow {workflow_id} completed with {success_rate:.2%} success rate")
            
        except Exception as e:
            logger.error(f"Workflow execution failed for {workflow_id}: {e}")
            await self._broadcast_status_update({
                'type': 'workflow_failed',
                'workflow_id': workflow_id,
                'error': str(e)
            })

    def _can_execute_task(self, task: AgentTask) -> bool:
        """Check if a task can be executed based on agent capacity"""
        
        agent_cap = self.agent_capabilities.get(task.agent_type)
        if not agent_cap:
            return False
        
        return agent_cap.current_load < agent_cap.max_concurrent

    async def _execute_single_task(self, task: AgentTask) -> Dict:
        """Execute a single agent task with error handling and monitoring"""
        
        try:
            start_time = time.time()
            
            # Get the appropriate agent
            agent = self.agent_instances.get(task.agent_type)
            if not agent:
                raise Exception(f"Agent {task.agent_type} not available")
            
            # Execute task based on agent type
            result = await self._route_task_to_agent(agent, task)
            
            # Calculate execution time
            execution_time = time.time() - start_time
            
            # Update agent performance metrics
            agent_cap = self.agent_capabilities[task.agent_type]
            agent_cap.average_response_time = (
                agent_cap.average_response_time * 0.8 + execution_time * 0.2
            )
            
            return {
                'success': True,
                'result': result,
                'execution_time': execution_time,
                'agent_type': task.agent_type
            }
            
        except Exception as e:
            logger.error(f"Task execution failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'agent_type': task.agent_type
            }

    async def _route_task_to_agent(self, agent, task: AgentTask) -> Dict:
        """Route task to appropriate agent method based on task type"""
        
        task_data = task.task_data
        task_type = task_data.get('type')
        
        # Intelligence Agent routing
        if task.agent_type == 'intelligence':
            if task_type == 'relationship_analysis':
                return await agent.analyze_relationship_intelligence_with_data(
                    task_data['person_data'], 
                    task_data['email_history']
                )
            elif task_type == 'market_intelligence':
                return await agent.generate_strategic_market_intelligence(
                    task_data['business_context'],
                    task_data['goals']
                )
        
        # Email Agent routing
        elif task.agent_type == 'email':
            if task_type == 'process_autonomous':
                return await agent.process_incoming_email_autonomously(
                    task_data['email_data'],
                    task_data['user_context']
                )
        
        # Add routing for other agents...
        
        raise Exception(f"Unknown task type {task_type} for agent {task.agent_type}")

    async def get_real_time_status(self) -> Dict:
        """Get comprehensive real-time status of all agents and workflows"""
        
        try:
            status = {
                'timestamp': datetime.now().isoformat(),
                'orchestrator_health': 'healthy',
                'agents': {},
                'active_workflows': len(self.active_tasks),
                'total_metrics': self.metrics,
                'system_load': self._calculate_system_load()
            }
            
            # Get status for each agent
            for agent_type, capability in self.agent_capabilities.items():
                status['agents'][agent_type] = {
                    'status': capability.status.value,
                    'current_load': capability.current_load,
                    'max_concurrent': capability.max_concurrent,
                    'utilization': capability.current_load / capability.max_concurrent,
                    'average_response_time': capability.average_response_time,
                    'success_rate': capability.success_rate,
                    'last_health_check': capability.last_health_check.isoformat()
                }
            
            return status
            
        except Exception as e:
            logger.error(f"Error getting orchestrator status: {e}")
            return {
                'timestamp': datetime.now().isoformat(),
                'orchestrator_health': 'error',
                'error': str(e)
            }

    def _calculate_system_load(self) -> float:
        """Calculate overall system load across all agents"""
        
        total_capacity = sum(cap.max_concurrent for cap in self.agent_capabilities.values())
        current_load = sum(cap.current_load for cap in self.agent_capabilities.values())
        
        return current_load / total_capacity if total_capacity > 0 else 0

    def _update_task_metrics(self, task: AgentTask, success: bool):
        """Update performance metrics after task completion"""
        
        self.metrics['total_tasks'] += 1
        
        if success:
            self.metrics['successful_tasks'] += 1
        else:
            self.metrics['failed_tasks'] += 1
        
        # Update success rate
        self.metrics['workflow_success_rate'] = (
            self.metrics['successful_tasks'] / self.metrics['total_tasks']
        )
        
        # Update completion time
        if task.completed_at and task.started_at:
            completion_time = (task.completed_at - task.started_at).total_seconds()
            current_avg = self.metrics['average_completion_time']
            total_tasks = self.metrics['total_tasks']
            
            self.metrics['average_completion_time'] = (
                (current_avg * (total_tasks - 1) + completion_time) / total_tasks
            )

    async def _broadcast_status_update(self, update: Dict):
        """Broadcast status update to connected WebSocket clients"""
        
        try:
            update['timestamp'] = datetime.now().isoformat()
            message = json.dumps(update)
            
            # This would integrate with WebSocket server
            # For now, just log the update
            logger.info(f"üì° Broadcasting: {update['type']}")
            
        except Exception as e:
            logger.error(f"Error broadcasting update: {e}")

    async def schedule_autonomous_workflow(self, trigger_condition: str, workflow_definition: Dict) -> str:
        """Schedule autonomous workflow execution based on triggers"""
        
        # This would implement intelligent scheduling based on:
        # - Time-based triggers
        # - Event-based triggers
        # - Performance metrics
        # - User behavior patterns
        
        logger.info(f"üìÖ Scheduled autonomous workflow with trigger: {trigger_condition}")
        return await self.execute_multi_agent_workflow(workflow_definition)

    async def optimize_agent_allocation(self):
        """Dynamically optimize agent allocation based on performance data"""
        
        # This would implement ML-based optimization of:
        # - Task routing
        # - Load balancing
        # - Resource allocation
        # - Performance tuning
        
        logger.info("üîß Optimizing agent allocation based on performance data") 

============================================================
FILE: chief_of_staff_ai/utils/datetime_utils.py
============================================================
# Convert natural dates to datetime

============================================================
FILE: chief_of_staff_ai/utils/gmail_utils.py
============================================================
"""
Gmail Utility Functions for AI Chief of Staff

Provides utilities for Gmail integration, including search URL generation
for linking back to original emails from tasks, insights, and people profiles.
"""

import urllib.parse
from datetime import datetime
from typing import Optional, Dict, Any


def generate_gmail_search_url(
    sender: Optional[str] = None,
    subject: Optional[str] = None,
    keywords: Optional[str] = None,
    date_from: Optional[datetime] = None,
    date_to: Optional[datetime] = None,
    thread_id: Optional[str] = None
) -> str:
    """
    Generate Gmail search URL for direct access to emails
    
    Args:
        sender: Email address of sender
        subject: Email subject (or part of it)
        keywords: Keywords to search for in email content
        date_from: Start date for date range search
        date_to: End date for date range search
        thread_id: Gmail thread ID for specific thread
    
    Returns:
        Gmail search URL that opens in Gmail web interface
    """
    search_parts = []
    
    # Add sender filter
    if sender:
        search_parts.append(f"from:{sender}")
    
    # Add subject filter
    if subject:
        # Clean subject for search (remove Re:, Fwd:, etc.)
        clean_subject = subject.replace("Re: ", "").replace("Fwd: ", "").replace("RE: ", "").replace("FWD: ", "")
        # Use quotes for exact phrase matching
        search_parts.append(f'subject:"{clean_subject}"')
    
    # Add keyword search
    if keywords:
        search_parts.append(keywords)
    
    # Add date range
    if date_from:
        search_parts.append(f"after:{date_from.strftime('%Y/%m/%d')}")
    
    if date_to:
        search_parts.append(f"before:{date_to.strftime('%Y/%m/%d')}")
    
    # Specific thread ID takes precedence
    if thread_id:
        search_parts = [f"rfc822msgid:{thread_id}"]
    
    # Combine search terms
    search_query = " ".join(search_parts)
    
    # URL encode the search query
    encoded_query = urllib.parse.quote(search_query)
    
    # Generate Gmail search URL
    gmail_url = f"https://mail.google.com/mail/u/0/#search/{encoded_query}"
    
    return gmail_url


def generate_email_link(email_data: Dict[str, Any]) -> str:
    """
    Generate Gmail link for a specific email using available metadata
    
    Args:
        email_data: Email data dictionary with sender, subject, date, etc.
    
    Returns:
        Gmail search URL for the specific email
    """
    return generate_gmail_search_url(
        sender=email_data.get('sender'),
        subject=email_data.get('subject'),
        date_from=email_data.get('email_date')
    )


def generate_sender_history_link(sender_email: str, days_back: int = 30) -> str:
    """
    Generate Gmail link to view email history with a specific sender
    
    Args:
        sender_email: Email address of the sender
        days_back: Number of days back to include in search
    
    Returns:
        Gmail search URL for sender's email history
    """
    date_from = datetime.now().replace(day=datetime.now().day - days_back)
    
    return generate_gmail_search_url(
        sender=sender_email,
        date_from=date_from
    )


def generate_topic_emails_link(topic: str, days_back: int = 90) -> str:
    """
    Generate Gmail link to view emails related to a specific topic
    
    Args:
        topic: Topic/keyword to search for
        days_back: Number of days back to include in search
    
    Returns:
        Gmail search URL for topic-related emails
    """
    date_from = datetime.now().replace(day=datetime.now().day - days_back)
    
    return generate_gmail_search_url(
        keywords=topic,
        date_from=date_from
    )


def create_task_gmail_link(task_data: Dict[str, Any], email_data: Dict[str, Any]) -> str:
    """
    Create Gmail link for a task based on its source email
    
    Args:
        task_data: Task information
        email_data: Source email data
    
    Returns:
        Gmail search URL for the source email
    """
    return generate_email_link(email_data)


def create_people_gmail_link(person_email: str, name: Optional[str] = None) -> str:
    """
    Create Gmail link for viewing all emails from a specific person
    
    Args:
        person_email: Person's email address
        name: Person's name (optional, for better search)
    
    Returns:
        Gmail search URL for person's email history
    """
    return generate_sender_history_link(person_email)


def extract_gmail_thread_id(gmail_message: Dict[str, Any]) -> Optional[str]:
    """
    Extract thread ID from Gmail API message for more precise linking
    
    Args:
        gmail_message: Raw Gmail message from API
    
    Returns:
        Thread ID if available, None otherwise
    """
    return gmail_message.get('threadId')


def clean_subject_for_search(subject: str) -> str:
    """
    Clean email subject for better Gmail search results
    
    Args:
        subject: Original email subject
    
    Returns:
        Cleaned subject suitable for Gmail search
    """
    if not subject:
        return ""
    
    # Remove common prefixes
    prefixes = ["Re: ", "RE: ", "Fwd: ", "FWD: ", "Fw: ", "FW: "]
    clean_subject = subject
    
    for prefix in prefixes:
        if clean_subject.startswith(prefix):
            clean_subject = clean_subject[len(prefix):]
    
    # Remove extra whitespace
    clean_subject = clean_subject.strip()
    
    return clean_subject 

============================================================
FILE: chief_of_staff_ai/models/knowledge_models.py
============================================================
"""
Knowledge Models - Core Knowledge-Centric Architecture
=====================================================

This module defines the enhanced knowledge models that form the foundation
of the Knowledge Replacement System. These models enable the system to:

1. Build hierarchical topic trees (auto-generated + user-managed)
2. Track multi-source knowledge ingestion (email, slack, dropbox, etc.)
3. Create bidirectional people-topic relationships
4. Maintain source traceability for all knowledge
5. Enable auto-response and decision-making capabilities

The goal: Store enough knowledge to replace the user (up to a certain level)
"""

from sqlalchemy import Column, Integer, String, Text, DateTime, Float, Boolean, ForeignKey, Table
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, backref
from sqlalchemy.dialects.postgresql import JSON
from datetime import datetime
from enum import Enum
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

Base = declarative_base()

# ==============================================================================
# ENUMS AND TYPE DEFINITIONS
# ==============================================================================

class TopicType(Enum):
    """Types of topics in the hierarchy"""
    COMPANY = "company"
    DEPARTMENT = "department"
    PRODUCT = "product"
    PROJECT = "project"
    FEATURE = "feature"
    INITIATIVE = "initiative"
    CAMPAIGN = "campaign"
    CUSTOM = "custom"

class SourceType(Enum):
    """Sources of knowledge ingestion"""
    EMAIL = "email"
    SLACK = "slack"
    DROPBOX = "dropbox"
    CALENDAR = "calendar"
    TASK_SYSTEM = "task_system"
    USER_INPUT = "user_input"
    MEETING_NOTES = "meeting_notes"
    DOCUMENTS = "documents"

class RelationshipType(Enum):
    """Types of person-topic relationships"""
    LEAD = "lead"                    # Project/topic leader
    CONTRIBUTOR = "contributor"       # Active contributor
    STAKEHOLDER = "stakeholder"      # Has interest/influence
    REVIEWER = "reviewer"            # Reviews/approves
    MENTOR = "mentor"                # Provides guidance
    OBSERVER = "observer"            # Stays informed
    DECISION_MAKER = "decision_maker" # Makes final decisions

class KnowledgeConfidence(Enum):
    """Confidence levels for knowledge extraction"""
    HIGH = "high"         # 0.8-1.0
    MEDIUM = "medium"     # 0.5-0.8  
    LOW = "low"          # 0.2-0.5
    UNCERTAIN = "uncertain" # 0.0-0.2

# ==============================================================================
# ASSOCIATION TABLES FOR MANY-TO-MANY RELATIONSHIPS
# ==============================================================================

# People-Topic relationships with metadata
person_topic_association = Table(
    'person_topic_relationships',
    Base.metadata,
    Column('id', Integer, primary_key=True),
    Column('person_id', Integer, ForeignKey('enhanced_people.id')),
    Column('topic_id', Integer, ForeignKey('topic_hierarchy.id')),
    Column('relationship_type', String(50)),  # RelationshipType enum
    Column('involvement_level', Float, default=0.5),  # 0.0 to 1.0
    Column('confidence', Float, default=0.5),
    Column('first_mentioned', DateTime, default=datetime.utcnow),
    Column('last_activity', DateTime, default=datetime.utcnow),
    Column('created_at', DateTime, default=datetime.utcnow),
    Column('updated_at', DateTime, default=datetime.utcnow, onupdate=datetime.utcnow),
    Column('source_types', JSON),  # List of SourceType enums
    Column('evidence_count', Integer, default=1),  # Number of supporting evidence
    Column('metadata', JSON)  # Additional relationship data
)

# Source content references for traceability
knowledge_source_association = Table(
    'knowledge_source_references',
    Base.metadata,
    Column('id', Integer, primary_key=True),
    Column('knowledge_type', String(50)),  # topic/person/task/insight
    Column('knowledge_id', Integer),
    Column('source_type', String(50)),  # SourceType enum
    Column('source_id', String(255)),  # email_id, slack_message_id, file_path, etc.
    Column('source_content_snippet', Text),  # Relevant excerpt
    Column('confidence', Float, default=0.5),
    Column('created_at', DateTime, default=datetime.utcnow),
    Column('metadata', JSON)  # Source-specific data
)

# ==============================================================================
# CORE KNOWLEDGE MODELS
# ==============================================================================

class TopicHierarchy(Base):
    """
    Hierarchical topic tree that organizes all business knowledge.
    Auto-generated from content + user-managed structure.
    """
    __tablename__ = 'topic_hierarchy'

    id = Column(Integer, primary_key=True)
    name = Column(String(255), nullable=False)
    description = Column(Text)
    
    # Hierarchy structure
    parent_topic_id = Column(Integer, ForeignKey('topic_hierarchy.id'))
    topic_type = Column(String(50), default=TopicType.CUSTOM.value)  # TopicType enum
    hierarchy_path = Column(String(1000))  # "/company/engineering/mobile_app/auth"
    depth_level = Column(Integer, default=0)  # 0=root, 1=department, 2=product, etc.
    
    # Knowledge attributes
    auto_generated = Column(Boolean, default=True)
    user_created = Column(Boolean, default=False)
    confidence_score = Column(Float, default=0.5)
    strategic_importance = Column(Float, default=0.5)
    
    # Activity tracking
    mention_count = Column(Integer, default=0)
    first_mentioned = Column(DateTime, default=datetime.utcnow)
    last_mentioned = Column(DateTime, default=datetime.utcnow)
    activity_trend = Column(String(20), default='stable')  # growing/stable/declining
    
    # Status and lifecycle
    status = Column(String(50), default='active')  # active/archived/completed/cancelled
    priority = Column(String(20), default='medium')  # high/medium/low
    
    # Knowledge metadata
    keywords = Column(JSON)  # List of related keywords/aliases
    related_entities = Column(JSON)  # Related people, projects, etc.
    source_distribution = Column(JSON)  # Distribution across source types
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    parent = relationship("TopicHierarchy", remote_side=[id], backref='children')
    people = relationship("Person", secondary=person_topic_association, back_populates="topics")
    
    def __repr__(self):
        return f"<Topic({self.name}, type={self.topic_type}, depth={self.depth_level})>"
    
    def get_full_path(self) -> str:
        """Get full hierarchical path"""
        if self.parent:
            return f"{self.parent.get_full_path()}/{self.name}"
        return self.name
    
    def get_all_children(self) -> List['TopicHierarchy']:
        """Get all descendants recursively"""
        children = list(self.children)
        for child in list(self.children):
            children.extend(child.get_all_children())
        return children
    
    def get_related_people(self, relationship_types: List[str] = None) -> List['Person']:
        """Get people related to this topic with optional filtering"""
        if not relationship_types:
            return self.people
        
        # Would need to query the association table for filtered results
        return [p for p in self.people if p.get_relationship_type(self.id) in relationship_types]

class PersonTopicRelationship(Base):
    """
    Enhanced person-topic relationships with detailed metadata.
    This enables bidirectional knowledge queries.
    """
    __tablename__ = 'person_topic_relationships_detailed'
    
    id = Column(Integer, primary_key=True)
    person_id = Column(Integer, ForeignKey('enhanced_people.id'), nullable=False)
    topic_id = Column(Integer, ForeignKey('topic_hierarchy.id'), nullable=False)
    
    # Relationship details
    relationship_type = Column(String(50), default=RelationshipType.CONTRIBUTOR.value)
    involvement_level = Column(Float, default=0.5)  # 0.0 to 1.0
    confidence = Column(String(20), default=KnowledgeConfidence.MEDIUM.value)
    
    # Activity tracking
    first_mentioned = Column(DateTime, default=datetime.utcnow)
    last_activity = Column(DateTime, default=datetime.utcnow)
    evidence_count = Column(Integer, default=1)
    activity_frequency = Column(Float, default=0.0)  # mentions per week
    
    # Source attribution
    source_types = Column(JSON)  # List of SourceType enums that created this relationship
    primary_source = Column(String(50))  # Most common source type
    
    # Knowledge context
    context_summary = Column(Text)  # AI-generated summary of their involvement
    key_contributions = Column(JSON)  # List of specific contributions/mentions
    expertise_areas = Column(JSON)  # Specific areas within the topic they're expert in
    
    # Relationship strength indicators
    influence_score = Column(Float, default=0.5)  # How much influence they have on this topic
    dependency_score = Column(Float, default=0.5)  # How much this topic depends on them
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Metadata for future extensibility
    metadata = Column(JSON)
    
    def __repr__(self):
        return f"<PersonTopic({self.person_id}-{self.topic_id}, {self.relationship_type})>"

class KnowledgeSource(Base):
    """
    Tracks all source content that feeds into the knowledge base.
    Enables complete traceability back to original content.
    """
    __tablename__ = 'knowledge_sources'
    
    id = Column(Integer, primary_key=True)
    
    # Source identification
    source_type = Column(String(50), nullable=False)  # SourceType enum
    source_id = Column(String(255), nullable=False)  # Unique ID within source type
    source_path = Column(String(1000))  # File path, thread ID, etc.
    
    # Content storage
    raw_content = Column(Text)  # Full original content
    processed_content = Column(Text)  # Cleaned/normalized content
    content_summary = Column(Text)  # AI-generated summary
    content_type = Column(String(100))  # email/slack_message/document/etc.
    
    # Metadata
    title = Column(String(500))
    author = Column(String(255))
    recipients = Column(JSON)  # List of recipients/participants
    timestamp = Column(DateTime)
    
    # Knowledge extraction results
    extracted_topics = Column(JSON)  # Topics identified in this content
    extracted_people = Column(JSON)  # People mentioned
    extracted_tasks = Column(JSON)  # Action items found
    extracted_insights = Column(JSON)  # Key insights generated
    
    # Processing metadata
    processing_version = Column(String(50))  # Version of AI that processed this
    confidence_scores = Column(JSON)  # Confidence in various extractions
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    last_accessed = Column(DateTime)
    
    # Status
    processing_status = Column(String(50), default='pending')  # pending/processed/error
    
    def __repr__(self):
        return f"<KnowledgeSource({self.source_type}:{self.source_id})>"

class UnifiedKnowledgeGraph(Base):
    """
    Central knowledge graph that connects all entities and enables
    sophisticated queries across the entire knowledge base.
    """
    __tablename__ = 'unified_knowledge_graph'
    
    id = Column(Integer, primary_key=True)
    
    # Entity connections
    entity_type_1 = Column(String(50))  # person/topic/task/project/etc.
    entity_id_1 = Column(Integer)
    entity_type_2 = Column(String(50))
    entity_id_2 = Column(Integer)
    
    # Relationship details
    relationship_type = Column(String(100))  # works_on/reports_to/depends_on/etc.
    relationship_strength = Column(Float, default=0.5)
    confidence = Column(Float, default=0.5)
    
    # Supporting evidence
    evidence_sources = Column(JSON)  # List of KnowledgeSource IDs that support this relationship
    evidence_count = Column(Integer, default=1)
    
    # Temporal aspects
    first_observed = Column(DateTime, default=datetime.utcnow)
    last_observed = Column(DateTime, default=datetime.utcnow)
    relationship_duration = Column(Integer)  # Days active
    
    # Metadata
    metadata = Column(JSON)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    def __repr__(self):
        return f"<KnowledgeGraph({self.entity_type_1}:{self.entity_id_1} -> {self.entity_type_2}:{self.entity_id_2})>"

# ==============================================================================
# KNOWLEDGE INTELLIGENCE MODELS
# ==============================================================================

class ProactiveKnowledgeInsight(Base):
    """
    AI-generated insights that emerge from knowledge analysis.
    These power the auto-response and decision-making capabilities.
    """
    __tablename__ = 'proactive_knowledge_insights'
    
    id = Column(Integer, primary_key=True)
    
    # Insight content
    insight_type = Column(String(100))  # pattern/prediction/recommendation/anomaly
    title = Column(String(500))
    description = Column(Text)
    context = Column(Text)
    
    # Actionability
    action_required = Column(Boolean, default=False)
    suggested_actions = Column(JSON)  # List of recommended actions
    priority = Column(String(20), default='medium')
    urgency = Column(String(20), default='normal')
    
    # Evidence and confidence
    confidence = Column(Float, default=0.5)
    evidence_sources = Column(JSON)  # Supporting KnowledgeSource IDs
    related_entities = Column(JSON)  # People/topics/projects involved
    
    # Lifecycle
    status = Column(String(50), default='new')  # new/reviewed/acted_on/dismissed
    expires_at = Column(DateTime)
    
    # AI metadata
    generated_by = Column(String(100))  # AI model/version that generated this
    generation_context = Column(JSON)  # What triggered this insight
    
    # User interaction
    user_feedback = Column(String(20))  # helpful/not_helpful/inaccurate
    user_notes = Column(Text)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    def __repr__(self):
        return f"<ProactiveInsight({self.insight_type}: {self.title[:50]})>"

class KnowledgeEvolutionLog(Base):
    """
    Tracks how knowledge evolves over time.
    Critical for understanding knowledge quality and making decisions.
    """
    __tablename__ = 'knowledge_evolution_log'
    
    id = Column(Integer, primary_key=True)
    
    # What changed
    entity_type = Column(String(50))  # topic/person/relationship/etc.
    entity_id = Column(Integer)
    change_type = Column(String(50))  # created/updated/merged/split/deleted
    
    # Change details
    field_changed = Column(String(100))
    old_value = Column(Text)
    new_value = Column(Text)
    change_reason = Column(String(200))  # auto_extraction/user_edit/ai_refinement
    
    # Context
    triggered_by_source = Column(Integer, ForeignKey('knowledge_sources.id'))
    confidence_before = Column(Float)
    confidence_after = Column(Float)
    
    # Metadata
    metadata = Column(JSON)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow)
    
    def __repr__(self):
        return f"<KnowledgeEvolution({self.entity_type}:{self.entity_id}, {self.change_type})>"

# ==============================================================================
# HELPER DATACLASSES FOR API RESPONSES
# ==============================================================================

@dataclass
class TopicSummary:
    """Summary view of a topic with key metrics"""
    id: int
    name: str
    topic_type: str
    hierarchy_path: str
    people_count: int
    mention_count: int
    confidence_score: float
    last_activity: datetime
    status: str

@dataclass
class PersonTopicContext:
    """Person's relationship to a specific topic"""
    person_id: int
    topic_id: int
    topic_name: str
    relationship_type: str
    involvement_level: float
    expertise_areas: List[str]
    key_contributions: List[str]
    last_activity: datetime

@dataclass
class KnowledgeTraceability:
    """Traceability back to source content"""
    entity_type: str
    entity_id: int
    source_type: str
    source_id: str
    source_content_snippet: str
    confidence: float
    timestamp: datetime
    can_access_full_content: bool 

============================================================
FILE: chief_of_staff_ai/models/database.py
============================================================
import os
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean, Float, ForeignKey, Index, text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship, Session
from sqlalchemy.dialects.postgresql import JSON
from sqlalchemy.types import TypeDecorator

from config.settings import settings

# Import enhanced models for entity-centric intelligence
from models.enhanced_models import (
    Topic as EnhancedTopic, Person as EnhancedPerson, Task as EnhancedTask, 
    Email as EnhancedEmail, CalendarEvent, Project as EnhancedProject,
    EntityRelationship, IntelligenceInsight,
    person_topic_association, task_topic_association, event_topic_association
)

logger = logging.getLogger(__name__)

# Base class for all models
Base = declarative_base()

# Custom JSON type that works with both SQLite and PostgreSQL
class JSONType(TypeDecorator):
    impl = Text
    
    def process_bind_param(self, value, dialect):
        if value is not None:
            return json.dumps(value)
        return value
    
    def process_result_value(self, value, dialect):
        if value is not None:
            return json.loads(value)
        return value

class User(Base):
    """User model for multi-tenant authentication"""
    __tablename__ = 'users'
    
    id = Column(Integer, primary_key=True)
    email = Column(String(255), unique=True, nullable=False, index=True)
    google_id = Column(String(255), unique=True, nullable=False)
    name = Column(String(255), nullable=False)
    
    # OAuth credentials (encrypted in production)
    access_token = Column(Text)
    refresh_token = Column(Text)
    token_expires_at = Column(DateTime)
    scopes = Column(JSONType)
    
    # Account metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    last_login = Column(DateTime, default=datetime.utcnow)
    is_active = Column(Boolean, default=True)
    
    # Processing preferences
    email_fetch_limit = Column(Integer, default=50)
    email_days_back = Column(Integer, default=30)
    auto_process_emails = Column(Boolean, default=True)
    
    # Relationships
    emails = relationship("Email", back_populates="user", cascade="all, delete-orphan")
    tasks = relationship("Task", back_populates="user", cascade="all, delete-orphan")
    people = relationship("Person", back_populates="user", cascade="all, delete-orphan")
    projects = relationship("Project", back_populates="user", cascade="all, delete-orphan")
    topics = relationship("Topic", back_populates="user", cascade="all, delete-orphan")
    
    def __repr__(self):
        return f"<User(email='{self.email}', name='{self.name}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'email': self.email,
            'name': self.name,
            'google_id': self.google_id,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'last_login': self.last_login.isoformat() if self.last_login else None,
            'is_active': self.is_active,
            'email_fetch_limit': self.email_fetch_limit,
            'email_days_back': self.email_days_back,
            'auto_process_emails': self.auto_process_emails
        }

class Email(Base):
    """Email model for storing processed emails per user"""
    __tablename__ = 'emails'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Gmail identifiers
    gmail_id = Column(String(255), nullable=False, index=True)
    thread_id = Column(String(255), index=True)
    
    # Email content
    sender = Column(String(255), index=True)
    sender_name = Column(String(255))
    subject = Column(Text)
    body_text = Column(Text)
    body_html = Column(Text)
    body_clean = Column(Text)
    body_preview = Column(Text)
    snippet = Column(Text)
    
    # Email metadata
    recipients = Column(JSONType)  # List of recipient emails
    cc = Column(JSONType)  # List of CC emails
    bcc = Column(JSONType)  # List of BCC emails
    labels = Column(JSONType)  # Gmail labels
    attachments = Column(JSONType)  # Attachment metadata
    entities = Column(JSONType)  # Extracted entities
    
    # Email properties
    email_date = Column(DateTime, index=True)
    size_estimate = Column(Integer)
    message_type = Column(String(50), index=True)  # regular, meeting, newsletter, etc.
    priority_score = Column(Float, index=True)
    
    # Email status
    is_read = Column(Boolean, default=False)
    is_important = Column(Boolean, default=False)
    is_starred = Column(Boolean, default=False)
    has_attachments = Column(Boolean, default=False)
    
    # Email classification and AI insights
    project_id = Column(Integer, ForeignKey('projects.id'), index=True)
    mentioned_people = Column(JSONType)  # List of person IDs mentioned in email
    ai_summary = Column(Text)  # Claude-generated summary
    ai_category = Column(String(100))  # AI-determined category
    sentiment_score = Column(Float)  # Sentiment analysis score
    urgency_score = Column(Float)  # AI-determined urgency
    key_insights = Column(JSONType)  # Key insights extracted by Claude
    topics = Column(JSONType)  # Main topics/themes identified
    action_required = Column(Boolean, default=False)  # Whether action is needed
    follow_up_required = Column(Boolean, default=False)  # Whether follow-up needed
    
    # Processing metadata
    processed_at = Column(DateTime, default=datetime.utcnow)
    created_at = Column(DateTime, default=datetime.utcnow)  # Add missing created_at column
    normalizer_version = Column(String(50))
    has_errors = Column(Boolean, default=False)
    error_message = Column(Text)
    
    # Enhanced intelligence fields (from migration)
    recipient_emails = Column(JSONType)  # List of recipient emails for analysis
    business_category = Column(String(100))  # Business context category
    sentiment = Column(Float)  # Alternative sentiment field
    strategic_importance = Column(Float, default=0.5)  # Strategic importance score
    content_hash = Column(String(255))  # Hash for duplicate detection
    blob_storage_key = Column(String(255))  # Key for large content storage
    primary_topic_id = Column(Integer, ForeignKey('topics.id'))  # Primary topic
    processing_version = Column(String(50))  # Processing version used
    
    # Relationships
    user = relationship("User", back_populates="emails")
    tasks = relationship("Task", back_populates="email", cascade="all, delete-orphan")
    
    # Indexes for performance - Fixed naming to avoid conflicts
    __table_args__ = (
        Index('idx_email_user_gmail', 'user_id', 'gmail_id'),
        Index('idx_email_user_date', 'user_id', 'email_date'),
        Index('idx_email_user_type', 'user_id', 'message_type'),
        Index('idx_email_user_priority', 'user_id', 'priority_score'),
    )
    
    def __repr__(self):
        return f"<Email(gmail_id='{self.gmail_id}', subject='{self.subject[:50]}...')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'gmail_id': self.gmail_id,
            'thread_id': self.thread_id,
            'sender': self.sender,
            'sender_name': self.sender_name,
            'subject': self.subject,
            'body_preview': self.body_preview,
            'snippet': self.snippet,
            'recipients': self.recipients,
            'email_date': self.email_date.isoformat() if self.email_date else None,
            'message_type': self.message_type,
            'priority_score': self.priority_score,
            'is_read': self.is_read,
            'is_important': self.is_important,
            'is_starred': self.is_starred,
            'has_attachments': self.has_attachments,
            'processed_at': self.processed_at.isoformat() if self.processed_at else None,
            'project_id': self.project_id,
            'mentioned_people': self.mentioned_people,
            'ai_summary': self.ai_summary,
            'ai_category': self.ai_category,
            'sentiment_score': self.sentiment_score,
            'urgency_score': self.urgency_score,
            'key_insights': self.key_insights,
            'topics': self.topics,
            'action_required': self.action_required,
            'follow_up_required': self.follow_up_required,
            'created_at': self.created_at.isoformat() if self.created_at else None
        }

class Task(Base):
    """Task model for storing extracted tasks per user"""
    __tablename__ = 'tasks'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    email_id = Column(Integer, ForeignKey('emails.id'), nullable=True, index=True)
    
    # Task content
    description = Column(Text, nullable=False)
    assignee = Column(String(255))
    due_date = Column(DateTime, index=True)
    due_date_text = Column(String(255))
    
    # Task metadata
    priority = Column(String(20), default='medium', index=True)  # high, medium, low
    category = Column(String(50), index=True)  # follow-up, deadline, meeting, etc.
    confidence = Column(Float)  # AI confidence score
    source_text = Column(Text)  # Original text from email
    
    # Task status
    status = Column(String(20), default='pending', index=True)  # pending, in_progress, completed, cancelled
    completed_at = Column(DateTime)
    
    # Enhanced intelligence fields (needed for entity engine compatibility)
    topics = Column(JSONType, default=lambda: [])  # List of related topic IDs
    
    # Extraction metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    extractor_version = Column(String(50))
    model_used = Column(String(100))
    
    # Relationships
    user = relationship("User", back_populates="tasks")
    email = relationship("Email", back_populates="tasks")
    
    # Indexes for performance - Fixed naming to avoid conflicts
    __table_args__ = (
        Index('idx_task_user_status', 'user_id', 'status'),
        Index('idx_task_user_priority_unique', 'user_id', 'priority'),
        Index('idx_task_user_due_date', 'user_id', 'due_date'),
        Index('idx_task_user_category', 'user_id', 'category'),
    )
    
    def __repr__(self):
        return f"<Task(description='{self.description[:50]}...', priority='{self.priority}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'email_id': self.email_id,
            'description': self.description,
            'assignee': self.assignee,
            'due_date': self.due_date.isoformat() if self.due_date else None,
            'due_date_text': self.due_date_text,
            'priority': self.priority,
            'category': self.category,
            'confidence': self.confidence,
            'source_text': self.source_text,
            'status': self.status,
            'completed_at': self.completed_at.isoformat() if self.completed_at else None,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'extractor_version': self.extractor_version,
            'model_used': self.model_used,
            'topics': self.topics
        }

class Person(Base):
    """Person model for tracking individuals mentioned in emails"""
    __tablename__ = 'people'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Person identification
    email_address = Column(String(255), index=True)
    name = Column(String(255), nullable=False)
    first_name = Column(String(100))
    last_name = Column(String(100))
    
    # Person details (extracted and augmented by Claude)
    title = Column(String(255))
    company = Column(String(255))
    role = Column(String(255))
    department = Column(String(255))
    
    # Relationship and context
    relationship_type = Column(String(100))  # colleague, client, vendor, etc.
    communication_frequency = Column(String(50))  # high, medium, low
    importance_level = Column(Float)  # 0.0 to 1.0
    
    # Knowledge base (JSON fields for flexible data)
    skills = Column(JSONType)  # List of skills/expertise
    interests = Column(JSONType)  # Personal/professional interests
    projects_involved = Column(JSONType)  # List of project IDs
    communication_style = Column(Text)  # Claude's analysis of communication style
    key_topics = Column(JSONType)  # Main topics discussed with this person
    
    # Extracted insights
    personality_traits = Column(JSONType)  # Claude-extracted personality insights
    preferences = Column(JSONType)  # Communication preferences, etc.
    notes = Column(Text)  # Accumulated notes about this person
    
    # Metadata
    first_mentioned = Column(DateTime, default=datetime.utcnow)
    last_interaction = Column(DateTime, default=datetime.utcnow)
    total_emails = Column(Integer, default=0)
    
    # Enhanced intelligence fields (from migration)
    phone = Column(String(50))  # Phone number
    last_contact = Column(DateTime)  # Last contact timestamp
    total_interactions = Column(Integer, default=0)  # Total interaction count
    linkedin_url = Column(String(500))  # LinkedIn profile URL
    bio = Column(Text)  # Professional bio
    professional_story = Column(Text)  # Professional story/background
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)  # Last update timestamp
    
    # AI processing metadata
    knowledge_confidence = Column(Float, default=0.5)  # Confidence in extracted data
    last_updated_by_ai = Column(DateTime)
    ai_version = Column(String(50))
    
    # NEW: Smart Contact Strategy fields
    is_trusted_contact = Column(Boolean, default=False, index=True)
    engagement_score = Column(Float, default=0.0)
    bidirectional_topics = Column(JSONType)  # Topics with back-and-forth discussion
    
    # Timestamps (created_at field needed for entity engine compatibility)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    user = relationship("User", back_populates="people")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_person_user_email', 'user_id', 'email_address'),
        Index('idx_person_user_name', 'user_id', 'name'),
        Index('idx_person_company', 'user_id', 'company'),
    )
    
    def __repr__(self):
        return f"<Person(name='{self.name}', email='{self.email_address}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'email_address': self.email_address,
            'name': self.name,
            'first_name': self.first_name,
            'last_name': self.last_name,
            'title': self.title,
            'company': self.company,
            'role': self.role,
            'department': self.department,
            'relationship_type': self.relationship_type,
            'communication_frequency': self.communication_frequency,
            'importance_level': self.importance_level,
            'skills': self.skills,
            'interests': self.interests,
            'projects_involved': self.projects_involved,
            'communication_style': self.communication_style,
            'key_topics': self.key_topics,
            'personality_traits': self.personality_traits,
            'preferences': self.preferences,
            'notes': self.notes,
            'first_mentioned': self.first_mentioned.isoformat() if self.first_mentioned else None,
            'last_interaction': self.last_interaction.isoformat() if self.last_interaction else None,
            'total_emails': self.total_emails,
            'knowledge_confidence': self.knowledge_confidence,
            'last_updated_by_ai': self.last_updated_by_ai.isoformat() if self.last_updated_by_ai else None,
            'ai_version': self.ai_version,
            'is_trusted_contact': self.is_trusted_contact,
            'engagement_score': self.engagement_score,
            'bidirectional_topics': self.bidirectional_topics,
            'created_at': self.created_at.isoformat() if self.created_at else None
        }

class Project(Base):
    """Project model for categorizing emails and tracking project-related information"""
    __tablename__ = 'projects'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Project identification
    name = Column(String(255), nullable=False)
    slug = Column(String(255), index=True)  # URL-friendly name
    description = Column(Text)
    
    # Project details
    status = Column(String(50), default='active')  # active, completed, paused, cancelled
    priority = Column(String(20), default='medium')  # high, medium, low
    category = Column(String(100))  # business, personal, client work, etc.
    
    # Timeline
    start_date = Column(DateTime)
    end_date = Column(DateTime)
    deadline = Column(DateTime)
    
    # People and relationships
    stakeholders = Column(JSONType)  # List of person IDs involved
    team_members = Column(JSONType)  # List of person IDs
    
    # Project insights (extracted by Claude)
    key_topics = Column(JSONType)  # Main topics/themes
    objectives = Column(JSONType)  # Project goals and objectives
    challenges = Column(JSONType)  # Identified challenges
    progress_indicators = Column(JSONType)  # Metrics and milestones
    
    # Communication patterns
    communication_frequency = Column(String(50))
    last_activity = Column(DateTime)
    total_emails = Column(Integer, default=0)
    
    # AI analysis
    sentiment_trend = Column(Float)  # Overall sentiment about project
    urgency_level = Column(Float)  # How urgent this project appears
    confidence_score = Column(Float)  # AI confidence in project categorization
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    ai_version = Column(String(50))
    
    # Relationships
    user = relationship("User", back_populates="projects")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_project_user_status', 'user_id', 'status'),
        Index('idx_project_user_priority', 'user_id', 'priority'),
        Index('idx_project_user_category', 'user_id', 'category'),
    )
    
    def __repr__(self):
        return f"<Project(name='{self.name}', status='{self.status}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'name': self.name,
            'slug': self.slug,
            'description': self.description,
            'status': self.status,
            'priority': self.priority,
            'category': self.category,
            'start_date': self.start_date.isoformat() if self.start_date else None,
            'end_date': self.end_date.isoformat() if self.end_date else None,
            'deadline': self.deadline.isoformat() if self.deadline else None,
            'stakeholders': self.stakeholders,
            'team_members': self.team_members,
            'key_topics': self.key_topics,
            'objectives': self.objectives,
            'challenges': self.challenges,
            'progress_indicators': self.progress_indicators,
            'communication_frequency': self.communication_frequency,
            'last_activity': self.last_activity.isoformat() if self.last_activity else None,
            'total_emails': self.total_emails,
            'sentiment_trend': self.sentiment_trend,
            'urgency_level': self.urgency_level,
            'confidence_score': self.confidence_score,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'ai_version': self.ai_version
        }

class Topic(Base):
    """Topic model for organizing and categorizing content"""
    __tablename__ = 'topics'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Topic identification
    name = Column(String(255), nullable=False)
    slug = Column(String(255), index=True)  # URL-friendly name
    description = Column(Text)
    
    # Topic properties
    is_official = Column(Boolean, default=False, index=True)  # Official vs AI-discovered
    parent_topic_id = Column(Integer, ForeignKey('topics.id'), index=True)  # For hierarchical topics
    merged_topics = Column(Text)  # JSON string of merged topic names
    keywords = Column(Text)  # JSON string of keywords for matching (changed from JSONType for compatibility)
    email_count = Column(Integer, default=0)  # Number of emails with this topic
    
    # Enhanced intelligence fields (added from migration)
    total_mentions = Column(Integer, default=0)
    last_mentioned = Column(DateTime)
    intelligence_summary = Column(Text)
    strategic_importance = Column(Float, default=0.5)
    version = Column(Integer, default=1)
    
    # Usage tracking
    last_used = Column(DateTime)
    usage_frequency = Column(Float)
    confidence_threshold = Column(Float)
    
    # AI analysis
    confidence_score = Column(Float, default=0.5)  # AI confidence in topic classification
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    ai_version = Column(String(50))
    
    # Relationships
    user = relationship("User", back_populates="topics")
    parent_topic = relationship("Topic", remote_side=[id], backref="child_topics")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_topic_user_official', 'user_id', 'is_official'),
        Index('idx_topic_user_name', 'user_id', 'name'),
        Index('idx_topic_slug', 'user_id', 'slug'),
        Index('idx_topic_parent', 'parent_topic_id'),
    )
    
    def __repr__(self):
        return f"<Topic(name='{self.name}', is_official={self.is_official})>"
    
    def to_dict(self):
        # Handle keywords field properly - could be JSON array or comma-separated string
        keywords_list = []
        if self.keywords:
            try:
                # Try to parse as JSON first
                keywords_list = json.loads(self.keywords)
            except (json.JSONDecodeError, TypeError):
                # If not valid JSON, treat as comma-separated string
                keywords_list = [k.strip() for k in self.keywords.split(',') if k.strip()]
        
        return {
            'id': self.id,
            'user_id': self.user_id,
            'name': self.name,
            'slug': self.slug,
            'description': self.description,
            'is_official': self.is_official,
            'keywords': keywords_list,
            'email_count': self.email_count,
            'confidence_score': self.confidence_score,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'ai_version': self.ai_version,
            'parent_topic_id': self.parent_topic_id,
            'last_used': self.last_used.isoformat() if self.last_used else None,
            'total_mentions': self.total_mentions,
            'last_mentioned': self.last_mentioned.isoformat() if self.last_mentioned else None,
            'intelligence_summary': self.intelligence_summary,
            'strategic_importance': self.strategic_importance,
            'version': self.version
        }

class TrustedContact(Base):
    """Trusted Contact model for engagement-based contact database"""
    __tablename__ = 'trusted_contacts'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Contact identification
    email_address = Column(String(255), nullable=False, index=True)
    name = Column(String(255))
    
    # Engagement metrics
    engagement_score = Column(Float, default=0.0, index=True)
    first_sent_date = Column(DateTime)
    last_sent_date = Column(DateTime, index=True)
    total_sent_emails = Column(Integer, default=0)
    total_received_emails = Column(Integer, default=0)
    bidirectional_threads = Column(Integer, default=0)
    
    # Topic analysis
    topics_discussed = Column(JSONType)  # List of topics from sent/received emails
    bidirectional_topics = Column(JSONType)  # Topics with back-and-forth discussion
    
    # Relationship assessment
    relationship_strength = Column(String(20), default='low', index=True)  # high, medium, low
    communication_frequency = Column(String(20))  # daily, weekly, monthly, occasional
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    last_analyzed = Column(DateTime)
    
    # Relationships
    user = relationship("User", backref="trusted_contacts")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_trusted_contact_user_email', 'user_id', 'email_address'),
        Index('idx_trusted_contact_engagement', 'user_id', 'engagement_score'),
        Index('idx_trusted_contact_strength', 'user_id', 'relationship_strength'),
    )
    
    def __repr__(self):
        return f"<TrustedContact(email='{self.email_address}', strength='{self.relationship_strength}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'email_address': self.email_address,
            'name': self.name,
            'engagement_score': self.engagement_score,
            'first_sent_date': self.first_sent_date.isoformat() if self.first_sent_date else None,
            'last_sent_date': self.last_sent_date.isoformat() if self.last_sent_date else None,
            'total_sent_emails': self.total_sent_emails,
            'total_received_emails': self.total_received_emails,
            'bidirectional_threads': self.bidirectional_threads,
            'topics_discussed': self.topics_discussed,
            'bidirectional_topics': self.bidirectional_topics,
            'relationship_strength': self.relationship_strength,
            'communication_frequency': self.communication_frequency,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'last_analyzed': self.last_analyzed.isoformat() if self.last_analyzed else None
        }

class ContactContext(Base):
    """Rich context information for contacts"""
    __tablename__ = 'contact_contexts'
    
    id = Column(Integer, primary_key=True)
    person_id = Column(Integer, ForeignKey('people.id'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Context details
    context_type = Column(String(50), nullable=False, index=True)  # communication_pattern, project_involvement, topic_expertise, relationship_notes
    title = Column(String(255), nullable=False)
    description = Column(Text)
    confidence_score = Column(Float, default=0.5)
    
    # Supporting evidence
    source_emails = Column(JSONType)  # List of email IDs that contributed to this context
    supporting_quotes = Column(JSONType)  # Relevant excerpts from emails
    tags = Column(JSONType)  # Flexible tagging system
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    person = relationship("Person", backref="contexts")
    user = relationship("User", backref="contact_contexts")
    
    # Indexes
    __table_args__ = (
        Index('idx_contact_context_person', 'person_id', 'context_type'),
        Index('idx_contact_context_user', 'user_id', 'context_type'),
    )
    
    def __repr__(self):
        return f"<ContactContext(type='{self.context_type}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'person_id': self.person_id,
            'user_id': self.user_id,
            'context_type': self.context_type,
            'title': self.title,
            'description': self.description,
            'confidence_score': self.confidence_score,
            'source_emails': self.source_emails,
            'supporting_quotes': self.supporting_quotes,
            'tags': self.tags,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

class TaskContext(Base):
    """Rich context information for tasks"""
    __tablename__ = 'task_contexts'
    
    id = Column(Integer, primary_key=True)
    task_id = Column(Integer, ForeignKey('tasks.id'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Context details
    context_type = Column(String(50), nullable=False, index=True)  # background, stakeholders, timeline, business_impact
    title = Column(String(255), nullable=False)
    description = Column(Text)
    
    # Related entities
    related_people = Column(JSONType)  # List of person IDs
    related_projects = Column(JSONType)  # List of project IDs
    related_topics = Column(JSONType)  # List of relevant topics
    
    # Source information
    source_email_id = Column(Integer, ForeignKey('emails.id'))
    source_thread_id = Column(String(255))
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    task = relationship("Task", backref="contexts")
    user = relationship("User", backref="task_contexts")
    source_email = relationship("Email")
    
    # Indexes
    __table_args__ = (
        Index('idx_task_context_task', 'task_id', 'context_type'),
        Index('idx_task_context_user', 'user_id', 'context_type'),
    )
    
    def __repr__(self):
        return f"<TaskContext(type='{self.context_type}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'task_id': self.task_id,
            'user_id': self.user_id,
            'context_type': self.context_type,
            'title': self.title,
            'description': self.description,
            'related_people': self.related_people,
            'related_projects': self.related_projects,
            'related_topics': self.related_topics,
            'source_email_id': self.source_email_id,
            'source_thread_id': self.source_thread_id,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

class TopicKnowledgeBase(Base):
    """Comprehensive knowledge base for topics"""
    __tablename__ = 'topic_knowledge_base'
    
    id = Column(Integer, primary_key=True)
    topic_id = Column(Integer, ForeignKey('topics.id'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Knowledge details
    knowledge_type = Column(String(50), nullable=False, index=True)  # methodology, key_people, challenges, success_patterns, tools, decisions
    title = Column(String(255), nullable=False)
    content = Column(Text)
    confidence_score = Column(Float, default=0.5)
    
    # Supporting evidence
    supporting_evidence = Column(JSONType)  # Email excerpts, patterns observed
    source_emails = Column(JSONType)  # List of email IDs that contributed
    patterns = Column(JSONType)  # Observed patterns and trends
    
    # Knowledge metadata
    relevance_score = Column(Float, default=0.5)  # How relevant this knowledge is
    engagement_weight = Column(Float, default=0.5)  # Weight based on user engagement
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    last_updated = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    topic = relationship("Topic", backref="knowledge_base")
    user = relationship("User", backref="topic_knowledge")
    
    # Indexes
    __table_args__ = (
        Index('idx_topic_knowledge_topic', 'topic_id', 'knowledge_type'),
        Index('idx_topic_knowledge_user', 'user_id', 'knowledge_type'),
        Index('idx_topic_knowledge_relevance', 'user_id', 'relevance_score'),
    )
    
    def __repr__(self):
        return f"<TopicKnowledgeBase(type='{self.knowledge_type}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'topic_id': self.topic_id,
            'user_id': self.user_id,
            'knowledge_type': self.knowledge_type,
            'title': self.title,
            'content': self.content,
            'confidence_score': self.confidence_score,
            'supporting_evidence': self.supporting_evidence,
            'source_emails': self.source_emails,
            'patterns': self.patterns,
            'relevance_score': self.relevance_score,
            'engagement_weight': self.engagement_weight,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'last_updated': self.last_updated.isoformat() if self.last_updated else None
        }

class Calendar(Base):
    """Calendar model for storing Google Calendar events per user"""
    __tablename__ = 'calendar_events'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Google Calendar identifiers
    event_id = Column(String(255), nullable=False, index=True)
    calendar_id = Column(String(255), nullable=False, index=True)
    recurring_event_id = Column(String(255), index=True)
    
    # Event content
    title = Column(Text)
    description = Column(Text)
    location = Column(Text)
    status = Column(String(50))  # confirmed, tentative, cancelled
    
    # Event timing
    start_time = Column(DateTime, index=True)
    end_time = Column(DateTime, index=True)
    timezone = Column(String(100))
    is_all_day = Column(Boolean, default=False)
    
    # Attendees and relationships
    organizer_email = Column(String(255), index=True)
    organizer_name = Column(String(255))
    attendees = Column(JSONType)  # List of attendee objects with email, name, status
    attendee_emails = Column(JSONType)  # List of attendee emails for quick lookup
    
    # Meeting metadata
    meeting_type = Column(String(100))  # in-person, video_call, phone, etc.
    conference_data = Column(JSONType)  # Google Meet, Zoom links, etc.
    visibility = Column(String(50))  # default, public, private
    
    # Event properties
    is_recurring = Column(Boolean, default=False)
    recurrence_rules = Column(JSONType)  # RRULE data
    is_busy = Column(Boolean, default=True)
    transparency = Column(String(20))  # opaque, transparent
    
    # AI analysis and insights
    ai_summary = Column(Text)  # Claude-generated meeting summary/purpose
    ai_category = Column(String(100))  # AI-determined category (business, personal, etc.)
    importance_score = Column(Float)  # AI-determined importance
    preparation_needed = Column(Boolean, default=False)
    follow_up_required = Column(Boolean, default=False)
    
    # Contact intelligence integration
    known_attendees = Column(JSONType)  # List of person IDs from People table
    unknown_attendees = Column(JSONType)  # Attendees not in contact database
    business_context = Column(Text)  # AI-generated business context based on attendees
    
    # Free time analysis
    is_free_time = Column(Boolean, default=False, index=True)  # For free time slot identification
    potential_duration = Column(Integer)  # Duration in minutes for free slots
    
    # Processing metadata
    fetched_at = Column(DateTime, default=datetime.utcnow)
    last_updated = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    ai_processed_at = Column(DateTime)
    ai_version = Column(String(50))
    
    # Google Calendar metadata
    html_link = Column(Text)  # Link to event in Google Calendar
    hangout_link = Column(Text)  # Google Meet link
    ical_uid = Column(String(255))
    sequence = Column(Integer)  # For tracking updates
    
    # Relationships
    user = relationship("User", backref="calendar_events")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_calendar_user_event', 'user_id', 'event_id'),
        Index('idx_calendar_user_time', 'user_id', 'start_time'),
        Index('idx_calendar_user_organizer', 'user_id', 'organizer_email'),
        Index('idx_calendar_free_time', 'user_id', 'is_free_time'),
        Index('idx_calendar_status', 'user_id', 'status'),
    )
    
    def __repr__(self):
        return f"<Calendar(event_id='{self.event_id}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'event_id': self.event_id,
            'calendar_id': self.calendar_id,
            'recurring_event_id': self.recurring_event_id,
            'title': self.title,
            'description': self.description,
            'location': self.location,
            'status': self.status,
            'start_time': self.start_time.isoformat() if self.start_time else None,
            'end_time': self.end_time.isoformat() if self.end_time else None,
            'timezone': self.timezone,
            'is_all_day': self.is_all_day,
            'organizer_email': self.organizer_email,
            'organizer_name': self.organizer_name,
            'attendees': self.attendees,
            'attendee_emails': self.attendee_emails,
            'meeting_type': self.meeting_type,
            'conference_data': self.conference_data,
            'visibility': self.visibility,
            'is_recurring': self.is_recurring,
            'recurrence_rules': self.recurrence_rules,
            'is_busy': self.is_busy,
            'transparency': self.transparency,
            'ai_summary': self.ai_summary,
            'ai_category': self.ai_category,
            'importance_score': self.importance_score,
            'preparation_needed': self.preparation_needed,
            'follow_up_required': self.follow_up_required,
            'known_attendees': self.known_attendees,
            'unknown_attendees': self.unknown_attendees,
            'business_context': self.business_context,
            'is_free_time': self.is_free_time,
            'potential_duration': self.potential_duration,
            'fetched_at': self.fetched_at.isoformat() if self.fetched_at else None,
            'last_updated': self.last_updated.isoformat() if self.last_updated else None,
            'ai_processed_at': self.ai_processed_at.isoformat() if self.ai_processed_at else None,
            'ai_version': self.ai_version,
            'html_link': self.html_link,
            'hangout_link': self.hangout_link,
            'ical_uid': self.ical_uid,
            'sequence': self.sequence
        }

class UserSession(Base):
    """User session model for authentication tracking"""
    __tablename__ = 'user_sessions'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    session_token = Column(String(255), unique=True, nullable=False)
    refresh_token = Column(String(255), unique=True)
    expires_at = Column(DateTime, nullable=False)
    last_activity = Column(DateTime, default=datetime.utcnow)
    is_active = Column(Boolean, default=True)
    user_agent = Column(Text)
    ip_address = Column(String(45))
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    user = relationship("User", backref="sessions")

class ApiKey(Base):
    """API key model for API authentication"""
    __tablename__ = 'api_keys'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    name = Column(String(255), nullable=False)
    key_hash = Column(String(255), unique=True, nullable=False)
    is_active = Column(Boolean, default=True)
    last_used = Column(DateTime)
    permissions = Column(JSONType)
    created_at = Column(DateTime, default=datetime.utcnow)
    expires_at = Column(DateTime)
    
    # Relationships
    user = relationship("User", backref="api_keys")

class DatabaseManager:
    """Database manager for handling connections and sessions"""
    
    def __init__(self):
        self.engine = None
        self.SessionLocal = None
        self.initialize_database()
    
    def initialize_database(self):
        """Initialize database connection and create tables"""
        try:
            # Use DATABASE_URL from environment or default to SQLite
            database_url = settings.DATABASE_URL
            
            # Handle PostgreSQL URL for Heroku
            if database_url and database_url.startswith('postgres://'):
                database_url = database_url.replace('postgres://', 'postgresql://', 1)
            
            # Create engine with appropriate settings
            if database_url.startswith('postgresql://'):
                # PostgreSQL settings for Heroku
                self.engine = create_engine(
                    database_url,
                    echo=settings.DEBUG,
                    pool_pre_ping=True,
                    pool_recycle=300
                )
            else:
                # SQLite settings for local development
                self.engine = create_engine(
                    database_url,
                    echo=settings.DEBUG,
                    connect_args={"check_same_thread": False}
                )
            
            # Create session factory
            self.SessionLocal = sessionmaker(bind=self.engine)
            
            # Create all tables
            Base.metadata.create_all(bind=self.engine)
            
            logger.info("Database initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize database: {str(e)}")
            raise
    
    def get_session(self) -> Session:
        """Get a new database session"""
        return self.SessionLocal()
    
    def get_user_by_email(self, email: str) -> Optional[User]:
        """Get user by email address"""
        with self.get_session() as session:
            return session.query(User).filter(User.email == email).first()
    
    def create_or_update_user(self, user_info: Dict, credentials: Dict) -> User:
        """Create or update user with OAuth info"""
        with self.get_session() as session:
            user = session.query(User).filter(User.email == user_info['email']).first()
            
            if user:
                # Update existing user
                user.name = user_info.get('name', user.name)
                user.last_login = datetime.utcnow()
                user.access_token = credentials.get('access_token')
                user.refresh_token = credentials.get('refresh_token')
                user.token_expires_at = credentials.get('expires_at')
                user.scopes = credentials.get('scopes', [])
            else:
                # Create new user
                user = User(
                    email=user_info['email'],
                    google_id=user_info['id'],
                    name=user_info.get('name', ''),
                    access_token=credentials.get('access_token'),
                    refresh_token=credentials.get('refresh_token'),
                    token_expires_at=credentials.get('expires_at'),
                    scopes=credentials.get('scopes', [])
                )
                session.add(user)
            
            session.commit()
            session.refresh(user)
            return user
    
    def save_email(self, user_id: int, email_data: Dict) -> Email:
        """Save processed email to database"""
        with self.get_session() as session:
            try:
                # Check if email already exists
                existing = session.query(Email).filter(
                    Email.user_id == user_id,
                    Email.gmail_id == email_data['id']
                ).first()
                
                if existing:
                    return existing
                
                # Create new email record
                email = Email(
                    user_id=user_id,
                    gmail_id=email_data['id'],
                    thread_id=email_data.get('thread_id'),
                    sender=email_data.get('sender'),
                    sender_name=email_data.get('sender_name'),
                    subject=email_data.get('subject'),
                    body_text=email_data.get('body_text'),
                    body_html=email_data.get('body_html'),
                    recipient_emails=email_data.get('recipient_emails', []),  # Store recipient emails
                    recipients=email_data.get('recipient_emails', []),  # For backwards compatibility
                    cc=email_data.get('cc', []),
                    bcc=email_data.get('bcc', []),
                    email_date=email_data.get('email_date') or email_data.get('timestamp'),
                    message_type=email_data.get('message_type', 'regular'),
                    is_read=email_data.get('is_read', False),
                    is_important=email_data.get('is_important', False),
                    is_starred=email_data.get('is_starred', False),
                    has_attachments=email_data.get('has_attachments', False),
                    processed_at=datetime.utcnow(),
                    created_at=datetime.utcnow(),
                    normalizer_version=email_data.get('processing_metadata', {}).get('fetcher_version', 'v1')
                )
                
                session.add(email)
                session.commit()
                session.refresh(email)
                return email
                
            except Exception as e:
                logger.error(f"Failed to save email: {str(e)}")
                session.rollback()
                raise
    
    def save_task(self, user_id: int, email_id: Optional[int], task_data: Dict) -> Task:
        """Save extracted task to database"""
        try:
            with self.get_session() as session:
                task = Task(
                    user_id=user_id,
                    email_id=email_id,
                    description=task_data['description'],
                    assignee=task_data.get('assignee'),
                    due_date=task_data.get('due_date'),
                    due_date_text=task_data.get('due_date_text'),
                    priority=task_data.get('priority', 'medium'),
                    category=task_data.get('category'),
                    confidence=task_data.get('confidence'),
                    source_text=task_data.get('source_text'),
                    status=task_data.get('status', 'pending'),
                    extractor_version=task_data.get('extractor_version'),
                    model_used=task_data.get('model_used')
                )
                
                session.add(task)
                session.commit()
                session.refresh(task)
                
                # Verify the task object is valid before returning
                if not task or not hasattr(task, 'id') or task.id is None:
                    raise ValueError("Failed to create task - invalid task object returned")
                
                return task
                
        except Exception as e:
            logger.error(f"Failed to save task to database: {str(e)}")
            logger.error(f"Task data: {task_data}")
            raise  # Re-raise the exception instead of returning a dict
    
    def get_user_emails(self, user_id: int, limit: int = 50) -> List[Email]:
        """Get emails for a user"""
        with self.get_session() as session:
            return session.query(Email).filter(
                Email.user_id == user_id
            ).order_by(Email.email_date.desc()).limit(limit).all()
    
    def get_user_tasks(self, user_id: int, status: str = None, limit: int = 500) -> List[Task]:
        """Get tasks for a user"""
        with self.get_session() as session:
            query = session.query(Task).filter(Task.user_id == user_id)
            if status:
                query = query.filter(Task.status == status)
            return query.order_by(Task.created_at.desc()).limit(limit).all()

    def create_or_update_person(self, user_id: int, person_data: Dict) -> Person:
        """Create or update a person record"""
        with self.get_session() as session:
            # Try to find existing person by email or name
            person = session.query(Person).filter(
                Person.user_id == user_id,
                Person.email_address == person_data.get('email_address')
            ).first()
            
            if not person and person_data.get('name'):
                # Try by name if email not found
                person = session.query(Person).filter(
                    Person.user_id == user_id,
                    Person.name == person_data.get('name')
                ).first()
            
            if person:
                # Update existing person
                for key, value in person_data.items():
                    if hasattr(person, key) and value is not None:
                        setattr(person, key, value)
                person.last_interaction = datetime.utcnow()
                person.total_emails += 1
                person.last_updated_by_ai = datetime.utcnow()
            else:
                # Create new person - remove conflicting fields from person_data
                person_data_clean = person_data.copy()
                person_data_clean.pop('total_emails', None)  # Remove if present
                person_data_clean.pop('last_updated_by_ai', None)  # Remove if present
                
                person = Person(
                    user_id=user_id,
                    **person_data_clean,
                    total_emails=1,
                    last_updated_by_ai=datetime.utcnow()
                )
                session.add(person)
            
            session.commit()
            session.refresh(person)
            return person
    
    def create_or_update_project(self, user_id: int, project_data: Dict) -> Project:
        """Create or update a project record"""
        with self.get_session() as session:
            # Try to find existing project by name or slug
            project = session.query(Project).filter(
                Project.user_id == user_id,
                Project.name == project_data.get('name')
            ).first()
            
            if project:
                # Update existing project
                for key, value in project_data.items():
                    if hasattr(project, key) and value is not None:
                        setattr(project, key, value)
                project.last_activity = datetime.utcnow()
                project.total_emails += 1
                project.updated_at = datetime.utcnow()
            else:
                # Create new project
                project = Project(
                    user_id=user_id,
                    **project_data,
                    total_emails=1,
                    updated_at=datetime.utcnow()
                )
                session.add(project)
            
            session.commit()
            session.refresh(project)
            return project
    
    def get_user_people(self, user_id: int, limit: int = 500) -> List[Person]:
        """Get people for a user"""
        with self.get_session() as session:
            query = session.query(Person).filter(Person.user_id == user_id)
            query = query.order_by(Person.last_interaction.desc())
            if limit:
                query = query.limit(limit)
            return query.all()
    
    def get_user_projects(self, user_id: int, status: str = None, limit: int = 200) -> List[Project]:
        """Get projects for a user"""
        with self.get_session() as session:
            query = session.query(Project).filter(Project.user_id == user_id)
            if status:
                query = query.filter(Project.status == status)
            query = query.order_by(Project.last_activity.desc())
            if limit:
                query = query.limit(limit)
            return query.all()
    
    def find_person_by_email(self, user_id: int, email: str) -> Optional[Person]:
        """Find person by email address"""
        with self.get_session() as session:
            return session.query(Person).filter(
                Person.user_id == user_id,
                Person.email_address == email
            ).first()
    
    def find_project_by_keywords(self, user_id: int, keywords: List[str]) -> Optional[Project]:
        """Find project by matching keywords against name, description, or topics - FIXED to prevent memory issues"""
        with self.get_session() as session:
            # CRITICAL FIX: Add limit to prevent loading too many projects
            projects = session.query(Project).filter(Project.user_id == user_id).limit(50).all()
            
            for project in projects:
                # Check name and description
                if any(keyword.lower() in (project.name or '').lower() for keyword in keywords):
                    return project
                if any(keyword.lower() in (project.description or '').lower() for keyword in keywords):
                    return project
                
                # Check key topics
                if project.key_topics:
                    project_topics = [topic.lower() for topic in project.key_topics]
                    if any(keyword.lower() in project_topics for keyword in keywords):
                        return project
            
            return None

    def get_user_topics(self, user_id: int, limit: int = 1000) -> List[Topic]:
        """Get all topics for a user"""
        with self.get_session() as session:
            return session.query(Topic).filter(
                Topic.user_id == user_id
            ).order_by(Topic.is_official.desc(), Topic.name.asc()).limit(limit).all()
    
    def create_or_update_topic(self, user_id: int, topic_data: Dict) -> Topic:
        """Create or update a topic record"""
        with self.get_session() as session:
            # Try to find existing topic by name
            topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.name == topic_data.get('name')
            ).first()
            
            # Handle keywords conversion to JSON string
            topic_data_copy = topic_data.copy()
            if 'keywords' in topic_data_copy and isinstance(topic_data_copy['keywords'], list):
                topic_data_copy['keywords'] = json.dumps(topic_data_copy['keywords'])
            
            if topic:
                # Update existing topic
                for key, value in topic_data_copy.items():
                    if hasattr(topic, key) and key != 'id':
                        setattr(topic, key, value)
                topic.updated_at = datetime.now()
            else:
                # Create new topic
                topic_data_copy['user_id'] = user_id
                topic_data_copy['created_at'] = datetime.now()
                topic_data_copy['updated_at'] = datetime.now()
                
                # Set default values for optional fields
                if 'slug' not in topic_data_copy:
                    topic_data_copy['slug'] = topic_data_copy['name'].lower().replace(' ', '-').replace('_', '-')
                
                if 'is_official' not in topic_data_copy:
                    topic_data_copy['is_official'] = False
                    
                if 'confidence_score' not in topic_data_copy:
                    topic_data_copy['confidence_score'] = 0.5
                    
                if 'email_count' not in topic_data_copy:
                    topic_data_copy['email_count'] = 0
                
                topic = Topic(**topic_data_copy)
                session.add(topic)
            
            session.commit()
            session.refresh(topic)
            return topic

    def update_topic(self, user_id: int, topic_id: int, topic_data: Dict) -> bool:
        """Update a specific topic by ID"""
        with self.get_session() as session:
            topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == topic_id
            ).first()
            
            if not topic:
                return False
            
            # Handle keywords conversion to JSON string
            for key, value in topic_data.items():
                if hasattr(topic, key) and value is not None:
                    if key == 'keywords' and isinstance(value, list):
                        setattr(topic, key, json.dumps(value))
                    else:
                        setattr(topic, key, value)
            
            topic.updated_at = datetime.utcnow()
            session.commit()
            return True

    def mark_topic_official(self, user_id: int, topic_id: int) -> bool:
        """Mark a topic as official"""
        with self.get_session() as session:
            topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == topic_id
            ).first()
            
            if not topic:
                return False
            
            topic.is_official = True
            topic.updated_at = datetime.utcnow()
            session.commit()
            return True

    def merge_topics(self, user_id: int, source_topic_id: int, target_topic_id: int) -> bool:
        """Merge one topic into another"""
        with self.get_session() as session:
            source_topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == source_topic_id
            ).first()
            
            target_topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == target_topic_id
            ).first()
            
            if not source_topic or not target_topic:
                return False
            
            try:
                # Update all emails that reference the source topic
                # This is a simplified version - in practice, you'd need to update
                # the topics JSON array in emails to replace source with target
                
                # For now, we'll merge the email counts and keywords
                target_topic.email_count = (target_topic.email_count or 0) + (source_topic.email_count or 0)
                
                # Merge keywords
                source_keywords = json.loads(source_topic.keywords) if source_topic.keywords else []
                target_keywords = json.loads(target_topic.keywords) if target_topic.keywords else []
                merged_keywords = list(set(source_keywords + target_keywords))
                target_topic.keywords = json.dumps(merged_keywords)
                
                # Update merge tracking
                merged_topics = json.loads(target_topic.merged_topics) if target_topic.merged_topics else []
                merged_topics.append(source_topic.name)
                target_topic.merged_topics = json.dumps(merged_topics)
                
                target_topic.updated_at = datetime.utcnow()
                
                # Delete the source topic
                session.delete(source_topic)
                session.commit()
                return True
                
            except Exception as e:
                session.rollback()
                logger.error(f"Failed to merge topics: {str(e)}")
                return False

    # ===== SMART CONTACT STRATEGY METHODS =====
    
    def create_or_update_trusted_contact(self, user_id: int, contact_data: Dict) -> TrustedContact:
        """Create or update a trusted contact record"""
        with self.get_session() as session:
            contact = session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.email_address == contact_data['email_address']
            ).first()
            
            if contact:
                # Update existing contact
                for key, value in contact_data.items():
                    if hasattr(contact, key) and value is not None:
                        setattr(contact, key, value)
                contact.updated_at = datetime.utcnow()
            else:
                # Create new trusted contact
                contact = TrustedContact(
                    user_id=user_id,
                    **contact_data,
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                session.add(contact)
            
            session.commit()
            session.refresh(contact)
            return contact
    
    def get_trusted_contacts(self, user_id: int, limit: int = 500) -> List[TrustedContact]:
        """Get trusted contacts for a user"""
        with self.get_session() as session:
            return session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id
            ).order_by(TrustedContact.engagement_score.desc()).limit(limit).all()
    
    def find_trusted_contact_by_email(self, user_id: int, email_address: str) -> Optional[TrustedContact]:
        """Find trusted contact by email address"""
        with self.get_session() as session:
            return session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.email_address == email_address
            ).first()
    
    def create_contact_context(self, user_id: int, person_id: int, context_data: Dict) -> ContactContext:
        """Create a new contact context record"""
        with self.get_session() as session:
            context = ContactContext(
                user_id=user_id,
                person_id=person_id,
                **context_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(context)
            session.commit()
            session.refresh(context)
            return context
    
    def get_contact_contexts(self, user_id: int, person_id: int = None, context_type: str = None) -> List[ContactContext]:
        """Get contact contexts for a user, optionally filtered by person or type"""
        with self.get_session() as session:
            query = session.query(ContactContext).filter(ContactContext.user_id == user_id)
            
            if person_id:
                query = query.filter(ContactContext.person_id == person_id)
            
            if context_type:
                query = query.filter(ContactContext.context_type == context_type)
            
            return query.order_by(ContactContext.created_at.desc()).all()
    
    def create_task_context(self, user_id: int, task_id: int, context_data: Dict) -> TaskContext:
        """Create a new task context record"""
        with self.get_session() as session:
            context = TaskContext(
                user_id=user_id,
                task_id=task_id,
                **context_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(context)
            session.commit()
            session.refresh(context)
            return context
    
    def get_task_contexts(self, user_id: int, task_id: int = None, context_type: str = None) -> List[TaskContext]:
        """Get task contexts for a user, optionally filtered by task or type"""
        with self.get_session() as session:
            query = session.query(TaskContext).filter(TaskContext.user_id == user_id)
            
            if task_id:
                query = query.filter(TaskContext.task_id == task_id)
            
            if context_type:
                query = query.filter(TaskContext.context_type == context_type)
            
            return query.order_by(TaskContext.created_at.desc()).all()
    
    def create_topic_knowledge(self, user_id: int, topic_id: int, knowledge_data: Dict) -> TopicKnowledgeBase:
        """Create a new topic knowledge record"""
        with self.get_session() as session:
            knowledge = TopicKnowledgeBase(
                user_id=user_id,
                topic_id=topic_id,
                **knowledge_data,
                created_at=datetime.utcnow(),
                last_updated=datetime.utcnow()
            )
            session.add(knowledge)
            session.commit()
            session.refresh(knowledge)
            return knowledge
    
    def get_topic_knowledge(self, user_id: int, topic_id: int = None, knowledge_type: str = None) -> List[TopicKnowledgeBase]:
        """Get topic knowledge for a user, optionally filtered by topic or type"""
        with self.get_session() as session:
            query = session.query(TopicKnowledgeBase).filter(TopicKnowledgeBase.user_id == user_id)
            
            if topic_id:
                query = query.filter(TopicKnowledgeBase.topic_id == topic_id)
            
            if knowledge_type:
                query = query.filter(TopicKnowledgeBase.knowledge_type == knowledge_type)
            
            return query.order_by(TopicKnowledgeBase.relevance_score.desc()).all()
    
    def update_people_engagement_data(self, user_id: int, person_id: int, engagement_data: Dict) -> bool:
        """Update people table with engagement-based data"""
        with self.get_session() as session:
            person = session.query(Person).filter(
                Person.user_id == user_id,
                Person.id == person_id
            ).first()
            
            if not person:
                return False
            
            # Add engagement fields to person if they don't exist
            if 'is_trusted_contact' in engagement_data:
                person.is_trusted_contact = engagement_data['is_trusted_contact']
            
            if 'engagement_score' in engagement_data:
                person.engagement_score = engagement_data['engagement_score']
            
            if 'bidirectional_topics' in engagement_data:
                person.bidirectional_topics = engagement_data['bidirectional_topics']
            
            session.commit()
            return True
    
    def get_engagement_analytics(self, user_id: int) -> Dict:
        """Get engagement analytics for Smart Contact Strategy reporting"""
        with self.get_session() as session:
            total_contacts = session.query(TrustedContact).filter(TrustedContact.user_id == user_id).count()
            high_engagement = session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.relationship_strength == 'high'
            ).count()
            
            recent_contacts = session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.last_sent_date >= datetime.utcnow() - timedelta(days=30)
            ).count()
            
            return {
                'total_trusted_contacts': total_contacts,
                'high_engagement_contacts': high_engagement,
                'recent_active_contacts': recent_contacts,
                'engagement_rate': (high_engagement / total_contacts * 100) if total_contacts > 0 else 0
            }

    def save_calendar_event(self, user_id: int, event_data: Dict) -> Calendar:
        """Save or update a calendar event"""
        try:
            with self.get_session() as session:
                # Try to find existing event
                existing_event = session.query(Calendar).filter_by(
                    user_id=user_id,
                    event_id=event_data.get('event_id')
                ).first()
                
                if existing_event:
                    # Update existing event
                    for key, value in event_data.items():
                        if hasattr(existing_event, key):
                            setattr(existing_event, key, value)
                    event = existing_event
                else:
                    # Create new event
                    event = Calendar(user_id=user_id, **event_data)
                    session.add(event)
                
                session.commit()
                session.refresh(event)
                return event
                
        except Exception as e:
            logger.error(f"Failed to save calendar event: {str(e)}")
            raise

    def get_user_calendar_events(self, user_id: int, start_date: datetime = None, end_date: datetime = None, limit: int = 500) -> List[Calendar]:
        """Get calendar events for a user within a date range"""
        try:
            with self.get_session() as session:
                query = session.query(Calendar).filter_by(user_id=user_id)
                
                if start_date:
                    query = query.filter(Calendar.start_time >= start_date)
                if end_date:
                    query = query.filter(Calendar.start_time <= end_date)
                
                events = query.order_by(Calendar.start_time.asc()).limit(limit).all()
                return events
                
        except Exception as e:
            logger.error(f"Failed to get user calendar events: {str(e)}")
            return []

    def get_free_time_slots(self, user_id: int, start_date: datetime, end_date: datetime) -> List[Dict]:
        """Identify free time slots between calendar events"""
        try:
            with self.get_session() as session:
                events = session.query(Calendar).filter(
                    Calendar.user_id == user_id,
                    Calendar.start_time >= start_date,
                    Calendar.start_time <= end_date,
                    Calendar.status.in_(['confirmed', 'tentative']),
                    Calendar.is_busy == True
                ).order_by(Calendar.start_time).all()
                
                free_slots = []
                current_time = start_date
                
                for event in events:
                    # If there's a gap before this event, it's free time
                    if event.start_time > current_time:
                        gap_duration = int((event.start_time - current_time).total_seconds() / 60)
                        if gap_duration >= 30:  # Minimum 30 minutes to be useful
                            free_slots.append({
                                'start_time': current_time,
                                'end_time': event.start_time,
                                'duration_minutes': gap_duration,
                                'type': 'free_time'
                            })
                    
                    # Update current time to end of this event
                    if event.end_time and event.end_time > current_time:
                        current_time = event.end_time
                
                # Check for free time after last event
                if current_time < end_date:
                    gap_duration = int((end_date - current_time).total_seconds() / 60)
                    if gap_duration >= 30:
                        free_slots.append({
                            'start_time': current_time,
                            'end_time': end_date,
                            'duration_minutes': gap_duration,
                            'type': 'free_time'
                        })
                
                return free_slots
                
        except Exception as e:
            logger.error(f"Failed to get free time slots: {str(e)}")
            return []

    def get_calendar_attendee_intelligence(self, user_id: int, event_id: str) -> Dict:
        """Get intelligence about calendar event attendees"""
        try:
            with self.get_session() as session:
                event = session.query(Calendar).filter_by(
                    user_id=user_id,
                    event_id=event_id
                ).first()
                
                if not event or not event.attendee_emails:
                    return {}
                
                # Find known attendees in People database
                known_people = []
                unknown_attendees = []
                
                for attendee_email in event.attendee_emails:
                    person = self.find_person_by_email(user_id, attendee_email)
                    if person:
                        known_people.append(person.to_dict())
                    else:
                        unknown_attendees.append(attendee_email)
                
                return {
                    'event_id': event_id,
                    'total_attendees': len(event.attendee_emails),
                    'known_attendees': known_people,
                    'unknown_attendees': unknown_attendees,
                    'known_percentage': len(known_people) / len(event.attendee_emails) * 100 if event.attendee_emails else 0
                }
                
        except Exception as e:
            logger.error(f"Failed to get calendar attendee intelligence: {str(e)}")
            return {}

    def update_calendar_ai_analysis(self, user_id: int, event_id: str, ai_data: Dict) -> bool:
        """Update calendar event with AI analysis"""
        try:
            with self.get_session() as session:
                event = session.query(Calendar).filter_by(
                    user_id=user_id,
                    event_id=event_id
                ).first()
                
                if not event:
                    return False
                
                # Update AI analysis fields
                if 'ai_summary' in ai_data:
                    event.ai_summary = ai_data['ai_summary']
                if 'ai_category' in ai_data:
                    event.ai_category = ai_data['ai_category']
                if 'importance_score' in ai_data:
                    event.importance_score = ai_data['importance_score']
                if 'business_context' in ai_data:
                    event.business_context = ai_data['business_context']
                if 'preparation_needed' in ai_data:
                    event.preparation_needed = ai_data['preparation_needed']
                if 'follow_up_required' in ai_data:
                    event.follow_up_required = ai_data['follow_up_required']
                
                event.ai_processed_at = datetime.utcnow()
                event.ai_version = ai_data.get('ai_version', 'claude-3.5-sonnet')
                
                session.commit()
                return True
                
        except Exception as e:
            logger.error(f"Failed to update calendar AI analysis: {str(e)}")
            return False

    # ===== ENHANCED ENTITY-CENTRIC INTELLIGENCE METHODS =====
    
    def get_user_topics_with_intelligence(self, user_id: int, limit: int = None) -> List[EnhancedTopic]:
        """Get topics with relationship intelligence"""
        with self.get_session() as session:
            query = session.query(EnhancedTopic).filter(EnhancedTopic.user_id == user_id)
            query = query.order_by(EnhancedTopic.strategic_importance.desc(), EnhancedTopic.total_mentions.desc())
            if limit:
                query = query.limit(limit)
            return query.all()
    
    def get_entity_relationships(self, user_id: int, entity_type: str = None) -> List[EntityRelationship]:
        """Get entity relationships for network analysis"""
        with self.get_session() as session:
            query = session.query(EntityRelationship).filter(EntityRelationship.user_id == user_id)
            if entity_type:
                query = query.filter(
                    (EntityRelationship.entity_type_a == entity_type) | 
                    (EntityRelationship.entity_type_b == entity_type)
                )
            return query.order_by(EntityRelationship.strength.desc()).all()
    
    def get_intelligence_insights(self, user_id: int, status: str = None) -> List[IntelligenceInsight]:
        """Get proactive intelligence insights"""
        with self.get_session() as session:
            query = session.query(IntelligenceInsight).filter(IntelligenceInsight.user_id == user_id)
            if status:
                query = query.filter(IntelligenceInsight.status == status)
            return query.order_by(IntelligenceInsight.priority.desc(), IntelligenceInsight.created_at.desc()).all()
    
    def create_enhanced_topic(self, user_id: int, topic_data: Dict) -> EnhancedTopic:
        """Create enhanced topic with intelligence accumulation"""
        with self.get_session() as session:
            topic = EnhancedTopic(
                user_id=user_id,
                **topic_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(topic)
            session.commit()
            session.refresh(topic)
            return topic
    
    def create_enhanced_person(self, user_id: int, person_data: Dict) -> EnhancedPerson:
        """Create enhanced person with relationship intelligence"""
        with self.get_session() as session:
            person = EnhancedPerson(
                user_id=user_id,
                **person_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(person)
            session.commit()
            session.refresh(person)
            return person
    
    def create_enhanced_task(self, user_id: int, task_data: Dict) -> EnhancedTask:
        """Create enhanced task with full context"""
        with self.get_session() as session:
            task = EnhancedTask(
                user_id=user_id,
                **task_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(task)
            session.commit()
            session.refresh(task)
            return task
    
    def create_entity_relationship(self, user_id: int, relationship_data: Dict) -> EntityRelationship:
        """Create entity relationship for network intelligence"""
        with self.get_session() as session:
            relationship = EntityRelationship(
                user_id=user_id,
                **relationship_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(relationship)
            session.commit()
            session.refresh(relationship)
            return relationship
    
    def create_intelligence_insight(self, user_id: int, insight_data: Dict) -> IntelligenceInsight:
        """Create proactive intelligence insight"""
        with self.get_session() as session:
            insight = IntelligenceInsight(
                user_id=user_id,
                **insight_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(insight)
            session.commit()
            session.refresh(insight)
            return insight
    
    def save_enhanced_email(self, user_id: int, email_data: Dict) -> EnhancedEmail:
        """Save enhanced email with intelligence focus"""
        with self.get_session() as session:
            # Check if email already exists
            existing_email = session.query(EnhancedEmail).filter(
                EnhancedEmail.user_id == user_id,
                EnhancedEmail.gmail_id == email_data.get('gmail_id')
            ).first()
            
            if existing_email:
                # Update existing email
                for key, value in email_data.items():
                    if hasattr(existing_email, key) and value is not None:
                        setattr(existing_email, key, value)
                return existing_email
            
            # Create new enhanced email
            email = EnhancedEmail(
                user_id=user_id,
                **email_data,
                created_at=datetime.utcnow()
            )
            session.add(email)
            session.commit()
            session.refresh(email)
            return email
    
    def save_calendar_event_enhanced(self, user_id: int, event_data: Dict) -> CalendarEvent:
        """Save calendar event with business intelligence"""
        with self.get_session() as session:
            # Check if event already exists
            existing_event = session.query(CalendarEvent).filter(
                CalendarEvent.user_id == user_id,
                CalendarEvent.google_event_id == event_data.get('google_event_id')
            ).first()
            
            if existing_event:
                # Update existing event
                for key, value in event_data.items():
                    if hasattr(existing_event, key) and value is not None:
                        setattr(existing_event, key, value)
                existing_event.updated_at = datetime.utcnow()
                session.commit()
                return existing_event
            
            # Create new enhanced calendar event
            event = CalendarEvent(
                user_id=user_id,
                **event_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(event)
            session.commit()
            session.refresh(event)
            return event

    def create_or_update_task(self, user_id: int, task_data: Dict) -> Task:
        """Create or update a task with enhanced intelligence data"""
        with self.get_session() as session:
            existing_task = None
            
            # Check if task already exists (by description similarity for deduplication)
            if task_data.get('description'):
                existing_tasks = session.query(Task).filter(
                    Task.user_id == user_id,
                    Task.description.like(f"%{task_data['description'][:50]}%")
                ).all()
                
                for task in existing_tasks:
                    # Simple similarity check to avoid duplicates
                    if len(set(task.description.split()) & set(task_data['description'].split())) > 3:
                        existing_task = task
                        break
            
            if existing_task:
                # Update existing task with new intelligence
                for key, value in task_data.items():
                    if hasattr(existing_task, key) and value is not None:
                        setattr(existing_task, key, value)
                existing_task.updated_at = datetime.utcnow()
                session.commit()
                return existing_task
            else:
                # Create new task
                task = Task(**task_data)
                task.user_id = user_id
                session.add(task)
                session.commit()
                return task

    def create_intelligence_insight(self, user_id: int, insight_data: Dict) -> IntelligenceInsight:
        """Create a new intelligence insight"""
        with self.get_session() as session:
            insight = IntelligenceInsight(**insight_data)
            insight.user_id = user_id
            session.add(insight)
            session.commit()
            return insight

    def get_intelligence_insights(self, user_id: int, status: str = None, limit: int = 50) -> List[IntelligenceInsight]:
        """Get intelligence insights for a user"""
        with self.get_session() as session:
            query = session.query(IntelligenceInsight).filter(IntelligenceInsight.user_id == user_id)
            
            if status:
                query = query.filter(IntelligenceInsight.status == status)
            
            insights = query.order_by(IntelligenceInsight.created_at.desc()).limit(limit).all()
            session.expunge_all()
            return insights

    def create_entity_relationship(self, user_id: int, relationship_data: Dict) -> EntityRelationship:
        """Create or update an entity relationship"""
        with self.get_session() as session:
            # Check if relationship already exists
            existing = session.query(EntityRelationship).filter(
                EntityRelationship.user_id == user_id,
                EntityRelationship.source_entity_type == relationship_data.get('source_entity_type'),
                EntityRelationship.source_entity_id == relationship_data.get('source_entity_id'),
                EntityRelationship.target_entity_type == relationship_data.get('target_entity_type'),
                EntityRelationship.target_entity_id == relationship_data.get('target_entity_id'),
                EntityRelationship.relationship_type == relationship_data.get('relationship_type')
            ).first()
            
            if existing:
                # Update existing relationship
                existing.evidence_count += 1
                existing.last_evidence_date = datetime.utcnow()
                if relationship_data.get('strength'):
                    existing.strength = max(existing.strength, relationship_data['strength'])
                session.commit()
                return existing
            else:
                # Create new relationship
                relationship = EntityRelationship(**relationship_data)
                relationship.user_id = user_id
                session.add(relationship)
                session.commit()
                return relationship

    def get_entity_relationships(self, user_id: int, entity_type: str = None, entity_id: int = None) -> List[EntityRelationship]:
        """Get entity relationships for a user"""
        with self.get_session() as session:
            query = session.query(EntityRelationship).filter(EntityRelationship.user_id == user_id)
            
            if entity_type and entity_id:
                query = query.filter(
                    ((EntityRelationship.source_entity_type == entity_type) & 
                     (EntityRelationship.source_entity_id == entity_id)) |
                    ((EntityRelationship.target_entity_type == entity_type) & 
                     (EntityRelationship.target_entity_id == entity_id))
                )
            
            relationships = query.order_by(EntityRelationship.strength.desc()).all()
            session.expunge_all()
            return relationships

    def enhance_calendar_event_with_intelligence(self, user_id: int, event_id: str, intelligence_data: Dict) -> bool:
        """Enhance calendar event with AI intelligence"""
        with self.get_session() as session:
            event = session.query(Calendar).filter(
                Calendar.user_id == user_id,
                Calendar.event_id == event_id
            ).first()
            
            if event:
                # Update with intelligence data
                if intelligence_data.get('business_context'):
                    event.business_context = intelligence_data['business_context']
                if intelligence_data.get('attendee_intelligence'):
                    event.business_context = intelligence_data['attendee_intelligence']  # Store in business_context for now
                if intelligence_data.get('importance_score'):
                    event.importance_score = intelligence_data['importance_score']
                if intelligence_data.get('preparation_needed'):
                    event.preparation_needed = intelligence_data['preparation_needed']
                
                event.ai_processed_at = datetime.utcnow()
                session.commit()
                return True
            
            return False

    def create_meeting_preparation_tasks(self, user_id: int, event_id: str, prep_tasks: List[Dict]) -> List[Task]:
        """Create meeting preparation tasks"""
        created_tasks = []
        
        for task_data in prep_tasks:
            # Add meeting context to task
            enhanced_task_data = {
                **task_data,
                'category': 'meeting_prep',
                'source_text': f"Preparation for meeting: {event_id}",
                'context': f"Meeting preparation task generated by AI for event {event_id}"
            }
            
            task = self.create_or_update_task(user_id, enhanced_task_data)
            if task:
                created_tasks.append(task)
        
        return created_tasks

    def get_upcoming_meetings_needing_prep(self, user_id: int, hours_ahead: int = 48) -> List[Calendar]:
        """Get upcoming meetings that need preparation"""
        with self.get_session() as session:
            cutoff_time = datetime.utcnow() + timedelta(hours=hours_ahead)
            
            meetings = session.query(Calendar).filter(
                Calendar.user_id == user_id,
                Calendar.start_time.between(datetime.utcnow(), cutoff_time),
                Calendar.preparation_needed == True
            ).order_by(Calendar.start_time.asc()).all()
            
            session.expunge_all()
            return meetings

    def update_person_intelligence(self, user_id: int, person_id: int, intelligence_data: Dict) -> bool:
        """Update person with enhanced intelligence data"""
        with self.get_session() as session:
            person = session.query(Person).filter(
                Person.user_id == user_id,
                Person.id == person_id
            ).first()
            
            if person:
                # Update intelligence fields
                for key, value in intelligence_data.items():
                    if hasattr(person, key) and value is not None:
                        setattr(person, key, value)
                
                person.updated_at = datetime.utcnow()
                session.commit()
                return True
            
            return False

    def get_business_intelligence_summary(self, user_id: int) -> Dict:
        """Get comprehensive business intelligence summary"""
        with self.get_session() as session:
            # Get active insights
            active_insights = session.query(IntelligenceInsight).filter(
                IntelligenceInsight.user_id == user_id,
                IntelligenceInsight.status.in_(['new', 'viewed'])
            ).count()
            
            # Get high-value relationships
            strong_relationships = session.query(EntityRelationship).filter(
                EntityRelationship.user_id == user_id,
                EntityRelationship.strength > 0.7
            ).count()
            
            # Get upcoming meetings needing prep
            upcoming_meetings = self.get_upcoming_meetings_needing_prep(user_id, 72)
            
            # Get recent strategic communications
            strategic_emails = session.query(Email).filter(
                Email.user_id == user_id,
                Email.strategic_importance > 0.7,
                Email.email_date > datetime.utcnow() - timedelta(days=7)
            ).count()
            
            return {
                'active_insights': active_insights,
                'strong_relationships': strong_relationships,
                'meetings_needing_prep': len(upcoming_meetings),
                'recent_strategic_communications': strategic_emails,
                'intelligence_quality_score': min(1.0, (active_insights + strong_relationships * 0.5) / 10)
            }

    def flush_user_data(self, user_id: int) -> bool:
        """
        Flush all data for a specific user from the database.
        This is a complete data wipe for the user while preserving the user account.
        
        Args:
            user_id: ID of the user whose data should be flushed
            
        Returns:
            True if successful, False otherwise
        """
        try:
            with self.get_session() as session:
                logger.warning(f"üóëÔ∏è Starting complete data flush for user ID {user_id}")
                
                # Delete in order to respect foreign key constraints
                
                # 1. Delete intelligence insights
                try:
                    insights_count = session.query(IntelligenceInsight).filter(IntelligenceInsight.user_id == user_id).count()
                    session.query(IntelligenceInsight).filter(IntelligenceInsight.user_id == user_id).delete()
                    logger.info(f"   Deleted {insights_count} intelligence insights")
                except Exception as e:
                    logger.warning(f"   Intelligence insights table issue: {e}")
                
                # 2. Delete entity relationships (using correct column names)
                try:
                    relationships_count = session.query(EntityRelationship).filter(EntityRelationship.user_id == user_id).count()
                    session.query(EntityRelationship).filter(EntityRelationship.user_id == user_id).delete()
                    logger.info(f"   Deleted {relationships_count} entity relationships")
                except Exception as e:
                    logger.warning(f"   Entity relationships table issue: {e}")
                
                # 3. Delete Smart Contact Strategy data (if exists)
                try:
                    contact_contexts_count = session.query(ContactContext).filter(ContactContext.user_id == user_id).count()
                    session.query(ContactContext).filter(ContactContext.user_id == user_id).delete()
                    logger.info(f"   Deleted {contact_contexts_count} contact contexts")
                except Exception as e:
                    logger.warning(f"   Contact contexts table issue: {e}")
                
                try:
                    task_contexts_count = session.query(TaskContext).filter(TaskContext.user_id == user_id).count()
                    session.query(TaskContext).filter(TaskContext.user_id == user_id).delete()
                    logger.info(f"   Deleted {task_contexts_count} task contexts")
                except Exception as e:
                    logger.warning(f"   Task contexts table issue: {e}")
                
                try:
                    topic_knowledge_count = session.query(TopicKnowledgeBase).filter(TopicKnowledgeBase.user_id == user_id).count()
                    session.query(TopicKnowledgeBase).filter(TopicKnowledgeBase.user_id == user_id).delete()
                    logger.info(f"   Deleted {topic_knowledge_count} topic knowledge entries")
                except Exception as e:
                    logger.warning(f"   Topic knowledge table issue: {e}")
                
                try:
                    trusted_contacts_count = session.query(TrustedContact).filter(TrustedContact.user_id == user_id).count()
                    session.query(TrustedContact).filter(TrustedContact.user_id == user_id).delete()
                    logger.info(f"   Deleted {trusted_contacts_count} trusted contacts")
                except Exception as e:
                    logger.warning(f"   Trusted contacts table issue: {e}")
                
                # 4. Delete calendar events
                try:
                    calendar_count = session.query(Calendar).filter(Calendar.user_id == user_id).count()
                    session.query(Calendar).filter(Calendar.user_id == user_id).delete()
                    logger.info(f"   Deleted {calendar_count} calendar events")
                except Exception as e:
                    logger.warning(f"   Calendar events table issue: {e}")
                
                # 5. Delete tasks
                try:
                    tasks_count = session.query(Task).filter(Task.user_id == user_id).count()
                    session.query(Task).filter(Task.user_id == user_id).delete()
                    logger.info(f"   Deleted {tasks_count} tasks")
                except Exception as e:
                    logger.warning(f"   Tasks table issue: {e}")
                
                # 6. Delete emails
                try:
                    emails_count = session.query(Email).filter(Email.user_id == user_id).count()
                    session.query(Email).filter(Email.user_id == user_id).delete()
                    logger.info(f"   Deleted {emails_count} emails")
                except Exception as e:
                    logger.warning(f"   Emails table issue: {e}")
                
                # 7. Delete people
                try:
                    people_count = session.query(Person).filter(Person.user_id == user_id).count()
                    session.query(Person).filter(Person.user_id == user_id).delete()
                    logger.info(f"   Deleted {people_count} people")
                except Exception as e:
                    logger.warning(f"   People table issue: {e}")
                
                # 8. Delete projects
                try:
                    projects_count = session.query(Project).filter(Project.user_id == user_id).count()
                    session.query(Project).filter(Project.user_id == user_id).delete()
                    logger.info(f"   Deleted {projects_count} projects")
                except Exception as e:
                    logger.warning(f"   Projects table issue: {e}")
                
                # 9. Delete topics
                try:
                    topics_count = session.query(Topic).filter(Topic.user_id == user_id).count()
                    session.query(Topic).filter(Topic.user_id == user_id).delete()
                    logger.info(f"   Deleted {topics_count} topics")
                except Exception as e:
                    logger.warning(f"   Topics table issue: {e}")
                
                # 10. Delete user sessions and API keys
                try:
                    sessions_count = session.query(UserSession).filter(UserSession.user_id == user_id).count()
                    session.query(UserSession).filter(UserSession.user_id == user_id).delete()
                    logger.info(f"   Deleted {sessions_count} user sessions")
                except Exception as e:
                    logger.warning(f"   User sessions table issue: {e}")
                
                try:
                    api_keys_count = session.query(ApiKey).filter(ApiKey.user_id == user_id).count()
                    session.query(ApiKey).filter(ApiKey.user_id == user_id).delete()
                    logger.info(f"   Deleted {api_keys_count} API keys")
                except Exception as e:
                    logger.warning(f"   API keys table issue: {e}")
                
                # Commit all deletions
                session.commit()
                
                logger.warning(f"‚úÖ Complete data flush successful for user ID {user_id}")
                return True
                
        except Exception as e:
            logger.error(f"‚ùå Database flush failed for user ID {user_id}: {str(e)}")
            return False

# Global database manager instance - Initialize lazily
_db_manager = None

def get_db_manager():
    """Get the global database manager instance (lazy initialization)"""
    global _db_manager
    if _db_manager is None:
        _db_manager = DatabaseManager()
    return _db_manager

# Export as db_manager for compatibility, but don't instantiate during import
db_manager = None  # Will be set by get_db_manager() when first called 

# At the end of the file, before the DatabaseManager class, add these enhanced intelligence models

class IntelligenceInsight(Base):
    """Proactive intelligence insights generated by AI"""
    __tablename__ = 'intelligence_insights'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Insight content
    insight_type = Column(String(50), nullable=False, index=True)  # meeting_prep, relationship_alert, topic_momentum, urgent_task
    title = Column(String(255), nullable=False)
    description = Column(Text)
    priority = Column(String(20), default='medium', index=True)  # high, medium, low
    confidence = Column(Float, default=0.5)
    
    # Related entities
    related_entity_type = Column(String(50), index=True)  # email, task, person, event
    related_entity_id = Column(Integer, index=True)
    
    # Actionable data
    action_required = Column(Boolean, default=False)
    action_due_date = Column(DateTime)
    action_taken = Column(Boolean, default=False)
    
    # Insight lifecycle
    status = Column(String(20), default='new', index=True)  # new, viewed, acted_on, dismissed
    user_feedback = Column(String(50))  # helpful, not_helpful, etc.
    expires_at = Column(DateTime)
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    viewed_at = Column(DateTime)
    acted_on_at = Column(DateTime)
    
    # Relationships
    user = relationship("User", backref="intelligence_insights")
    
    # Indexes
    __table_args__ = (
        Index('idx_insight_user_type', 'user_id', 'insight_type'),
        Index('idx_insight_user_status', 'user_id', 'status'),
        Index('idx_insight_user_priority', 'user_id', 'priority'),
        Index('idx_insight_expires', 'expires_at'),
    )
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'insight_type': self.insight_type,
            'title': self.title,
            'description': self.description,
            'priority': self.priority,
            'confidence': self.confidence,
            'related_entity_type': self.related_entity_type,
            'related_entity_id': self.related_entity_id,
            'action_required': self.action_required,
            'action_due_date': self.action_due_date.isoformat() if self.action_due_date else None,
            'action_taken': self.action_taken,
            'status': self.status,
            'user_feedback': self.user_feedback,
            'expires_at': self.expires_at.isoformat() if self.expires_at else None,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'viewed_at': self.viewed_at.isoformat() if self.viewed_at else None,
            'acted_on_at': self.acted_on_at.isoformat() if self.acted_on_at else None
        }


class EntityRelationship(Base):
    """Relationships between entities (people, tasks, events, topics)"""
    __tablename__ = 'entity_relationships'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Relationship entities
    source_entity_type = Column(String(50), nullable=False, index=True)  # person, task, event, topic
    source_entity_id = Column(Integer, nullable=False, index=True)
    target_entity_type = Column(String(50), nullable=False, index=True)
    target_entity_id = Column(Integer, nullable=False, index=True)
    
    # Relationship properties
    relationship_type = Column(String(100), nullable=False, index=True)  # works_with, mentioned_in, assigned_to, discussed_in
    strength = Column(Float, default=0.5)  # 0.0 to 1.0
    direction = Column(String(20), default='bidirectional')  # unidirectional, bidirectional
    
    # Supporting evidence
    evidence_count = Column(Integer, default=1)
    last_evidence_date = Column(DateTime, default=datetime.utcnow)
    source_emails = Column(JSONType)  # Email IDs that support this relationship
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    user = relationship("User", backref="entity_relationships")
    
    # Indexes
    __table_args__ = (
        Index('idx_relationship_source', 'user_id', 'source_entity_type', 'source_entity_id'),
        Index('idx_relationship_target', 'user_id', 'target_entity_type', 'target_entity_id'),
        Index('idx_relationship_type', 'user_id', 'relationship_type'),
        Index('idx_relationship_strength', 'user_id', 'strength'),
    )
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'source_entity_type': self.source_entity_type,
            'source_entity_id': self.source_entity_id,
            'target_entity_type': self.target_entity_type,
            'target_entity_id': self.target_entity_id,
            'relationship_type': self.relationship_type,
            'strength': self.strength,
            'direction': self.direction,
            'evidence_count': self.evidence_count,
            'last_evidence_date': self.last_evidence_date.isoformat() if self.last_evidence_date else None,
            'source_emails': self.source_emails,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

# Enhance existing models with comprehensive intelligence fields
# Add these columns to Task model (after the existing fields)
# comprehensive_context_story = Column(Text)  # Rich narrative about task background
# detailed_task_meaning = Column(Text)  # Detailed explanation of what the task means
# comprehensive_importance_analysis = Column(Text)  # Why this task is important
# comprehensive_origin_details = Column(Text)  # Where this task came from
# business_intelligence = Column(JSONType)  # Additional business intelligence metadata

# Add these columns to Person model (after the existing fields)  
# comprehensive_relationship_story = Column(Text)  # Rich narrative about the relationship
# relationship_insights = Column(Text)  # Actionable relationship insights
# relationship_intelligence = Column(JSONType)  # Comprehensive relationship metadata
# business_context = Column(JSONType)  # Enhanced business context data
# relationship_analytics = Column(JSONType)  # Relationship analytics and patterns

# Add these columns to Calendar model (after the existing fields)
# meeting_preparation_tasks = Column(JSONType)  # List of preparation task IDs
# attendee_intelligence = Column(Text)  # Intelligence about meeting attendees
# meeting_context_story = Column(Text)  # Rich narrative about meeting purpose
# preparation_priority = Column(Float, default=0.5)  # How important prep is
# strategic_importance = Column(Float, default=0.5)  # Strategic value of meeting
# preparation_insights = Column(JSONType)  # Specific preparation insights
# outcome_prediction = Column(JSONType)  # Predicted meeting outcomes

# ... existing code ...
FILE: chief_of_staff_ai/models/__init__.py - Package initialization file

============================================================
FILE: chief_of_staff_ai/models/enhanced_models.py
============================================================
# Enhanced Database Models for Entity-Centric Intelligence

from sqlalchemy import Column, Integer, String, Text, DateTime, Boolean, Float, ForeignKey, Table, JSON
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime
import json

Base = declarative_base()

# Association Tables for Many-to-Many Relationships
person_topic_association = Table(
    'person_topics',
    Base.metadata,
    Column('person_id', Integer, ForeignKey('people.id')),
    Column('topic_id', Integer, ForeignKey('topics.id')),
    Column('affinity_score', Float, default=0.5),  # How connected this person is to this topic
    Column('created_at', DateTime, default=datetime.utcnow),
    Column('last_interaction', DateTime, default=datetime.utcnow)
)

task_topic_association = Table(
    'task_topics',
    Base.metadata,
    Column('task_id', Integer, ForeignKey('tasks.id')),
    Column('topic_id', Integer, ForeignKey('topics.id')),
    Column('relevance_score', Float, default=0.5),
    Column('created_at', DateTime, default=datetime.utcnow)
)

event_topic_association = Table(
    'event_topics', 
    Base.metadata,
    Column('event_id', Integer, ForeignKey('calendar_events.id')),
    Column('topic_id', Integer, ForeignKey('topics.id')),
    Column('relevance_score', Float, default=0.5),
    Column('created_at', DateTime, default=datetime.utcnow)
)

class Topic(Base):
    """Topics as the central brain - persistent memory containers"""
    __tablename__ = 'topics'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    name = Column(String(200), nullable=False)
    description = Column(Text)
    keywords = Column(Text)  # Comma-separated for now, can be normalized later
    is_official = Column(Boolean, default=False)
    confidence_score = Column(Float, default=0.5)
    
    # Intelligence accumulation fields
    total_mentions = Column(Integer, default=0)
    last_mentioned = Column(DateTime)
    intelligence_summary = Column(Text)  # AI-generated summary of what we know about this topic
    strategic_importance = Column(Float, default=0.5)  # How important this topic is to the user
    
    # Topic evolution tracking
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    version = Column(Integer, default=1)  # Track topic evolution
    
    # Relationships - Topics as the central hub
    people = relationship("Person", secondary=person_topic_association, back_populates="topics")
    tasks = relationship("Task", secondary=task_topic_association, back_populates="topics")
    events = relationship("CalendarEvent", secondary=event_topic_association, back_populates="topics")
    
    # Direct content relationships
    emails = relationship("Email", back_populates="primary_topic")
    projects = relationship("Project", back_populates="primary_topic")

    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'name': self.name,
            'description': self.description,
            'keywords': json.loads(self.keywords) if self.keywords else [],
            'is_official': self.is_official,
            'confidence_score': self.confidence_score,
            'total_mentions': self.total_mentions,
            'last_mentioned': self.last_mentioned.isoformat() if self.last_mentioned else None,
            'intelligence_summary': self.intelligence_summary,
            'strategic_importance': self.strategic_importance,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'version': self.version
        }

class Person(Base):
    """Enhanced Person model with relationship intelligence"""
    __tablename__ = 'people'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    name = Column(String(200), nullable=False)
    email_address = Column(String(255))
    phone = Column(String(50))
    company = Column(String(200))
    title = Column(String(200))
    
    # Relationship intelligence
    relationship_type = Column(String(100))  # colleague, client, partner, etc.
    importance_level = Column(Float, default=0.5)
    communication_frequency = Column(String(50))  # daily, weekly, monthly, etc.
    last_contact = Column(DateTime)
    total_interactions = Column(Integer, default=0)
    
    # Professional context (extracted from signatures, etc.)
    linkedin_url = Column(String(255))
    bio = Column(Text)
    professional_story = Column(Text)  # AI-generated summary of professional relationship
    
    # Intelligence accumulation
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    topics = relationship("Topic", secondary=person_topic_association, back_populates="people")
    tasks_assigned = relationship("Task", foreign_keys="Task.assignee_id", back_populates="assignee")
    tasks_mentioned = relationship("Task", foreign_keys="Task.mentioned_person_id", back_populates="mentioned_person")

    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'name': self.name,
            'email_address': self.email_address,
            'phone': self.phone,
            'company': self.company,
            'title': self.title,
            'relationship_type': self.relationship_type,
            'importance_level': self.importance_level,
            'communication_frequency': self.communication_frequency,
            'last_contact': self.last_contact.isoformat() if self.last_contact else None,
            'total_interactions': self.total_interactions,
            'linkedin_url': self.linkedin_url,
            'bio': self.bio,
            'professional_story': self.professional_story,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

class Task(Base):
    """Enhanced Task model with full context"""
    __tablename__ = 'tasks'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    description = Column(Text, nullable=False)
    context_story = Column(Text)  # WHY this task exists - the narrative context
    
    # Assignments and ownership
    assignee_id = Column(Integer, ForeignKey('people.id'))
    mentioned_person_id = Column(Integer, ForeignKey('people.id'))  # Person mentioned in task
    
    # Task metadata
    priority = Column(String(20), default='medium')
    status = Column(String(20), default='pending')
    category = Column(String(100))
    confidence = Column(Float, default=0.8)
    
    # Source tracking
    source_email_id = Column(Integer, ForeignKey('emails.id'))
    source_event_id = Column(Integer, ForeignKey('calendar_events.id'))
    
    # Temporal information
    due_date = Column(DateTime)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    completed_at = Column(DateTime)
    
    # Relationships
    topics = relationship("Topic", secondary=task_topic_association, back_populates="tasks")
    assignee = relationship("Person", foreign_keys=[assignee_id], back_populates="tasks_assigned")
    mentioned_person = relationship("Person", foreign_keys=[mentioned_person_id], back_populates="tasks_mentioned")
    source_email = relationship("Email", back_populates="generated_tasks")

    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'description': self.description,
            'context_story': self.context_story,
            'assignee_id': self.assignee_id,
            'mentioned_person_id': self.mentioned_person_id,
            'priority': self.priority,
            'status': self.status,
            'category': self.category,
            'confidence': self.confidence,
            'source_email_id': self.source_email_id,
            'source_event_id': self.source_event_id,
            'due_date': self.due_date.isoformat() if self.due_date else None,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'completed_at': self.completed_at.isoformat() if self.completed_at else None
        }

class Email(Base):
    """Streamlined Email model focused on intelligence, not storage"""
    __tablename__ = 'emails'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    gmail_id = Column(String(255), unique=True, nullable=False)
    
    # Essential metadata only
    subject = Column(String(500))
    sender = Column(String(255))
    sender_name = Column(String(255))
    recipient_emails = Column(Text)  # JSON array of recipients
    email_date = Column(DateTime)
    
    # Intelligence fields
    ai_summary = Column(Text)  # Concise summary for display
    business_category = Column(String(100))  # meeting, project, decision, etc.
    sentiment = Column(String(50))
    urgency_score = Column(Float, default=0.5)
    strategic_importance = Column(Float, default=0.5)
    
    # Content storage strategy: metadata in DB, content in blob storage
    content_hash = Column(String(64))  # SHA-256 of content for deduplication
    blob_storage_key = Column(String(255))  # Reference to external content storage
    
    # Primary topic assignment (Topics as brain concept)
    primary_topic_id = Column(Integer, ForeignKey('topics.id'))
    
    # Processing metadata
    processed_at = Column(DateTime)
    processing_version = Column(String(50))
    
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    primary_topic = relationship("Topic", back_populates="emails")
    generated_tasks = relationship("Task", back_populates="source_email")

    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'gmail_id': self.gmail_id,
            'subject': self.subject,
            'sender': self.sender,
            'sender_name': self.sender_name,
            'recipient_emails': json.loads(self.recipient_emails) if self.recipient_emails else [],
            'email_date': self.email_date.isoformat() if self.email_date else None,
            'ai_summary': self.ai_summary,
            'business_category': self.business_category,
            'sentiment': self.sentiment,
            'urgency_score': self.urgency_score,
            'strategic_importance': self.strategic_importance,
            'content_hash': self.content_hash,
            'blob_storage_key': self.blob_storage_key,
            'primary_topic_id': self.primary_topic_id,
            'processed_at': self.processed_at.isoformat() if self.processed_at else None,
            'processing_version': self.processing_version,
            'created_at': self.created_at.isoformat() if self.created_at else None
        }

class CalendarEvent(Base):
    """Enhanced Calendar model with business intelligence"""
    __tablename__ = 'calendar_events'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    google_event_id = Column(String(255), unique=True, nullable=False)
    
    # Event basics
    title = Column(String(500))
    description = Column(Text)
    location = Column(String(500))
    start_time = Column(DateTime)
    end_time = Column(DateTime)
    
    # Business intelligence
    business_context = Column(Text)  # AI-generated context about meeting purpose
    attendee_intelligence = Column(Text)  # Summary of known attendees and relationships
    preparation_priority = Column(Float, default=0.5)  # How important prep is for this meeting
    
    # Meeting outcome tracking
    outcome_summary = Column(Text)  # Post-meeting AI analysis
    follow_up_needed = Column(Boolean, default=False)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    topics = relationship("Topic", secondary=event_topic_association, back_populates="events")

    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'google_event_id': self.google_event_id,
            'title': self.title,
            'description': self.description,
            'location': self.location,
            'start_time': self.start_time.isoformat() if self.start_time else None,
            'end_time': self.end_time.isoformat() if self.end_time else None,
            'business_context': self.business_context,
            'attendee_intelligence': self.attendee_intelligence,
            'preparation_priority': self.preparation_priority,
            'outcome_summary': self.outcome_summary,
            'follow_up_needed': self.follow_up_needed,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

class Project(Base):
    """Projects as coherent business initiatives"""
    __tablename__ = 'projects'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    name = Column(String(200), nullable=False)
    description = Column(Text)
    status = Column(String(50), default='active')
    priority = Column(String(20), default='medium')
    
    # Project intelligence
    stakeholder_summary = Column(Text)  # AI summary of key people involved
    objective = Column(Text)
    current_phase = Column(String(100))
    challenges = Column(Text)
    opportunities = Column(Text)
    
    # Primary topic assignment
    primary_topic_id = Column(Integer, ForeignKey('topics.id'))
    
    # Timeline
    start_date = Column(DateTime)
    target_completion = Column(DateTime)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    primary_topic = relationship("Topic", back_populates="projects")

    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'name': self.name,
            'description': self.description,
            'status': self.status,
            'priority': self.priority,
            'stakeholder_summary': self.stakeholder_summary,
            'objective': self.objective,
            'current_phase': self.current_phase,
            'challenges': self.challenges,
            'opportunities': self.opportunities,
            'primary_topic_id': self.primary_topic_id,
            'start_date': self.start_date.isoformat() if self.start_date else None,
            'target_completion': self.target_completion.isoformat() if self.target_completion else None,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

class EntityRelationship(Base):
    """Track relationships between any entities for advanced intelligence"""
    __tablename__ = 'entity_relationships'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    
    # Generic entity references
    entity_type_a = Column(String(50), nullable=False)  # person, topic, project, etc.
    entity_id_a = Column(Integer, nullable=False)
    entity_type_b = Column(String(50), nullable=False)
    entity_id_b = Column(Integer, nullable=False)
    
    # Relationship metadata
    relationship_type = Column(String(100))  # collaborates_on, leads, discusses, etc.
    strength = Column(Float, default=0.5)  # How strong this relationship is
    frequency = Column(String(50))  # How often they interact
    
    # Intelligence context
    context_summary = Column(Text)  # AI summary of this relationship
    last_interaction = Column(DateTime)
    total_interactions = Column(Integer, default=1)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'entity_type_a': self.entity_type_a,
            'entity_id_a': self.entity_id_a,
            'entity_type_b': self.entity_type_b,
            'entity_id_b': self.entity_id_b,
            'relationship_type': self.relationship_type,
            'strength': self.strength,
            'frequency': self.frequency,
            'context_summary': self.context_summary,
            'last_interaction': self.last_interaction.isoformat() if self.last_interaction else None,
            'total_interactions': self.total_interactions,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

class IntelligenceInsight(Base):
    """Capture proactive insights generated by the system"""
    __tablename__ = 'intelligence_insights'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    
    # Insight metadata
    insight_type = Column(String(100), nullable=False)  # relationship_alert, opportunity, decision_needed
    title = Column(String(200), nullable=False)
    description = Column(Text, nullable=False)
    priority = Column(String(20), default='medium')
    confidence = Column(Float, default=0.8)
    
    # Entity connections
    related_entity_type = Column(String(50))  # What entity triggered this insight
    related_entity_id = Column(Integer)
    
    # User interaction
    status = Column(String(50), default='new')  # new, viewed, acted_on, dismissed
    user_feedback = Column(String(50))  # helpful, not_helpful, etc.
    
    # Temporal
    expires_at = Column(DateTime)  # Some insights are time-sensitive
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'insight_type': self.insight_type,
            'title': self.title,
            'description': self.description,
            'priority': self.priority,
            'confidence': self.confidence,
            'related_entity_type': self.related_entity_type,
            'related_entity_id': self.related_entity_id,
            'status': self.status,
            'user_feedback': self.user_feedback,
            'expires_at': self.expires_at.isoformat() if self.expires_at else None,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

# Migration strategy: Create these tables alongside existing ones,
# then populate with data transformation scripts 

============================================================
FILE: chief_of_staff_ai/models/database.py.backup
============================================================
import os
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean, Float, ForeignKey, Index, text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship, Session
from sqlalchemy.dialects.postgresql import JSON
from sqlalchemy.types import TypeDecorator

from config.settings import settings

# Import enhanced models for entity-centric intelligence
from models.enhanced_models import (
    Topic as EnhancedTopic, Person as EnhancedPerson, Task as EnhancedTask, 
    Email as EnhancedEmail, CalendarEvent, Project as EnhancedProject,
    EntityRelationship, IntelligenceInsight,
    person_topic_association, task_topic_association, event_topic_association
)

logger = logging.getLogger(__name__)

# Base class for all models
Base = declarative_base()

# Custom JSON type that works with both SQLite and PostgreSQL
class JSONType(TypeDecorator):
    impl = Text
    
    def process_bind_param(self, value, dialect):
        if value is not None:
            return json.dumps(value)
        return value
    
    def process_result_value(self, value, dialect):
        if value is not None:
            return json.loads(value)
        return value

class User(Base):
    """User model for multi-tenant authentication"""
    __tablename__ = 'users'
    
    id = Column(Integer, primary_key=True)
    email = Column(String(255), unique=True, nullable=False, index=True)
    google_id = Column(String(255), unique=True, nullable=False)
    name = Column(String(255), nullable=False)
    
    # OAuth credentials (encrypted in production)
    access_token = Column(Text)
    refresh_token = Column(Text)
    token_expires_at = Column(DateTime)
    scopes = Column(JSONType)
    
    # Account metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    last_login = Column(DateTime, default=datetime.utcnow)
    is_active = Column(Boolean, default=True)
    
    # Processing preferences
    email_fetch_limit = Column(Integer, default=50)
    email_days_back = Column(Integer, default=30)
    auto_process_emails = Column(Boolean, default=True)
    
    # Relationships
    emails = relationship("Email", back_populates="user", cascade="all, delete-orphan")
    tasks = relationship("Task", back_populates="user", cascade="all, delete-orphan")
    people = relationship("Person", back_populates="user", cascade="all, delete-orphan")
    projects = relationship("Project", back_populates="user", cascade="all, delete-orphan")
    topics = relationship("Topic", back_populates="user", cascade="all, delete-orphan")
    
    def __repr__(self):
        return f"<User(email='{self.email}', name='{self.name}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'email': self.email,
            'name': self.name,
            'google_id': self.google_id,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'last_login': self.last_login.isoformat() if self.last_login else None,
            'is_active': self.is_active,
            'email_fetch_limit': self.email_fetch_limit,
            'email_days_back': self.email_days_back,
            'auto_process_emails': self.auto_process_emails
        }

class Email(Base):
    """Email model for storing processed emails per user"""
    __tablename__ = 'emails'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Gmail identifiers
    gmail_id = Column(String(255), nullable=False, index=True)
    thread_id = Column(String(255), index=True)
    
    # Email content
    sender = Column(String(255), index=True)
    sender_name = Column(String(255))
    subject = Column(Text)
    body_text = Column(Text)
    body_html = Column(Text)
    body_clean = Column(Text)
    body_preview = Column(Text)
    snippet = Column(Text)
    
    # Email metadata
    recipients = Column(JSONType)  # List of recipient emails
    cc = Column(JSONType)  # List of CC emails
    bcc = Column(JSONType)  # List of BCC emails
    labels = Column(JSONType)  # Gmail labels
    attachments = Column(JSONType)  # Attachment metadata
    entities = Column(JSONType)  # Extracted entities
    
    # Email properties
    email_date = Column(DateTime, index=True)
    size_estimate = Column(Integer)
    message_type = Column(String(50), index=True)  # regular, meeting, newsletter, etc.
    priority_score = Column(Float, index=True)
    
    # Email status
    is_read = Column(Boolean, default=False)
    is_important = Column(Boolean, default=False)
    is_starred = Column(Boolean, default=False)
    has_attachments = Column(Boolean, default=False)
    
    # Email classification and AI insights
    project_id = Column(Integer, ForeignKey('projects.id'), index=True)
    mentioned_people = Column(JSONType)  # List of person IDs mentioned in email
    ai_summary = Column(Text)  # Claude-generated summary
    ai_category = Column(String(100))  # AI-determined category
    sentiment_score = Column(Float)  # Sentiment analysis score
    urgency_score = Column(Float)  # AI-determined urgency
    key_insights = Column(JSONType)  # Key insights extracted by Claude
    topics = Column(JSONType)  # Main topics/themes identified
    action_required = Column(Boolean, default=False)  # Whether action is needed
    follow_up_required = Column(Boolean, default=False)  # Whether follow-up needed
    
    # Processing metadata
    processed_at = Column(DateTime, default=datetime.utcnow)
    normalizer_version = Column(String(50))
    has_errors = Column(Boolean, default=False)
    error_message = Column(Text)
    
    # Relationships
    user = relationship("User", back_populates="emails")
    tasks = relationship("Task", back_populates="email", cascade="all, delete-orphan")
    
    # Indexes for performance - Fixed naming to avoid conflicts
    __table_args__ = (
        Index('idx_email_user_gmail', 'user_id', 'gmail_id'),
        Index('idx_email_user_date', 'user_id', 'email_date'),
        Index('idx_email_user_type', 'user_id', 'message_type'),
        Index('idx_email_user_priority', 'user_id', 'priority_score'),
    )
    
    def __repr__(self):
        return f"<Email(gmail_id='{self.gmail_id}', subject='{self.subject[:50]}...')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'gmail_id': self.gmail_id,
            'thread_id': self.thread_id,
            'sender': self.sender,
            'sender_name': self.sender_name,
            'subject': self.subject,
            'body_preview': self.body_preview,
            'snippet': self.snippet,
            'recipients': self.recipients,
            'email_date': self.email_date.isoformat() if self.email_date else None,
            'message_type': self.message_type,
            'priority_score': self.priority_score,
            'is_read': self.is_read,
            'is_important': self.is_important,
            'is_starred': self.is_starred,
            'has_attachments': self.has_attachments,
            'processed_at': self.processed_at.isoformat() if self.processed_at else None,
            'project_id': self.project_id,
            'mentioned_people': self.mentioned_people,
            'ai_summary': self.ai_summary,
            'ai_category': self.ai_category,
            'sentiment_score': self.sentiment_score,
            'urgency_score': self.urgency_score,
            'key_insights': self.key_insights,
            'topics': self.topics,
            'action_required': self.action_required,
            'follow_up_required': self.follow_up_required
        }

class Task(Base):
    """Task model for storing extracted tasks per user"""
    __tablename__ = 'tasks'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    email_id = Column(Integer, ForeignKey('emails.id'), nullable=True, index=True)
    
    # Task content
    description = Column(Text, nullable=False)
    assignee = Column(String(255))
    due_date = Column(DateTime, index=True)
    due_date_text = Column(String(255))
    
    # Task metadata
    priority = Column(String(20), default='medium', index=True)  # high, medium, low
    category = Column(String(50), index=True)  # follow-up, deadline, meeting, etc.
    confidence = Column(Float)  # AI confidence score
    source_text = Column(Text)  # Original text from email
    
    # Task status
    status = Column(String(20), default='pending', index=True)  # pending, in_progress, completed, cancelled
    completed_at = Column(DateTime)
    
    # Extraction metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    extractor_version = Column(String(50))
    model_used = Column(String(100))
    
    # Relationships
    user = relationship("User", back_populates="tasks")
    email = relationship("Email", back_populates="tasks")
    
    # Indexes for performance - Fixed naming to avoid conflicts
    __table_args__ = (
        Index('idx_task_user_status', 'user_id', 'status'),
        Index('idx_task_user_priority_unique', 'user_id', 'priority'),
        Index('idx_task_user_due_date', 'user_id', 'due_date'),
        Index('idx_task_user_category', 'user_id', 'category'),
    )
    
    def __repr__(self):
        return f"<Task(description='{self.description[:50]}...', priority='{self.priority}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'email_id': self.email_id,
            'description': self.description,
            'assignee': self.assignee,
            'due_date': self.due_date.isoformat() if self.due_date else None,
            'due_date_text': self.due_date_text,
            'priority': self.priority,
            'category': self.category,
            'confidence': self.confidence,
            'source_text': self.source_text,
            'status': self.status,
            'completed_at': self.completed_at.isoformat() if self.completed_at else None,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'extractor_version': self.extractor_version,
            'model_used': self.model_used
        }

class Person(Base):
    """Person model for tracking individuals mentioned in emails"""
    __tablename__ = 'people'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Person identification
    email_address = Column(String(255), index=True)
    name = Column(String(255), nullable=False)
    first_name = Column(String(100))
    last_name = Column(String(100))
    
    # Person details (extracted and augmented by Claude)
    title = Column(String(255))
    company = Column(String(255))
    role = Column(String(255))
    department = Column(String(255))
    
    # Relationship and context
    relationship_type = Column(String(100))  # colleague, client, vendor, etc.
    communication_frequency = Column(String(50))  # high, medium, low
    importance_level = Column(Float)  # 0.0 to 1.0
    
    # Knowledge base (JSON fields for flexible data)
    skills = Column(JSONType)  # List of skills/expertise
    interests = Column(JSONType)  # Personal/professional interests
    projects_involved = Column(JSONType)  # List of project IDs
    communication_style = Column(Text)  # Claude's analysis of communication style
    key_topics = Column(JSONType)  # Main topics discussed with this person
    
    # Extracted insights
    personality_traits = Column(JSONType)  # Claude-extracted personality insights
    preferences = Column(JSONType)  # Communication preferences, etc.
    notes = Column(Text)  # Accumulated notes about this person
    
    # Metadata
    first_mentioned = Column(DateTime, default=datetime.utcnow)
    last_interaction = Column(DateTime, default=datetime.utcnow)
    total_emails = Column(Integer, default=0)
    
    # AI processing metadata
    knowledge_confidence = Column(Float, default=0.5)  # Confidence in extracted data
    last_updated_by_ai = Column(DateTime)
    ai_version = Column(String(50))
    
    # NEW: Smart Contact Strategy fields
    is_trusted_contact = Column(Boolean, default=False, index=True)
    engagement_score = Column(Float, default=0.0)
    bidirectional_topics = Column(JSONType)  # Topics with back-and-forth discussion
    
    # Relationships
    user = relationship("User", back_populates="people")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_person_user_email', 'user_id', 'email_address'),
        Index('idx_person_user_name', 'user_id', 'name'),
        Index('idx_person_company', 'user_id', 'company'),
    )
    
    def __repr__(self):
        return f"<Person(name='{self.name}', email='{self.email_address}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'email_address': self.email_address,
            'name': self.name,
            'first_name': self.first_name,
            'last_name': self.last_name,
            'title': self.title,
            'company': self.company,
            'role': self.role,
            'department': self.department,
            'relationship_type': self.relationship_type,
            'communication_frequency': self.communication_frequency,
            'importance_level': self.importance_level,
            'skills': self.skills,
            'interests': self.interests,
            'projects_involved': self.projects_involved,
            'communication_style': self.communication_style,
            'key_topics': self.key_topics,
            'personality_traits': self.personality_traits,
            'preferences': self.preferences,
            'notes': self.notes,
            'first_mentioned': self.first_mentioned.isoformat() if self.first_mentioned else None,
            'last_interaction': self.last_interaction.isoformat() if self.last_interaction else None,
            'total_emails': self.total_emails,
            'knowledge_confidence': self.knowledge_confidence,
            'last_updated_by_ai': self.last_updated_by_ai.isoformat() if self.last_updated_by_ai else None,
            'ai_version': self.ai_version,
            'is_trusted_contact': self.is_trusted_contact,
            'engagement_score': self.engagement_score,
            'bidirectional_topics': self.bidirectional_topics
        }

class Project(Base):
    """Project model for categorizing emails and tracking project-related information"""
    __tablename__ = 'projects'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Project identification
    name = Column(String(255), nullable=False)
    slug = Column(String(255), index=True)  # URL-friendly name
    description = Column(Text)
    
    # Project details
    status = Column(String(50), default='active')  # active, completed, paused, cancelled
    priority = Column(String(20), default='medium')  # high, medium, low
    category = Column(String(100))  # business, personal, client work, etc.
    
    # Timeline
    start_date = Column(DateTime)
    end_date = Column(DateTime)
    deadline = Column(DateTime)
    
    # People and relationships
    stakeholders = Column(JSONType)  # List of person IDs involved
    team_members = Column(JSONType)  # List of person IDs
    
    # Project insights (extracted by Claude)
    key_topics = Column(JSONType)  # Main topics/themes
    objectives = Column(JSONType)  # Project goals and objectives
    challenges = Column(JSONType)  # Identified challenges
    progress_indicators = Column(JSONType)  # Metrics and milestones
    
    # Communication patterns
    communication_frequency = Column(String(50))
    last_activity = Column(DateTime)
    total_emails = Column(Integer, default=0)
    
    # AI analysis
    sentiment_trend = Column(Float)  # Overall sentiment about project
    urgency_level = Column(Float)  # How urgent this project appears
    confidence_score = Column(Float)  # AI confidence in project categorization
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    ai_version = Column(String(50))
    
    # Relationships
    user = relationship("User", back_populates="projects")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_project_user_status', 'user_id', 'status'),
        Index('idx_project_user_priority', 'user_id', 'priority'),
        Index('idx_project_user_category', 'user_id', 'category'),
    )
    
    def __repr__(self):
        return f"<Project(name='{self.name}', status='{self.status}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'name': self.name,
            'slug': self.slug,
            'description': self.description,
            'status': self.status,
            'priority': self.priority,
            'category': self.category,
            'start_date': self.start_date.isoformat() if self.start_date else None,
            'end_date': self.end_date.isoformat() if self.end_date else None,
            'deadline': self.deadline.isoformat() if self.deadline else None,
            'stakeholders': self.stakeholders,
            'team_members': self.team_members,
            'key_topics': self.key_topics,
            'objectives': self.objectives,
            'challenges': self.challenges,
            'progress_indicators': self.progress_indicators,
            'communication_frequency': self.communication_frequency,
            'last_activity': self.last_activity.isoformat() if self.last_activity else None,
            'total_emails': self.total_emails,
            'sentiment_trend': self.sentiment_trend,
            'urgency_level': self.urgency_level,
            'confidence_score': self.confidence_score,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'ai_version': self.ai_version
        }

class Topic(Base):
    """Topic model for organizing and categorizing content"""
    __tablename__ = 'topics'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Topic identification
    name = Column(String(255), nullable=False)
    slug = Column(String(255), index=True)  # URL-friendly name
    description = Column(Text)
    
    # Topic properties
    is_official = Column(Boolean, default=False, index=True)  # Official vs AI-discovered
    parent_topic_id = Column(Integer, ForeignKey('topics.id'), index=True)  # For hierarchical topics
    merged_topics = Column(Text)  # JSON string of merged topic names
    keywords = Column(Text)  # JSON string of keywords for matching (changed from JSONType for compatibility)
    email_count = Column(Integer, default=0)  # Number of emails with this topic
    
    # Usage tracking
    last_used = Column(DateTime)
    usage_frequency = Column(Float)
    confidence_threshold = Column(Float)
    
    # AI analysis
    confidence_score = Column(Float, default=0.5)  # AI confidence in topic classification
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    ai_version = Column(String(50))
    
    # Relationships
    user = relationship("User", back_populates="topics")
    parent_topic = relationship("Topic", remote_side=[id], backref="child_topics")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_topic_user_official', 'user_id', 'is_official'),
        Index('idx_topic_user_name', 'user_id', 'name'),
        Index('idx_topic_slug', 'user_id', 'slug'),
        Index('idx_topic_parent', 'parent_topic_id'),
    )
    
    def __repr__(self):
        return f"<Topic(name='{self.name}', is_official={self.is_official})>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'name': self.name,
            'slug': self.slug,
            'description': self.description,
            'is_official': self.is_official,
            'keywords': json.loads(self.keywords) if self.keywords else [],
            'email_count': self.email_count,
            'confidence_score': self.confidence_score,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'ai_version': self.ai_version,
            'parent_topic_id': self.parent_topic_id,
            'last_used': self.last_used.isoformat() if self.last_used else None
        }

class TrustedContact(Base):
    """Trusted Contact model for engagement-based contact database"""
    __tablename__ = 'trusted_contacts'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Contact identification
    email_address = Column(String(255), nullable=False, index=True)
    name = Column(String(255))
    
    # Engagement metrics
    engagement_score = Column(Float, default=0.0, index=True)
    first_sent_date = Column(DateTime)
    last_sent_date = Column(DateTime, index=True)
    total_sent_emails = Column(Integer, default=0)
    total_received_emails = Column(Integer, default=0)
    bidirectional_threads = Column(Integer, default=0)
    
    # Topic analysis
    topics_discussed = Column(JSONType)  # List of topics from sent/received emails
    bidirectional_topics = Column(JSONType)  # Topics with back-and-forth discussion
    
    # Relationship assessment
    relationship_strength = Column(String(20), default='low', index=True)  # high, medium, low
    communication_frequency = Column(String(20))  # daily, weekly, monthly, occasional
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    last_analyzed = Column(DateTime)
    
    # Relationships
    user = relationship("User", backref="trusted_contacts")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_trusted_contact_user_email', 'user_id', 'email_address'),
        Index('idx_trusted_contact_engagement', 'user_id', 'engagement_score'),
        Index('idx_trusted_contact_strength', 'user_id', 'relationship_strength'),
    )
    
    def __repr__(self):
        return f"<TrustedContact(email='{self.email_address}', strength='{self.relationship_strength}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'email_address': self.email_address,
            'name': self.name,
            'engagement_score': self.engagement_score,
            'first_sent_date': self.first_sent_date.isoformat() if self.first_sent_date else None,
            'last_sent_date': self.last_sent_date.isoformat() if self.last_sent_date else None,
            'total_sent_emails': self.total_sent_emails,
            'total_received_emails': self.total_received_emails,
            'bidirectional_threads': self.bidirectional_threads,
            'topics_discussed': self.topics_discussed,
            'bidirectional_topics': self.bidirectional_topics,
            'relationship_strength': self.relationship_strength,
            'communication_frequency': self.communication_frequency,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'last_analyzed': self.last_analyzed.isoformat() if self.last_analyzed else None
        }

class ContactContext(Base):
    """Rich context information for contacts"""
    __tablename__ = 'contact_contexts'
    
    id = Column(Integer, primary_key=True)
    person_id = Column(Integer, ForeignKey('people.id'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Context details
    context_type = Column(String(50), nullable=False, index=True)  # communication_pattern, project_involvement, topic_expertise, relationship_notes
    title = Column(String(255), nullable=False)
    description = Column(Text)
    confidence_score = Column(Float, default=0.5)
    
    # Supporting evidence
    source_emails = Column(JSONType)  # List of email IDs that contributed to this context
    supporting_quotes = Column(JSONType)  # Relevant excerpts from emails
    tags = Column(JSONType)  # Flexible tagging system
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    person = relationship("Person", backref="contexts")
    user = relationship("User", backref="contact_contexts")
    
    # Indexes
    __table_args__ = (
        Index('idx_contact_context_person', 'person_id', 'context_type'),
        Index('idx_contact_context_user', 'user_id', 'context_type'),
    )
    
    def __repr__(self):
        return f"<ContactContext(type='{self.context_type}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'person_id': self.person_id,
            'user_id': self.user_id,
            'context_type': self.context_type,
            'title': self.title,
            'description': self.description,
            'confidence_score': self.confidence_score,
            'source_emails': self.source_emails,
            'supporting_quotes': self.supporting_quotes,
            'tags': self.tags,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

class TaskContext(Base):
    """Rich context information for tasks"""
    __tablename__ = 'task_contexts'
    
    id = Column(Integer, primary_key=True)
    task_id = Column(Integer, ForeignKey('tasks.id'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Context details
    context_type = Column(String(50), nullable=False, index=True)  # background, stakeholders, timeline, business_impact
    title = Column(String(255), nullable=False)
    description = Column(Text)
    
    # Related entities
    related_people = Column(JSONType)  # List of person IDs
    related_projects = Column(JSONType)  # List of project IDs
    related_topics = Column(JSONType)  # List of relevant topics
    
    # Source information
    source_email_id = Column(Integer, ForeignKey('emails.id'))
    source_thread_id = Column(String(255))
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    task = relationship("Task", backref="contexts")
    user = relationship("User", backref="task_contexts")
    source_email = relationship("Email")
    
    # Indexes
    __table_args__ = (
        Index('idx_task_context_task', 'task_id', 'context_type'),
        Index('idx_task_context_user', 'user_id', 'context_type'),
    )
    
    def __repr__(self):
        return f"<TaskContext(type='{self.context_type}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'task_id': self.task_id,
            'user_id': self.user_id,
            'context_type': self.context_type,
            'title': self.title,
            'description': self.description,
            'related_people': self.related_people,
            'related_projects': self.related_projects,
            'related_topics': self.related_topics,
            'source_email_id': self.source_email_id,
            'source_thread_id': self.source_thread_id,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

class TopicKnowledgeBase(Base):
    """Comprehensive knowledge base for topics"""
    __tablename__ = 'topic_knowledge_base'
    
    id = Column(Integer, primary_key=True)
    topic_id = Column(Integer, ForeignKey('topics.id'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Knowledge details
    knowledge_type = Column(String(50), nullable=False, index=True)  # methodology, key_people, challenges, success_patterns, tools, decisions
    title = Column(String(255), nullable=False)
    content = Column(Text)
    confidence_score = Column(Float, default=0.5)
    
    # Supporting evidence
    supporting_evidence = Column(JSONType)  # Email excerpts, patterns observed
    source_emails = Column(JSONType)  # List of email IDs that contributed
    patterns = Column(JSONType)  # Observed patterns and trends
    
    # Knowledge metadata
    relevance_score = Column(Float, default=0.5)  # How relevant this knowledge is
    engagement_weight = Column(Float, default=0.5)  # Weight based on user engagement
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    last_updated = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    topic = relationship("Topic", backref="knowledge_base")
    user = relationship("User", backref="topic_knowledge")
    
    # Indexes
    __table_args__ = (
        Index('idx_topic_knowledge_topic', 'topic_id', 'knowledge_type'),
        Index('idx_topic_knowledge_user', 'user_id', 'knowledge_type'),
        Index('idx_topic_knowledge_relevance', 'user_id', 'relevance_score'),
    )
    
    def __repr__(self):
        return f"<TopicKnowledgeBase(type='{self.knowledge_type}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'topic_id': self.topic_id,
            'user_id': self.user_id,
            'knowledge_type': self.knowledge_type,
            'title': self.title,
            'content': self.content,
            'confidence_score': self.confidence_score,
            'supporting_evidence': self.supporting_evidence,
            'source_emails': self.source_emails,
            'patterns': self.patterns,
            'relevance_score': self.relevance_score,
            'engagement_weight': self.engagement_weight,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'last_updated': self.last_updated.isoformat() if self.last_updated else None
        }

class Calendar(Base):
    """Calendar model for storing Google Calendar events per user"""
    __tablename__ = 'calendar_events'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Google Calendar identifiers
    event_id = Column(String(255), nullable=False, index=True)
    calendar_id = Column(String(255), nullable=False, index=True)
    recurring_event_id = Column(String(255), index=True)
    
    # Event content
    title = Column(Text)
    description = Column(Text)
    location = Column(Text)
    status = Column(String(50))  # confirmed, tentative, cancelled
    
    # Event timing
    start_time = Column(DateTime, index=True)
    end_time = Column(DateTime, index=True)
    timezone = Column(String(100))
    is_all_day = Column(Boolean, default=False)
    
    # Attendees and relationships
    organizer_email = Column(String(255), index=True)
    organizer_name = Column(String(255))
    attendees = Column(JSONType)  # List of attendee objects with email, name, status
    attendee_emails = Column(JSONType)  # List of attendee emails for quick lookup
    
    # Meeting metadata
    meeting_type = Column(String(100))  # in-person, video_call, phone, etc.
    conference_data = Column(JSONType)  # Google Meet, Zoom links, etc.
    visibility = Column(String(50))  # default, public, private
    
    # Event properties
    is_recurring = Column(Boolean, default=False)
    recurrence_rules = Column(JSONType)  # RRULE data
    is_busy = Column(Boolean, default=True)
    transparency = Column(String(20))  # opaque, transparent
    
    # AI analysis and insights
    ai_summary = Column(Text)  # Claude-generated meeting summary/purpose
    ai_category = Column(String(100))  # AI-determined category (business, personal, etc.)
    importance_score = Column(Float)  # AI-determined importance
    preparation_needed = Column(Boolean, default=False)
    follow_up_required = Column(Boolean, default=False)
    
    # Contact intelligence integration
    known_attendees = Column(JSONType)  # List of person IDs from People table
    unknown_attendees = Column(JSONType)  # Attendees not in contact database
    business_context = Column(Text)  # AI-generated business context based on attendees
    
    # Free time analysis
    is_free_time = Column(Boolean, default=False, index=True)  # For free time slot identification
    potential_duration = Column(Integer)  # Duration in minutes for free slots
    
    # Processing metadata
    fetched_at = Column(DateTime, default=datetime.utcnow)
    last_updated = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    ai_processed_at = Column(DateTime)
    ai_version = Column(String(50))
    
    # Google Calendar metadata
    html_link = Column(Text)  # Link to event in Google Calendar
    hangout_link = Column(Text)  # Google Meet link
    ical_uid = Column(String(255))
    sequence = Column(Integer)  # For tracking updates
    
    # Relationships
    user = relationship("User", backref="calendar_events")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_calendar_user_event', 'user_id', 'event_id'),
        Index('idx_calendar_user_time', 'user_id', 'start_time'),
        Index('idx_calendar_user_organizer', 'user_id', 'organizer_email'),
        Index('idx_calendar_free_time', 'user_id', 'is_free_time'),
        Index('idx_calendar_status', 'user_id', 'status'),
    )
    
    def __repr__(self):
        return f"<Calendar(event_id='{self.event_id}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'event_id': self.event_id,
            'calendar_id': self.calendar_id,
            'recurring_event_id': self.recurring_event_id,
            'title': self.title,
            'description': self.description,
            'location': self.location,
            'status': self.status,
            'start_time': self.start_time.isoformat() if self.start_time else None,
            'end_time': self.end_time.isoformat() if self.end_time else None,
            'timezone': self.timezone,
            'is_all_day': self.is_all_day,
            'organizer_email': self.organizer_email,
            'organizer_name': self.organizer_name,
            'attendees': self.attendees,
            'attendee_emails': self.attendee_emails,
            'meeting_type': self.meeting_type,
            'conference_data': self.conference_data,
            'visibility': self.visibility,
            'is_recurring': self.is_recurring,
            'recurrence_rules': self.recurrence_rules,
            'is_busy': self.is_busy,
            'transparency': self.transparency,
            'ai_summary': self.ai_summary,
            'ai_category': self.ai_category,
            'importance_score': self.importance_score,
            'preparation_needed': self.preparation_needed,
            'follow_up_required': self.follow_up_required,
            'known_attendees': self.known_attendees,
            'unknown_attendees': self.unknown_attendees,
            'business_context': self.business_context,
            'is_free_time': self.is_free_time,
            'potential_duration': self.potential_duration,
            'fetched_at': self.fetched_at.isoformat() if self.fetched_at else None,
            'last_updated': self.last_updated.isoformat() if self.last_updated else None,
            'ai_processed_at': self.ai_processed_at.isoformat() if self.ai_processed_at else None,
            'ai_version': self.ai_version,
            'html_link': self.html_link,
            'hangout_link': self.hangout_link,
            'ical_uid': self.ical_uid,
            'sequence': self.sequence
        }

class DatabaseManager:
    """Database manager for handling connections and sessions"""
    
    def __init__(self):
        self.engine = None
        self.SessionLocal = None
        self.initialize_database()
    
    def initialize_database(self):
        """Initialize database connection and create tables"""
        try:
            # Use DATABASE_URL from environment or default to SQLite
            database_url = settings.DATABASE_URL
            
            # Handle PostgreSQL URL for Heroku
            if database_url and database_url.startswith('postgres://'):
                database_url = database_url.replace('postgres://', 'postgresql://', 1)
            
            # Create engine with appropriate settings
            if database_url.startswith('postgresql://'):
                # PostgreSQL settings for Heroku
                self.engine = create_engine(
                    database_url,
                    echo=settings.DEBUG,
                    pool_pre_ping=True,
                    pool_recycle=300
                )
            else:
                # SQLite settings for local development
                self.engine = create_engine(
                    database_url,
                    echo=settings.DEBUG,
                    connect_args={"check_same_thread": False}
                )
            
            # Create session factory
            self.SessionLocal = sessionmaker(bind=self.engine)
            
            # Create all tables
            Base.metadata.create_all(bind=self.engine)
            
            logger.info("Database initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize database: {str(e)}")
            raise
    
    def get_session(self) -> Session:
        """Get a new database session"""
        return self.SessionLocal()
    
    def get_user_by_email(self, email: str) -> Optional[User]:
        """Get user by email address"""
        with self.get_session() as session:
            return session.query(User).filter(User.email == email).first()
    
    def create_or_update_user(self, user_info: Dict, credentials: Dict) -> User:
        """Create or update user with OAuth info"""
        with self.get_session() as session:
            user = session.query(User).filter(User.email == user_info['email']).first()
            
            if user:
                # Update existing user
                user.name = user_info.get('name', user.name)
                user.last_login = datetime.utcnow()
                user.access_token = credentials.get('access_token')
                user.refresh_token = credentials.get('refresh_token')
                user.token_expires_at = credentials.get('expires_at')
                user.scopes = credentials.get('scopes', [])
            else:
                # Create new user
                user = User(
                    email=user_info['email'],
                    google_id=user_info['id'],
                    name=user_info.get('name', ''),
                    access_token=credentials.get('access_token'),
                    refresh_token=credentials.get('refresh_token'),
                    token_expires_at=credentials.get('expires_at'),
                    scopes=credentials.get('scopes', [])
                )
                session.add(user)
            
            session.commit()
            session.refresh(user)
            return user
    
    def save_email(self, user_id: int, email_data: Dict) -> Email:
        """Save processed email to database"""
        with self.get_session() as session:
            # Check if email already exists
            existing = session.query(Email).filter(
                Email.user_id == user_id,
                Email.gmail_id == email_data['id']
            ).first()
            
            if existing:
                return existing
            
            # Create new email record
            email = Email(
                user_id=user_id,
                gmail_id=email_data['id'],
                thread_id=email_data.get('thread_id'),
                sender=email_data.get('sender'),
                sender_name=email_data.get('sender_name'),
                subject=email_data.get('subject'),
                body_text=email_data.get('body_text'),
                body_html=email_data.get('body_html'),
                body_clean=email_data.get('body_clean'),
                body_preview=email_data.get('body_preview'),
                snippet=email_data.get('snippet'),
                recipients=email_data.get('recipients', []),
                cc=email_data.get('cc', []),
                bcc=email_data.get('bcc', []),
                labels=email_data.get('labels', []),
                attachments=email_data.get('attachments', []),
                entities=email_data.get('entities', {}),
                email_date=email_data.get('timestamp'),
                size_estimate=email_data.get('size_estimate'),
                message_type=email_data.get('message_type'),
                priority_score=email_data.get('priority_score'),
                is_read=email_data.get('is_read', False),
                is_important=email_data.get('is_important', False),
                is_starred=email_data.get('is_starred', False),
                has_attachments=email_data.get('has_attachments', False),
                normalizer_version=email_data.get('processing_metadata', {}).get('normalizer_version'),
                has_errors=email_data.get('error', False),
                error_message=email_data.get('error_message')
            )
            
            session.add(email)
            session.commit()
            session.refresh(email)
            return email
    
    def save_task(self, user_id: int, email_id: Optional[int], task_data: Dict) -> Task:
        """Save extracted task to database"""
        try:
            with self.get_session() as session:
                task = Task(
                    user_id=user_id,
                    email_id=email_id,
                    description=task_data['description'],
                    assignee=task_data.get('assignee'),
                    due_date=task_data.get('due_date'),
                    due_date_text=task_data.get('due_date_text'),
                    priority=task_data.get('priority', 'medium'),
                    category=task_data.get('category'),
                    confidence=task_data.get('confidence'),
                    source_text=task_data.get('source_text'),
                    status=task_data.get('status', 'pending'),
                    extractor_version=task_data.get('extractor_version'),
                    model_used=task_data.get('model_used')
                )
                
                session.add(task)
                session.commit()
                session.refresh(task)
                
                # Verify the task object is valid before returning
                if not task or not hasattr(task, 'id') or task.id is None:
                    raise ValueError("Failed to create task - invalid task object returned")
                
                return task
                
        except Exception as e:
            logger.error(f"Failed to save task to database: {str(e)}")
            logger.error(f"Task data: {task_data}")
            raise  # Re-raise the exception instead of returning a dict
    
    def get_user_emails(self, user_id: int, limit: int = 50) -> List[Email]:
        """Get emails for a user"""
        with self.get_session() as session:
            return session.query(Email).filter(
                Email.user_id == user_id
            ).order_by(Email.email_date.desc()).limit(limit).all()
    
    def get_user_tasks(self, user_id: int, status: str = None, limit: int = 500) -> List[Task]:
        """Get tasks for a user"""
        with self.get_session() as session:
            query = session.query(Task).filter(Task.user_id == user_id)
            if status:
                query = query.filter(Task.status == status)
            return query.order_by(Task.created_at.desc()).limit(limit).all()

    def create_or_update_person(self, user_id: int, person_data: Dict) -> Person:
        """Create or update a person record"""
        with self.get_session() as session:
            # Try to find existing person by email or name
            person = session.query(Person).filter(
                Person.user_id == user_id,
                Person.email_address == person_data.get('email_address')
            ).first()
            
            if not person and person_data.get('name'):
                # Try by name if email not found
                person = session.query(Person).filter(
                    Person.user_id == user_id,
                    Person.name == person_data.get('name')
                ).first()
            
            if person:
                # Update existing person
                for key, value in person_data.items():
                    if hasattr(person, key) and value is not None:
                        setattr(person, key, value)
                person.last_interaction = datetime.utcnow()
                person.total_emails += 1
                person.last_updated_by_ai = datetime.utcnow()
            else:
                # Create new person - remove conflicting fields from person_data
                person_data_clean = person_data.copy()
                person_data_clean.pop('total_emails', None)  # Remove if present
                person_data_clean.pop('last_updated_by_ai', None)  # Remove if present
                
                person = Person(
                    user_id=user_id,
                    **person_data_clean,
                    total_emails=1,
                    last_updated_by_ai=datetime.utcnow()
                )
                session.add(person)
            
            session.commit()
            session.refresh(person)
            return person
    
    def create_or_update_project(self, user_id: int, project_data: Dict) -> Project:
        """Create or update a project record"""
        with self.get_session() as session:
            # Try to find existing project by name or slug
            project = session.query(Project).filter(
                Project.user_id == user_id,
                Project.name == project_data.get('name')
            ).first()
            
            if project:
                # Update existing project
                for key, value in project_data.items():
                    if hasattr(project, key) and value is not None:
                        setattr(project, key, value)
                project.last_activity = datetime.utcnow()
                project.total_emails += 1
                project.updated_at = datetime.utcnow()
            else:
                # Create new project
                project = Project(
                    user_id=user_id,
                    **project_data,
                    total_emails=1,
                    updated_at=datetime.utcnow()
                )
                session.add(project)
            
            session.commit()
            session.refresh(project)
            return project
    
    def get_user_people(self, user_id: int, limit: int = 500) -> List[Person]:
        """Get people for a user"""
        with self.get_session() as session:
            query = session.query(Person).filter(Person.user_id == user_id)
            query = query.order_by(Person.last_interaction.desc())
            if limit:
                query = query.limit(limit)
            return query.all()
    
    def get_user_projects(self, user_id: int, status: str = None, limit: int = 200) -> List[Project]:
        """Get projects for a user"""
        with self.get_session() as session:
            query = session.query(Project).filter(Project.user_id == user_id)
            if status:
                query = query.filter(Project.status == status)
            query = query.order_by(Project.last_activity.desc())
            if limit:
                query = query.limit(limit)
            return query.all()
    
    def find_person_by_email(self, user_id: int, email: str) -> Optional[Person]:
        """Find person by email address"""
        with self.get_session() as session:
            return session.query(Person).filter(
                Person.user_id == user_id,
                Person.email_address == email
            ).first()
    
    def find_project_by_keywords(self, user_id: int, keywords: List[str]) -> Optional[Project]:
        """Find project by matching keywords against name, description, or topics - FIXED to prevent memory issues"""
        with self.get_session() as session:
            # CRITICAL FIX: Add limit to prevent loading too many projects
            projects = session.query(Project).filter(Project.user_id == user_id).limit(50).all()
            
            for project in projects:
                # Check name and description
                if any(keyword.lower() in (project.name or '').lower() for keyword in keywords):
                    return project
                if any(keyword.lower() in (project.description or '').lower() for keyword in keywords):
                    return project
                
                # Check key topics
                if project.key_topics:
                    project_topics = [topic.lower() for topic in project.key_topics]
                    if any(keyword.lower() in project_topics for keyword in keywords):
                        return project
            
            return None

    def get_user_topics(self, user_id: int, limit: int = 1000) -> List[Topic]:
        """Get all topics for a user"""
        with self.get_session() as session:
            return session.query(Topic).filter(
                Topic.user_id == user_id
            ).order_by(Topic.is_official.desc(), Topic.name.asc()).limit(limit).all()
    
    def create_or_update_topic(self, user_id: int, topic_data: Dict) -> Topic:
        """Create or update a topic record"""
        with self.get_session() as session:
            # Try to find existing topic by name
            topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.name == topic_data.get('name')
            ).first()
            
            # Handle keywords conversion to JSON string
            topic_data_copy = topic_data.copy()
            if 'keywords' in topic_data_copy and isinstance(topic_data_copy['keywords'], list):
                topic_data_copy['keywords'] = json.dumps(topic_data_copy['keywords'])
            
            if topic:
                # Update existing topic
                for key, value in topic_data_copy.items():
                    if hasattr(topic, key) and key != 'id':
                        setattr(topic, key, value)
                topic.updated_at = datetime.now()
            else:
                # Create new topic
                topic_data_copy['user_id'] = user_id
                topic_data_copy['created_at'] = datetime.now()
                topic_data_copy['updated_at'] = datetime.now()
                
                # Set default values for optional fields
                if 'slug' not in topic_data_copy:
                    topic_data_copy['slug'] = topic_data_copy['name'].lower().replace(' ', '-').replace('_', '-')
                
                if 'is_official' not in topic_data_copy:
                    topic_data_copy['is_official'] = False
                    
                if 'confidence_score' not in topic_data_copy:
                    topic_data_copy['confidence_score'] = 0.5
                    
                if 'email_count' not in topic_data_copy:
                    topic_data_copy['email_count'] = 0
                
                topic = Topic(**topic_data_copy)
                session.add(topic)
            
            session.commit()
            session.refresh(topic)
            return topic

    def update_topic(self, user_id: int, topic_id: int, topic_data: Dict) -> bool:
        """Update a specific topic by ID"""
        with self.get_session() as session:
            topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == topic_id
            ).first()
            
            if not topic:
                return False
            
            # Handle keywords conversion to JSON string
            for key, value in topic_data.items():
                if hasattr(topic, key) and value is not None:
                    if key == 'keywords' and isinstance(value, list):
                        setattr(topic, key, json.dumps(value))
                    else:
                        setattr(topic, key, value)
            
            topic.updated_at = datetime.utcnow()
            session.commit()
            return True

    def mark_topic_official(self, user_id: int, topic_id: int) -> bool:
        """Mark a topic as official"""
        with self.get_session() as session:
            topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == topic_id
            ).first()
            
            if not topic:
                return False
            
            topic.is_official = True
            topic.updated_at = datetime.utcnow()
            session.commit()
            return True

    def merge_topics(self, user_id: int, source_topic_id: int, target_topic_id: int) -> bool:
        """Merge one topic into another"""
        with self.get_session() as session:
            source_topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == source_topic_id
            ).first()
            
            target_topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == target_topic_id
            ).first()
            
            if not source_topic or not target_topic:
                return False
            
            try:
                # Update all emails that reference the source topic
                # This is a simplified version - in practice, you'd need to update
                # the topics JSON array in emails to replace source with target
                
                # For now, we'll merge the email counts and keywords
                target_topic.email_count = (target_topic.email_count or 0) + (source_topic.email_count or 0)
                
                # Merge keywords
                source_keywords = json.loads(source_topic.keywords) if source_topic.keywords else []
                target_keywords = json.loads(target_topic.keywords) if target_topic.keywords else []
                merged_keywords = list(set(source_keywords + target_keywords))
                target_topic.keywords = json.dumps(merged_keywords)
                
                # Update merge tracking
                merged_topics = json.loads(target_topic.merged_topics) if target_topic.merged_topics else []
                merged_topics.append(source_topic.name)
                target_topic.merged_topics = json.dumps(merged_topics)
                
                target_topic.updated_at = datetime.utcnow()
                
                # Delete the source topic
                session.delete(source_topic)
                session.commit()
                return True
                
            except Exception as e:
                session.rollback()
                logger.error(f"Failed to merge topics: {str(e)}")
                return False

    # ===== SMART CONTACT STRATEGY METHODS =====
    
    def create_or_update_trusted_contact(self, user_id: int, contact_data: Dict) -> TrustedContact:
        """Create or update a trusted contact record"""
        with self.get_session() as session:
            contact = session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.email_address == contact_data['email_address']
            ).first()
            
            if contact:
                # Update existing contact
                for key, value in contact_data.items():
                    if hasattr(contact, key) and value is not None:
                        setattr(contact, key, value)
                contact.updated_at = datetime.utcnow()
            else:
                # Create new trusted contact
                contact = TrustedContact(
                    user_id=user_id,
                    **contact_data,
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                session.add(contact)
            
            session.commit()
            session.refresh(contact)
            return contact
    
    def get_trusted_contacts(self, user_id: int, limit: int = 500) -> List[TrustedContact]:
        """Get trusted contacts for a user"""
        with self.get_session() as session:
            return session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id
            ).order_by(TrustedContact.engagement_score.desc()).limit(limit).all()
    
    def find_trusted_contact_by_email(self, user_id: int, email_address: str) -> Optional[TrustedContact]:
        """Find trusted contact by email address"""
        with self.get_session() as session:
            return session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.email_address == email_address
            ).first()
    
    def create_contact_context(self, user_id: int, person_id: int, context_data: Dict) -> ContactContext:
        """Create a new contact context record"""
        with self.get_session() as session:
            context = ContactContext(
                user_id=user_id,
                person_id=person_id,
                **context_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(context)
            session.commit()
            session.refresh(context)
            return context
    
    def get_contact_contexts(self, user_id: int, person_id: int = None, context_type: str = None) -> List[ContactContext]:
        """Get contact contexts for a user, optionally filtered by person or type"""
        with self.get_session() as session:
            query = session.query(ContactContext).filter(ContactContext.user_id == user_id)
            
            if person_id:
                query = query.filter(ContactContext.person_id == person_id)
            
            if context_type:
                query = query.filter(ContactContext.context_type == context_type)
            
            return query.order_by(ContactContext.created_at.desc()).all()
    
    def create_task_context(self, user_id: int, task_id: int, context_data: Dict) -> TaskContext:
        """Create a new task context record"""
        with self.get_session() as session:
            context = TaskContext(
                user_id=user_id,
                task_id=task_id,
                **context_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(context)
            session.commit()
            session.refresh(context)
            return context
    
    def get_task_contexts(self, user_id: int, task_id: int = None, context_type: str = None) -> List[TaskContext]:
        """Get task contexts for a user, optionally filtered by task or type"""
        with self.get_session() as session:
            query = session.query(TaskContext).filter(TaskContext.user_id == user_id)
            
            if task_id:
                query = query.filter(TaskContext.task_id == task_id)
            
            if context_type:
                query = query.filter(TaskContext.context_type == context_type)
            
            return query.order_by(TaskContext.created_at.desc()).all()
    
    def create_topic_knowledge(self, user_id: int, topic_id: int, knowledge_data: Dict) -> TopicKnowledgeBase:
        """Create a new topic knowledge record"""
        with self.get_session() as session:
            knowledge = TopicKnowledgeBase(
                user_id=user_id,
                topic_id=topic_id,
                **knowledge_data,
                created_at=datetime.utcnow(),
                last_updated=datetime.utcnow()
            )
            session.add(knowledge)
            session.commit()
            session.refresh(knowledge)
            return knowledge
    
    def get_topic_knowledge(self, user_id: int, topic_id: int = None, knowledge_type: str = None) -> List[TopicKnowledgeBase]:
        """Get topic knowledge for a user, optionally filtered by topic or type"""
        with self.get_session() as session:
            query = session.query(TopicKnowledgeBase).filter(TopicKnowledgeBase.user_id == user_id)
            
            if topic_id:
                query = query.filter(TopicKnowledgeBase.topic_id == topic_id)
            
            if knowledge_type:
                query = query.filter(TopicKnowledgeBase.knowledge_type == knowledge_type)
            
            return query.order_by(TopicKnowledgeBase.relevance_score.desc()).all()
    
    def update_people_engagement_data(self, user_id: int, person_id: int, engagement_data: Dict) -> bool:
        """Update people table with engagement-based data"""
        with self.get_session() as session:
            person = session.query(Person).filter(
                Person.user_id == user_id,
                Person.id == person_id
            ).first()
            
            if not person:
                return False
            
            # Add engagement fields to person if they don't exist
            if 'is_trusted_contact' in engagement_data:
                person.is_trusted_contact = engagement_data['is_trusted_contact']
            
            if 'engagement_score' in engagement_data:
                person.engagement_score = engagement_data['engagement_score']
            
            if 'bidirectional_topics' in engagement_data:
                person.bidirectional_topics = engagement_data['bidirectional_topics']
            
            session.commit()
            return True
    
    def get_engagement_analytics(self, user_id: int) -> Dict:
        """Get engagement analytics for Smart Contact Strategy reporting"""
        with self.get_session() as session:
            total_contacts = session.query(TrustedContact).filter(TrustedContact.user_id == user_id).count()
            high_engagement = session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.relationship_strength == 'high'
            ).count()
            
            recent_contacts = session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.last_sent_date >= datetime.utcnow() - timedelta(days=30)
            ).count()
            
            return {
                'total_trusted_contacts': total_contacts,
                'high_engagement_contacts': high_engagement,
                'recent_active_contacts': recent_contacts,
                'engagement_rate': (high_engagement / total_contacts * 100) if total_contacts > 0 else 0
            }

    def save_calendar_event(self, user_id: int, event_data: Dict) -> Calendar:
        """Save or update a calendar event"""
        try:
            with self.get_session() as session:
                # Try to find existing event
                existing_event = session.query(Calendar).filter_by(
                    user_id=user_id,
                    event_id=event_data.get('event_id')
                ).first()
                
                if existing_event:
                    # Update existing event
                    for key, value in event_data.items():
                        if hasattr(existing_event, key):
                            setattr(existing_event, key, value)
                    event = existing_event
                else:
                    # Create new event
                    event = Calendar(user_id=user_id, **event_data)
                    session.add(event)
                
                session.commit()
                session.refresh(event)
                return event
                
        except Exception as e:
            logger.error(f"Failed to save calendar event: {str(e)}")
            raise

    def get_user_calendar_events(self, user_id: int, start_date: datetime = None, end_date: datetime = None, limit: int = 500) -> List[Calendar]:
        """Get calendar events for a user within a date range"""
        try:
            with self.get_session() as session:
                query = session.query(Calendar).filter_by(user_id=user_id)
                
                if start_date:
                    query = query.filter(Calendar.start_time >= start_date)
                if end_date:
                    query = query.filter(Calendar.start_time <= end_date)
                
                events = query.order_by(Calendar.start_time.asc()).limit(limit).all()
                return events
                
        except Exception as e:
            logger.error(f"Failed to get user calendar events: {str(e)}")
            return []

    def get_free_time_slots(self, user_id: int, start_date: datetime, end_date: datetime) -> List[Dict]:
        """Identify free time slots between calendar events"""
        try:
            with self.get_session() as session:
                events = session.query(Calendar).filter(
                    Calendar.user_id == user_id,
                    Calendar.start_time >= start_date,
                    Calendar.start_time <= end_date,
                    Calendar.status.in_(['confirmed', 'tentative']),
                    Calendar.is_busy == True
                ).order_by(Calendar.start_time).all()
                
                free_slots = []
                current_time = start_date
                
                for event in events:
                    # If there's a gap before this event, it's free time
                    if event.start_time > current_time:
                        gap_duration = int((event.start_time - current_time).total_seconds() / 60)
                        if gap_duration >= 30:  # Minimum 30 minutes to be useful
                            free_slots.append({
                                'start_time': current_time,
                                'end_time': event.start_time,
                                'duration_minutes': gap_duration,
                                'type': 'free_time'
                            })
                    
                    # Update current time to end of this event
                    if event.end_time and event.end_time > current_time:
                        current_time = event.end_time
                
                # Check for free time after last event
                if current_time < end_date:
                    gap_duration = int((end_date - current_time).total_seconds() / 60)
                    if gap_duration >= 30:
                        free_slots.append({
                            'start_time': current_time,
                            'end_time': end_date,
                            'duration_minutes': gap_duration,
                            'type': 'free_time'
                        })
                
                return free_slots
                
        except Exception as e:
            logger.error(f"Failed to get free time slots: {str(e)}")
            return []

    def get_calendar_attendee_intelligence(self, user_id: int, event_id: str) -> Dict:
        """Get intelligence about calendar event attendees"""
        try:
            with self.get_session() as session:
                event = session.query(Calendar).filter_by(
                    user_id=user_id,
                    event_id=event_id
                ).first()
                
                if not event or not event.attendee_emails:
                    return {}
                
                # Find known attendees in People database
                known_people = []
                unknown_attendees = []
                
                for attendee_email in event.attendee_emails:
                    person = self.find_person_by_email(user_id, attendee_email)
                    if person:
                        known_people.append(person.to_dict())
                    else:
                        unknown_attendees.append(attendee_email)
                
                return {
                    'event_id': event_id,
                    'total_attendees': len(event.attendee_emails),
                    'known_attendees': known_people,
                    'unknown_attendees': unknown_attendees,
                    'known_percentage': len(known_people) / len(event.attendee_emails) * 100 if event.attendee_emails else 0
                }
                
        except Exception as e:
            logger.error(f"Failed to get calendar attendee intelligence: {str(e)}")
            return {}

    def update_calendar_ai_analysis(self, user_id: int, event_id: str, ai_data: Dict) -> bool:
        """Update calendar event with AI analysis"""
        try:
            with self.get_session() as session:
                event = session.query(Calendar).filter_by(
                    user_id=user_id,
                    event_id=event_id
                ).first()
                
                if not event:
                    return False
                
                # Update AI analysis fields
                if 'ai_summary' in ai_data:
                    event.ai_summary = ai_data['ai_summary']
                if 'ai_category' in ai_data:
                    event.ai_category = ai_data['ai_category']
                if 'importance_score' in ai_data:
                    event.importance_score = ai_data['importance_score']
                if 'business_context' in ai_data:
                    event.business_context = ai_data['business_context']
                if 'preparation_needed' in ai_data:
                    event.preparation_needed = ai_data['preparation_needed']
                if 'follow_up_required' in ai_data:
                    event.follow_up_required = ai_data['follow_up_required']
                
                event.ai_processed_at = datetime.utcnow()
                event.ai_version = ai_data.get('ai_version', 'claude-3.5-sonnet')
                
                session.commit()
                return True
                
        except Exception as e:
            logger.error(f"Failed to update calendar AI analysis: {str(e)}")
            return False

    # ===== ENHANCED ENTITY-CENTRIC INTELLIGENCE METHODS =====
    
    def get_user_topics_with_intelligence(self, user_id: int, limit: int = None) -> List[EnhancedTopic]:
        """Get topics with relationship intelligence"""
        with self.get_session() as session:
            query = session.query(EnhancedTopic).filter(EnhancedTopic.user_id == user_id)
            query = query.order_by(EnhancedTopic.strategic_importance.desc(), EnhancedTopic.total_mentions.desc())
            if limit:
                query = query.limit(limit)
            return query.all()
    
    def get_entity_relationships(self, user_id: int, entity_type: str = None) -> List[EntityRelationship]:
        """Get entity relationships for network analysis"""
        with self.get_session() as session:
            query = session.query(EntityRelationship).filter(EntityRelationship.user_id == user_id)
            if entity_type:
                query = query.filter(
                    (EntityRelationship.entity_type_a == entity_type) | 
                    (EntityRelationship.entity_type_b == entity_type)
                )
            return query.order_by(EntityRelationship.strength.desc()).all()
    
    def get_intelligence_insights(self, user_id: int, status: str = None) -> List[IntelligenceInsight]:
        """Get proactive intelligence insights"""
        with self.get_session() as session:
            query = session.query(IntelligenceInsight).filter(IntelligenceInsight.user_id == user_id)
            if status:
                query = query.filter(IntelligenceInsight.status == status)
            return query.order_by(IntelligenceInsight.priority.desc(), IntelligenceInsight.created_at.desc()).all()
    
    def create_enhanced_topic(self, user_id: int, topic_data: Dict) -> EnhancedTopic:
        """Create enhanced topic with intelligence accumulation"""
        with self.get_session() as session:
            topic = EnhancedTopic(
                user_id=user_id,
                **topic_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(topic)
            session.commit()
            session.refresh(topic)
            return topic
    
    def create_enhanced_person(self, user_id: int, person_data: Dict) -> EnhancedPerson:
        """Create enhanced person with relationship intelligence"""
        with self.get_session() as session:
            person = EnhancedPerson(
                user_id=user_id,
                **person_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(person)
            session.commit()
            session.refresh(person)
            return person
    
    def create_enhanced_task(self, user_id: int, task_data: Dict) -> EnhancedTask:
        """Create enhanced task with full context"""
        with self.get_session() as session:
            task = EnhancedTask(
                user_id=user_id,
                **task_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(task)
            session.commit()
            session.refresh(task)
            return task
    
    def create_entity_relationship(self, user_id: int, relationship_data: Dict) -> EntityRelationship:
        """Create entity relationship for network intelligence"""
        with self.get_session() as session:
            relationship = EntityRelationship(
                user_id=user_id,
                **relationship_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(relationship)
            session.commit()
            session.refresh(relationship)
            return relationship
    
    def create_intelligence_insight(self, user_id: int, insight_data: Dict) -> IntelligenceInsight:
        """Create proactive intelligence insight"""
        with self.get_session() as session:
            insight = IntelligenceInsight(
                user_id=user_id,
                **insight_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(insight)
            session.commit()
            session.refresh(insight)
            return insight
    
    def save_enhanced_email(self, user_id: int, email_data: Dict) -> EnhancedEmail:
        """Save enhanced email with intelligence focus"""
        with self.get_session() as session:
            # Check if email already exists
            existing_email = session.query(EnhancedEmail).filter(
                EnhancedEmail.user_id == user_id,
                EnhancedEmail.gmail_id == email_data.get('gmail_id')
            ).first()
            
            if existing_email:
                # Update existing email
                for key, value in email_data.items():
                    if hasattr(existing_email, key) and value is not None:
                        setattr(existing_email, key, value)
                return existing_email
            
            # Create new enhanced email
            email = EnhancedEmail(
                user_id=user_id,
                **email_data,
                created_at=datetime.utcnow()
            )
            session.add(email)
            session.commit()
            session.refresh(email)
            return email
    
    def save_calendar_event_enhanced(self, user_id: int, event_data: Dict) -> CalendarEvent:
        """Save calendar event with business intelligence"""
        with self.get_session() as session:
            # Check if event already exists
            existing_event = session.query(CalendarEvent).filter(
                CalendarEvent.user_id == user_id,
                CalendarEvent.google_event_id == event_data.get('google_event_id')
            ).first()
            
            if existing_event:
                # Update existing event
                for key, value in event_data.items():
                    if hasattr(existing_event, key) and value is not None:
                        setattr(existing_event, key, value)
                existing_event.updated_at = datetime.utcnow()
                session.commit()
                return existing_event
            
            # Create new enhanced calendar event
            event = CalendarEvent(
                user_id=user_id,
                **event_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(event)
            session.commit()
            session.refresh(event)
            return event

# Global database manager instance - Initialize lazily
_db_manager = None

def get_db_manager():
    """Get the global database manager instance (lazy initialization)"""
    global _db_manager
    if _db_manager is None:
        _db_manager = DatabaseManager()
    return _db_manager

# Export as db_manager for compatibility, but don't instantiate during import
db_manager = None  # Will be set by get_db_manager() when first called 

============================================================
FILE: chief_of_staff_ai/storage/vector_store.py
============================================================
# Manage FAISS or similar DB

============================================================
FILE: chief_of_staff_ai/processors/realtime_processing.py
============================================================
# Real-Time Processing Pipeline - Proactive Intelligence
# This transforms the system from batch processing to continuous intelligence

import asyncio
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import json
from dataclasses import dataclass, asdict
from enum import Enum
import threading
import queue
import time

from processors.enhanced_ai_pipeline import enhanced_ai_processor
from processors.unified_entity_engine import entity_engine, EntityContext
from config.settings import settings
from models.database import IntelligenceInsight, Person, Topic, Task, CalendarEvent

logger = logging.getLogger(__name__)

class EventType(Enum):
    NEW_EMAIL = "new_email"
    NEW_CALENDAR_EVENT = "new_calendar_event"
    ENTITY_UPDATE = "entity_update"
    USER_ACTION = "user_action"
    SCHEDULED_ANALYSIS = "scheduled_analysis"

@dataclass
class ProcessingEvent:
    event_type: EventType
    user_id: int
    data: Dict
    timestamp: datetime
    priority: int = 5  # 1-10, 1 = highest priority
    correlation_id: Optional[str] = None

class RealTimeProcessor:
    """
    Real-time processing engine that provides continuous intelligence.
    This is what transforms your system from reactive to proactive.
    """
    
    def __init__(self):
        self.processing_queue = queue.PriorityQueue()
        self.running = False
        self.worker_threads = []
        self.user_contexts = {}  # Cache user contexts for efficiency
        self.insight_callbacks = {}  # User-specific insight delivery callbacks
        
    def start(self, num_workers: int = 3):
        """Start the real-time processing engine"""
        self.running = True
        
        # Start worker threads
        for i in range(num_workers):
            worker = threading.Thread(target=self._process_events_worker, name=f"RTProcessor-{i}")
            worker.daemon = True
            worker.start()
            self.worker_threads.append(worker)
        
        # Start periodic analysis thread
        scheduler = threading.Thread(target=self._scheduled_analysis_worker, name="RTScheduler")
        scheduler.daemon = True
        scheduler.start()
        self.worker_threads.append(scheduler)
        
        logger.info(f"Started real-time processor with {num_workers} workers")
    
    def stop(self):
        """Stop the real-time processing engine"""
        self.running = False
        for worker in self.worker_threads:
            worker.join(timeout=5)
        logger.info("Stopped real-time processor")
    
    # =====================================================================
    # EVENT INGESTION METHODS
    # =====================================================================
    
    def process_new_email(self, email_data: Dict, user_id: int, priority: int = 5):
        """Process new email in real-time"""
        event = ProcessingEvent(
            event_type=EventType.NEW_EMAIL,
            user_id=user_id,
            data=email_data,
            timestamp=datetime.utcnow(),
            priority=priority
        )
        self._queue_event(event)
    
    def process_new_calendar_event(self, event_data: Dict, user_id: int, priority: int = 5):
        """Process new calendar event in real-time"""
        event = ProcessingEvent(
            event_type=EventType.NEW_CALENDAR_EVENT,
            user_id=user_id,
            data=event_data,
            timestamp=datetime.utcnow(),
            priority=priority
        )
        self._queue_event(event)
    
    def process_entity_update(self, entity_type: str, entity_id: int, update_data: Dict, user_id: int):
        """Process entity update and trigger related intelligence updates"""
        event = ProcessingEvent(
            event_type=EventType.ENTITY_UPDATE,
            user_id=user_id,
            data={
                'entity_type': entity_type,
                'entity_id': entity_id,
                'update_data': update_data
            },
            timestamp=datetime.utcnow(),
            priority=3  # Higher priority for entity updates
        )
        self._queue_event(event)
    
    def process_user_action(self, action_type: str, action_data: Dict, user_id: int):
        """Process user action and learn from feedback"""
        event = ProcessingEvent(
            event_type=EventType.USER_ACTION,
            user_id=user_id,
            data={
                'action_type': action_type,
                'action_data': action_data
            },
            timestamp=datetime.utcnow(),
            priority=4
        )
        self._queue_event(event)
    
    # =====================================================================
    # CORE PROCESSING WORKERS
    # =====================================================================
    
    def _process_events_worker(self):
        """Main event processing worker"""
        while self.running:
            try:
                # Get event from queue (blocks until available or timeout)
                try:
                    priority, event = self.processing_queue.get(timeout=1.0)
                except queue.Empty:
                    continue
                
                logger.debug(f"Processing {event.event_type.value} for user {event.user_id}")
                
                # Process based on event type
                if event.event_type == EventType.NEW_EMAIL:
                    self._process_new_email_event(event)
                elif event.event_type == EventType.NEW_CALENDAR_EVENT:
                    self._process_new_calendar_event(event)
                elif event.event_type == EventType.ENTITY_UPDATE:
                    self._process_entity_update_event(event)
                elif event.event_type == EventType.USER_ACTION:
                    self._process_user_action_event(event)
                elif event.event_type == EventType.SCHEDULED_ANALYSIS:
                    self._process_scheduled_analysis_event(event)
                
                # Mark task as done
                self.processing_queue.task_done()
                
            except Exception as e:
                logger.error(f"Error in event processing worker: {str(e)}")
                time.sleep(0.1)  # Brief pause on error
    
    def _scheduled_analysis_worker(self):
        """Worker for periodic intelligence analysis"""
        while self.running:
            try:
                # Run scheduled analysis every 15 minutes
                time.sleep(900)  # 15 minutes
                
                # Get active users (those with recent activity)
                active_users = self._get_active_users_for_analysis()
                
                for user_id in active_users:
                    event = ProcessingEvent(
                        event_type=EventType.SCHEDULED_ANALYSIS,
                        user_id=user_id,
                        data={'analysis_type': 'proactive_insights'},
                        timestamp=datetime.utcnow(),
                        priority=7  # Lower priority for scheduled analysis
                    )
                    self._queue_event(event)
                
            except Exception as e:
                logger.error(f"Error in scheduled analysis worker: {str(e)}")
    
    # =====================================================================
    # EVENT PROCESSING METHODS
    # =====================================================================
    
    def _process_new_email_event(self, event: ProcessingEvent):
        """Process new email with real-time intelligence generation"""
        try:
            email_data = event.data
            user_id = event.user_id
            
            # Get cached user context for efficiency
            context = self._get_cached_user_context(user_id)
            
            # Process email with enhanced AI pipeline
            result = enhanced_ai_processor.process_email_with_context(email_data, user_id, context)
            
            if result.success:
                # Update cached context with new information
                self._update_cached_context(user_id, result)
                
                # Generate immediate insights
                immediate_insights = self._generate_immediate_insights(email_data, result, user_id)
                
                # Deliver insights to user
                self._deliver_insights_to_user(user_id, immediate_insights)
                
                # Check for entity cross-references and augmentations
                self._check_cross_entity_augmentations(result, user_id)
                
                logger.info(f"Processed new email in real-time for user {user_id}: "
                           f"{result.entities_created} entities created, {len(immediate_insights)} insights")
            
        except Exception as e:
            logger.error(f"Failed to process new email event: {str(e)}")
    
    def _process_new_calendar_event(self, event: ProcessingEvent):
        """Process new calendar event with intelligence enhancement"""
        try:
            event_data = event.data
            user_id = event.user_id
            
            # Enhance calendar event with email intelligence
            result = enhanced_ai_processor.enhance_calendar_event_with_intelligence(event_data, user_id)
            
            if result.success:
                # Generate meeting preparation insights
                prep_insights = self._generate_meeting_prep_insights(event_data, result, user_id)
                
                # Deliver insights to user
                self._deliver_insights_to_user(user_id, prep_insights)
                
                # Update cached context
                self._update_cached_context(user_id, result)
                
                logger.info(f"Enhanced calendar event in real-time for user {user_id}: "
                           f"{result.entities_created['tasks']} prep tasks created")
            
        except Exception as e:
            logger.error(f"Failed to process new calendar event: {str(e)}")
    
    def _process_entity_update_event(self, event: ProcessingEvent):
        """Process entity updates and propagate intelligence"""
        try:
            entity_type = event.data['entity_type']
            entity_id = event.data['entity_id']
            update_data = event.data['update_data']
            user_id = event.user_id
            
            # Create entity context
            context = EntityContext(
                source_type='update',
                user_id=user_id,
                confidence=0.9
            )
            
            # Augment entity with new data
            entity_engine.augment_entity_from_source(entity_type, entity_id, update_data, context)
            
            # Find related entities that might need updates
            related_entities = self._find_related_entities(entity_type, entity_id, user_id)
            
            # Propagate intelligence to related entities
            for related_entity in related_entities:
                self._propagate_intelligence_update(
                    related_entity['type'], 
                    related_entity['id'], 
                    entity_type, 
                    entity_id, 
                    update_data, 
                    user_id
                )
            
            # Generate insights from entity updates
            update_insights = self._generate_entity_update_insights(entity_type, entity_id, update_data, user_id)
            self._deliver_insights_to_user(user_id, update_insights)
            
            logger.info(f"Processed entity update for {entity_type}:{entity_id}, "
                       f"propagated to {len(related_entities)} related entities")
            
        except Exception as e:
            logger.error(f"Failed to process entity update event: {str(e)}")
    
    def _process_user_action_event(self, event: ProcessingEvent):
        """Process user actions and learn from feedback"""
        try:
            action_type = event.data['action_type']
            action_data = event.data['action_data']
            user_id = event.user_id
            
            # Learning from user feedback
            if action_type == 'insight_feedback':
                self._learn_from_insight_feedback(action_data, user_id)
            elif action_type == 'task_completion':
                self._learn_from_task_completion(action_data, user_id)
            elif action_type == 'topic_management':
                self._learn_from_topic_management(action_data, user_id)
            elif action_type == 'relationship_update':
                self._learn_from_relationship_update(action_data, user_id)
            
            logger.debug(f"Processed user action: {action_type} for user {user_id}")
            
        except Exception as e:
            logger.error(f"Failed to process user action event: {str(e)}")
    
    def _process_scheduled_analysis_event(self, event: ProcessingEvent):
        """Process scheduled proactive analysis"""
        try:
            user_id = event.user_id
            analysis_type = event.data.get('analysis_type', 'proactive_insights')
            
            if analysis_type == 'proactive_insights':
                # Generate proactive insights
                insights = entity_engine.generate_proactive_insights(user_id)
                
                if insights:
                    self._deliver_insights_to_user(user_id, insights)
                    logger.info(f"Generated {len(insights)} proactive insights for user {user_id}")
            
        except Exception as e:
            logger.error(f"Failed to process scheduled analysis: {str(e)}")
    
    # =====================================================================
    # INTELLIGENCE GENERATION METHODS
    # =====================================================================
    
    def _generate_immediate_insights(self, email_data: Dict, processing_result: Any, user_id: int) -> List[IntelligenceInsight]:
        """Generate immediate insights from new email processing"""
        insights = []
        
        try:
            # Insight 1: Important person contact
            sender = email_data.get('sender', '')
            if sender and self._is_important_person(sender, user_id):
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='important_contact',
                    title=f"New email from important contact",
                    description=f"Received email from {email_data.get('sender_name', sender)}. "
                               f"Subject: {email_data.get('subject', 'No subject')}",
                    priority='high',
                    confidence=0.9,
                    related_entity_type='person',
                    status='new'
                )
                insights.append(insight)
            
            # Insight 2: Urgent task detection
            if hasattr(processing_result, 'entities_created') and processing_result.entities_created.get('tasks', 0) > 0:
                # Check if any high-priority tasks were created
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='urgent_task',
                    title=f"New tasks extracted from email",
                    description=f"Created {processing_result.entities_created['tasks']} tasks from recent email. "
                               f"Review and prioritize action items.",
                    priority='medium',
                    confidence=0.8,
                    related_entity_type='task',
                    status='new'
                )
                insights.append(insight)
            
            # Insight 3: Topic momentum detection
            if hasattr(processing_result, 'entities_created') and (processing_result.entities_created.get('topics', 0) > 0 or processing_result.entities_updated.get('topics', 0) > 0):
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='topic_momentum',
                    title=f"Business topic activity detected",
                    description=f"Recent email activity relates to your business topics. "
                               f"Consider scheduling focused time for strategic planning.",
                    priority='medium',
                    confidence=0.7,
                    related_entity_type='topic',
                    status='new'
                )
                insights.append(insight)
            
        except Exception as e:
            logger.error(f"Failed to generate immediate insights: {str(e)}")
        
        return insights
    
    def _generate_meeting_prep_insights(self, event_data: Dict, processing_result: Any, user_id: int) -> List[IntelligenceInsight]:
        """Generate meeting preparation insights"""
        insights = []
        
        try:
            meeting_title = event_data.get('title', 'Unknown Meeting')
            meeting_time = event_data.get('start_time')
            
            # Calculate time until meeting
            if meeting_time:
                if isinstance(meeting_time, str):
                    from dateutil import parser
                    meeting_time = parser.parse(meeting_time)
                
                time_until = meeting_time - datetime.utcnow()
                
                if time_until.total_seconds() > 0 and time_until.days <= 2:  # Within 48 hours
                    # High-priority preparation insight
                    insight = IntelligenceInsight(
                        user_id=user_id,
                        insight_type='meeting_prep',
                        title=f"Prepare for '{meeting_title}'",
                        description=f"Meeting in {time_until.days} days, {time_until.seconds // 3600} hours. "
                                   f"AI has generated preparation tasks based on attendee intelligence.",
                        priority='high' if time_until.days == 0 else 'medium',
                        confidence=0.9,
                        related_entity_type='event',
                        status='new',
                        expires_at=meeting_time
                    )
                    insights.append(insight)
            
            # Insight about preparation tasks created
            if hasattr(processing_result, 'entities_created') and processing_result.entities_created.get('tasks', 0) > 0:
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='prep_tasks_generated',
                    title=f"Meeting preparation tasks created",
                    description=f"Generated {processing_result.entities_created['tasks']} preparation tasks "
                               f"for '{meeting_title}' based on your email history with attendees.",
                    priority='medium',
                    confidence=0.8,
                    related_entity_type='task',
                    status='new'
                )
                insights.append(insight)
            
        except Exception as e:
            logger.error(f"Failed to generate meeting prep insights: {str(e)}")
        
        return insights
    
    def _generate_entity_update_insights(self, entity_type: str, entity_id: int, update_data: Dict, user_id: int) -> List[IntelligenceInsight]:
        """Generate insights from entity updates"""
        insights = []
        
        try:
            if entity_type == 'topic' and update_data.get('mentions', 0) > 0:
                # Topic becoming hot
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='topic_momentum',
                    title=f"Topic gaining momentum",
                    description=f"Business topic receiving increased attention. "
                               f"Consider preparing materials or scheduling focused discussion.",
                    priority='medium',
                    confidence=0.7,
                    related_entity_type='topic',
                    related_entity_id=entity_id,
                    status='new'
                )
                insights.append(insight)
            
            elif entity_type == 'person' and update_data.get('interaction'):
                # Relationship activity
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='relationship_activity',
                    title=f"Recent contact activity",
                    description=f"Ongoing communication with important contact. "
                               f"Relationship engagement is active.",
                    priority='low',
                    confidence=0.6,
                    related_entity_type='person',
                    related_entity_id=entity_id,
                    status='new'
                )
                insights.append(insight)
            
        except Exception as e:
            logger.error(f"Failed to generate entity update insights: {str(e)}")
        
        return insights
    
    # =====================================================================
    # CONTEXT MANAGEMENT AND CACHING
    # =====================================================================
    
    def _get_cached_user_context(self, user_id: int) -> Dict:
        """Get cached user context for efficient processing"""
        if user_id not in self.user_contexts:
            # Load context from enhanced AI processor
            context = enhanced_ai_processor._gather_user_context(user_id)
            self.user_contexts[user_id] = {
                'context': context,
                'last_updated': datetime.utcnow(),
                'version': 1
            }
        else:
            # Check if context needs refresh (every 30 minutes)
            cached = self.user_contexts[user_id]
            if datetime.utcnow() - cached['last_updated'] > timedelta(minutes=30):
                context = enhanced_ai_processor._gather_user_context(user_id)
                cached['context'] = context
                cached['last_updated'] = datetime.utcnow()
                cached['version'] += 1
        
        return self.user_contexts[user_id]['context']
    
    def _update_cached_context(self, user_id: int, processing_result: Any):
        """Update cached context with new processing results"""
        if user_id not in self.user_contexts:
            return
        
        cached = self.user_contexts[user_id]
        
        # Update context with new entities
        if hasattr(processing_result, 'entities_created'):
            # This would update the cached context with newly created entities
            # Implementation would depend on the specific structure
            cached['last_updated'] = datetime.utcnow()
            cached['version'] += 1
    
    def _find_related_entities(self, entity_type: str, entity_id: int, user_id: int) -> List[Dict]:
        """Find entities related to the updated entity"""
        related_entities = []
        
        try:
            from models.database import get_db_manager
            from models.database import EntityRelationship
            
            with get_db_manager().get_session() as session:
                # Find direct relationships
                relationships = session.query(EntityRelationship).filter(
                    EntityRelationship.user_id == user_id,
                    ((EntityRelationship.entity_type_a == entity_type) & (EntityRelationship.entity_id_a == entity_id)) |
                    ((EntityRelationship.entity_type_b == entity_type) & (EntityRelationship.entity_id_b == entity_id))
                ).all()
                
                for rel in relationships:
                    if rel.entity_type_a == entity_type and rel.entity_id_a == entity_id:
                        related_entities.append({
                            'type': rel.entity_type_b,
                            'id': rel.entity_id_b,
                            'relationship': rel.relationship_type
                        })
                    else:
                        related_entities.append({
                            'type': rel.entity_type_a,
                            'id': rel.entity_id_a,
                            'relationship': rel.relationship_type
                        })
            
        except Exception as e:
            logger.error(f"Failed to find related entities: {str(e)}")
        
        return related_entities
    
    def _propagate_intelligence_update(self, target_entity_type: str, target_entity_id: int, 
                                     source_entity_type: str, source_entity_id: int, 
                                     update_data: Dict, user_id: int):
        """Propagate intelligence updates to related entities"""
        try:
            # Create propagation context
            context = EntityContext(
                source_type='propagation',
                user_id=user_id,
                confidence=0.7,
                processing_metadata={
                    'source_entity': f"{source_entity_type}:{source_entity_id}",
                    'propagation_data': update_data
                }
            )
            
            # Determine what intelligence to propagate based on entity types
            propagation_data = {}
            
            if source_entity_type == 'topic' and target_entity_type == 'person':
                # Topic update affecting person
                propagation_data = {
                    'topic_activity': True,
                    'related_topic_update': update_data
                }
            elif source_entity_type == 'person' and target_entity_type == 'topic':
                # Person update affecting topic
                propagation_data = {
                    'person_interaction': True,
                    'related_person_update': update_data
                }
            
            if propagation_data:
                entity_engine.augment_entity_from_source(
                    target_entity_type, target_entity_id, propagation_data, context
                )
            
        except Exception as e:
            logger.error(f"Failed to propagate intelligence update: {str(e)}")
    
    def _check_cross_entity_augmentations(self, processing_result: Any, user_id: int):
        """Check for opportunities to augment existing entities with new information"""
        try:
            # This would analyze the processing result and find opportunities to
            # augment existing entities with new information from the processing
            pass
            
        except Exception as e:
            logger.error(f"Failed to check cross-entity augmentations: {str(e)}")
    
    # =====================================================================
    # USER FEEDBACK AND LEARNING
    # =====================================================================
    
    def _learn_from_insight_feedback(self, feedback_data: Dict, user_id: int):
        """Learn from user feedback on insights"""
        try:
            insight_id = feedback_data.get('insight_id')
            feedback_type = feedback_data.get('feedback')  # helpful, not_helpful, etc.
            
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                insight = session.query(IntelligenceInsight).filter(
                    IntelligenceInsight.id == insight_id,
                    IntelligenceInsight.user_id == user_id
                ).first()
                
                if insight:
                    insight.user_feedback = feedback_type
                    insight.updated_at = datetime.utcnow()
                    session.commit()
                    
                    # Adjust future insight generation based on feedback
                    self._adjust_insight_generation(insight.insight_type, feedback_type, user_id)
            
        except Exception as e:
            logger.error(f"Failed to learn from insight feedback: {str(e)}")
    
    def _learn_from_task_completion(self, completion_data: Dict, user_id: int):
        """Learn from task completion patterns"""
        try:
            task_id = completion_data.get('task_id')
            completion_time = completion_data.get('completion_time')
            
            # This would analyze task completion patterns to improve future task extraction
            # For example: tasks that take longer than estimated, tasks that are never completed, etc.
            
        except Exception as e:
            logger.error(f"Failed to learn from task completion: {str(e)}")
    
    def _learn_from_topic_management(self, topic_data: Dict, user_id: int):
        """Learn from user topic management actions"""
        try:
            action = topic_data.get('action')  # create, merge, delete, etc.
            
            # This would learn user preferences for topic organization
            # and improve future topic extraction and categorization
            
        except Exception as e:
            logger.error(f"Failed to learn from topic management: {str(e)}")
    
    def _learn_from_relationship_update(self, relationship_data: Dict, user_id: int):
        """Learn from relationship updates"""
        try:
            # Learn how users categorize and prioritize relationships
            # to improve future relationship intelligence
            pass
            
        except Exception as e:
            logger.error(f"Failed to learn from relationship update: {str(e)}")
    
    def _adjust_insight_generation(self, insight_type: str, feedback: str, user_id: int):
        """Adjust future insight generation based on user feedback"""
        # This would implement adaptive insight generation
        # For example: if user consistently marks "relationship_alert" as not helpful,
        # reduce frequency or adjust criteria for that insight type
        pass
    
    # =====================================================================
    # UTILITY METHODS
    # =====================================================================
    
    def _queue_event(self, event: ProcessingEvent):
        """Queue event for processing"""
        # Priority queue uses tuple (priority, item)
        self.processing_queue.put((event.priority, event))
    
    def _get_active_users_for_analysis(self) -> List[int]:
        """Get users with recent activity for scheduled analysis"""
        try:
            from models.database import get_db_manager
            
            # Users with activity in last 24 hours
            cutoff = datetime.utcnow() - timedelta(hours=24)
            
            with get_db_manager().get_session() as session:
                # This would query for users with recent email processing or other activity
                # For now, return empty list - would be implemented with proper user activity tracking
                return []
            
        except Exception as e:
            logger.error(f"Failed to get active users: {str(e)}")
            return []
    
    def _is_important_person(self, email: str, user_id: int) -> bool:
        """Check if person is marked as important"""
        try:
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                person = session.query(Person).filter(
                    Person.user_id == user_id,
                    Person.email_address == email.lower(),
                    Person.importance_level > 0.7
                ).first()
                
                return person is not None
                
        except Exception as e:
            logger.error(f"Failed to check person importance: {str(e)}")
            return False
    
    def _deliver_insights_to_user(self, user_id: int, insights: List[IntelligenceInsight]):
        """Deliver insights to user through registered callbacks"""
        if not insights:
            return
        
        try:
            # Store insights in database
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                for insight in insights:
                    session.add(insight)
                session.commit()
            
            # Deliver through callbacks (WebSocket, push notifications, etc.)
            if user_id in self.insight_callbacks:
                callback = self.insight_callbacks[user_id]
                callback(insights)
            
            logger.info(f"Delivered {len(insights)} insights to user {user_id}")
            
        except Exception as e:
            logger.error(f"Failed to deliver insights to user: {str(e)}")
    
    def register_insight_callback(self, user_id: int, callback):
        """Register callback for delivering insights to specific user"""
        self.insight_callbacks[user_id] = callback
    
    def unregister_insight_callback(self, user_id: int):
        """Unregister insight callback for user"""
        if user_id in self.insight_callbacks:
            del self.insight_callbacks[user_id]

# Global instance
realtime_processor = RealTimeProcessor() 

============================================================
FILE: chief_of_staff_ai/processors/enhanced_ai_pipeline.py
============================================================
"""
Enhanced AI Processing Pipeline - Context-Aware Intelligence
This replaces the scattered AI processing with unified, context-aware analysis
"""

import json
import logging
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timezone, timedelta
import anthropic
from dataclasses import dataclass
import hashlib

from config.settings import settings
from processors.unified_entity_engine import entity_engine, EntityContext
from models.database import Email, Topic, Person, Task, Project, EntityRelationship, IntelligenceInsight

logger = logging.getLogger(__name__)

@dataclass
class ProcessingResult:
    """Result container for AI processing"""
    success: bool
    entities_created: Dict[str, int]  # Type -> count
    entities_updated: Dict[str, int]
    insights_generated: List[str]
    processing_time: float
    error: Optional[str] = None

class EnhancedAIProcessor:
    """
    Context-aware AI processing that builds on existing knowledge.
    This is the brain that turns raw data into intelligent insights.
    """
    
    def __init__(self):
        from config.settings import settings
        
        self.client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        self.model = settings.CLAUDE_MODEL  # Now uses Claude 4 Opus from settings
        self.entity_engine = entity_engine
        
    # =====================================================================
    # UNIFIED EMAIL PROCESSING - SINGLE PASS WITH CONTEXT
    # =====================================================================
    
    def process_email_with_context(self, email_data: Dict, user_id: int, existing_context: Dict = None) -> ProcessingResult:
        """
        Process email with full context awareness in a single AI call.
        This replaces multiple separate prompts with one intelligent analysis.
        """
        start_time = datetime.utcnow()
        result = ProcessingResult(
            success=False,
            entities_created={'people': 0, 'topics': 0, 'tasks': 0, 'projects': 0},
            entities_updated={'people': 0, 'topics': 0, 'tasks': 0, 'projects': 0},
            insights_generated=[],
            processing_time=0.0
        )
        
        try:
            # Step 1: Gather existing context for this user
            context = self._gather_user_context(user_id, existing_context)
            
            # Step 2: Prepare comprehensive prompt with existing knowledge
            analysis_prompt = self._prepare_unified_email_prompt(email_data, context)
            
            # Step 3: Single AI analysis call
            claude_response = self._call_claude_unified_analysis(analysis_prompt)
            
            if not claude_response:
                result.error = "Failed to get AI analysis"
                return result
            
            # Step 4: Parse comprehensive response
            analysis = self._parse_unified_analysis(claude_response)
            
            # Step 5: Create/update entities with context
            processing_context = EntityContext(
                source_type='email',
                source_id=email_data.get('id'),
                user_id=user_id,
                confidence=analysis.get('overall_confidence', 0.8)
            )
            
            # Process people (including signature analysis)
            people_result = self._process_people_from_analysis(analysis.get('people', []), processing_context)
            result.entities_created['people'] = people_result['created']
            result.entities_updated['people'] = people_result['updated']
            
            # Process topics (check existing first)
            topics_result = self._process_topics_from_analysis(analysis.get('topics', []), processing_context)
            result.entities_created['topics'] = topics_result['created']
            result.entities_updated['topics'] = topics_result['updated']
            
            # Process tasks (with full context story)
            tasks_result = self._process_tasks_from_analysis(analysis.get('tasks', []), processing_context)
            result.entities_created['tasks'] = tasks_result['created']
            
            # Process projects (check for augmentation)
            projects_result = self._process_projects_from_analysis(analysis.get('projects', []), processing_context)
            result.entities_created['projects'] = projects_result['created']
            result.entities_updated['projects'] = projects_result['updated']
            
            # Create entity relationships
            self._create_entity_relationships(analysis, processing_context)
            
            # Store email intelligence
            self._store_email_intelligence(email_data, analysis, user_id)
            
            # Generate insights
            result.insights_generated = analysis.get('strategic_insights', [])
            
            result.success = True
            result.processing_time = (datetime.utcnow() - start_time).total_seconds()
            
            logger.info(f"Successfully processed email with context in {result.processing_time:.2f}s")
            
        except Exception as e:
            logger.error(f"Failed to process email with context: {str(e)}")
            result.error = str(e)
            result.processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        return result
    
    # =====================================================================
    # CALENDAR EVENT ENHANCEMENT WITH EMAIL INTELLIGENCE
    # =====================================================================
    
    def enhance_calendar_event_with_intelligence(self, event_data: Dict, user_id: int) -> ProcessingResult:
        """
        Enhance calendar events with email intelligence and create prep tasks.
        This addresses connecting email insights to calendar events.
        """
        start_time = datetime.utcnow()
        result = ProcessingResult(
            success=False,
            entities_created={'tasks': 0, 'people': 0},
            entities_updated={'events': 0, 'people': 0},
            insights_generated=[],
            processing_time=0.0
        )
        
        try:
            # Step 1: Analyze attendees and find existing relationships
            attendee_intelligence = self._analyze_event_attendees(event_data, user_id)
            
            # Step 2: Find related email intelligence for these people
            email_context = self._find_related_email_intelligence(attendee_intelligence, user_id)
            
            # Step 3: Generate enhanced meeting context
            enhancement_prompt = self._prepare_meeting_enhancement_prompt(event_data, attendee_intelligence, email_context)
            
            # Step 4: AI analysis for meeting preparation
            claude_response = self._call_claude_meeting_enhancement(enhancement_prompt)
            
            if claude_response:
                enhancement = self._parse_meeting_enhancement(claude_response)
                
                processing_context = EntityContext(
                    source_type='calendar',
                    source_id=event_data.get('id'),
                    user_id=user_id,
                    confidence=0.8
                )
                
                # Create preparation tasks
                if enhancement.get('prep_tasks'):
                    for task_data in enhancement['prep_tasks']:
                        task = self.entity_engine.create_task_with_full_context(
                            description=task_data['description'],
                            assignee_email=None,  # User's own prep tasks
                            topic_names=task_data.get('topics', []),
                            context=processing_context,
                            priority=task_data.get('priority', 'medium')
                        )
                        if task:
                            result.entities_created['tasks'] += 1
                
                # Update event with business context
                self._update_event_intelligence(event_data, enhancement, user_id)
                result.entities_updated['events'] = 1
                
                result.insights_generated = enhancement.get('insights', [])
                result.success = True
            
            result.processing_time = (datetime.utcnow() - start_time).total_seconds()
            
        except Exception as e:
            logger.error(f"Failed to enhance calendar event: {str(e)}")
            result.error = str(e)
            result.processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        return result
    
    # =====================================================================
    # CONTEXT GATHERING AND PROMPT PREPARATION
    # =====================================================================
    
    def _gather_user_context(self, user_id: int, existing_context: Dict = None) -> Dict:
        """Gather comprehensive user context for AI processing"""
        try:
            from models.database import get_db_manager
            
            context = {
                'existing_people': [],
                'existing_topics': [],
                'active_projects': [],
                'recent_insights': [],
                'communication_patterns': {}
            }
            
            if existing_context:
                context.update(existing_context)
                return context
            
            with get_db_manager().get_session() as session:
                # Get recent people (last 30 days)
                recent_people = session.query(Person).filter(
                    Person.user_id == user_id,
                    Person.last_interaction > datetime.utcnow() - timedelta(days=30)
                ).limit(20).all()
                
                context['existing_people'] = [
                    {
                        'name': p.name,
                        'email': p.email_address,
                        'company': p.company,
                        'relationship': p.relationship_type,
                        'importance': p.importance_level
                    }
                    for p in recent_people
                ]
                
                # Get active topics
                active_topics = session.query(Topic).filter(
                    Topic.user_id == user_id,
                    Topic.total_mentions > 1
                ).order_by(Topic.last_mentioned.desc()).limit(15).all()
                
                context['existing_topics'] = [
                    {
                        'name': t.name,
                        'description': t.description,
                        'keywords': t.keywords,
                        'mentions': t.total_mentions,
                        'is_official': t.is_official
                    }
                    for t in active_topics
                ]
                
                # Get active projects
                active_projects = session.query(Project).filter(
                    Project.user_id == user_id,
                    Project.status == 'active'
                ).limit(10).all()
                
                context['active_projects'] = [
                    {
                        'name': p.name,
                        'description': p.description,
                        'status': p.status,
                        'priority': p.priority
                    }
                    for p in active_projects
                ]
            
            return context
            
        except Exception as e:
            logger.error(f"Failed to gather user context: {str(e)}")
            return {}
    
    def _prepare_unified_email_prompt(self, email_data: Dict, context: Dict) -> str:
        """Prepare comprehensive email analysis prompt with existing context"""
        
        # Format existing context for Claude
        context_summary = self._format_context_for_claude(context)
        
        prompt = f"""You are an AI Chief of Staff analyzing business email communication with access to the user's existing business intelligence context.

EXISTING BUSINESS CONTEXT:
{context_summary}

EMAIL TO ANALYZE:
From: {email_data.get('sender_name', '')} <{email_data.get('sender', '')}>
Subject: {email_data.get('subject', '')}
Date: {email_data.get('email_date', '')}

Content:
{email_data.get('body_text', email_data.get('body_clean', ''))}

ANALYSIS INSTRUCTIONS:
Provide comprehensive analysis in the following JSON format. Use the existing context to:
1. Match people to existing contacts (avoid duplicates)
2. Connect topics to existing business themes
3. Identify project connections and updates
4. Generate contextual tasks with business rationale
5. Extract strategic insights based on patterns

{{
    "overall_confidence": 0.0-1.0,
    "business_summary": "Concise business-focused summary for display",
    "category": "meeting|project|decision|information|relationship",
    "sentiment": "positive|neutral|negative|urgent",
    "strategic_importance": 0.0-1.0,
    
    "people": [
        {{
            "email": "required",
            "name": "extracted or inferred name",
            "is_existing": true/false,
            "existing_person_match": "name if matched to existing context",
            "role_in_email": "sender|recipient|mentioned",
            "professional_context": "title, company, relationship insights",
            "signature_data": "extracted title, company, phone, etc if available",
            "importance_level": 0.0-1.0
        }}
    ],
    
    "topics": [
        {{
            "name": "topic name",
            "is_existing": true/false,
            "existing_topic_match": "name if matched to existing",
            "description": "what this topic covers",
            "keywords": ["keyword1", "keyword2"],
            "strategic_importance": 0.0-1.0,
            "new_information": "what's new about this topic from this email"
        }}
    ],
    
    "tasks": [
        {{
            "description": "clear actionable task",
            "assignee_email": "who should do this or null for user",
            "context_rationale": "WHY this task exists - business context",
            "related_topics": ["topic names"],
            "related_people": ["email addresses"],
            "priority": "high|medium|low",
            "due_date_hint": "extracted date or timing hint",
            "confidence": 0.0-1.0
        }}
    ],
    
    "projects": [
        {{
            "name": "project name",
            "is_existing": true/false,
            "existing_project_match": "name if matched",
            "description": "project description",
            "new_information": "what's new about this project",
            "stakeholders": ["email addresses of involved people"],
            "status_update": "current status or progress",
            "priority": "high|medium|low"
        }}
    ],
    
    "strategic_insights": [
        "Key business insights that connect to existing context or reveal new patterns"
    ],
    
    "entity_relationships": [
        {{
            "entity_a": {{"type": "person|topic|project", "identifier": "email or name"}},
            "entity_b": {{"type": "person|topic|project", "identifier": "email or name"}},
            "relationship_type": "collaborates_on|discusses|leads|reports_to",
            "strength": 0.0-1.0
        }}
    ]
}}

Focus on business intelligence that builds on existing context rather than isolated data extraction."""
        
        return prompt
    
    def _format_context_for_claude(self, context: Dict) -> str:
        """Format user context in a readable way for Claude"""
        sections = []
        
        if context.get('existing_people'):
            people_summary = []
            for person in context['existing_people'][:10]:  # Limit for token efficiency
                people_summary.append(f"- {person['name']} ({person['email']}) - {person.get('company', 'Unknown')} - {person.get('relationship', 'contact')}")
            sections.append(f"EXISTING PEOPLE:\n" + "\n".join(people_summary))
        
        if context.get('existing_topics'):
            topics_summary = []
            for topic in context['existing_topics'][:10]:
                status = "OFFICIAL" if topic.get('is_official') else f"{topic.get('mentions', 0)} mentions"
                topics_summary.append(f"- {topic['name']} ({status}) - {topic.get('description', 'No description')}")
            sections.append(f"EXISTING TOPICS:\n" + "\n".join(topics_summary))
        
        if context.get('active_projects'):
            projects_summary = []
            for project in context['active_projects'][:5]:
                projects_summary.append(f"- {project['name']} ({project['status']}) - {project.get('description', '')}")
            sections.append(f"ACTIVE PROJECTS:\n" + "\n".join(projects_summary))
        
        return "\n\n".join(sections) if sections else "No existing context available."
    
    # =====================================================================
    # AI RESPONSE PROCESSING
    # =====================================================================
    
    def _call_claude_unified_analysis(self, prompt: str) -> Optional[str]:
        """Call Claude for unified email analysis"""
        try:
            message = self.client.messages.create(
                model=self.model,
                max_tokens=4000,  # Increased for comprehensive analysis
                temperature=0.1,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return message.content[0].text.strip()
            
        except Exception as e:
            logger.error(f"Failed to call Claude for unified analysis: {str(e)}")
            return None
    
    def _parse_unified_analysis(self, response: str) -> Dict:
        """Parse Claude's comprehensive analysis response"""
        try:
            # Find JSON in response
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                logger.warning("No JSON found in Claude response")
                return {}
            
            json_text = response[json_start:json_end]
            analysis = json.loads(json_text)
            
            logger.info(f"Parsed unified analysis with {len(analysis.get('people', []))} people, "
                       f"{len(analysis.get('topics', []))} topics, {len(analysis.get('tasks', []))} tasks")
            
            return analysis
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Claude analysis JSON: {str(e)}")
            return {}
        except Exception as e:
            logger.error(f"Failed to parse Claude analysis: {str(e)}")
            return {}
    
    def _process_people_from_analysis(self, people_data: List[Dict], context: EntityContext) -> Dict:
        """Process people from unified analysis"""
        result = {'created': 0, 'updated': 0}
        
        for person_data in people_data:
            email = person_data.get('email')
            name = person_data.get('name')
            
            if not email:
                continue
            
            # Add signature data to processing metadata
            if person_data.get('signature_data'):
                context.processing_metadata = {'signature': person_data['signature_data']}
            
            person = self.entity_engine.create_or_update_person(email, name, context)
            
            if person:
                if person_data.get('is_existing'):
                    result['updated'] += 1
                else:
                    result['created'] += 1
        
        return result
    
    def _process_topics_from_analysis(self, topics_data: List[Dict], context: EntityContext) -> Dict:
        """Process topics from unified analysis with existing topic checking"""
        result = {'created': 0, 'updated': 0}
        
        for topic_data in topics_data:
            topic_name = topic_data.get('name')
            description = topic_data.get('description')
            keywords = topic_data.get('keywords', [])
            
            if not topic_name:
                continue
            
            topic = self.entity_engine.create_or_update_topic(
                topic_name=topic_name,
                description=description,
                keywords=keywords,
                context=context
            )
            
            if topic:
                if topic_data.get('is_existing'):
                    result['updated'] += 1
                else:
                    result['created'] += 1
        
        return result
    
    def _process_tasks_from_analysis(self, tasks_data: List[Dict], context: EntityContext) -> Dict:
        """Process tasks from unified analysis with full context stories"""
        result = {'created': 0}
        
        for task_data in tasks_data:
            description = task_data.get('description')
            assignee_email = task_data.get('assignee_email')
            related_topics = task_data.get('related_topics', [])
            priority = task_data.get('priority', 'medium')
            
            if not description:
                continue
            
            # Parse due date hint
            due_date = None
            due_date_hint = task_data.get('due_date_hint')
            if due_date_hint:
                due_date = self._parse_due_date_hint(due_date_hint)
            
            # Set confidence from analysis
            context.confidence = task_data.get('confidence', 0.8)
            
            # Add context rationale to processing metadata
            if task_data.get('context_rationale'):
                context.processing_metadata = {
                    'context_rationale': task_data['context_rationale'],
                    'related_people': task_data.get('related_people', [])
                }
            
            task = self.entity_engine.create_task_with_full_context(
                description=description,
                assignee_email=assignee_email,
                topic_names=related_topics,
                context=context,
                due_date=due_date,
                priority=priority
            )
            
            if task:
                result['created'] += 1
        
        return result
    
    def _process_projects_from_analysis(self, projects_data: List[Dict], context: EntityContext) -> Dict:
        """Process projects from unified analysis with augmentation logic"""
        result = {'created': 0, 'updated': 0}
        
        try:
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                for project_data in projects_data:
                    project_name = project_data.get('name')
                    
                    if not project_name:
                        continue
                    
                    # Check for existing project
                    existing_project = session.query(Project).filter(
                        Project.user_id == context.user_id,
                        Project.name.ilike(f"%{project_name}%")
                    ).first()
                    
                    if existing_project and project_data.get('is_existing'):
                        # Augment existing project
                        updated = self._augment_existing_project(existing_project, project_data, session)
                        if updated:
                            result['updated'] += 1
                    
                    elif not existing_project:
                        # Create new project
                        new_project = self._create_new_project(project_data, context, session)
                        if new_project:
                            result['created'] += 1
                
                session.commit()
        
        except Exception as e:
            logger.error(f"Failed to process projects: {str(e)}")
        
        return result
    
    def _create_entity_relationships(self, analysis: Dict, context: EntityContext):
        """Create relationships between entities based on analysis"""
        relationships = analysis.get('entity_relationships', [])
        
        for rel_data in relationships:
            entity_a = rel_data.get('entity_a', {})
            entity_b = rel_data.get('entity_b', {})
            relationship_type = rel_data.get('relationship_type', 'related')
            
            # Find actual entity IDs
            entity_a_id = self._find_entity_id(entity_a, context.user_id)
            entity_b_id = self._find_entity_id(entity_b, context.user_id)
            
            if entity_a_id and entity_b_id:
                self.entity_engine.create_entity_relationship(
                    entity_a['type'], entity_a_id,
                    entity_b['type'], entity_b_id,
                    relationship_type,
                    context
                )
    
    def _store_email_intelligence(self, email_data: Dict, analysis: Dict, user_id: int):
        """Store processed email intelligence in optimized format"""
        try:
            from models.database import get_db_manager, Email
            import hashlib
            
            # Create content hash for deduplication
            content = email_data.get('body_clean', '')
            content_hash = hashlib.sha256(content.encode()).hexdigest()
            
            # Store in blob storage (simplified for now - would use S3/GCS in production)
            blob_key = f"emails/{user_id}/{content_hash}.txt"
            
            # Convert sentiment string to float for database storage
            sentiment_value = self._convert_sentiment_to_float(analysis.get('sentiment'))
            
            with get_db_manager().get_session() as session:
                # Check if email already exists
                existing_email = session.query(Email).filter(
                    Email.gmail_id == email_data.get('gmail_id')
                ).first()
                
                if existing_email:
                    # Update existing email with new intelligence
                    existing_email.ai_summary = analysis.get('business_summary')
                    existing_email.business_category = analysis.get('category')
                    existing_email.sentiment = sentiment_value
                    existing_email.strategic_importance = analysis.get('strategic_importance', 0.5)
                    existing_email.processed_at = datetime.utcnow()
                else:
                    # Create new email record
                    email_record = Email(
                        user_id=user_id,
                        gmail_id=email_data.get('gmail_id') or email_data.get('id'),  # Use gmail_id or id
                        subject=email_data.get('subject'),
                        sender=email_data.get('sender'),
                        sender_name=email_data.get('sender_name'),
                        email_date=email_data.get('email_date'),
                        ai_summary=analysis.get('business_summary'),
                        business_category=analysis.get('category'),
                        sentiment=sentiment_value,
                        strategic_importance=analysis.get('strategic_importance', 0.5),
                        content_hash=content_hash,
                        blob_storage_key=blob_key,
                        processed_at=datetime.utcnow(),
                        processing_version="unified_v2.0"
                    )
                    session.add(email_record)
                
                session.commit()
                
        except Exception as e:
            logger.error(f"Failed to store email intelligence: {str(e)}")
    
    def _convert_sentiment_to_float(self, sentiment):
        """Convert sentiment string to float value for database storage"""
        if isinstance(sentiment, (int, float)):
            return float(sentiment)
        
        if isinstance(sentiment, str):
            sentiment_lower = sentiment.lower()
            if sentiment_lower in ['positive', 'good', 'happy']:
                return 0.7
            elif sentiment_lower in ['negative', 'bad', 'sad', 'angry']:
                return -0.7
            elif sentiment_lower in ['neutral', 'mixed', 'balanced']:
                return 0.0
            else:
                # Try to parse as float
                try:
                    return float(sentiment)
                except ValueError:
                    return 0.0
        
        return 0.0  # Default neutral
    
    # =====================================================================
    # MEETING ENHANCEMENT METHODS (simplified for space)
    # =====================================================================
    
    def _analyze_event_attendees(self, event_data: Dict, user_id: int) -> Dict:
        """Analyze meeting attendees and find existing relationships"""
        # Implementation for attendee analysis
        return {'known_attendees': [], 'unknown_attendees': [], 'relationship_strength': 0.0}
    
    def _find_related_email_intelligence(self, attendee_intelligence: Dict, user_id: int) -> Dict:
        """Find email intelligence related to meeting attendees"""
        # Implementation for finding related email context
        return {'recent_communications': [], 'shared_topics': []}
    
    def _prepare_meeting_enhancement_prompt(self, event_data: Dict, attendee_intelligence: Dict, email_context: Dict) -> str:
        """Prepare prompt for meeting enhancement with intelligence"""
        return f"Analyze meeting: {event_data.get('title', '')} with context"
    
    def _call_claude_meeting_enhancement(self, prompt: str) -> Optional[str]:
        """Call Claude for meeting enhancement analysis"""
        try:
            message = self.client.messages.create(
                model=self.model,
                max_tokens=3000,
                temperature=0.1,
                messages=[{"role": "user", "content": prompt}]
            )
            return message.content[0].text.strip()
        except Exception as e:
            logger.error(f"Failed to call Claude for meeting enhancement: {str(e)}")
            return None
    
    def _parse_meeting_enhancement(self, response: str) -> Dict:
        """Parse Claude's meeting enhancement response"""
        try:
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end > 0:
                return json.loads(response[json_start:json_end])
        except:
            pass
        return {}
    
    def _update_event_intelligence(self, event_data: Dict, enhancement: Dict, user_id: int):
        """Update calendar event with intelligence"""
        # Implementation for updating event intelligence
        pass
    
    # =====================================================================
    # UTILITY METHODS
    # =====================================================================
    
    def _parse_due_date_hint(self, hint: str) -> Optional[datetime]:
        """Parse due date hint into actual datetime"""
        from dateutil import parser as date_parser
        try:
            return date_parser.parse(hint, fuzzy=True)
        except:
            return None
    
    def _find_entity_id(self, entity_info: Dict, user_id: int) -> Optional[int]:
        """Find actual entity ID from analysis data"""
        try:
            from models.database import get_db_manager
            
            entity_type = entity_info.get('type')
            identifier = entity_info.get('identifier')
            
            if not entity_type or not identifier:
                return None
            
            with get_db_manager().get_session() as session:
                if entity_type == 'person':
                    entity = session.query(Person).filter(
                        Person.user_id == user_id,
                        Person.email_address == identifier
                    ).first()
                elif entity_type == 'topic':
                    entity = session.query(Topic).filter(
                        Topic.user_id == user_id,
                        Topic.name == identifier
                    ).first()
                elif entity_type == 'project':
                    entity = session.query(Project).filter(
                        Project.user_id == user_id,
                        Project.name == identifier
                    ).first()
                else:
                    return None
                
                return entity.id if entity else None
                
        except Exception as e:
            logger.error(f"Failed to find entity ID: {str(e)}")
            return None
    
    def _augment_existing_project(self, project: Project, project_data: Dict, session) -> bool:
        """Augment existing project with new information"""
        updated = False
        
        new_info = project_data.get('new_information')
        if new_info:
            if project.description:
                project.description = f"{project.description}. {new_info}"
            else:
                project.description = new_info
            updated = True
        
        status_update = project_data.get('status_update')
        if status_update:
            project.updated_at = datetime.utcnow()
            updated = True
        
        return updated
    
    def _create_new_project(self, project_data: Dict, context: EntityContext, session) -> Optional[Project]:
        """Create new project from analysis"""
        try:
            project = Project(
                user_id=context.user_id,
                name=project_data['name'],
                description=project_data.get('description'),
                status='active',
                priority=project_data.get('priority', 'medium'),
                created_at=datetime.utcnow()
            )
            
            session.add(project)
            return project
            
        except Exception as e:
            logger.error(f"Failed to create new project: {str(e)}")
            return None

# Global instance
enhanced_ai_processor = EnhancedAIProcessor() 

============================================================
FILE: chief_of_staff_ai/processors/integration_manager.py
============================================================
# Integration Manager - Unified Processor Coordination
# This coordinates between all enhanced processors and provides unified interfaces

import logging
from typing import Dict, List, Optional, Any, Union
from datetime import datetime, timezone
from dataclasses import dataclass

# Import enhanced processors
from processors.enhanced_processors.enhanced_task_processor import enhanced_task_processor
from processors.enhanced_processors.enhanced_email_processor import enhanced_email_processor
from processors.enhanced_processors.enhanced_data_normalizer import enhanced_data_normalizer

# Import unified processors
from processors.unified_entity_engine import entity_engine, EntityContext
from processors.enhanced_ai_pipeline import enhanced_ai_processor
from processors.realtime_processor import realtime_processor

# Import adapter layer for backward compatibility
from processors.adapter_layer import task_extractor, email_intelligence, email_normalizer

logger = logging.getLogger(__name__)

@dataclass
class ProcessingPlan:
    """Plan for processing various types of data"""
    data_type: str
    processing_steps: List[str]
    expected_entities: List[str]
    real_time: bool
    priority: int

class IntegrationManager:
    """
    Central integration manager that coordinates all processors.
    This is the main interface for the application to interact with processors.
    """
    
    def __init__(self):
        # Enhanced processors
        self.task_processor = enhanced_task_processor
        self.email_processor = enhanced_email_processor
        self.data_normalizer = enhanced_data_normalizer
        
        # Unified processors
        self.entity_engine = entity_engine
        self.ai_processor = enhanced_ai_processor
        self.realtime_processor = realtime_processor
        
        # Adapter layer for backward compatibility
        self.legacy_task_extractor = task_extractor
        self.legacy_email_intelligence = email_intelligence
        self.legacy_email_normalizer = email_normalizer
        
        # Processing statistics
        self.processing_stats = {
            'emails_processed': 0,
            'tasks_created': 0,
            'entities_created': 0,
            'insights_generated': 0,
            'processing_time_total': 0.0
        }
        
        logger.info("Integration Manager initialized with enhanced processor architecture")
    
    # =====================================================================
    # UNIFIED PROCESSING INTERFACES
    # =====================================================================
    
    def process_email_complete(self, email_data: Dict, user_id: int, 
                             real_time: bool = True, 
                             legacy_mode: bool = False) -> Dict[str, Any]:
        """
        Complete email processing with full entity creation and intelligence.
        This is the main email processing interface.
        """
        try:
            start_time = datetime.utcnow()
            
            if legacy_mode:
                # Use legacy adapters for backward compatibility
                logger.info(f"Processing email in legacy mode for user {user_id}")
                result = self._process_email_legacy_mode(email_data, user_id)
            else:
                # Use enhanced processors
                logger.info(f"Processing email with enhanced processors for user {user_id}")
                result = self._process_email_enhanced_mode(email_data, user_id, real_time)
            
            # Update processing statistics
            processing_time = (datetime.utcnow() - start_time).total_seconds()
            self._update_processing_stats('email', result, processing_time)
            
            return result
            
        except Exception as e:
            logger.error(f"Error in complete email processing: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'processing_mode': 'legacy' if legacy_mode else 'enhanced'
            }
    
    def process_calendar_event_complete(self, event_data: Dict, user_id: int,
                                      real_time: bool = True) -> Dict[str, Any]:
        """
        Complete calendar event processing with meeting preparation.
        """
        try:
            start_time = datetime.utcnow()
            
            # Step 1: Normalize calendar data
            logger.info(f"Processing calendar event for user {user_id}")
            normalization_result = self.data_normalizer.normalize_calendar_data(event_data)
            
            if not normalization_result.success:
                return {
                    'success': False,
                    'error': f"Calendar normalization failed: {normalization_result.issues_found}"
                }
            
            normalized_event = normalization_result.normalized_data
            
            # Step 2: Process with enhanced processors
            if real_time:
                # Queue for real-time processing
                self.realtime_processor.process_new_calendar_event(normalized_event, user_id)
                result = {
                    'success': True,
                    'status': 'queued_for_realtime',
                    'event_id': normalized_event.get('google_event_id'),
                    'message': 'Calendar event queued for real-time processing'
                }
            else:
                # Process immediately
                ai_result = self.ai_processor.enhance_calendar_event_with_intelligence(
                    normalized_event, user_id
                )
                
                # Create preparation tasks
                task_result = self.task_processor.process_tasks_from_calendar_event(
                    normalized_event, user_id
                )
                
                result = {
                    'success': True,
                    'event_processing': ai_result,
                    'task_processing': task_result,
                    'normalized_event': normalized_event
                }
            
            # Update statistics
            processing_time = (datetime.utcnow() - start_time).total_seconds()
            self._update_processing_stats('calendar', result, processing_time)
            
            return result
            
        except Exception as e:
            logger.error(f"Error in calendar event processing: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def create_manual_task_complete(self, task_description: str, user_id: int,
                                  assignee_email: str = None,
                                  topic_names: List[str] = None,
                                  project_name: str = None,
                                  due_date: datetime = None,
                                  priority: str = 'medium') -> Dict[str, Any]:
        """
        Complete manual task creation with entity relationships.
        """
        try:
            result = self.task_processor.create_manual_task_with_context(
                task_description=task_description,
                assignee_email=assignee_email,
                topic_names=topic_names,
                project_name=project_name,
                due_date=due_date,
                priority=priority,
                user_id=user_id
            )
            
            # Update statistics
            if result['success']:
                self.processing_stats['tasks_created'] += 1
            
            return result
            
        except Exception as e:
            logger.error(f"Error in manual task creation: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    # =====================================================================
    # BATCH PROCESSING INTERFACES
    # =====================================================================
    
    def process_email_batch(self, email_list: List[Dict], user_id: int,
                           batch_size: int = 10,
                           real_time: bool = False) -> Dict[str, Any]:
        """
        Process multiple emails in optimized batches.
        """
        try:
            logger.info(f"Processing email batch of {len(email_list)} emails for user {user_id}")
            
            # Use enhanced email processor for batch processing
            result = self.email_processor.process_email_batch(email_list, user_id, batch_size)
            
            # Update statistics
            if result['success']:
                batch_result = result['result']
                self.processing_stats['emails_processed'] += batch_result['processed']
                self.processing_stats['processing_time_total'] += batch_result['batch_summary']['processing_time']
                
                # Update entity stats
                entities_created = batch_result['batch_summary']['total_entities_created']
                for entity_type, count in entities_created.items():
                    if entity_type == 'tasks':
                        self.processing_stats['tasks_created'] += count
                    self.processing_stats['entities_created'] += count
            
            return result
            
        except Exception as e:
            logger.error(f"Error in email batch processing: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    # =====================================================================
    # ANALYTICS AND INSIGHTS
    # =====================================================================
    
    def generate_user_insights(self, user_id: int, analysis_type: str = 'comprehensive') -> Dict[str, Any]:
        """
        Generate comprehensive user insights from all data sources.
        """
        try:
            insights = {
                'user_id': user_id,
                'analysis_type': analysis_type,
                'generated_at': datetime.utcnow().isoformat(),
                'insights': {}
            }
            
            if analysis_type in ['comprehensive', 'email']:
                # Email pattern analysis
                email_insights = self.email_processor.analyze_email_patterns(user_id)
                if email_insights['success']:
                    insights['insights']['email_patterns'] = email_insights['result']
            
            if analysis_type in ['comprehensive', 'tasks']:
                # Task pattern analysis
                task_insights = self.task_processor.analyze_task_patterns(user_id)
                if task_insights['success']:
                    insights['insights']['task_patterns'] = task_insights['result']
            
            if analysis_type in ['comprehensive', 'proactive']:
                # Proactive insights from entity engine
                proactive_insights = self.entity_engine.generate_proactive_insights(user_id)
                insights['insights']['proactive_insights'] = [
                    {
                        'type': insight.insight_type,
                        'title': insight.title,
                        'description': insight.description,
                        'priority': insight.priority,
                        'confidence': insight.confidence
                    }
                    for insight in proactive_insights
                ]
            
            return {
                'success': True,
                'result': insights
            }
            
        except Exception as e:
            logger.error(f"Error generating user insights: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """
        Get comprehensive processing statistics.
        """
        return {
            'success': True,
            'result': {
                'processing_stats': self.processing_stats.copy(),
                'processor_status': {
                    'enhanced_processors_active': True,
                    'legacy_adapters_available': True,
                    'real_time_processing': self.realtime_processor.running,
                    'entity_engine_active': True
                },
                'performance_metrics': {
                    'avg_processing_time': (
                        self.processing_stats['processing_time_total'] / 
                        max(1, self.processing_stats['emails_processed'])
                    ),
                    'entities_per_email': (
                        self.processing_stats['entities_created'] / 
                        max(1, self.processing_stats['emails_processed'])
                    )
                }
            }
        }
    
    # =====================================================================
    # LEGACY COMPATIBILITY METHODS
    # =====================================================================
    
    def get_legacy_task_extractor(self):
        """Get legacy task extractor for backward compatibility"""
        return self.legacy_task_extractor
    
    def get_legacy_email_intelligence(self):
        """Get legacy email intelligence for backward compatibility"""
        return self.legacy_email_intelligence
    
    def get_legacy_email_normalizer(self):
        """Get legacy email normalizer for backward compatibility"""
        return self.legacy_email_normalizer
    
    # =====================================================================
    # REAL-TIME PROCESSING CONTROL
    # =====================================================================
    
    def start_realtime_processing(self, num_workers: int = 3):
        """Start real-time processing"""
        try:
            self.realtime_processor.start(num_workers)
            logger.info(f"Started real-time processing with {num_workers} workers")
            return {'success': True, 'message': 'Real-time processing started'}
        except Exception as e:
            logger.error(f"Failed to start real-time processing: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def stop_realtime_processing(self):
        """Stop real-time processing"""
        try:
            self.realtime_processor.stop()
            logger.info("Stopped real-time processing")
            return {'success': True, 'message': 'Real-time processing stopped'}
        except Exception as e:
            logger.error(f"Failed to stop real-time processing: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def register_insight_callback(self, user_id: int, callback_function):
        """Register callback for real-time insight delivery"""
        self.realtime_processor.register_insight_callback(user_id, callback_function)
        logger.info(f"Registered insight callback for user {user_id}")
    
    # =====================================================================
    # PRIVATE HELPER METHODS
    # =====================================================================
    
    def _process_email_enhanced_mode(self, email_data: Dict, user_id: int, real_time: bool) -> Dict[str, Any]:
        """Process email using enhanced processors"""
        # Step 1: Normalize email data
        normalization_result = self.data_normalizer.normalize_email_data(email_data)
        
        if not normalization_result.success:
            return {
                'success': False,
                'error': f"Email normalization failed: {normalization_result.issues_found}",
                'processing_mode': 'enhanced'
            }
        
        normalized_email = normalization_result.normalized_data
        
        # Step 2: Process with enhanced email processor
        processing_result = self.email_processor.process_email_comprehensive(
            normalized_email, user_id, real_time
        )
        
        # Add normalization info to result
        if processing_result['success']:
            processing_result['normalization'] = {
                'quality_score': normalization_result.quality_score,
                'issues_found': normalization_result.issues_found
            }
        
        processing_result['processing_mode'] = 'enhanced'
        return processing_result
    
    def _process_email_legacy_mode(self, email_data: Dict, user_id: int) -> Dict[str, Any]:
        """Process email using legacy adapters"""
        # Step 1: Normalize with legacy adapter
        normalized_email = self.legacy_email_normalizer.normalize_gmail_email(email_data)
        
        if normalized_email.get('error'):
            return {
                'success': False,
                'error': normalized_email.get('error_message'),
                'processing_mode': 'legacy'
            }
        
        # Step 2: Extract tasks
        task_result = self.legacy_task_extractor.extract_tasks_from_email(normalized_email, user_id)
        
        # Step 3: Process with email intelligence
        intelligence_result = self.legacy_email_intelligence.process_email(normalized_email, user_id)
        
        # Combine results
        combined_result = {
            'success': True,
            'processing_mode': 'legacy',
            'normalized_email': normalized_email,
            'task_extraction': task_result,
            'email_intelligence': intelligence_result
        }
        
        return combined_result
    
    def _update_processing_stats(self, data_type: str, result: Dict, processing_time: float):
        """Update internal processing statistics"""
        self.processing_stats['processing_time_total'] += processing_time
        
        if data_type == 'email' and result.get('success'):
            self.processing_stats['emails_processed'] += 1
            
            # Count entities if available
            if 'processing_summary' in result.get('result', {}):
                entities_created = result['result']['processing_summary'].get('entities_created', {})
                for entity_type, count in entities_created.items():
                    if entity_type == 'tasks':
                        self.processing_stats['tasks_created'] += count
                    self.processing_stats['entities_created'] += count
                
                insights_count = result['result']['processing_summary'].get('insights_generated', 0)
                self.processing_stats['insights_generated'] += insights_count

# =====================================================================
# GLOBAL INTEGRATION MANAGER INSTANCE
# =====================================================================

# Create global integration manager instance
integration_manager = IntegrationManager()

# Export for easy import
__all__ = ['integration_manager', 'IntegrationManager', 'ProcessingPlan']

logger.info("Integration Manager module loaded - unified processor coordination active") 

============================================================
FILE: chief_of_staff_ai/processors/intelligence_engine.py
============================================================
# Enhanced Intelligence Engine - The Core That Makes Everything Useful
# This generates meaningful meeting preparation, attendee intelligence, and proactive insights

import json
import logging
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple
import anthropic
from dataclasses import dataclass

from config.settings import settings
from models.database import get_db_manager, Calendar, Person, Email, Task, IntelligenceInsight, EntityRelationship

logger = logging.getLogger(__name__)

@dataclass
class AttendeeIntelligence:
    """Rich intelligence about meeting attendees"""
    name: str
    email: str
    relationship_score: float
    recent_communications: List[Dict]
    business_context: str
    preparation_notes: str
    conversation_starters: List[str]

@dataclass
class MeetingIntelligence:
    """Comprehensive meeting intelligence"""
    meeting_title: str
    business_context: str
    attendee_intelligence: List[AttendeeIntelligence]
    preparation_tasks: List[Dict]
    discussion_topics: List[Dict]
    strategic_importance: float
    success_probability: float
    outcome_predictions: List[str]

class IntelligenceEngine:
    """The core intelligence engine that makes everything useful"""
    
    def __init__(self):
        from config.settings import settings
        
        self.claude_client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        self.model = settings.CLAUDE_MODEL  # Now uses Claude 4 Opus from settings
        
    def generate_meeting_intelligence(self, user_id: int, event: Calendar) -> Optional[MeetingIntelligence]:
        """Generate comprehensive meeting intelligence with preparation tasks"""
        try:
            logger.info(f"Generating meeting intelligence for: {event.title}")
            
            # Step 1: Gather attendee intelligence
            attendee_intelligence = self._gather_attendee_intelligence(user_id, event)
            
            # Step 2: Find related email context
            email_context = self._find_meeting_email_context(user_id, event, attendee_intelligence)
            
            # Step 3: Generate AI-powered meeting analysis
            meeting_analysis = self._generate_ai_meeting_analysis(event, attendee_intelligence, email_context)
            
            if not meeting_analysis:
                return None
                
            # Step 4: Create preparation tasks based on analysis
            prep_tasks = self._create_preparation_tasks(user_id, event, meeting_analysis, attendee_intelligence)
            
            # Step 5: Generate proactive insights
            self._create_meeting_insights(user_id, event, meeting_analysis, attendee_intelligence)
            
            return MeetingIntelligence(
                meeting_title=event.title or "Unknown Meeting",
                business_context=meeting_analysis.get('business_context', ''),
                attendee_intelligence=attendee_intelligence,
                preparation_tasks=prep_tasks,
                discussion_topics=meeting_analysis.get('discussion_topics', []),
                strategic_importance=meeting_analysis.get('strategic_importance', 0.5),
                success_probability=meeting_analysis.get('success_probability', 0.5),
                outcome_predictions=meeting_analysis.get('outcome_predictions', [])
            )
            
        except Exception as e:
            logger.error(f"Failed to generate meeting intelligence: {str(e)}")
            return None
    
    def _gather_attendee_intelligence(self, user_id: int, event: Calendar) -> List[AttendeeIntelligence]:
        """Gather rich intelligence about meeting attendees"""
        attendees = []
        
        if not event.attendee_emails:
            return attendees
            
        db_manager = get_db_manager()
        
        for email in event.attendee_emails:
            if not email:
                continue
                
            # Find person in database
            person = db_manager.find_person_by_email(user_id, email)
            
            if person:
                # Get recent communications
                recent_comms = self._get_recent_communications(user_id, email)
                
                # Calculate relationship score
                relationship_score = self._calculate_relationship_score(person, recent_comms)
                
                # Generate business context
                business_context = self._generate_person_business_context(person, recent_comms)
                
                # Generate preparation notes
                prep_notes = self._generate_preparation_notes(person, recent_comms, event)
                
                # Generate conversation starters
                conversation_starters = self._generate_conversation_starters(person, recent_comms)
                
                attendee_intel = AttendeeIntelligence(
                    name=person.name,
                    email=email,
                    relationship_score=relationship_score,
                    recent_communications=recent_comms,
                    business_context=business_context,
                    preparation_notes=prep_notes,
                    conversation_starters=conversation_starters
                )
                attendees.append(attendee_intel)
            else:
                # Unknown attendee - still provide basic intelligence
                attendee_intel = AttendeeIntelligence(
                    name=email.split('@')[0].replace('.', ' ').title(),
                    email=email,
                    relationship_score=0.1,
                    recent_communications=[],
                    business_context=f"New contact from {email.split('@')[1]}",
                    preparation_notes=f"First meeting with {email} - opportunity to build new relationship",
                    conversation_starters=[f"How did you get involved with this project?", "What's your role at {email.split('@')[1]}?"]
                )
                attendees.append(attendee_intel)
        
        return attendees
    
    def _find_meeting_email_context(self, user_id: int, event: Calendar, attendees: List[AttendeeIntelligence]) -> Dict:
        """Find email context related to this meeting"""
        db_manager = get_db_manager()
        
        with db_manager.get_session() as session:
            # Look for emails mentioning meeting topics or from attendees
            attendee_emails = [a.email for a in attendees]
            
            # Get emails from attendees in the last 30 days
            cutoff_date = datetime.utcnow() - timedelta(days=30)
            
            related_emails = session.query(Email).filter(
                Email.user_id == user_id,
                Email.sender.in_(attendee_emails),
                Email.email_date > cutoff_date
            ).order_by(Email.email_date.desc()).limit(10).all()
            
            # Also look for emails mentioning meeting title keywords
            if event.title:
                title_keywords = [word.lower() for word in event.title.split() if len(word) > 3]
                if title_keywords:
                    keyword_emails = session.query(Email).filter(
                        Email.user_id == user_id,
                        Email.email_date > cutoff_date
                    ).all()
                    
                    # Filter by keyword matching
                    keyword_matches = []
                    for email in keyword_emails:
                        content = f"{email.subject or ''} {email.ai_summary or ''}".lower()
                        if any(keyword in content for keyword in title_keywords):
                            keyword_matches.append(email)
                    
                    related_emails.extend(keyword_matches[:5])  # Add top 5 keyword matches
            
            session.expunge_all()
            
            return {
                'related_emails': related_emails,
                'attendee_communications': len(related_emails),
                'recent_topics': self._extract_topics_from_emails(related_emails),
                'key_decisions': self._extract_decisions_from_emails(related_emails),
                'shared_context': self._extract_shared_context(related_emails)
            }
    
    def _generate_ai_meeting_analysis(self, event: Calendar, attendees: List[AttendeeIntelligence], email_context: Dict) -> Optional[Dict]:
        """Generate AI-powered meeting analysis"""
        try:
            # Prepare context for Claude
            meeting_context = self._prepare_meeting_context(event, attendees, email_context)
            
            system_prompt = """You are an expert executive assistant AI that specializes in meeting preparation and business intelligence. Your goal is to generate actionable meeting intelligence that will help the user have more effective, productive meetings.

Analyze the meeting and attendee information to provide:

1. **Business Context**: What is this meeting really about? What are the underlying business objectives?
2. **Strategic Importance**: How important is this meeting to business success?
3. **Preparation Tasks**: Specific, actionable tasks the user should complete before the meeting
4. **Discussion Topics**: Key topics to discuss based on recent communications and relationships
5. **Success Probability**: Likelihood this meeting will achieve its objectives
6. **Outcome Predictions**: Likely outcomes and next steps from this meeting

Focus on being practical and actionable. Generate insights that will actually help the user succeed in this meeting.

Return a JSON object with this structure:
{
    "business_context": "Detailed explanation of what this meeting is really about and why it matters",
    "strategic_importance": 0.8,
    "preparation_tasks": [
        {
            "task": "Specific preparation task",
            "rationale": "Why this is important",
            "priority": "high|medium|low",
            "time_needed": "15 minutes",
            "category": "research|review|prepare_materials|contact_followup"
        }
    ],
    "discussion_topics": [
        {
            "topic": "Key discussion topic",
            "context": "Background based on recent communications",
            "talking_points": ["Specific point 1", "Specific point 2"],
            "priority": "high|medium|low"
        }
    ],
    "success_probability": 0.7,
    "outcome_predictions": [
        "Likely outcome 1 based on context",
        "Potential challenge to prepare for"
    ],
    "attendee_dynamics": "Analysis of how attendees might interact",
    "key_opportunities": ["Opportunity 1", "Opportunity 2"],
    "potential_obstacles": ["Challenge 1", "Challenge 2"]
}"""

            user_prompt = f"""Analyze this meeting and generate comprehensive preparation intelligence:

{meeting_context}

Focus on generating actionable preparation tasks and discussion topics that will help make this meeting successful. Be specific and practical."""

            message = self.claude_client.messages.create(
                model=self.model,
                max_tokens=3000,
                temperature=0.1,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )
            
            response_text = message.content[0].text.strip()
            
            # Parse JSON response
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_text = response_text[json_start:json_end]
                return json.loads(json_text)
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to generate AI meeting analysis: {str(e)}")
            return None
    
    def _create_preparation_tasks(self, user_id: int, event: Calendar, analysis: Dict, attendees: List[AttendeeIntelligence]) -> List[Dict]:
        """Create specific preparation tasks based on meeting analysis"""
        db_manager = get_db_manager()
        created_tasks = []
        
        # Get preparation tasks from AI analysis
        prep_tasks = analysis.get('preparation_tasks', [])
        
        for task_info in prep_tasks:
            # Calculate due date (before meeting)
            meeting_time = event.start_time
            if meeting_time:
                # Tasks should be completed 1-2 hours before meeting
                due_time = meeting_time - timedelta(hours=2)
            else:
                due_time = datetime.utcnow() + timedelta(hours=24)
            
            task_data = {
                'description': task_info.get('task', 'Meeting preparation task'),
                'category': 'meeting_prep',
                'priority': task_info.get('priority', 'medium'),
                'due_date': due_time,
                'due_date_text': f"Before {event.title}",
                'source_text': f"AI-generated preparation for: {event.title}",
                'context': f"{task_info.get('rationale', '')} | Time needed: {task_info.get('time_needed', '15 minutes')}",
                'confidence': 0.9,
                'comprehensive_context_story': self._generate_task_context_story(task_info, event, attendees),
                'detailed_task_meaning': self._generate_task_meaning(task_info, event),
                'comprehensive_importance_analysis': self._generate_task_importance(task_info, event, analysis),
                'comprehensive_origin_details': f"Generated by AI for meeting '{event.title}' scheduled for {event.start_time}"
            }
            
            task = db_manager.create_or_update_task(user_id, task_data)
            if task:
                created_tasks.append(task.to_dict())
        
        return created_tasks
    
    def _create_meeting_insights(self, user_id: int, event: Calendar, analysis: Dict, attendees: List[AttendeeIntelligence]):
        """Create proactive insights about the meeting"""
        db_manager = get_db_manager()
        
        # Meeting preparation insight
        if analysis.get('strategic_importance', 0) > 0.6:
            insight_data = {
                'insight_type': 'meeting_prep',
                'title': f"High-value meeting preparation needed: {event.title}",
                'description': f"This meeting has high strategic importance ({analysis.get('strategic_importance', 0):.1f}/1.0). {analysis.get('business_context', '')}",
                'priority': 'high' if analysis.get('strategic_importance', 0) > 0.8 else 'medium',
                'confidence': 0.9,
                'related_entity_type': 'event',
                'related_entity_id': event.id,
                'action_required': True,
                'action_due_date': event.start_time - timedelta(hours=2) if event.start_time else None,
                'expires_at': event.start_time
            }
            db_manager.create_intelligence_insight(user_id, insight_data)
        
        # Relationship insights
        for attendee in attendees:
            if attendee.relationship_score > 0.7:
                insight_data = {
                    'insight_type': 'relationship_opportunity',
                    'title': f"Strong relationship opportunity with {attendee.name}",
                    'description': f"You have a strong relationship with {attendee.name} (score: {attendee.relationship_score:.1f}). {attendee.preparation_notes}",
                    'priority': 'medium',
                    'confidence': 0.8,
                    'related_entity_type': 'person',
                    'expires_at': event.start_time + timedelta(days=1)
                }
                db_manager.create_intelligence_insight(user_id, insight_data)
    
    def generate_proactive_insights(self, user_id: int) -> List[IntelligenceInsight]:
        """Generate proactive business insights"""
        db_manager = get_db_manager()
        insights = []
        
        # Check for upcoming meetings needing preparation
        upcoming_meetings = db_manager.get_upcoming_meetings_needing_prep(user_id, 48)
        
        for meeting in upcoming_meetings:
            hours_until = (meeting.start_time - datetime.utcnow()).total_seconds() / 3600
            
            if hours_until > 0 and hours_until < 24:
                insight_data = {
                    'insight_type': 'meeting_prep',
                    'title': f"Meeting preparation needed: {meeting.title}",
                    'description': f"Meeting in {hours_until:.1f} hours. Preparation recommended based on meeting importance and attendees.",
                    'priority': 'high' if hours_until < 4 else 'medium',
                    'confidence': 0.9,
                    'related_entity_type': 'event',
                    'related_entity_id': meeting.id,
                    'action_required': True,
                    'action_due_date': meeting.start_time - timedelta(hours=1),
                    'expires_at': meeting.start_time
                }
                insight = db_manager.create_intelligence_insight(user_id, insight_data)
                insights.append(insight)
        
        # Check for relationship maintenance opportunities
        self._generate_relationship_insights(user_id, insights)
        
        # Check for topic momentum opportunities
        self._generate_topic_insights(user_id, insights)
        
        return insights
    
    def _generate_relationship_insights(self, user_id: int, insights: List[IntelligenceInsight]):
        """Generate relationship maintenance insights"""
        db_manager = get_db_manager()
        
        # Find people who haven't been contacted recently but have high engagement
        with db_manager.get_session() as session:
            cutoff_date = datetime.utcnow() - timedelta(days=14)
            
            inactive_relationships = session.query(Person).filter(
                Person.user_id == user_id,
                Person.importance_level > 0.7,
                Person.last_interaction < cutoff_date
            ).limit(5).all()
            
            for person in inactive_relationships:
                days_since = (datetime.utcnow() - person.last_interaction).days
                
                insight_data = {
                    'insight_type': 'relationship_maintenance',
                    'title': f"Reconnect with {person.name}",
                    'description': f"It's been {days_since} days since your last interaction with {person.name} ({person.company or 'important contact'}). Consider reaching out to maintain this valuable relationship.",
                    'priority': 'medium',
                    'confidence': 0.7,
                    'related_entity_type': 'person',
                    'related_entity_id': person.id,
                    'action_required': True
                }
                insight = db_manager.create_intelligence_insight(user_id, insight_data)
                insights.append(insight)
    
    def _generate_topic_insights(self, user_id: int, insights: List[IntelligenceInsight]):
        """Generate topic momentum insights"""
        db_manager = get_db_manager()
        
        # Find topics with recent momentum
        with db_manager.get_session() as session:
            recent_date = datetime.utcnow() - timedelta(days=7)
            
            # Get recent emails with topics
            recent_emails = session.query(Email).filter(
                Email.user_id == user_id,
                Email.email_date > recent_date,
                Email.topics.isnot(None)
            ).all()
            
            # Count topic mentions
            topic_counts = {}
            for email in recent_emails:
                if email.topics:
                    for topic in email.topics:
                        topic_counts[topic] = topic_counts.get(topic, 0) + 1
            
            # Find trending topics
            for topic, count in topic_counts.items():
                if count >= 3:  # Mentioned in 3+ emails this week
                    insight_data = {
                        'insight_type': 'topic_momentum',
                        'title': f"Trending topic: {topic}",
                        'description': f"'{topic}' has been mentioned in {count} recent communications. This might be a good time to take action or follow up on this topic.",
                        'priority': 'medium',
                        'confidence': 0.6,
                        'action_required': False
                    }
                    insight = db_manager.create_intelligence_insight(user_id, insight_data)
                    insights.append(insight)
    
    # Helper methods for context generation
    def _prepare_meeting_context(self, event: Calendar, attendees: List[AttendeeIntelligence], email_context: Dict) -> str:
        """Prepare meeting context for AI analysis"""
        context = f"""MEETING ANALYSIS REQUEST

Meeting Details:
- Title: {event.title or 'Unknown'}
- Date/Time: {event.start_time}
- Duration: {(event.end_time - event.start_time).total_seconds() / 3600:.1f} hours
- Location: {event.location or 'Not specified'}
- Description: {event.description or 'No description'}

Attendees ({len(attendees)} people):"""

        for attendee in attendees:
            context += f"\n- {attendee.name} ({attendee.email})"
            context += f"\n  * Relationship Score: {attendee.relationship_score:.1f}/1.0"
            context += f"\n  * Business Context: {attendee.business_context}"
            if attendee.recent_communications:
                context += f"\n  * Recent Communications: {len(attendee.recent_communications)} emails"

        context += f"\n\nEmail Context:"
        context += f"\n- Related emails found: {email_context.get('attendee_communications', 0)}"
        if email_context.get('recent_topics'):
            context += f"\n- Recent topics: {', '.join(email_context['recent_topics'][:5])}"
        if email_context.get('key_decisions'):
            context += f"\n- Key decisions mentioned: {', '.join(email_context['key_decisions'][:3])}"

        return context
    
    def _get_recent_communications(self, user_id: int, email: str) -> List[Dict]:
        """Get recent communications with a person"""
        db_manager = get_db_manager()
        
        with db_manager.get_session() as session:
            cutoff_date = datetime.utcnow() - timedelta(days=30)
            
            emails = session.query(Email).filter(
                Email.user_id == user_id,
                Email.sender == email,
                Email.email_date > cutoff_date
            ).order_by(Email.email_date.desc()).limit(5).all()
            
            return [
                {
                    'subject': email.subject,
                    'date': email.email_date,
                    'summary': email.ai_summary,
                    'strategic_importance': email.strategic_importance or 0.5
                }
                for email in emails
            ]
    
    def _calculate_relationship_score(self, person: Person, recent_comms: List[Dict]) -> float:
        """Calculate relationship strength score"""
        base_score = person.importance_level or 0.5
        
        # Boost for recent communications
        comm_boost = min(0.3, len(recent_comms) * 0.1)
        
        # Boost for strategic communications
        strategic_boost = 0
        for comm in recent_comms:
            if comm.get('strategic_importance', 0) > 0.7:
                strategic_boost += 0.1
        
        return min(1.0, base_score + comm_boost + strategic_boost)
    
    def _generate_person_business_context(self, person: Person, recent_comms: List[Dict]) -> str:
        """Generate business context for a person"""
        context_parts = []
        
        if person.title and person.company:
            context_parts.append(f"{person.title} at {person.company}")
        elif person.company:
            context_parts.append(f"Works at {person.company}")
        elif person.title:
            context_parts.append(f"{person.title}")
        
        if person.relationship_type:
            context_parts.append(f"Relationship: {person.relationship_type}")
        
        if recent_comms:
            recent_topics = []
            for comm in recent_comms[:2]:
                if comm.get('summary'):
                    recent_topics.append(comm['summary'][:100])
            if recent_topics:
                context_parts.append(f"Recent discussions: {'; '.join(recent_topics)}")
        
        return '. '.join(context_parts) if context_parts else "Professional contact"
    
    def _generate_preparation_notes(self, person: Person, recent_comms: List[Dict], event: Calendar) -> str:
        """Generate preparation notes for meeting with this person"""
        notes = []
        
        if recent_comms:
            notes.append(f"Recent context: {recent_comms[0].get('summary', 'Previous communication')}")
        
        if person.key_topics:
            topics = person.key_topics[:3] if isinstance(person.key_topics, list) else [str(person.key_topics)]
            notes.append(f"Key interests: {', '.join(topics)}")
        
        if person.notes:
            notes.append(f"Background: {person.notes[:150]}")
        
        return '. '.join(notes) if notes else f"First documented meeting with {person.name}"
    
    def _generate_conversation_starters(self, person: Person, recent_comms: List[Dict]) -> List[str]:
        """Generate conversation starters"""
        starters = ["How has your week been?"]
        
        if recent_comms and recent_comms[0].get('summary'):
            starters.append(f"Following up on {recent_comms[0]['summary'][:50]}...")
        
        if person.company:
            starters.append(f"How are things going at {person.company}?")
        
        if person.key_topics and isinstance(person.key_topics, list):
            if person.key_topics:
                starters.append(f"Any updates on {person.key_topics[0]}?")
        
        return starters[:3]
    
    def _extract_topics_from_emails(self, emails: List[Email]) -> List[str]:
        """Extract topics from emails"""
        topics = set()
        for email in emails:
            if email.topics and isinstance(email.topics, list):
                topics.update(email.topics[:3])
        return list(topics)[:5]
    
    def _extract_decisions_from_emails(self, emails: List[Email]) -> List[str]:
        """Extract key decisions from emails"""
        decisions = []
        for email in emails:
            if email.key_insights and isinstance(email.key_insights, dict):
                email_decisions = email.key_insights.get('key_decisions', [])
                if isinstance(email_decisions, list):
                    decisions.extend(email_decisions[:2])
        return decisions[:3]
    
    def _extract_shared_context(self, emails: List[Email]) -> str:
        """Extract shared context from emails"""
        if not emails:
            return "No recent shared context found"
        
        summaries = [email.ai_summary for email in emails[:3] if email.ai_summary]
        if summaries:
            return f"Recent shared discussions: {' | '.join(summaries[:2])}"
        
        return "Recent communications available"
    
    def _generate_task_context_story(self, task_info: Dict, event: Calendar, attendees: List[AttendeeIntelligence]) -> str:
        """Generate comprehensive context story for preparation task"""
        story_parts = []
        
        story_parts.append(f"üìÖ **Meeting Preparation Task for:** {event.title}")
        story_parts.append(f"‚è∞ **Meeting Time:** {event.start_time.strftime('%A, %B %d at %I:%M %p')}")
        
        if attendees:
            attendee_names = [a.name for a in attendees[:3]]
            story_parts.append(f"üë• **Key Attendees:** {', '.join(attendee_names)}")
        
        story_parts.append(f"üéØ **Task Purpose:** {task_info.get('rationale', 'Meeting preparation')}")
        story_parts.append(f"‚è±Ô∏è **Time Investment:** {task_info.get('time_needed', '15 minutes')}")
        
        return "\n".join(story_parts)
    
    def _generate_task_meaning(self, task_info: Dict, event: Calendar) -> str:
        """Generate detailed task meaning"""
        meaning_parts = []
        
        meaning_parts.append(f"üìã **What You Need to Do:** {task_info.get('task', 'Complete preparation task')}")
        meaning_parts.append(f"üéØ **Why This Matters:** {task_info.get('rationale', 'This preparation will help ensure meeting success')}")
        meaning_parts.append(f"üìÖ **Context:** This task should be completed before your meeting '{event.title}'")
        
        category = task_info.get('category', 'general')
        if category == 'research':
            meaning_parts.append("üîç **Action Type:** Research and information gathering")
        elif category == 'review':
            meaning_parts.append("üìñ **Action Type:** Review and analysis of existing materials")
        elif category == 'prepare_materials':
            meaning_parts.append("üìÅ **Action Type:** Prepare documents or presentation materials")
        elif category == 'contact_followup':
            meaning_parts.append("üìû **Action Type:** Reach out to contacts for information or confirmation")
        
        return "\n".join(meaning_parts)
    
    def _generate_task_importance(self, task_info: Dict, event: Calendar, analysis: Dict) -> str:
        """Generate comprehensive importance analysis"""
        importance_parts = []
        
        priority = task_info.get('priority', 'medium')
        if priority == 'high':
            importance_parts.append("üö® **Priority Level:** HIGH - This task is critical for meeting success")
        elif priority == 'medium':
            importance_parts.append("‚ö†Ô∏è **Priority Level:** MEDIUM - This task will significantly improve meeting outcomes")
        else:
            importance_parts.append("üìù **Priority Level:** Standard preparation task")
        
        strategic_importance = analysis.get('strategic_importance', 0.5)
        if strategic_importance > 0.8:
            importance_parts.append("‚≠ê **Strategic Value:** This meeting has very high strategic importance")
        elif strategic_importance > 0.6:
            importance_parts.append("‚≠ê **Strategic Value:** This meeting has significant strategic value")
        
        importance_parts.append(f"üíº **Business Impact:** {analysis.get('business_context', 'Important for business relationships and outcomes')}")
        
        return "\n".join(importance_parts)

# Global intelligence engine instance
intelligence_engine = IntelligenceEngine() 

============================================================
FILE: chief_of_staff_ai/processors/unified_entity_engine.py
============================================================
# Unified Entity Engine - Central Intelligence Hub
# This replaces the scattered entity creation across multiple files

import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timezone, timedelta
import json
import hashlib
from sqlalchemy.orm import Session
from dataclasses import dataclass
import re

from models.database import get_db_manager
from config.settings import settings
from models.database import (
    Topic, Person, Task, Email, CalendarEvent, Project,
    EntityRelationship, IntelligenceInsight, 
    person_topic_association, task_topic_association, event_topic_association
)

logger = logging.getLogger(__name__)

@dataclass
class EntityContext:
    """Container for entity creation context"""
    source_type: str  # email, calendar, manual
    source_id: Optional[int] = None
    confidence: float = 0.8
    user_id: int = None
    processing_metadata: Dict = None

class UnifiedEntityEngine:
    """
    Central hub for all entity creation, updating, and relationship management.
    This is the brain that ensures consistency across all data sources.
    """
    
    def __init__(self):
        self.db_manager = get_db_manager()
        
    # =====================================================================
    # CORE ENTITY CREATION METHODS
    # =====================================================================
    
    def create_or_update_person(self, 
                               email: str, 
                               name: str = None, 
                               context: EntityContext = None) -> Person:
        """
        Unified person creation from ANY source (email, calendar, manual).
        This solves the asymmetry problem you identified.
        """
        try:
            with self.db_manager.get_session() as session:
                # Always check for existing person first
                existing_person = session.query(Person).filter(
                    Person.user_id == context.user_id,
                    Person.email_address == email.lower()
                ).first()
                
                if existing_person:
                    # Update existing person with new information
                    updated = self._update_person_intelligence(existing_person, name, context, session)
                    if updated:
                        session.commit()
                        logger.info(f"Updated existing person: {existing_person.name} ({email})")
                    return existing_person
                
                # Create new person
                person = Person(
                    user_id=context.user_id,
                    email_address=email.lower(),
                    name=name or self._extract_name_from_email(email),
                    created_at=datetime.utcnow()
                )
                
                # Add source-specific intelligence
                self._enrich_person_from_context(person, context)
                
                session.add(person)
                session.commit()
                
                logger.info(f"Created new person: {person.name} ({email}) from {context.source_type}")
                return person
                
        except Exception as e:
            logger.error(f"Failed to create/update person {email}: {str(e)}")
            return None
    
    def create_or_update_topic(self, 
                              topic_name: str, 
                              description: str = None,
                              keywords: List[str] = None,
                              context: EntityContext = None) -> Topic:
        """
        Topics as the central brain - always check existing first, then augment.
        This solves your topic duplication concern.
        """
        try:
            with self.db_manager.get_session() as session:
                # Intelligent topic matching - exact name or similar
                existing_topic = self._find_matching_topic(topic_name, context.user_id, session)
                
                if existing_topic:
                    # Augment existing topic with new intelligence
                    updated = self._augment_topic_intelligence(existing_topic, description, keywords, context, session)
                    if updated:
                        existing_topic.updated_at = datetime.utcnow()
                        existing_topic.version += 1
                        session.commit()
                        logger.info(f"Augmented existing topic: {existing_topic.name}")
                    return existing_topic
                
                # Create new topic
                topic = Topic(
                    user_id=context.user_id,
                    name=topic_name.strip().title(),
                    description=description,
                    keywords=','.join(keywords) if keywords else None,
                    confidence_score=context.confidence,
                    total_mentions=1,
                    last_mentioned=datetime.utcnow(),
                    created_at=datetime.utcnow()
                )
                
                # Generate AI intelligence summary for new topic
                topic.intelligence_summary = self._generate_topic_intelligence_summary(topic_name, description, keywords)
                
                session.add(topic)
                session.commit()
                
                logger.info(f"Created new topic: {topic_name}")
                return topic
                
        except Exception as e:
            logger.error(f"Failed to create/update topic {topic_name}: {str(e)}")
            return None
    
    def create_task_with_full_context(self,
                                    description: str,
                                    assignee_email: str = None,
                                    topic_names: List[str] = None,
                                    context: EntityContext = None,
                                    due_date: datetime = None,
                                    priority: str = 'medium') -> Task:
        """
        Create tasks with full context story and entity relationships.
        This addresses your concern about tasks existing "in the air".
        """
        try:
            with self.db_manager.get_session() as session:
                # Create the task
                task = Task(
                    user_id=context.user_id,
                    description=description,
                    priority=priority,
                    due_date=due_date,
                    confidence=context.confidence,
                    created_at=datetime.utcnow()
                )
                
                # Generate context story - WHY this task exists
                task.context_story = self._generate_task_context_story(
                    description, assignee_email, topic_names, context
                )
                
                # Link to assignee if provided
                if assignee_email:
                    assignee = self.create_or_update_person(assignee_email, context=context)
                    if assignee:
                        task.assignee_id = assignee.id
                
                # Link to topics
                if topic_names:
                    topic_ids = []
                    for topic_name in topic_names:
                        topic = self.create_or_update_topic(topic_name, context=context)
                        if topic:
                            topic_ids.append(topic.id)
                    task.topics = topic_ids  # Store as JSON list of topic IDs
                
                # Link to source
                if context.source_type == 'email' and context.source_id:
                    task.source_email_id = context.source_id
                elif context.source_type == 'calendar' and context.source_id:
                    task.source_event_id = context.source_id
                
                session.add(task)
                session.commit()
                
                logger.info(f"Created task with full context: {description[:50]}...")
                return task
                
        except Exception as e:
            logger.error(f"Failed to create task: {str(e)}")
            return None
    
    # =====================================================================
    # RELATIONSHIP INTELLIGENCE METHODS
    # =====================================================================
    
    def create_entity_relationship(self, 
                                 entity_a_type: str, entity_a_id: int,
                                 entity_b_type: str, entity_b_id: int,
                                 relationship_type: str,
                                 context: EntityContext) -> EntityRelationship:
        """Create intelligent relationships between any entities"""
        try:
            with self.db_manager.get_session() as session:
                # Check if relationship already exists
                existing = session.query(EntityRelationship).filter(
                    EntityRelationship.user_id == context.user_id,
                    EntityRelationship.entity_type_a == entity_a_type,
                    EntityRelationship.entity_id_a == entity_a_id,
                    EntityRelationship.entity_type_b == entity_b_type,
                    EntityRelationship.entity_id_b == entity_b_id
                ).first()
                
                if existing:
                    # Strengthen existing relationship
                    existing.total_interactions += 1
                    existing.last_interaction = datetime.utcnow()
                    existing.strength = min(1.0, existing.strength + 0.1)
                    session.commit()
                    return existing
                
                # Create new relationship
                relationship = EntityRelationship(
                    user_id=context.user_id,
                    entity_type_a=entity_a_type,
                    entity_id_a=entity_a_id,
                    entity_type_b=entity_b_type,
                    entity_id_b=entity_b_id,
                    relationship_type=relationship_type,
                    strength=0.5,
                    last_interaction=datetime.utcnow(),
                    total_interactions=1
                )
                
                # Generate context summary
                relationship.context_summary = self._generate_relationship_context(
                    entity_a_type, entity_a_id, entity_b_type, entity_b_id, session
                )
                
                session.add(relationship)
                session.commit()
                
                logger.info(f"Created relationship: {entity_a_type}:{entity_a_id} -> {entity_b_type}:{entity_b_id}")
                return relationship
                
        except Exception as e:
            logger.error(f"Failed to create entity relationship: {str(e)}")
            return None
    
    def generate_proactive_insights(self, user_id: int) -> List[IntelligenceInsight]:
        """
        Generate proactive insights based on entity patterns and relationships.
        This is where the predictive intelligence happens.
        """
        insights = []
        
        try:
            with self.db_manager.get_session() as session:
                # Insight 1: Relationship gaps (haven't contacted important people)
                relationship_insights = self._detect_relationship_gaps(user_id, session)
                insights.extend(relationship_insights)
                
                # Insight 2: Topic momentum (topics getting hot)
                topic_insights = self._detect_topic_momentum(user_id, session)
                insights.extend(topic_insights)
                
                # Insight 3: Meeting preparation needs
                meeting_prep_insights = self._detect_meeting_prep_needs(user_id, session)
                insights.extend(meeting_prep_insights)
                
                # Insight 4: Project attention needed
                project_insights = self._detect_project_attention_needs(user_id, session)
                insights.extend(project_insights)
                
                # Save insights to database
                for insight in insights:
                    session.add(insight)
                
                session.commit()
                logger.info(f"Generated {len(insights)} proactive insights for user {user_id}")
                
        except Exception as e:
            logger.error(f"Failed to generate proactive insights: {str(e)}")
            
        return insights
    
    # =====================================================================
    # HELPER METHODS
    # =====================================================================
    
    def _find_matching_topic(self, topic_name: str, user_id: int, session: Session) -> Optional[Topic]:
        """Intelligent topic matching using exact name, keywords, or similarity"""
        # Exact match first
        exact_match = session.query(Topic).filter(
            Topic.user_id == user_id,
            Topic.name.ilike(f"%{topic_name}%")
        ).first()
        
        if exact_match:
            return exact_match
        
        # Keyword matching
        topics = session.query(Topic).filter(Topic.user_id == user_id).all()
        for topic in topics:
            if topic.keywords:
                keywords = [k.strip().lower() for k in topic.keywords.split(',')]
                if topic_name.lower() in keywords:
                    return topic
        
        return None
    
    def _generate_task_context_story(self, description: str, assignee_email: str, 
                                   topic_names: List[str], context: EntityContext) -> str:
        """Generate WHY this task exists - the narrative context"""
        story_parts = []
        
        # Source context
        if context.source_type == 'email':
            story_parts.append(f"Task extracted from email communication")
        elif context.source_type == 'calendar':
            story_parts.append(f"Task generated for meeting preparation")
        
        # Topic context
        if topic_names:
            story_parts.append(f"Related to: {', '.join(topic_names)}")
        
        # Assignee context
        if assignee_email:
            story_parts.append(f"Assigned to: {assignee_email}")
        
        # Confidence context
        confidence_level = "high" if context.confidence > 0.8 else "medium" if context.confidence > 0.5 else "low"
        story_parts.append(f"Confidence: {confidence_level}")
        
        return ". ".join(story_parts)
    
    def _generate_topic_intelligence_summary(self, name: str, description: str, keywords: List[str]) -> str:
        """Generate AI summary of what we know about this topic"""
        # This would call Claude for intelligence generation
        # For now, return a structured summary
        parts = [f"Topic: {name}"]
        if description:
            parts.append(f"Description: {description}")
        if keywords:
            parts.append(f"Keywords: {', '.join(keywords)}")
        return ". ".join(parts)
    
    def _detect_relationship_gaps(self, user_id: int, session: Session) -> List[IntelligenceInsight]:
        """Detect important people user hasn't contacted recently"""
        insights = []
        
        # Find high-importance people with no recent contact
        important_people = session.query(Person).filter(
            Person.user_id == user_id,
            Person.importance_level > 0.7,
            Person.last_interaction < datetime.utcnow() - timedelta(days=14)
        ).limit(5).all()
        
        for person in important_people:
            insight = IntelligenceInsight(
                user_id=user_id,
                insight_type='relationship_alert',
                title=f"Haven't connected with {person.name} recently",
                description=f"Last contact was {person.last_interaction.strftime('%Y-%m-%d') if person.last_interaction else 'unknown'}. "
                           f"Consider reaching out about relevant topics.",
                priority='medium',
                confidence=0.8,
                related_entity_type='person',
                related_entity_id=person.id
            )
            insights.append(insight)
        
        return insights
    
    def _detect_topic_momentum(self, user_id: int, session: Session) -> List[IntelligenceInsight]:
        """Detect topics that are gaining momentum"""
        insights = []
        
        # Topics mentioned frequently in recent emails (last 7 days)
        week_ago = datetime.utcnow() - timedelta(days=7)
        
        hot_topics = session.query(Topic).filter(
            Topic.user_id == user_id,
            Topic.last_mentioned > week_ago,
            Topic.total_mentions > 3
        ).order_by(Topic.total_mentions.desc()).limit(3).all()
        
        for topic in hot_topics:
            insight = IntelligenceInsight(
                user_id=user_id,
                insight_type='topic_momentum',
                title=f"'{topic.name}' is trending in your communications",
                description=f"Mentioned {topic.total_mentions} times recently. "
                           f"Consider preparing materials or scheduling focused time.",
                priority='medium',
                confidence=0.7,
                related_entity_type='topic',
                related_entity_id=topic.id
            )
            insights.append(insight)
        
        return insights
    
    def _detect_meeting_prep_needs(self, user_id: int, session: Session) -> List[IntelligenceInsight]:
        """Detect upcoming meetings that need preparation"""
        insights = []
        
        # Meetings in next 48 hours with high prep priority
        tomorrow = datetime.utcnow() + timedelta(hours=48)
        
        upcoming_meetings = session.query(CalendarEvent).filter(
            CalendarEvent.user_id == user_id,
            CalendarEvent.start_time.between(datetime.utcnow(), tomorrow),
            CalendarEvent.preparation_priority > 0.7
        ).all()
        
        for meeting in upcoming_meetings:
            insight = IntelligenceInsight(
                user_id=user_id,
                insight_type='meeting_prep',
                title=f"Prepare for '{meeting.title}'",
                description=f"High-priority meeting on {meeting.start_time.strftime('%Y-%m-%d %H:%M')}. "
                           f"{meeting.business_context or 'No context available.'}",
                priority='high',
                confidence=0.9,
                related_entity_type='event',
                related_entity_id=meeting.id,
                expires_at=meeting.start_time
            )
            insights.append(insight)
        
        return insights
    
    def _detect_project_attention_needs(self, user_id: int, session: Session) -> List[IntelligenceInsight]:
        """Detect projects that need attention"""
        insights = []
        
        # Projects with no recent activity
        stale_projects = session.query(Project).filter(
            Project.user_id == user_id,
            Project.status == 'active',
            Project.updated_at < datetime.utcnow() - timedelta(days=7)
        ).limit(3).all()
        
        for project in stale_projects:
            insight = IntelligenceInsight(
                user_id=user_id,
                insight_type='project_attention',
                title=f"Project '{project.name}' needs attention",
                description=f"No recent activity since {project.updated_at.strftime('%Y-%m-%d')}. "
                           f"Consider checking in with stakeholders.",
                priority='medium',
                confidence=0.6,
                related_entity_type='project',
                related_entity_id=project.id
            )
            insights.append(insight)
        
        return insights
    
    def _update_person_intelligence(self, person: Person, name: str, context: EntityContext, session: Session) -> bool:
        """Update existing person with new intelligence"""
        updated = False
        
        # Update name if we have a better one
        if name and name != person.name and len(name) > len(person.name or ""):
            person.name = name
            updated = True
        
        # Update interaction tracking
        person.total_interactions += 1
        person.last_interaction = datetime.utcnow()
        person.updated_at = datetime.utcnow()
        updated = True
        
        # Add any signature data from processing metadata
        if context.processing_metadata and context.processing_metadata.get('signature'):
            sig_data = context.processing_metadata['signature']
            if sig_data.get('company') and not person.company:
                person.company = sig_data['company']
                updated = True
            if sig_data.get('title') and not person.title:
                person.title = sig_data['title']
                updated = True
            if sig_data.get('phone') and not person.phone:
                person.phone = sig_data['phone']
                updated = True
        
        return updated
    
    def _extract_name_from_email(self, email: str) -> str:
        """Extract a reasonable name from email address"""
        local_part = email.split('@')[0]
        # Handle common formats like first.last, first_last, firstlast
        if '.' in local_part:
            parts = local_part.split('.')
            return ' '.join(part.capitalize() for part in parts)
        elif '_' in local_part:
            parts = local_part.split('_')
            return ' '.join(part.capitalize() for part in parts)
        else:
            return local_part.capitalize()
    
    def _enrich_person_from_context(self, person: Person, context: EntityContext):
        """Enrich person with context-specific information"""
        if context.source_type == 'email':
            person.relationship_type = 'email_contact'
        elif context.source_type == 'calendar':
            person.relationship_type = 'meeting_attendee'
        
        # Set initial importance based on context
        person.importance_level = context.confidence * 0.6  # Scale down initial importance
        person.total_interactions = 1
        person.last_interaction = datetime.utcnow()
    
    def _augment_topic_intelligence(self, topic: Topic, description: str, keywords: List[str], 
                                   context: EntityContext, session: Session) -> bool:
        """Augment existing topic with new intelligence"""
        updated = False
        
        # Update mention tracking
        topic.total_mentions += 1
        topic.last_mentioned = datetime.utcnow()
        updated = True
        
        # Add new description if we don't have one
        if description and not topic.description:
            topic.description = description
            updated = True
        
        # Merge keywords
        if keywords:
            existing_keywords = set(topic.keywords.split(',')) if topic.keywords else set()
            new_keywords = set(keywords)
            merged_keywords = existing_keywords.union(new_keywords)
            topic.keywords = ','.join(merged_keywords)
            updated = True
        
        return updated
    
    def _generate_relationship_context(self, entity_a_type: str, entity_a_id: int, 
                                     entity_b_type: str, entity_b_id: int, session: Session) -> str:
        """Generate context summary for entity relationships"""
        try:
            # Get entity names for context
            entity_a_name = self._get_entity_name(entity_a_type, entity_a_id, session)
            entity_b_name = self._get_entity_name(entity_b_type, entity_b_id, session)
            
            return f"{entity_a_type.title()} '{entity_a_name}' connected to {entity_b_type} '{entity_b_name}'"
        except:
            return f"Relationship between {entity_a_type}:{entity_a_id} and {entity_b_type}:{entity_b_id}"
    
    def _get_entity_name(self, entity_type: str, entity_id: int, session: Session) -> str:
        """Get display name for any entity"""
        if entity_type == 'person':
            person = session.query(Person).get(entity_id)
            return person.name if person else f"Person {entity_id}"
        elif entity_type == 'topic':
            topic = session.query(Topic).get(entity_id)
            return topic.name if topic else f"Topic {entity_id}"
        elif entity_type == 'project':
            project = session.query(Project).get(entity_id)
            return project.name if project else f"Project {entity_id}"
        else:
            return f"{entity_type.title()} {entity_id}"
    
    def _extract_from_signature(self, signature_text: str) -> Dict[str, str]:
        """Extract structured data from email signature"""
        info = {}
        
        # Extract title (common patterns)
        title_patterns = [
            r'(?:CEO|CTO|CFO|President|Director|Manager|VP|Vice President)',
            r'(?:Senior|Lead|Principal|Head of|Chief)\s+[A-Za-z\s]+',
            r'(?:^|\n)([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s*(?:\n|$)'
        ]
        
        for pattern in title_patterns:
            match = re.search(pattern, signature_text, re.IGNORECASE)
            if match:
                info['title'] = match.group(0).strip()
                break
        
        # Extract company (usually follows title)
        company_patterns = [
            r'(?:^|\n)([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*(?:\s+Inc\.?|\s+LLC|\s+Corp\.?|\s+Co\.?))\s*(?:\n|$)',
            r'(?:@\s*)?([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s*(?:\n|$)'
        ]
        
        for pattern in company_patterns:
            match = re.search(pattern, signature_text)
            if match:
                potential_company = match.group(1).strip()
                if potential_company and len(potential_company) > 2:
                    info['company'] = potential_company
                    break
        
        # Extract phone with improved pattern
        phone_pattern = r'(\+?1?[-.\s]?\(?[0-9]{3}\)?[-.\s]?[0-9]{3}[-.\s]?[0-9]{4})'
        phone_match = re.search(phone_pattern, signature_text)
        if phone_match:
            info['phone'] = phone_match.group(1).strip()
        
        # Extract LinkedIn with robust pattern
        linkedin_pattern = r'(?:linkedin\.com/in/|linkedin\.com/pub/)([a-zA-Z0-9-]+)'
        linkedin_match = re.search(linkedin_pattern, signature_text, re.IGNORECASE)
        if linkedin_match:
            info['linkedin'] = f"https://linkedin.com/in/{linkedin_match.group(1)}"
        
        # Extract email if different from sender
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, signature_text)
        if emails:
            info['additional_emails'] = emails
        
        return info

# Global instance
entity_engine = UnifiedEntityEngine() 

============================================================
FILE: chief_of_staff_ai/processors/adapter_layer.py
============================================================
# Adapter Layer - Stub Implementation for Legacy Compatibility
import logging
from typing import Dict, Any

logger = logging.getLogger(__name__)

class TaskExtractor:
    """Stub implementation of legacy task extractor"""
    
    def extract_tasks_from_email(self, email_data: Dict, user_id: int) -> Dict[str, Any]:
        """Extract tasks from email - legacy adapter"""
        try:
            return {
                'success': True,
                'tasks_found': 0,
                'tasks': [],
                'processing_notes': ['Legacy adapter stub - no tasks extracted']
            }
        except Exception as e:
            logger.error(f"Error in legacy task extraction: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }

class EmailIntelligence:
    """Stub implementation of legacy email intelligence"""
    
    def process_email(self, email_data: Dict, user_id: int) -> Dict[str, Any]:
        """Process email intelligence - legacy adapter"""
        try:
            return {
                'success': True,
                'intelligence': {},
                'processing_notes': ['Legacy adapter stub - no intelligence extracted']
            }
        except Exception as e:
            logger.error(f"Error in legacy email intelligence: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }

class EmailNormalizer:
    """Stub implementation of legacy email normalizer"""
    
    def normalize_gmail_email(self, email_data: Dict) -> Dict[str, Any]:
        """Normalize Gmail email - legacy adapter"""
        try:
            # Basic normalization - just pass through the data
            normalized = email_data.copy()
            normalized['normalized'] = True
            normalized['processing_notes'] = ['Legacy adapter stub - basic normalization']
            return normalized
        except Exception as e:
