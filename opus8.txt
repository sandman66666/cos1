e == name,
                ApiKey.is_active == True
            ).first()
            
            if existing_key:
                return error_response("API key with this name already exists", "NAME_EXISTS", 409)
            
            # Create new API key
            new_api_key = ApiKey(
                user_id=g.user_id,
                name=name,
                key_hash=key_hash,
                permissions=permissions,
                created_at=datetime.utcnow(),
                expires_at=datetime.utcnow() + timedelta(days=expires_in_days) if expires_in_days else None,
                is_active=True
            )
            
            session.add(new_api_key)
            session.commit()
            
            logger.info(f"API key created for user: {g.user_email} (ID: {g.user_id}, Key: {name})")
            
            return success_response({
                'api_key': {
                    'id': new_api_key.id,
                    'name': new_api_key.name,
                    'key': api_key,  # Only shown once during creation
                    'permissions': new_api_key.permissions,
                    'created_at': new_api_key.created_at.isoformat(),
                    'expires_at': new_api_key.expires_at.isoformat() if new_api_key.expires_at else None
                },
                'warning': 'Store this API key securely. It will not be shown again.'
            }, "API key created successfully")
            
    except Exception as e:
        logger.error(f"Error creating API key: {str(e)}")
        return error_response(f"API key creation failed: {str(e)}", "API_KEY_CREATE_ERROR", 500)

@auth_api_bp.route('/api-keys/<int:key_id>', methods=['DELETE'])
@require_jwt_auth()
def revoke_api_key(key_id: int):
    """
    Revoke (deactivate) an API key.
    """
    try:
        with get_db_manager().get_session() as session:
            api_key = session.query(ApiKey).filter(
                ApiKey.id == key_id,
                ApiKey.user_id == g.user_id
            ).first()
            
            if not api_key:
                return error_response("API key not found", "KEY_NOT_FOUND", 404)
            
            api_key.is_active = False
            api_key.revoked_at = datetime.utcnow()
            session.commit()
            
            logger.info(f"API key revoked: {api_key.name} for user {g.user_email}")
            
            return success_response({}, "API key revoked successfully")
            
    except Exception as e:
        logger.error(f"Error revoking API key: {str(e)}")
        return error_response(f"API key revocation failed: {str(e)}", "API_KEY_REVOKE_ERROR", 500)

# =====================================================================
# SESSION MANAGEMENT ENDPOINTS
# =====================================================================

@auth_api_bp.route('/sessions', methods=['GET'])
@require_jwt_auth()
def get_user_sessions():
    """
    Get user's active sessions.
    """
    try:
        with get_db_manager().get_session() as session:
            user_sessions = session.query(UserSession).filter(
                UserSession.user_id == g.user_id,
                UserSession.is_active == True,
                UserSession.expires_at > datetime.utcnow()
            ).order_by(UserSession.last_activity.desc()).all()
            
            sessions_data = []
            for user_session in user_sessions:
                sessions_data.append({
                    'id': user_session.id,
                    'created_at': user_session.created_at.isoformat(),
                    'last_activity': user_session.last_activity.isoformat(),
                    'expires_at': user_session.expires_at.isoformat(),
                    'ip_address': user_session.ip_address,
                    'user_agent': user_session.user_agent[:100] if user_session.user_agent else None,
                    'is_current': True  # Would need to check against current session
                })
            
            return success_response({
                'sessions': sessions_data,
                'count': len(sessions_data)
            })
            
    except Exception as e:
        logger.error(f"Error getting user sessions: {str(e)}")
        return error_response(f"Sessions retrieval failed: {str(e)}", "SESSIONS_ERROR", 500)

@auth_api_bp.route('/sessions/<int:session_id>', methods=['DELETE'])
@require_jwt_auth()
def revoke_session(session_id: int):
    """
    Revoke a specific user session.
    """
    try:
        with get_db_manager().get_session() as session:
            user_session = session.query(UserSession).filter(
                UserSession.id == session_id,
                UserSession.user_id == g.user_id
            ).first()
            
            if not user_session:
                return error_response("Session not found", "SESSION_NOT_FOUND", 404)
            
            user_session.is_active = False
            user_session.logged_out_at = datetime.utcnow()
            session.commit()
            
            return success_response({}, "Session revoked successfully")
            
    except Exception as e:
        logger.error(f"Error revoking session: {str(e)}")
        return error_response(f"Session revocation failed: {str(e)}", "SESSION_REVOKE_ERROR", 500)

# =====================================================================
# ADMIN ENDPOINTS (ROLE-BASED ACCESS)
# =====================================================================

@auth_api_bp.route('/admin/users', methods=['GET'])
@require_jwt_auth(roles=['admin', 'super_admin'])
def admin_get_users():
    """
    Admin endpoint to list all users.
    """
    try:
        limit = int(request.args.get('limit', 100))
        offset = int(request.args.get('offset', 0))
        role_filter = request.args.get('role')
        is_active = request.args.get('is_active')
        
        with get_db_manager().get_session() as session:
            query = session.query(User)
            
            if role_filter:
                query = query.filter(User.role == role_filter)
            
            if is_active is not None:
                query = query.filter(User.is_active == (is_active.lower() == 'true'))
            
            total_count = query.count()
            users = query.order_by(User.created_at.desc()).offset(offset).limit(limit).all()
            
            users_data = []
            for user in users:
                users_data.append({
                    'id': user.id,
                    'email': user.email,
                    'full_name': user.full_name,
                    'role': user.role,
                    'is_active': user.is_active,
                    'email_verified': user.email_verified,
                    'created_at': user.created_at.isoformat() if user.created_at else None,
                    'last_login': user.last_login.isoformat() if user.last_login else None
                })
            
            return success_response({
                'users': users_data,
                'pagination': {
                    'total_count': total_count,
                    'limit': limit,
                    'offset': offset,
                    'has_more': offset + limit < total_count
                }
            })
            
    except Exception as e:
        logger.error(f"Error in admin get users: {str(e)}")
        return error_response(f"Admin user retrieval failed: {str(e)}", "ADMIN_ERROR", 500)

# =====================================================================
# UTILITY ENDPOINTS
# =====================================================================

@auth_api_bp.route('/verify-token', methods=['POST'])
def verify_token():
    """
    Verify if a token is valid.
    """
    try:
        data = request.get_json() or {}
        token = data.get('token')
        
        if not token:
            return error_response("Token is required", "MISSING_TOKEN")
        
        payload = verify_jwt_token(token)
        
        if payload:
            return success_response({
                'valid': True,
                'user_id': payload.get('user_id'),
                'email': payload.get('email'),
                'role': payload.get('role'),
                'expires_at': datetime.fromtimestamp(payload.get('exp')).isoformat()
            })
        else:
            return success_response({'valid': False})
            
    except Exception as e:
        logger.error(f"Error verifying token: {str(e)}")
        return error_response(f"Token verification failed: {str(e)}", "VERIFY_ERROR", 500)

@auth_api_bp.route('/health', methods=['GET'])
def auth_health_check():
    """
    Health check for authentication endpoints.
    """
    try:
        health_status = {
            'status': 'healthy',
            'auth_endpoints': 'available',
            'jwt_configured': bool(JWT_SECRET_KEY),
            'timestamp': datetime.utcnow().isoformat()
        }
        
        # Test database connection
        try:
            with get_db_manager().get_session() as session:
                session.execute('SELECT 1')
        except Exception as e:
            health_status['status'] = 'degraded'
            health_status['database'] = f'error: {str(e)}'
        
        status_code = 200 if health_status['status'] == 'healthy' else 503
        return jsonify(health_status), status_code
        
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.utcnow().isoformat()
        }), 500

# Export the blueprint
__all__ = ['auth_api_bp', 'require_jwt_auth', 'verify_jwt_token'] 

============================================================
FILE: chief_of_staff_ai/api/docs_endpoints.py
============================================================
# API Documentation and Testing Endpoints
# These endpoints provide comprehensive documentation, testing tools, and API exploration

import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timezone
from flask import Blueprint, request, jsonify, render_template, send_from_directory
from functools import wraps
import json
import inspect
import os
from pathlib import Path

# Import the integration manager and other API modules
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'chief_of_staff_ai'))

from api.versioning import API_VERSIONS, CURRENT_VERSION

logger = logging.getLogger(__name__)

# Create Blueprint
docs_api_bp = Blueprint('docs_api', __name__, url_prefix='/api/docs')

# =====================================================================
# API DOCUMENTATION CONFIGURATION
# =====================================================================

API_DOCUMENTATION = {
    'info': {
        'title': 'AI Chief of Staff API',
        'description': 'Comprehensive entity-centric AI Chief of Staff API with real-time processing, analytics, and intelligent automation',
        'version': '2.0.0',
        'contact': {
            'name': 'AI Chief of Staff Team',
            'email': 'support@chief-of-staff.ai'
        },
        'license': {
            'name': 'MIT',
            'url': 'https://opensource.org/licenses/MIT'
        }
    },
    'servers': [
        {
            'url': '/api/v2',
            'description': 'Production API (Current)'
        },
        {
            'url': '/api/v1',
            'description': 'Legacy API (Deprecated)'
        }
    ],
    'tags': [
        {
            'name': 'Authentication',
            'description': 'User authentication and session management'
        },
        {
            'name': 'Email Processing',
            'description': 'Email analysis and entity extraction'
        },
        {
            'name': 'Task Management',
            'description': 'Task creation, management, and tracking'
        },
        {
            'name': 'Entity Management',
            'description': 'People, topics, and relationship management'
        },
        {
            'name': 'Analytics',
            'description': 'Business intelligence and insights'
        },
        {
            'name': 'Real-time',
            'description': 'WebSocket-based real-time processing'
        },
        {
            'name': 'Batch Processing',
            'description': 'High-volume batch operations'
        }
    ]
}

ENDPOINT_EXAMPLES = {
    '/api/auth/login': {
        'method': 'POST',
        'description': 'Authenticate user and receive JWT tokens',
        'request_example': {
            'email': 'user@example.com',
            'password': 'SecurePassword123!',
            'remember_me': False
        },
        'response_example': {
            'success': True,
            'data': {
                'user': {
                    'id': 1,
                    'email': 'user@example.com',
                    'full_name': 'John Doe',
                    'role': 'user'
                },
                'tokens': {
                    'access_token': 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9...',
                    'refresh_token': 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9...',
                    'token_type': 'Bearer',
                    'expires_in': 86400
                }
            }
        }
    },
    '/api/v2/emails/process': {
        'method': 'POST',
        'description': 'Process emails with comprehensive entity creation',
        'request_example': {
            'email_data': [
                {
                    'subject': 'Meeting with Client',
                    'body': 'Hi, let\'s schedule a meeting to discuss the project.',
                    'sender': 'client@company.com',
                    'date': '2024-01-15T10:00:00Z'
                }
            ],
            'real_time': True,
            'batch_size': 10
        },
        'response_example': {
            'success': True,
            'data': {
                'total_processed': 1,
                'entities_created': {
                    'people': 1,
                    'topics': 2,
                    'tasks': 1
                },
                'processing_time': 2.5
            }
        }
    },
    '/api/entities/people': {
        'method': 'GET',
        'description': 'Get all people entities with optional filtering',
        'parameters': {
            'limit': 100,
            'search': 'john',
            'company': 'TechCorp',
            'include_relationships': True
        },
        'response_example': {
            'success': True,
            'data': {
                'people': [
                    {
                        'id': 1,
                        'name': 'John Doe',
                        'email_address': 'john@techcorp.com',
                        'company': 'TechCorp',
                        'importance_level': 0.8,
                        'relationships': []
                    }
                ],
                'pagination': {
                    'total_count': 1,
                    'has_more': False
                }
            }
        }
    }
}

# =====================================================================
# API DOCUMENTATION ENDPOINTS
# =====================================================================

@docs_api_bp.route('/', methods=['GET'])
def api_documentation_home():
    """
    Serve the main API documentation page.
    """
    try:
        return render_template('api_docs.html', 
                             api_info=API_DOCUMENTATION['info'],
                             api_versions=API_VERSIONS,
                             current_version=CURRENT_VERSION)
    except Exception as e:
        # Fallback to JSON if template not available
        return jsonify({
            'success': True,
            'data': {
                'message': 'AI Chief of Staff API Documentation',
                'api_info': API_DOCUMENTATION['info'],
                'available_endpoints': [
                    '/api/docs/openapi.json - OpenAPI specification',
                    '/api/docs/endpoints - List all endpoints',
                    '/api/docs/examples - API usage examples',
                    '/api/docs/testing - Interactive API testing',
                    '/api/versions - API version information'
                ]
            }
        })

@docs_api_bp.route('/openapi.json', methods=['GET'])
def get_openapi_spec():
    """
    Generate OpenAPI 3.0 specification for the API.
    """
    try:
        openapi_spec = {
            'openapi': '3.0.0',
            'info': API_DOCUMENTATION['info'],
            'servers': API_DOCUMENTATION['servers'],
            'tags': API_DOCUMENTATION['tags'],
            'paths': _generate_openapi_paths(),
            'components': _generate_openapi_components(),
            'security': [
                {'BearerAuth': []}
            ]
        }
        
        return jsonify(openapi_spec)
        
    except Exception as e:
        logger.error(f"Error generating OpenAPI spec: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@docs_api_bp.route('/endpoints', methods=['GET'])
def list_api_endpoints():
    """
    List all available API endpoints with descriptions.
    """
    try:
        version = request.args.get('version', CURRENT_VERSION)
        include_deprecated = request.args.get('include_deprecated', 'false').lower() == 'true'
        
        endpoints = _get_api_endpoints(version, include_deprecated)
        
        return jsonify({
            'success': True,
            'data': {
                'version': version,
                'endpoints': endpoints,
                'total_count': len(endpoints),
                'include_deprecated': include_deprecated
            }
        })
        
    except Exception as e:
        logger.error(f"Error listing endpoints: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@docs_api_bp.route('/examples', methods=['GET'])
def get_api_examples():
    """
    Get API usage examples for different endpoints.
    """
    try:
        endpoint = request.args.get('endpoint')
        
        if endpoint:
            # Get specific endpoint example
            if endpoint in ENDPOINT_EXAMPLES:
                return jsonify({
                    'success': True,
                    'data': {
                        'endpoint': endpoint,
                        'example': ENDPOINT_EXAMPLES[endpoint]
                    }
                })
            else:
                return jsonify({
                    'success': False,
                    'error': f'No example available for endpoint: {endpoint}'
                }), 404
        else:
            # Get all examples
            return jsonify({
                'success': True,
                'data': {
                    'examples': ENDPOINT_EXAMPLES,
                    'available_endpoints': list(ENDPOINT_EXAMPLES.keys())
                }
            })
            
    except Exception as e:
        logger.error(f"Error getting examples: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@docs_api_bp.route('/testing', methods=['GET'])
def api_testing_interface():
    """
    Serve interactive API testing interface.
    """
    try:
        return render_template('api_testing.html',
                             api_info=API_DOCUMENTATION['info'],
                             endpoints=ENDPOINT_EXAMPLES,
                             api_versions=API_VERSIONS,
                             current_version=CURRENT_VERSION)
    except Exception:
        # Fallback to JSON interface description
        return jsonify({
            'success': True,
            'data': {
                'message': 'Interactive API Testing Interface',
                'usage': 'This endpoint provides an interactive interface to test API endpoints',
                'features': [
                    'Live API testing',
                    'Request/response examples',
                    'Authentication testing',
                    'Error handling demonstration',
                    'Response validation'
                ],
                'available_tests': list(ENDPOINT_EXAMPLES.keys())
            }
        })

@docs_api_bp.route('/test/<path:endpoint>', methods=['POST'])
def test_api_endpoint(endpoint: str):
    """
    Test a specific API endpoint with provided data.
    """
    try:
        test_data = request.get_json() or {}
        
        # This would make actual API calls to test endpoints
        # For now, return a mock testing response
        
        test_result = {
            'endpoint': f'/{endpoint}',
            'method': test_data.get('method', 'GET'),
            'test_data': test_data,
            'simulated_response': {
                'success': True,
                'message': 'This is a simulated test response',
                'actual_testing': 'Would require implementing actual API calls'
            },
            'validation': {
                'request_valid': True,
                'response_valid': True,
                'authentication_required': endpoint not in ['health', 'versions'],
                'estimated_response_time': '< 1s'
            }
        }
        
        return jsonify({
            'success': True,
            'data': test_result
        })
        
    except Exception as e:
        logger.error(f"Error testing endpoint {endpoint}: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# =====================================================================
# API SCHEMA AND VALIDATION ENDPOINTS
# =====================================================================

@docs_api_bp.route('/schema/<model_name>', methods=['GET'])
def get_data_schema(model_name: str):
    """
    Get JSON schema for data models.
    """
    try:
        schemas = _get_data_schemas()
        
        if model_name in schemas:
            return jsonify({
                'success': True,
                'data': {
                    'model': model_name,
                    'schema': schemas[model_name]
                }
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Schema not found for model: {model_name}',
                'available_schemas': list(schemas.keys())
            }), 404
            
    except Exception as e:
        logger.error(f"Error getting schema for {model_name}: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@docs_api_bp.route('/validate', methods=['POST'])
def validate_api_request():
    """
    Validate API request data against schemas.
    """
    try:
        validation_data = request.get_json() or {}
        endpoint = validation_data.get('endpoint')
        data = validation_data.get('data', {})
        
        if not endpoint:
            return jsonify({
                'success': False,
                'error': 'Endpoint is required for validation'
            }), 400
        
        # Perform validation (simplified implementation)
        validation_result = _validate_request_data(endpoint, data)
        
        return jsonify({
            'success': True,
            'data': validation_result
        })
        
    except Exception as e:
        logger.error(f"Error validating request: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# =====================================================================
# API STATISTICS AND MONITORING
# =====================================================================

@docs_api_bp.route('/stats', methods=['GET'])
def get_api_statistics():
    """
    Get API usage statistics and performance metrics.
    """
    try:
        # This would integrate with actual monitoring system
        # For now, return mock statistics
        
        stats = {
            'api_version': CURRENT_VERSION,
            'total_endpoints': len(_get_all_endpoints()),
            'endpoints_by_category': {
                'authentication': 8,
                'email_processing': 3,
                'task_management': 6,
                'entity_management': 12,
                'analytics': 15,
                'real_time': 8,
                'batch_processing': 7
            },
            'deprecation_status': {
                'deprecated_endpoints': 5,
                'current_endpoints': 54,
                'beta_endpoints': 0
            },
            'usage_statistics': {
                'daily_requests': 12847,
                'avg_response_time': '145ms',
                'success_rate': '99.2%',
                'most_used_endpoints': [
                    '/api/v2/emails/process',
                    '/api/v2/tasks',
                    '/api/auth/login'
                ]
            }
        }
        
        return jsonify({
            'success': True,
            'data': stats,
            'timestamp': datetime.utcnow().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error getting API statistics: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@docs_api_bp.route('/health-check', methods=['GET'])
def docs_health_check():
    """
    Health check for documentation endpoints.
    """
    try:
        health_status = {
            'status': 'healthy',
            'documentation': 'available',
            'openapi_spec': 'generated',
            'testing_interface': 'available',
            'examples': len(ENDPOINT_EXAMPLES),
            'timestamp': datetime.utcnow().isoformat()
        }
        
        return jsonify(health_status)
        
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.utcnow().isoformat()
        }), 500

# =====================================================================
# HELPER FUNCTIONS
# =====================================================================

def _generate_openapi_paths() -> Dict:
    """Generate OpenAPI paths specification"""
    paths = {}
    
    # Add key endpoints with basic documentation
    endpoints = [
        {
            'path': '/api/auth/login',
            'method': 'post',
            'summary': 'User Login',
            'description': 'Authenticate user and receive JWT tokens',
            'tags': ['Authentication']
        },
        {
            'path': '/api/v2/emails/process',
            'method': 'post',
            'summary': 'Process Emails',
            'description': 'Process emails with entity extraction',
            'tags': ['Email Processing']
        },
        {
            'path': '/api/entities/people',
            'method': 'get',
            'summary': 'List People',
            'description': 'Get all people entities',
            'tags': ['Entity Management']
        }
    ]
    
    for endpoint in endpoints:
        if endpoint['path'] not in paths:
            paths[endpoint['path']] = {}
        
        paths[endpoint['path']][endpoint['method']] = {
            'summary': endpoint['summary'],
            'description': endpoint['description'],
            'tags': endpoint['tags'],
            'responses': {
                '200': {
                    'description': 'Successful response',
                    'content': {
                        'application/json': {
                            'schema': {'type': 'object'}
                        }
                    }
                }
            }
        }
    
    return paths

def _generate_openapi_components() -> Dict:
    """Generate OpenAPI components specification"""
    return {
        'securitySchemes': {
            'BearerAuth': {
                'type': 'http',
                'scheme': 'bearer',
                'bearerFormat': 'JWT'
            }
        },
        'schemas': _get_data_schemas()
    }

def _get_data_schemas() -> Dict:
    """Get JSON schemas for data models"""
    return {
        'User': {
            'type': 'object',
            'properties': {
                'id': {'type': 'integer'},
                'email': {'type': 'string', 'format': 'email'},
                'full_name': {'type': 'string'},
                'role': {'type': 'string', 'enum': ['user', 'admin']}
            }
        },
        'Person': {
            'type': 'object',
            'properties': {
                'id': {'type': 'integer'},
                'name': {'type': 'string'},
                'email_address': {'type': 'string'},
                'company': {'type': 'string'},
                'importance_level': {'type': 'number'}
            }
        },
        'Task': {
            'type': 'object',
            'properties': {
                'id': {'type': 'integer'},
                'description': {'type': 'string'},
                'status': {'type': 'string'},
                'priority': {'type': 'string'},
                'due_date': {'type': 'string', 'format': 'date-time'}
            }
        }
    }

def _get_api_endpoints(version: str, include_deprecated: bool) -> List[Dict]:
    """Get list of API endpoints for specified version"""
    endpoints = [
        {
            'path': '/api/auth/login',
            'method': 'POST',
            'description': 'User authentication',
            'category': 'Authentication',
            'deprecated': False
        },
        {
            'path': '/api/v2/emails/process',
            'method': 'POST',
            'description': 'Process emails with entity extraction',
            'category': 'Email Processing',
            'deprecated': False
        },
        {
            'path': '/api/entities/people',
            'method': 'GET',
            'description': 'List people entities',
            'category': 'Entity Management',
            'deprecated': False
        },
        {
            'path': '/api/v1/process-email',
            'method': 'POST',
            'description': 'Legacy email processing',
            'category': 'Email Processing',
            'deprecated': True
        }
    ]
    
    if not include_deprecated:
        endpoints = [ep for ep in endpoints if not ep['deprecated']]
    
    return endpoints

def _get_all_endpoints() -> List[str]:
    """Get list of all available endpoints"""
    return [
        '/api/auth/login', '/api/auth/register', '/api/auth/refresh',
        '/api/v2/emails/process', '/api/v2/emails/normalize',
        '/api/v2/tasks', '/api/v2/tasks/from-email',
        '/api/entities/people', '/api/entities/topics',
        '/api/analytics/insights', '/api/analytics/email/patterns',
        '/api/realtime/start', '/api/realtime/stream/emails',
        '/api/batch/emails/process', '/api/batch/tasks/create'
    ]

def _validate_request_data(endpoint: str, data: Dict) -> Dict:
    """Validate request data for endpoint"""
    validation_result = {
        'endpoint': endpoint,
        'valid': True,
        'errors': [],
        'warnings': []
    }
    
    # Add validation logic based on endpoint
    if 'auth/login' in endpoint:
        if not data.get('email'):
            validation_result['errors'].append('Email is required')
            validation_result['valid'] = False
        if not data.get('password'):
            validation_result['errors'].append('Password is required')
            validation_result['valid'] = False
    
    return validation_result

# Export the blueprint
__all__ = ['docs_api_bp'] 
FILE: chief_of_staff_ai/api/__init__.py - Package initialization file

============================================================
FILE: chief_of_staff_ai/api/batch_endpoints.py
============================================================
# Batch Processing API Endpoints - Bulk Operations and Efficient Processing
# These endpoints provide batch processing capabilities for high-volume operations

import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timezone
from flask import Blueprint, request, jsonify, session, g
from functools import wraps
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# Import the integration manager and enhanced processors
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'chief_of_staff_ai'))

from processors.integration_manager import integration_manager
from models.database import get_db_manager
from config.settings import settings

logger = logging.getLogger(__name__)

# Create Blueprint
batch_api_bp = Blueprint('batch_api', __name__, url_prefix='/api/batch')

# =====================================================================
# AUTHENTICATION AND UTILITIES
# =====================================================================

def require_auth(f):
    """Decorator to require authentication for API endpoints"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if 'user_email' not in session or 'db_user_id' not in session:
            return jsonify({
                'success': False,
                'error': 'Authentication required',
                'code': 'AUTH_REQUIRED'
            }), 401
        
        g.user_id = session['db_user_id']
        g.user_email = session['user_email']
        
        return f(*args, **kwargs)
    return decorated_function

def success_response(data: Any, message: str = None) -> tuple:
    """Create standardized success response"""
    response = {
        'success': True,
        'data': data,
        'timestamp': datetime.utcnow().isoformat()
    }
    if message:
        response['message'] = message
    return jsonify(response), 200

def error_response(error: str, code: str = None, status_code: int = 400) -> tuple:
    """Create standardized error response"""
    response = {
        'success': False,
        'error': error,
        'timestamp': datetime.utcnow().isoformat()
    }
    if code:
        response['code'] = code
    return jsonify(response), status_code

# =====================================================================
# BATCH EMAIL PROCESSING ENDPOINTS
# =====================================================================

@batch_api_bp.route('/emails/process', methods=['POST'])
@require_auth
def batch_process_emails():
    """
    Process multiple emails in batch with configurable concurrency and options.
    """
    try:
        data = request.get_json() or {}
        
        # Batch configuration
        emails_data = data.get('emails', [])
        batch_size = data.get('batch_size', 10)
        max_workers = data.get('max_workers', 3)
        real_time = data.get('real_time', False)
        legacy_mode = data.get('legacy_mode', False)
        priority = data.get('priority', 5)
        
        if not emails_data:
            return error_response("No emails provided for batch processing", "NO_EMAILS")
        
        if len(emails_data) > 1000:
            return error_response("Batch size too large. Maximum 1000 emails per batch.", "BATCH_TOO_LARGE")
        
        logger.info(f"Starting batch email processing: {len(emails_data)} emails, batch_size={batch_size}, workers={max_workers}")
        
        # Process emails in batches
        start_time = time.time()
        batch_results = []
        total_processed = 0
        total_errors = 0
        
        # Split emails into batches
        email_batches = [emails_data[i:i + batch_size] for i in range(0, len(emails_data), batch_size)]
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit batch processing jobs
            future_to_batch = {
                executor.submit(
                    _process_email_batch_worker,
                    batch, g.user_id, real_time, legacy_mode, batch_idx
                ): batch_idx
                for batch_idx, batch in enumerate(email_batches)
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_batch):
                batch_idx = future_to_batch[future]
                try:
                    batch_result = future.result()
                    batch_results.append({
                        'batch_index': batch_idx,
                        'result': batch_result,
                        'processing_time': batch_result.get('processing_time', 0)
                    })
                    
                    total_processed += batch_result.get('processed_count', 0)
                    total_errors += batch_result.get('error_count', 0)
                    
                except Exception as e:
                    logger.error(f"Error in batch {batch_idx}: {str(e)}")
                    batch_results.append({
                        'batch_index': batch_idx,
                        'error': str(e)
                    })
                    total_errors += len(email_batches[batch_idx])
        
        total_time = time.time() - start_time
        
        # Compile comprehensive results
        batch_summary = {
            'total_emails': len(emails_data),
            'total_processed': total_processed,
            'total_errors': total_errors,
            'success_rate': (total_processed / len(emails_data)) * 100 if emails_data else 0,
            'total_processing_time': total_time,
            'avg_emails_per_second': len(emails_data) / total_time if total_time > 0 else 0,
            'batch_results': batch_results,
            'configuration': {
                'batch_size': batch_size,
                'max_workers': max_workers,
                'real_time': real_time,
                'legacy_mode': legacy_mode
            }
        }
        
        return success_response(batch_summary, f"Batch processing completed: {total_processed}/{len(emails_data)} emails processed")
        
    except Exception as e:
        logger.error(f"Error in batch email processing: {str(e)}")
        return error_response(f"Batch processing error: {str(e)}", "BATCH_ERROR", 500)

@batch_api_bp.route('/emails/normalize', methods=['POST'])
@require_auth
def batch_normalize_emails():
    """
    Normalize multiple emails in batch.
    """
    try:
        data = request.get_json() or {}
        emails_data = data.get('emails', [])
        batch_size = data.get('batch_size', 20)
        
        if not emails_data:
            return error_response("No emails provided for normalization", "NO_EMAILS")
        
        logger.info(f"Starting batch email normalization: {len(emails_data)} emails")
        
        # Process in batches for memory efficiency
        normalized_results = []
        error_results = []
        
        for i in range(0, len(emails_data), batch_size):
            batch = emails_data[i:i + batch_size]
            
            for email_data in batch:
                try:
                    result = integration_manager.data_normalizer.normalize_email_data(email_data)
                    normalized_results.append({
                        'original_index': i + batch.index(email_data),
                        'success': result.success,
                        'normalized_data': result.normalized_data if result.success else None,
                        'quality_score': result.quality_score,
                        'issues_found': result.issues_found
                    })
                except Exception as e:
                    error_results.append({
                        'original_index': i + batch.index(email_data),
                        'error': str(e)
                    })
        
        normalization_summary = {
            'total_emails': len(emails_data),
            'successfully_normalized': len([r for r in normalized_results if r['success']]),
            'normalization_errors': len(error_results),
            'processing_errors': len([r for r in normalized_results if not r['success']]),
            'average_quality_score': sum(r['quality_score'] for r in normalized_results if r['success']) / max(len([r for r in normalized_results if r['success']]), 1),
            'results': normalized_results,
            'errors': error_results
        }
        
        return success_response(normalization_summary, "Batch normalization completed")
        
    except Exception as e:
        logger.error(f"Error in batch email normalization: {str(e)}")
        return error_response(f"Batch normalization error: {str(e)}", "BATCH_NORMALIZE_ERROR", 500)

# =====================================================================
# BATCH TASK PROCESSING ENDPOINTS
# =====================================================================

@batch_api_bp.route('/tasks/create', methods=['POST'])
@require_auth
def batch_create_tasks():
    """
    Create multiple tasks in batch with entity relationships.
    """
    try:
        data = request.get_json() or {}
        tasks_data = data.get('tasks', [])
        
        if not tasks_data:
            return error_response("No tasks provided for creation", "NO_TASKS")
        
        if len(tasks_data) > 500:
            return error_response("Batch size too large. Maximum 500 tasks per batch.", "BATCH_TOO_LARGE")
        
        logger.info(f"Starting batch task creation: {len(tasks_data)} tasks")
        
        created_tasks = []
        error_tasks = []
        
        for idx, task_data in enumerate(tasks_data):
            try:
                if 'description' not in task_data:
                    error_tasks.append({
                        'index': idx,
                        'error': 'Missing required field: description',
                        'task_data': task_data
                    })
                    continue
                
                result = integration_manager.create_manual_task_complete(
                    task_description=task_data['description'],
                    user_id=g.user_id,
                    assignee_email=task_data.get('assignee_email'),
                    topic_names=task_data.get('topic_names', []),
                    project_name=task_data.get('project_name'),
                    due_date=datetime.fromisoformat(task_data['due_date']) if task_data.get('due_date') else None,
                    priority=task_data.get('priority', 'medium')
                )
                
                if result['success']:
                    created_tasks.append({
                        'index': idx,
                        'task_id': result['result']['task']['id'],
                        'description': result['result']['task']['description']
                    })
                else:
                    error_tasks.append({
                        'index': idx,
                        'error': result['error'],
                        'task_data': task_data
                    })
                    
            except Exception as e:
                error_tasks.append({
                    'index': idx,
                    'error': str(e),
                    'task_data': task_data
                })
        
        task_creation_summary = {
            'total_tasks': len(tasks_data),
            'successfully_created': len(created_tasks),
            'creation_errors': len(error_tasks),
            'success_rate': (len(created_tasks) / len(tasks_data)) * 100 if tasks_data else 0,
            'created_tasks': created_tasks,
            'errors': error_tasks
        }
        
        return success_response(task_creation_summary, f"Batch task creation completed: {len(created_tasks)}/{len(tasks_data)} tasks created")
        
    except Exception as e:
        logger.error(f"Error in batch task creation: {str(e)}")
        return error_response(f"Batch task creation error: {str(e)}", "BATCH_TASK_ERROR", 500)

@batch_api_bp.route('/tasks/update-status', methods=['POST'])
@require_auth
def batch_update_task_status():
    """
    Update status of multiple tasks in batch.
    """
    try:
        data = request.get_json() or {}
        task_updates = data.get('task_updates', [])
        
        if not task_updates:
            return error_response("No task updates provided", "NO_UPDATES")
        
        logger.info(f"Starting batch task status updates: {len(task_updates)} tasks")
        
        updated_tasks = []
        error_updates = []
        
        for idx, update_data in enumerate(task_updates):
            try:
                if 'task_id' not in update_data or 'status' not in update_data:
                    error_updates.append({
                        'index': idx,
                        'error': 'Missing required fields: task_id and status',
                        'update_data': update_data
                    })
                    continue
                
                result = integration_manager.task_processor.update_task_status(
                    update_data['task_id'],
                    update_data['status'],
                    g.user_id,
                    update_data.get('completion_notes')
                )
                
                if result['success']:
                    updated_tasks.append({
                        'index': idx,
                        'task_id': update_data['task_id'],
                        'new_status': update_data['status']
                    })
                else:
                    error_updates.append({
                        'index': idx,
                        'error': result['error'],
                        'update_data': update_data
                    })
                    
            except Exception as e:
                error_updates.append({
                    'index': idx,
                    'error': str(e),
                    'update_data': update_data
                })
        
        update_summary = {
            'total_updates': len(task_updates),
            'successfully_updated': len(updated_tasks),
            'update_errors': len(error_updates),
            'success_rate': (len(updated_tasks) / len(task_updates)) * 100 if task_updates else 0,
            'updated_tasks': updated_tasks,
            'errors': error_updates
        }
        
        return success_response(update_summary, f"Batch task updates completed: {len(updated_tasks)}/{len(task_updates)} tasks updated")
        
    except Exception as e:
        logger.error(f"Error in batch task updates: {str(e)}")
        return error_response(f"Batch task update error: {str(e)}", "BATCH_UPDATE_ERROR", 500)

# =====================================================================
# BATCH CALENDAR PROCESSING ENDPOINTS
# =====================================================================

@batch_api_bp.route('/calendar/process', methods=['POST'])
@require_auth
def batch_process_calendar_events():
    """
    Process multiple calendar events in batch.
    """
    try:
        data = request.get_json() or {}
        events_data = data.get('events', [])
        max_workers = data.get('max_workers', 2)  # Calendar processing is more intensive
        real_time = data.get('real_time', False)
        
        if not events_data:
            return error_response("No calendar events provided", "NO_EVENTS")
        
        if len(events_data) > 200:
            return error_response("Batch size too large. Maximum 200 events per batch.", "BATCH_TOO_LARGE")
        
        logger.info(f"Starting batch calendar processing: {len(events_data)} events")
        
        processed_events = []
        error_events = []
        
        # Process events with limited concurrency due to AI processing intensity
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_event = {
                executor.submit(
                    _process_calendar_event_worker,
                    event_data, g.user_id, real_time, idx
                ): idx
                for idx, event_data in enumerate(events_data)
            }
            
            for future in as_completed(future_to_event):
                idx = future_to_event[future]
                try:
                    result = future.result()
                    if result['success']:
                        processed_events.append({
                            'index': idx,
                            'event_id': result['result'].get('event_id'),
                            'title': result['result'].get('title')
                        })
                    else:
                        error_events.append({
                            'index': idx,
                            'error': result['error']
                        })
                except Exception as e:
                    error_events.append({
                        'index': idx,
                        'error': str(e)
                    })
        
        calendar_summary = {
            'total_events': len(events_data),
            'successfully_processed': len(processed_events),
            'processing_errors': len(error_events),
            'success_rate': (len(processed_events) / len(events_data)) * 100 if events_data else 0,
            'processed_events': processed_events,
            'errors': error_events
        }
        
        return success_response(calendar_summary, f"Batch calendar processing completed: {len(processed_events)}/{len(events_data)} events processed")
        
    except Exception as e:
        logger.error(f"Error in batch calendar processing: {str(e)}")
        return error_response(f"Batch calendar processing error: {str(e)}", "BATCH_CALENDAR_ERROR", 500)

# =====================================================================
# BATCH ANALYTICS ENDPOINTS
# =====================================================================

@batch_api_bp.route('/analytics/generate-insights', methods=['POST'])
@require_auth
def batch_generate_insights():
    """
    Generate analytics and insights for multiple users or time periods.
    """
    try:
        data = request.get_json() or {}
        analysis_requests = data.get('requests', [])
        
        if not analysis_requests:
            # Default to current user comprehensive analysis
            analysis_requests = [{'type': 'comprehensive', 'user_id': g.user_id}]
        
        logger.info(f"Starting batch analytics generation: {len(analysis_requests)} requests")
        
        analytics_results = []
        error_results = []
        
        for idx, request_data in enumerate(analysis_requests):
            try:
                analysis_type = request_data.get('type', 'comprehensive')
                target_user_id = request_data.get('user_id', g.user_id)
                
                # Ensure user can only analyze their own data (security check)
                if target_user_id != g.user_id:
                    error_results.append({
                        'index': idx,
                        'error': 'Cannot analyze data for other users',
                        'request': request_data
                    })
                    continue
                
                result = integration_manager.generate_user_insights(target_user_id, analysis_type)
                
                if result['success']:
                    analytics_results.append({
                        'index': idx,
                        'analysis_type': analysis_type,
                        'insights_count': len(result['result'].get('insights', [])),
                        'summary': result['result'].get('summary', {})
                    })
                else:
                    error_results.append({
                        'index': idx,
                        'error': result['error'],
                        'request': request_data
                    })
                    
            except Exception as e:
                error_results.append({
                    'index': idx,
                    'error': str(e),
                    'request': request_data
                })
        
        analytics_summary = {
            'total_requests': len(analysis_requests),
            'successfully_generated': len(analytics_results),
            'generation_errors': len(error_results),
            'success_rate': (len(analytics_results) / len(analysis_requests)) * 100 if analysis_requests else 0,
            'analytics_results': analytics_results,
            'errors': error_results
        }
        
        return success_response(analytics_summary, f"Batch analytics generation completed: {len(analytics_results)}/{len(analysis_requests)} analyses generated")
        
    except Exception as e:
        logger.error(f"Error in batch analytics generation: {str(e)}")
        return error_response(f"Batch analytics error: {str(e)}", "BATCH_ANALYTICS_ERROR", 500)

# =====================================================================
# BATCH ENTITY MANAGEMENT ENDPOINTS
# =====================================================================

@batch_api_bp.route('/entities/create', methods=['POST'])
@require_auth
def batch_create_entities():
    """
    Create multiple entities and their relationships in batch.
    """
    try:
        data = request.get_json() or {}
        entities_data = data.get('entities', [])
        create_relationships = data.get('create_relationships', True)
        
        if not entities_data:
            return error_response("No entities provided for creation", "NO_ENTITIES")
        
        logger.info(f"Starting batch entity creation: {len(entities_data)} entities")
        
        created_entities = []
        error_entities = []
        
        from processors.unified_entity_engine import EntityContext
        
        context = EntityContext(
            source_type='batch_manual',
            user_id=g.user_id,
            confidence=1.0
        )
        
        for idx, entity_data in enumerate(entities_data):
            try:
                entity_type = entity_data.get('type')
                
                if entity_type == 'person':
                    entity = integration_manager.entity_engine.create_or_update_person(
                        email=entity_data.get('email', ''),
                        name=entity_data.get('name', ''),
                        company=entity_data.get('company', ''),
                        context=context,
                        bio=entity_data.get('bio', ''),
                        title=entity_data.get('title', ''),
                        linkedin_url=entity_data.get('linkedin_url', '')
                    )
                    
                elif entity_type == 'topic':
                    entity = integration_manager.entity_engine.create_or_update_topic(
                        topic_name=entity_data.get('name', ''),
                        description=entity_data.get('description', ''),
                        context=context
                    )
                    
                else:
                    error_entities.append({
                        'index': idx,
                        'error': f'Unsupported entity type: {entity_type}',
                        'entity_data': entity_data
                    })
                    continue
                
                if entity:
                    created_entities.append({
                        'index': idx,
                        'entity_type': entity_type,
                        'entity_id': entity.id,
                        'name': getattr(entity, 'name', 'Unknown')
                    })
                else:
                    error_entities.append({
                        'index': idx,
                        'error': 'Failed to create entity',
                        'entity_data': entity_data
                    })
                    
            except Exception as e:
                error_entities.append({
                    'index': idx,
                    'error': str(e),
                    'entity_data': entity_data
                })
        
        entity_creation_summary = {
            'total_entities': len(entities_data),
            'successfully_created': len(created_entities),
            'creation_errors': len(error_entities),
            'success_rate': (len(created_entities) / len(entities_data)) * 100 if entities_data else 0,
            'created_entities': created_entities,
            'errors': error_entities
        }
        
        return success_response(entity_creation_summary, f"Batch entity creation completed: {len(created_entities)}/{len(entities_data)} entities created")
        
    except Exception as e:
        logger.error(f"Error in batch entity creation: {str(e)}")
        return error_response(f"Batch entity creation error: {str(e)}", "BATCH_ENTITY_ERROR", 500)

# =====================================================================
# BATCH STATUS AND MONITORING ENDPOINTS
# =====================================================================

@batch_api_bp.route('/status', methods=['GET'])
@require_auth
def get_batch_processing_status():
    """
    Get current status of all batch processing operations.
    """
    try:
        # This would integrate with a job queue system in production
        # For now, return static status information
        status_info = {
            'batch_processing_available': True,
            'max_batch_sizes': {
                'emails': 1000,
                'tasks': 500,
                'calendar_events': 200,
                'entities': 500
            },
            'default_batch_sizes': {
                'emails': 10,
                'tasks': 20,
                'calendar_events': 5,
                'entities': 25
            },
            'max_workers': {
                'emails': 3,
                'tasks': 4,
                'calendar_events': 2,
                'entities': 3
            },
            'processing_statistics': integration_manager.get_processing_statistics()['result'] if integration_manager.get_processing_statistics()['success'] else {}
        }
        
        return success_response(status_info, "Batch processing status retrieved")
        
    except Exception as e:
        logger.error(f"Error getting batch status: {str(e)}")
        return error_response(f"Status error: {str(e)}", "STATUS_ERROR", 500)

@batch_api_bp.route('/health', methods=['GET'])
def batch_health_check():
    """
    Health check for batch processing endpoints.
    """
    try:
        health_status = {
            'status': 'healthy',
            'batch_endpoints': 'available',
            'integration_manager': 'connected',
            'timestamp': datetime.utcnow().isoformat()
        }
        
        # Test integration manager connection
        try:
            integration_manager.get_processing_statistics()
        except Exception as e:
            health_status['status'] = 'degraded'
            health_status['integration_manager'] = f'error: {str(e)}'
        
        status_code = 200 if health_status['status'] == 'healthy' else 503
        return jsonify(health_status), status_code
        
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.utcnow().isoformat()
        }), 500

# =====================================================================
# WORKER FUNCTIONS FOR BATCH PROCESSING
# =====================================================================

def _process_email_batch_worker(email_batch: List[Dict], user_id: int, real_time: bool, legacy_mode: bool, batch_idx: int) -> Dict:
    """
    Worker function to process a batch of emails.
    """
    try:
        start_time = time.time()
        
        result = integration_manager.process_email_batch(
            email_batch, user_id, len(email_batch), real_time
        )
        
        processing_time = time.time() - start_time
        
        return {
            'batch_index': batch_idx,
            'success': result['success'],
            'processed_count': len(email_batch) if result['success'] else 0,
            'error_count': 0 if result['success'] else len(email_batch),
            'processing_time': processing_time,
            'details': result['result'] if result['success'] else result['error']
        }
        
    except Exception as e:
        logger.error(f"Error in email batch worker {batch_idx}: {str(e)}")
        return {
            'batch_index': batch_idx,
            'success': False,
            'processed_count': 0,
            'error_count': len(email_batch),
            'processing_time': 0,
            'error': str(e)
        }

def _process_calendar_event_worker(event_data: Dict, user_id: int, real_time: bool, idx: int) -> Dict:
    """
    Worker function to process a single calendar event.
    """
    try:
        result = integration_manager.process_calendar_event_complete(
            event_data, user_id, real_time
        )
        
        return {
            'index': idx,
            'success': result['success'],
            'result': result['result'] if result['success'] else None,
            'error': result['error'] if not result['success'] else None
        }
        
    except Exception as e:
        logger.error(f"Error in calendar event worker {idx}: {str(e)}")
        return {
            'index': idx,
            'success': False,
            'result': None,
            'error': str(e)
        }

# Export the blueprint
__all__ = ['batch_api_bp'] 

============================================================
FILE: chief_of_staff_ai/api/versioning.py
============================================================
# API Versioning and Backward Compatibility System
# This module provides comprehensive API versioning, deprecation management, and backward compatibility

import logging
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime, timezone
from flask import Blueprint, request, jsonify, g
from functools import wraps
import json
import re
from packaging import version

logger = logging.getLogger(__name__)

# =====================================================================
# API VERSION CONFIGURATION
# =====================================================================

API_VERSIONS = {
    'v1': {
        'version': '1.0.0',
        'status': 'deprecated',
        'supported_until': '2024-12-31',
        'description': 'Legacy API endpoints',
        'prefix': '/api/v1',
        'features': ['basic_email_processing', 'task_extraction'],
        'deprecation_warnings': True
    },
    'v2': {
        'version': '2.0.0',
        'status': 'current',
        'supported_until': None,
        'description': 'Enhanced entity-centric API',
        'prefix': '/api/v2',
        'features': ['entity_processing', 'real_time', 'analytics', 'batch_processing'],
        'deprecation_warnings': False
    },
    'v3': {
        'version': '3.0.0',
        'status': 'beta',
        'supported_until': None,
        'description': 'Next-generation AI Chief of Staff API',
        'prefix': '/api/v3',
        'features': ['ai_agents', 'predictive_intelligence', 'auto_optimization'],
        'deprecation_warnings': False
    }
}

CURRENT_VERSION = 'v2'
DEFAULT_VERSION = 'v2'

# =====================================================================
# VERSION DETECTION AND VALIDATION
# =====================================================================

def get_api_version_from_request() -> str:
    """
    Determine API version from request headers or URL.
    Priority: Header > URL > Default
    """
    # Check for version in headers
    version_header = request.headers.get('API-Version', '').strip()
    if version_header and version_header in API_VERSIONS:
        return version_header
    
    # Check for version in Accept header
    accept_header = request.headers.get('Accept', '')
    version_match = re.search(r'application/vnd\.chief-of-staff\.v(\d+)', accept_header)
    if version_match:
        api_version = f"v{version_match.group(1)}"
        if api_version in API_VERSIONS:
            return api_version
    
    # Check URL path for version
    path = request.path
    version_match = re.search(r'/api/v(\d+)/', path)
    if version_match:
        api_version = f"v{version_match.group(1)}"
        if api_version in API_VERSIONS:
            return api_version
    
    # Default version
    return DEFAULT_VERSION

def validate_api_version(api_version: str) -> tuple[bool, Optional[str]]:
    """
    Validate if API version is supported.
    Returns (is_valid, error_message)
    """
    if api_version not in API_VERSIONS:
        return False, f"Unsupported API version: {api_version}. Supported versions: {', '.join(API_VERSIONS.keys())}"
    
    version_info = API_VERSIONS[api_version]
    
    if version_info['status'] == 'deprecated':
        # Check if still within support period
        if version_info.get('supported_until'):
            support_end = datetime.fromisoformat(version_info['supported_until'] + 'T23:59:59')
            if datetime.utcnow() > support_end:
                return False, f"API version {api_version} is no longer supported. Please upgrade to v{CURRENT_VERSION}."
    
    return True, None

def get_version_features(api_version: str) -> List[str]:
    """Get list of features available in the specified API version"""
    return API_VERSIONS.get(api_version, {}).get('features', [])

def is_feature_available(api_version: str, feature: str) -> bool:
    """Check if a feature is available in the specified API version"""
    return feature in get_version_features(api_version)

# =====================================================================
# VERSION DECORATORS
# =====================================================================

def api_version(min_version: str = None, max_version: str = None, 
                required_features: List[str] = None, deprecated_in: str = None):
    """
    Decorator to specify API version requirements for endpoints.
    
    Args:
        min_version: Minimum API version required
        max_version: Maximum API version supported
        required_features: List of features required for this endpoint
        deprecated_in: Version in which this endpoint was deprecated
    """
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            # Get and validate API version
            api_version = get_api_version_from_request()
            is_valid, error_msg = validate_api_version(api_version)
            
            if not is_valid:
                return jsonify({
                    'success': False,
                    'error': error_msg,
                    'code': 'UNSUPPORTED_API_VERSION',
                    'supported_versions': list(API_VERSIONS.keys()),
                    'current_version': CURRENT_VERSION
                }), 400
            
            # Check version range
            if min_version and version.parse(API_VERSIONS[api_version]['version']) < version.parse(API_VERSIONS[min_version]['version']):
                return jsonify({
                    'success': False,
                    'error': f"This endpoint requires API version {min_version} or higher",
                    'code': 'VERSION_TOO_LOW',
                    'current_request_version': api_version,
                    'minimum_required': min_version
                }), 400
            
            if max_version and version.parse(API_VERSIONS[api_version]['version']) > version.parse(API_VERSIONS[max_version]['version']):
                return jsonify({
                    'success': False,
                    'error': f"This endpoint is not available in API version {api_version}",
                    'code': 'VERSION_TOO_HIGH',
                    'current_request_version': api_version,
                    'maximum_supported': max_version
                }), 400
            
            # Check required features
            if required_features:
                available_features = get_version_features(api_version)
                missing_features = [f for f in required_features if f not in available_features]
                
                if missing_features:
                    return jsonify({
                        'success': False,
                        'error': f"Required features not available in {api_version}: {', '.join(missing_features)}",
                        'code': 'MISSING_FEATURES',
                        'required_features': required_features,
                        'available_features': available_features
                    }), 400
            
            # Store version info in g
            g.api_version = api_version
            g.api_version_info = API_VERSIONS[api_version]
            
            # Add deprecation warning if needed
            response_headers = {}
            if deprecated_in and version.parse(API_VERSIONS[api_version]['version']) >= version.parse(API_VERSIONS[deprecated_in]['version']):
                response_headers['Deprecation'] = 'true'
                response_headers['Sunset'] = API_VERSIONS[api_version].get('supported_until', 'TBD')
                response_headers['Link'] = f'</api/{CURRENT_VERSION}>; rel="successor-version"'
            
            # Execute the function
            result = f(*args, **kwargs)
            
            # Add version headers to response
            if isinstance(result, tuple) and len(result) == 2:
                response, status_code = result
                if hasattr(response, 'headers'):
                    for header, value in response_headers.items():
                        response.headers[header] = value
                    response.headers['API-Version'] = api_version
                    response.headers['API-Current-Version'] = CURRENT_VERSION
                return response, status_code
            
            return result
            
        return decorated_function
    return decorator

def requires_feature(feature_name: str):
    """Decorator to require a specific feature to be available in the API version"""
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            api_version = getattr(g, 'api_version', get_api_version_from_request())
            
            if not is_feature_available(api_version, feature_name):
                return jsonify({
                    'success': False,
                    'error': f"Feature '{feature_name}' is not available in API version {api_version}",
                    'code': 'FEATURE_NOT_AVAILABLE',
                    'required_feature': feature_name,
                    'available_features': get_version_features(api_version)
                }), 400
            
            return f(*args, **kwargs)
        return decorated_function
    return decorator

def deprecated(since_version: str, remove_in_version: str = None, alternative: str = None):
    """
    Decorator to mark endpoints as deprecated.
    
    Args:
        since_version: Version when deprecation started
        remove_in_version: Version when endpoint will be removed
        alternative: Suggested alternative endpoint
    """
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            api_version = getattr(g, 'api_version', get_api_version_from_request())
            
            # Add deprecation warning to response
            result = f(*args, **kwargs)
            
            deprecation_message = f"This endpoint is deprecated since API version {since_version}."
            if remove_in_version:
                deprecation_message += f" It will be removed in version {remove_in_version}."
            if alternative:
                deprecation_message += f" Please use {alternative} instead."
            
            # Handle different response types
            if isinstance(result, tuple) and len(result) == 2:
                response_data, status_code = result
                
                # Add deprecation info to JSON response
                if hasattr(response_data, 'get_json'):
                    json_data = response_data.get_json() or {}
                    if isinstance(json_data, dict):
                        json_data['deprecation_warning'] = deprecation_message
                        response_data.data = json.dumps(json_data)
                
                # Add deprecation headers
                if hasattr(response_data, 'headers'):
                    response_data.headers['Deprecation'] = 'true'
                    response_data.headers['Sunset'] = API_VERSIONS.get(remove_in_version, {}).get('supported_until', 'TBD')
                    if alternative:
                        response_data.headers['Link'] = f'<{alternative}>; rel="alternate"'
                
                return response_data, status_code
            
            return result
            
        return decorated_function
    return decorator

# =====================================================================
# BACKWARD COMPATIBILITY HANDLERS
# =====================================================================

class BackwardCompatibilityMapper:
    """Maps old API endpoints to new ones with data transformation"""
    
    def __init__(self):
        self.mappings = {}
        self.transformers = {}
    
    def register_mapping(self, old_endpoint: str, new_endpoint: str, 
                        request_transformer: Callable = None, 
                        response_transformer: Callable = None):
        """
        Register a mapping from old endpoint to new endpoint.
        
        Args:
            old_endpoint: Old API endpoint pattern
            new_endpoint: New API endpoint pattern
            request_transformer: Function to transform request data
            response_transformer: Function to transform response data
        """
        self.mappings[old_endpoint] = new_endpoint
        self.transformers[old_endpoint] = {
            'request': request_transformer,
            'response': response_transformer
        }
    
    def get_mapping(self, endpoint: str) -> Optional[Dict]:
        """Get mapping information for an endpoint"""
        for old_endpoint, new_endpoint in self.mappings.items():
            if re.match(old_endpoint.replace('*', '.*'), endpoint):
                return {
                    'new_endpoint': new_endpoint,
                    'transformers': self.transformers.get(old_endpoint, {})
                }
        return None

# Global compatibility mapper
compatibility_mapper = BackwardCompatibilityMapper()

# =====================================================================
# V1 TO V2 COMPATIBILITY MAPPINGS
# =====================================================================

def transform_v1_email_request(data: Dict) -> Dict:
    """Transform V1 email processing request to V2 format"""
    if not data:
        return data
    
    # V1 used 'email_content', V2 uses 'email_data'
    if 'email_content' in data:
        data['email_data'] = data.pop('email_content')
    
    # V1 used 'extract_tasks', V2 uses 'real_time'
    if 'extract_tasks' in data:
        data['real_time'] = data.pop('extract_tasks')
    
    # Add default V2 parameters
    data.setdefault('batch_size', 10)
    data.setdefault('legacy_mode', True)
    
    return data

def transform_v1_email_response(data: Dict) -> Dict:
    """Transform V2 email processing response to V1 format"""
    if not data or not isinstance(data, dict):
        return data
    
    # V1 expected simpler structure
    if 'result' in data and isinstance(data['result'], dict):
        result = data['result']
        
        # Extract key V1 fields
        v1_response = {
            'success': data.get('success', True),
            'processed_emails': result.get('total_processed', 0),
            'extracted_tasks': result.get('tasks_created', 0),
            'processing_time': result.get('total_processing_time', 0)
        }
        
        # Add tasks if available
        if 'tasks' in result:
            v1_response['tasks'] = result['tasks']
        
        return v1_response
    
    return data

def transform_v1_task_request(data: Dict) -> Dict:
    """Transform V1 task creation request to V2 format"""
    if not data:
        return data
    
    # V1 used 'task_text', V2 uses 'description'
    if 'task_text' in data:
        data['description'] = data.pop('task_text')
    
    # V1 used 'assigned_to', V2 uses 'assignee_email'
    if 'assigned_to' in data:
        data['assignee_email'] = data.pop('assigned_to')
    
    return data

# Register compatibility mappings
compatibility_mapper.register_mapping(
    r'/api/v1/process-email',
    '/api/v2/emails/process',
    request_transformer=transform_v1_email_request,
    response_transformer=transform_v1_email_response
)

compatibility_mapper.register_mapping(
    r'/api/v1/extract-tasks',
    '/api/v2/tasks/from-email',
    request_transformer=transform_v1_email_request,
    response_transformer=transform_v1_email_response
)

compatibility_mapper.register_mapping(
    r'/api/v1/create-task',
    '/api/v2/tasks',
    request_transformer=transform_v1_task_request
)

# =====================================================================
# VERSION MANAGEMENT ENDPOINTS
# =====================================================================

# Create versioning blueprint
versioning_bp = Blueprint('versioning', __name__, url_prefix='/api')

@versioning_bp.route('/versions', methods=['GET'])
def get_api_versions():
    """Get information about all available API versions"""
    try:
        versions_info = {}
        
        for version_key, version_data in API_VERSIONS.items():
            versions_info[version_key] = {
                'version': version_data['version'],
                'status': version_data['status'],
                'description': version_data['description'],
                'prefix': version_data['prefix'],
                'features': version_data['features'],
                'supported_until': version_data.get('supported_until'),
                'is_current': version_key == CURRENT_VERSION,
                'is_default': version_key == DEFAULT_VERSION
            }
        
        return jsonify({
            'success': True,
            'data': {
                'versions': versions_info,
                'current_version': CURRENT_VERSION,
                'default_version': DEFAULT_VERSION,
                'version_selection_methods': [
                    'Header: API-Version',
                    'Accept: application/vnd.chief-of-staff.v{N}+json',
                    'URL: /api/v{N}/'
                ]
            },
            'timestamp': datetime.utcnow().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"Error getting API versions: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.utcnow().isoformat()
        }), 500

@versioning_bp.route('/version', methods=['GET'])
def get_current_version():
    """Get current API version information"""
    try:
        api_version = get_api_version_from_request()
        version_info = API_VERSIONS.get(api_version, {})
        
        return jsonify({
            'success': True,
            'data': {
                'detected_version': api_version,
                'version_info': version_info,
                'current_version': CURRENT_VERSION,
                'is_deprecated': version_info.get('status') == 'deprecated',
                'features_available': version_info.get('features', [])
            },
            'timestamp': datetime.utcnow().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"Error getting current version: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.utcnow().isoformat()
        }), 500

@versioning_bp.route('/migration-guide', methods=['GET'])
def get_migration_guide():
    """Get migration guide for upgrading between API versions"""
    try:
        from_version = request.args.get('from', 'v1')
        to_version = request.args.get('to', CURRENT_VERSION)
        
        if from_version not in API_VERSIONS or to_version not in API_VERSIONS:
            return jsonify({
                'success': False,
                'error': 'Invalid version specified',
                'available_versions': list(API_VERSIONS.keys())
            }), 400
        
        migration_guide = {
            'from_version': from_version,
            'to_version': to_version,
            'breaking_changes': _get_breaking_changes(from_version, to_version),
            'new_features': _get_new_features(from_version, to_version),
            'deprecated_endpoints': _get_deprecated_endpoints(from_version),
            'endpoint_mappings': _get_endpoint_mappings(from_version, to_version),
            'migration_steps': _get_migration_steps(from_version, to_version)
        }
        
        return jsonify({
            'success': True,
            'data': migration_guide,
            'timestamp': datetime.utcnow().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"Error getting migration guide: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.utcnow().isoformat()
        }), 500

# =====================================================================
# MIGRATION GUIDE HELPERS
# =====================================================================

def _get_breaking_changes(from_version: str, to_version: str) -> List[Dict]:
    """Get list of breaking changes between versions"""
    breaking_changes = []
    
    if from_version == 'v1' and to_version in ['v2', 'v3']:
        breaking_changes.extend([
            {
                'type': 'request_format',
                'description': 'Email processing request format changed',
                'old_format': '{"email_content": "...", "extract_tasks": true}',
                'new_format': '{"email_data": "...", "real_time": true}',
                'impact': 'medium'
            },
            {
                'type': 'response_format',
                'description': 'Response structure is now more detailed',
                'old_format': '{"success": true, "tasks": [...]}',
                'new_format': '{"success": true, "data": {"result": {...}}}',
                'impact': 'high'
            },
            {
                'type': 'authentication',
                'description': 'JWT authentication now required for most endpoints',
                'old_format': 'Session-based authentication',
                'new_format': 'Bearer token in Authorization header',
                'impact': 'high'
            }
        ])
    
    return breaking_changes

def _get_new_features(from_version: str, to_version: str) -> List[Dict]:
    """Get list of new features available in target version"""
    new_features = []
    
    if from_version == 'v1' and to_version in ['v2', 'v3']:
        new_features.extend([
            {
                'feature': 'entity_processing',
                'description': 'Entity-centric processing with relationships',
                'endpoints': ['/api/v2/entities/*']
            },
            {
                'feature': 'real_time',
                'description': 'Real-time processing and WebSocket support',
                'endpoints': ['/api/realtime/*']
            },
            {
                'feature': 'analytics',
                'description': 'Comprehensive analytics and insights',
                'endpoints': ['/api/analytics/*']
            },
            {
                'feature': 'batch_processing',
                'description': 'Batch processing for high-volume operations',
                'endpoints': ['/api/batch/*']
            }
        ])
    
    return new_features

def _get_deprecated_endpoints(version: str) -> List[Dict]:
    """Get list of deprecated endpoints in the specified version"""
    deprecated = []
    
    if version == 'v1':
        deprecated.extend([
            {
                'endpoint': '/api/v1/process-email',
                'alternative': '/api/v2/emails/process',
                'removal_date': '2024-12-31'
            },
            {
                'endpoint': '/api/v1/extract-tasks',
                'alternative': '/api/v2/tasks/from-email',
                'removal_date': '2024-12-31'
            }
        ])
    
    return deprecated

def _get_endpoint_mappings(from_version: str, to_version: str) -> Dict:
    """Get endpoint mappings between versions"""
    mappings = {}
    
    if from_version == 'v1' and to_version == 'v2':
        mappings = {
            '/api/v1/process-email': '/api/v2/emails/process',
            '/api/v1/extract-tasks': '/api/v2/tasks/from-email',
            '/api/v1/create-task': '/api/v2/tasks',
            '/api/v1/get-tasks': '/api/v2/tasks',
            '/api/v1/health': '/api/v2/health'
        }
    
    return mappings

def _get_migration_steps(from_version: str, to_version: str) -> List[Dict]:
    """Get step-by-step migration instructions"""
    steps = []
    
    if from_version == 'v1' and to_version == 'v2':
        steps = [
            {
                'step': 1,
                'title': 'Update Authentication',
                'description': 'Implement JWT authentication using /api/auth/login',
                'code_example': 'Authorization: Bearer <jwt_token>'
            },
            {
                'step': 2,
                'title': 'Update Request Formats',
                'description': 'Change request field names according to breaking changes',
                'code_example': '{"email_data": "...", "real_time": true}'
            },
            {
                'step': 3,
                'title': 'Update Response Handling',
                'description': 'Handle new response structure with data wrapper',
                'code_example': 'response.data.result instead of response directly'
            },
            {
                'step': 4,
                'title': 'Leverage New Features',
                'description': 'Optionally integrate entity processing and analytics',
                'code_example': 'Use /api/v2/entities/* for relationship management'
            }
        ]
    
    return steps

# =====================================================================
# UTILITY FUNCTIONS
# =====================================================================

def add_version_headers(response, api_version: str = None):
    """Add version-related headers to response"""
    if api_version is None:
        api_version = get_api_version_from_request()
    
    if hasattr(response, 'headers'):
        response.headers['API-Version'] = api_version
        response.headers['API-Current-Version'] = CURRENT_VERSION
        response.headers['API-Supported-Versions'] = ', '.join(API_VERSIONS.keys())
        
        # Add deprecation warning if needed
        version_info = API_VERSIONS.get(api_version, {})
        if version_info.get('status') == 'deprecated':
            response.headers['Deprecation'] = 'true'
            if version_info.get('supported_until'):
                response.headers['Sunset'] = version_info['supported_until']
    
    return response

def get_version_from_url(url: str) -> Optional[str]:
    """Extract API version from URL"""
    match = re.search(r'/api/v(\d+)/', url)
    if match:
        return f"v{match.group(1)}"
    return None

# Export all components
__all__ = [
    'api_version',
    'requires_feature', 
    'deprecated',
    'get_api_version_from_request',
    'validate_api_version',
    'get_version_features',
    'is_feature_available',
    'compatibility_mapper',
    'versioning_bp',
    'add_version_headers',
    'API_VERSIONS',
    'CURRENT_VERSION'
] 

============================================================
FILE: chief_of_staff_ai/monitoring/realtime_server.py
============================================================
import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Set, Optional
import websockets
import ssl
from dataclasses import dataclass, asdict
from enum import Enum
import uuid
import time
from collections import defaultdict
import threading

logger = logging.getLogger(__name__)

class EventType(Enum):
    AGENT_STATUS_UPDATE = "agent_status_update"
    WORKFLOW_STARTED = "workflow_started"
    WORKFLOW_PROGRESS = "workflow_progress"
    WORKFLOW_COMPLETED = "workflow_completed"
    SECURITY_ALERT = "security_alert"
    PERFORMANCE_METRIC = "performance_metric"
    SYSTEM_HEALTH = "system_health"
    USER_ACTIVITY = "user_activity"
    ERROR_OCCURRED = "error_occurred"

@dataclass
class RealtimeEvent:
    event_id: str
    event_type: EventType
    timestamp: datetime
    data: Dict
    user_id: Optional[str] = None
    agent_type: Optional[str] = None
    priority: int = 1  # 1=low, 5=critical

class RealtimeConnection:
    """Represents a WebSocket connection with subscription preferences"""
    
    def __init__(self, websocket, user_id: str):
        self.websocket = websocket
        self.user_id = user_id
        self.connection_id = str(uuid.uuid4())
        self.connected_at = datetime.now()
        self.last_ping = datetime.now()
        self.subscriptions: Set[EventType] = set()
        self.is_admin = False
        self.rate_limit_counter = 0
        self.last_rate_limit_reset = time.time()

class RealtimeMonitoringServer:
    """
    Advanced real-time monitoring server for AI Chief of Staff
    
    Features:
    - Real-time WebSocket connections for all agent activities
    - Multi-channel subscriptions with filtering
    - Advanced performance monitoring and analytics
    - Security event streaming
    - Auto-scaling WebSocket management
    - Historical data streaming
    - Rate limiting and abuse protection
    - Admin dashboard streaming
    """
    
    def __init__(self, host: str = "localhost", port: int = 8765):
        self.host = host
        self.port = port
        self.connections: Dict[str, RealtimeConnection] = {}
        self.event_history: List[RealtimeEvent] = []
        self.event_queue = asyncio.Queue()
        
        # Performance metrics
        self.metrics = {
            'total_connections': 0,
            'active_connections': 0,
            'events_sent': 0,
            'events_per_second': 0,
            'data_throughput': 0,
            'connection_errors': 0,
            'last_metric_reset': time.time()
        }
        
        # Rate limiting
        self.max_events_per_second_per_user = 50
        self.max_connections_per_user = 5
        
        # Event filtering and aggregation
        self.event_aggregators = defaultdict(list)
        self.batch_size = 10
        self.batch_timeout = 1.0  # seconds
        
        # Health monitoring
        self.server_health = {
            'status': 'starting',
            'uptime': 0,
            'memory_usage': 0,
            'cpu_usage': 0,
            'connection_quality': 'good'
        }
        
        # Background tasks
        self.cleanup_task = None
        self.metrics_task = None
        self.health_task = None
        
        logger.info(f" Real-time Monitoring Server initializing on {host}:{port}")

    async def start_server(self):
        """Start the WebSocket server with production configurations"""
        
        try:
            # SSL configuration for production
            ssl_context = None
            if hasattr(self, 'ssl_cert_path') and hasattr(self, 'ssl_key_path'):
                ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
                ssl_context.load_cert_chain(self.ssl_cert_path, self.ssl_key_path)
            
            # Start WebSocket server
            server = await websockets.serve(
                self.handle_connection,
                self.host,
                self.port,
                ssl=ssl_context,
                ping_interval=30,
                ping_timeout=10,
                max_size=10**6,  # 1MB max message size
                max_queue=100,   # Max queued messages
                compression="deflate"
            )
            
            # Start background tasks
            self.cleanup_task = asyncio.create_task(self.cleanup_connections())
            self.metrics_task = asyncio.create_task(self.update_metrics())
            self.health_task = asyncio.create_task(self.monitor_health())
            
            # Start event processing
            asyncio.create_task(self.process_event_queue())
            
            self.server_health['status'] = 'running'
            logger.info(f" Real-time Monitoring Server started on {self.host}:{self.port}")
            
            # Keep server running
            await server.wait_closed()
            
        except Exception as e:
            logger.error(f"Failed to start real-time server: {e}")
            self.server_health['status'] = 'error'
            raise

    async def handle_connection(self, websocket, path):
        """Handle new WebSocket connection with authentication and setup"""
        
        connection_id = None
        try:
            # Extract authentication from query params or headers
            user_id = await self.authenticate_connection(websocket, path)
            if not user_id:
                await websocket.close(4001, "Authentication required")
                return
            
            # Check connection limits
            user_connections = [c for c in self.connections.values() if c.user_id == user_id]
            if len(user_connections) >= self.max_connections_per_user:
                await websocket.close(4002, "Connection limit exceeded")
                return
            
            # Create connection object
            connection = RealtimeConnection(websocket, user_id)
            connection_id = connection.connection_id
            self.connections[connection_id] = connection
            self.metrics['total_connections'] += 1
            self.metrics['active_connections'] += 1
            
            logger.info(f" New real-time connection: {user_id} ({connection_id})")
            
            # Send welcome message with capabilities
            await self.send_to_connection(connection, {
                'type': 'connection_established',
                'connection_id': connection_id,
                'server_capabilities': {
                    'event_types': [e.value for e in EventType],
                    'max_rate': self.max_events_per_second_per_user,
                    'batch_support': True,
                    'filtering_support': True
                },
                'server_health': self.server_health
            })
            
            # Handle messages from client
            async for message in websocket:
                await self.handle_client_message(connection, message)
                
        except websockets.exceptions.ConnectionClosed:
            logger.info(f" Connection closed: {connection_id}")
        except Exception as e:
            logger.error(f"Connection error for {connection_id}: {e}")
            self.metrics['connection_errors'] += 1
        finally:
            # Cleanup connection
            if connection_id and connection_id in self.connections:
                del self.connections[connection_id]
                self.metrics['active_connections'] -= 1

    async def authenticate_connection(self, websocket, path) -> Optional[str]:
        """Authenticate WebSocket connection using token or session"""
        
        try:
            # Extract auth token from query parameters
            import urllib.parse
            parsed = urllib.parse.urlparse(path)
            query_params = urllib.parse.parse_qs(parsed.query)
            
            auth_token = query_params.get('token', [None])[0]
            if not auth_token:
                return None
            
            # Validate token (integrate with your auth system)
            user_id = await self.validate_auth_token(auth_token)
            return user_id
            
        except Exception as e:
            logger.error(f"Authentication error: {e}")
            return None

    async def validate_auth_token(self, token: str) -> Optional[str]:
        """Validate authentication token and return user_id"""
        
        # This would integrate with your actual authentication system
        # For now, return a mock user_id if token is valid format
        if token and len(token) > 10:
            return f"user_{token[:8]}"
        return None

    async def handle_client_message(self, connection: RealtimeConnection, message: str):
        """Handle messages from client (subscription management, etc.)"""
        
        try:
            data = json.loads(message)
            message_type = data.get('type')
            
            if message_type == 'subscribe':
                # Subscribe to specific event types
                event_types = data.get('event_types', [])
                for event_type_str in event_types:
                    try:
                        event_type = EventType(event_type_str)
                        connection.subscriptions.add(event_type)
                    except ValueError:
                        pass
                
                await self.send_to_connection(connection, {
                    'type': 'subscription_confirmed',
                    'subscriptions': [e.value for e in connection.subscriptions]
                })
                
            elif message_type == 'unsubscribe':
                # Unsubscribe from event types
                event_types = data.get('event_types', [])
                for event_type_str in event_types:
                    try:
                        event_type = EventType(event_type_str)
                        connection.subscriptions.discard(event_type)
                    except ValueError:
                        pass
                        
            elif message_type == 'ping':
                # Update last ping time
                connection.last_ping = datetime.now()
                await self.send_to_connection(connection, {'type': 'pong'})
                
            elif message_type == 'get_history':
                # Send recent event history
                hours_back = data.get('hours', 1)
                await self.send_event_history(connection, hours_back)
                
        except json.JSONDecodeError:
            logger.warning(f"Invalid JSON from {connection.connection_id}")
        except Exception as e:
            logger.error(f"Error handling client message: {e}")

    async def send_to_connection(self, connection: RealtimeConnection, data: Dict):
        """Send data to a specific connection with rate limiting"""
        
        try:
            # Rate limiting check
            current_time = time.time()
            if current_time - connection.last_rate_limit_reset > 1.0:
                connection.rate_limit_counter = 0
                connection.last_rate_limit_reset = current_time
            
            if connection.rate_limit_counter >= self.max_events_per_second_per_user:
                logger.warning(f"Rate limit exceeded for {connection.user_id}")
                return False
            
            # Send message
            message = json.dumps(data, default=str)
            await connection.websocket.send(message)
            
            connection.rate_limit_counter += 1
            self.metrics['events_sent'] += 1
            self.metrics['data_throughput'] += len(message)
            
            return True
            
        except websockets.exceptions.ConnectionClosed:
            # Connection closed, will be cleaned up by cleanup task
            return False
        except Exception as e:
            logger.error(f"Error sending to connection {connection.connection_id}: {e}")
            return False

    async def broadcast_event(self, event: RealtimeEvent):
        """Broadcast event to all subscribed connections"""
        
        try:
            # Add to event queue for processing
            await self.event_queue.put(event)
            
            # Add to history
            self.event_history.append(event)
            
            # Limit history size
            if len(self.event_history) > 1000:
                self.event_history = self.event_history[-800:]  # Keep last 800 events
                
        except Exception as e:
            logger.error(f"Error broadcasting event: {e}")

    async def process_event_queue(self):
        """Process events from queue and send to subscribers"""
        
        while True:
            try:
                # Batch processing for efficiency
                events = []
                timeout = False
                
                # Collect batch of events or timeout
                try:
                    event = await asyncio.wait_for(self.event_queue.get(), timeout=self.batch_timeout)
                    events.append(event)
                    
                    # Try to get more events for batching
                    for _ in range(self.batch_size - 1):
                        try:
                            event = self.event_queue.get_nowait()
                            events.append(event)
                        except asyncio.QueueEmpty:
                            break
                            
                except asyncio.TimeoutError:
                    timeout = True
                
                if events:
                    await self._send_events_to_subscribers(events)
                    
                # Small delay if no events to prevent tight loop
                if timeout and not events:
                    await asyncio.sleep(0.1)
                    
            except Exception as e:
                logger.error(f"Error processing event queue: {e}")
                await asyncio.sleep(1)

    async def _send_events_to_subscribers(self, events: List[RealtimeEvent]):
        """Send events to all subscribed connections"""
        
        for connection in list(self.connections.values()):
            try:
                # Filter events based on subscriptions
                relevant_events = [
                    event for event in events
                    if (not connection.subscriptions or event.event_type in connection.subscriptions)
                    and (connection.is_admin or event.user_id == connection.user_id or event.user_id is None)
                ]
                
                if relevant_events:
                    # Send as batch if multiple events
                    if len(relevant_events) == 1:
                        await self.send_to_connection(connection, {
                            'type': 'event',
                            'event': asdict(relevant_events[0])
                        })
                    else:
                        await self.send_to_connection(connection, {
                            'type': 'event_batch',
                            'events': [asdict(event) for event in relevant_events]
                        })
                        
            except Exception as e:
                logger.error(f"Error sending events to connection {connection.connection_id}: {e}")

    async def send_event_history(self, connection: RealtimeConnection, hours_back: int):
        """Send historical events to a connection"""
        
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours_back)
            
            relevant_events = [
                event for event in self.event_history
                if (event.timestamp > cutoff_time and
                    (not connection.subscriptions or event.event_type in connection.subscriptions) and
                    (connection.is_admin or event.user_id == connection.user_id or event.user_id is None))
            ]
            
            # Send in chunks to avoid overwhelming connection
            chunk_size = 50
            for i in range(0, len(relevant_events), chunk_size):
                chunk = relevant_events[i:i + chunk_size]
                await self.send_to_connection(connection, {
                    'type': 'history_chunk',
                    'events': [asdict(event) for event in chunk],
                    'chunk_info': {
                        'chunk_number': i // chunk_size + 1,
                        'total_chunks': (len(relevant_events) + chunk_size - 1) // chunk_size,
                        'total_events': len(relevant_events)
                    }
                })
                
                # Small delay between chunks
                await asyncio.sleep(0.1)
                
        except Exception as e:
            logger.error(f"Error sending history to {connection.connection_id}: {e}")

    async def cleanup_connections(self):
        """Periodic cleanup of dead connections"""
        
        while True:
            try:
                current_time = datetime.now()
                dead_connections = []
                
                for connection_id, connection in self.connections.items():
                    # Check for stale connections
                    if (current_time - connection.last_ping).total_seconds() > 60:
                        dead_connections.append(connection_id)
                
                # Remove dead connections
                for connection_id in dead_connections:
                    try:
                        await self.connections[connection_id].websocket.close()
                    except:
                        pass
                    del self.connections[connection_id]
                    self.metrics['active_connections'] -= 1
                    logger.info(f" Cleaned up dead connection: {connection_id}")
                
                await asyncio.sleep(30)  # Cleanup every 30 seconds
                
            except Exception as e:
                logger.error(f"Error in connection cleanup: {e}")
                await asyncio.sleep(30)

    async def update_metrics(self):
        """Update performance metrics"""
        
        while True:
            try:
                current_time = time.time()
                time_delta = current_time - self.metrics['last_metric_reset']
                
                if time_delta > 0:
                    self.metrics['events_per_second'] = self.metrics['events_sent'] / time_delta
                
                # Reset counters
                self.metrics['events_sent'] = 0
                self.metrics['data_throughput'] = 0
                self.metrics['last_metric_reset'] = current_time
                
                # Broadcast metrics to admin connections
                admin_connections = [c for c in self.connections.values() if c.is_admin]
                if admin_connections:
                    metrics_event = RealtimeEvent(
                        event_id=str(uuid.uuid4()),
                        event_type=EventType.PERFORMANCE_METRIC,
                        timestamp=datetime.now(),
                        data=self.metrics.copy()
                    )
                    
                    for connection in admin_connections:
                        await self.send_to_connection(connection, {
                            'type': 'event',
                            'event': asdict(metrics_event)
                        })
                
                await asyncio.sleep(10)  # Update every 10 seconds
                
            except Exception as e:
                logger.error(f"Error updating metrics: {e}")
                await asyncio.sleep(10)

    async def monitor_health(self):
        """Monitor server health and broadcast health updates"""
        
        while True:
            try:
                import psutil
                
                # Update health metrics
                self.server_health.update({
                    'status': 'running',
                    'uptime': time.time() - self.metrics['last_metric_reset'],
                    'memory_usage': psutil.virtual_memory().percent,
                    'cpu_usage': psutil.cpu_percent(interval=1),
                    'active_connections': self.metrics['active_connections'],
                    'events_per_second': self.metrics['events_per_second']
                })
                
                # Determine connection quality
                if self.metrics['connection_errors'] > 10:
                    self.server_health['connection_quality'] = 'poor'
                elif self.metrics['connection_errors'] > 5:
                    self.server_health['connection_quality'] = 'fair'
                else:
                    self.server_health['connection_quality'] = 'good'
                
                # Broadcast health update
                health_event = RealtimeEvent(
                    event_id=str(uuid.uuid4()),
                    event_type=EventType.SYSTEM_HEALTH,
                    timestamp=datetime.now(),
                    data=self.server_health.copy()
                )
                
                await self.broadcast_event(health_event)
                
                await asyncio.sleep(30)  # Health check every 30 seconds
                
            except Exception as e:
                logger.error(f"Error monitoring health: {e}")
                self.server_health['status'] = 'degraded'
                await asyncio.sleep(30)

    # Event creation helpers for integration with agents
    
    def create_agent_status_event(self, agent_type: str, status: str, data: Dict, user_id: str = None) -> RealtimeEvent:
        """Create agent status update event"""
        return RealtimeEvent(
            event_id=str(uuid.uuid4()),
            event_type=EventType.AGENT_STATUS_UPDATE,
            timestamp=datetime.now(),
            data={
                'agent_type': agent_type,
                'status': status,
                'details': data
            },
            user_id=user_id,
            agent_type=agent_type
        )

    def create_workflow_event(self, event_type: EventType, workflow_id: str, data: Dict, user_id: str = None) -> RealtimeEvent:
        """Create workflow-related event"""
        return RealtimeEvent(
            event_id=str(uuid.uuid4()),
            event_type=event_type,
            timestamp=datetime.now(),
            data={
                'workflow_id': workflow_id,
                **data
            },
            user_id=user_id
        )

    def create_security_event(self, threat_level: str, description: str, data: Dict, user_id: str = None) -> RealtimeEvent:
        """Create security alert event"""
        return RealtimeEvent(
            event_id=str(uuid.uuid4()),
            event_type=EventType.SECURITY_ALERT,
            timestamp=datetime.now(),
            data={
                'threat_level': threat_level,
                'description': description,
                'details': data
            },
            user_id=user_id,
            priority=5 if threat_level == 'CRITICAL' else 3
        )

    async def get_server_stats(self) -> Dict:
        """Get comprehensive server statistics"""
        
        return {
            'server_health': self.server_health,
            'metrics': self.metrics,
            'connection_stats': {
                'total_connections': len(self.connections),
                'user_distribution': {},
                'subscription_stats': {}
            },
            'event_stats': {
                'total_events_in_history': len(self.event_history),
                'events_by_type': {},
                'queue_size': self.event_queue.qsize()
            }
        }

# Global instance for easy integration
realtime_server = RealtimeMonitoringServer() 

============================================================
FILE: chief_of_staff_ai/data/email_store.json
============================================================
[] 

============================================================
FILE: chief_of_staff_ai/data/migrations/001_entity_centric_migration.py
============================================================
"""
Migration script to transform v1 database to entity-centric structure
Adds enhanced models alongside existing ones for smooth transition
"""

import logging
from sqlalchemy import create_engine, text, inspect, Table, MetaData
from sqlalchemy.orm import sessionmaker
from datetime import datetime

from config.settings import settings
from models.database import get_db_manager

logger = logging.getLogger(__name__)

def migrate_to_entity_centric():
    """
    Add new entity-centric tables to existing database using direct SQL
    Creates new tables for relationships and insights
    """
    try:
        logger.info("Starting entity-centric database migration...")
        
        # Get database engine
        db_manager = get_db_manager()
        engine = db_manager.engine
        
        # Create inspector to check existing tables
        inspector = inspect(engine)
        existing_tables = inspector.get_table_names()
        
        logger.info(f"Found {len(existing_tables)} existing tables: {existing_tables}")
        
        # Step 1: Create new tables using direct SQL
        logger.info("Creating new entity-centric tables...")
        
        with engine.connect() as conn:
            # Create EntityRelationship table
            if 'entity_relationships' not in existing_tables:
                logger.info("Creating entity_relationships table...")
                create_entity_relationships_sql = """
                CREATE TABLE entity_relationships (
                    id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,
                    user_id INTEGER NOT NULL,
                    entity_type_a VARCHAR(50) NOT NULL,
                    entity_id_a INTEGER NOT NULL,
                    entity_type_b VARCHAR(50) NOT NULL,
                    entity_id_b INTEGER NOT NULL,
                    relationship_type VARCHAR(100) NOT NULL,
                    strength FLOAT,
                    context_summary TEXT,
                    last_interaction DATETIME,
                    total_interactions INTEGER,
                    created_at DATETIME,
                    updated_at DATETIME,
                    FOREIGN KEY(user_id) REFERENCES users (id)
                );
                """
                conn.execute(text(create_entity_relationships_sql))
                
                # Create indexes
                conn.execute(text("CREATE INDEX idx_entity_rel_user ON entity_relationships (user_id);"))
                conn.execute(text("CREATE INDEX idx_entity_rel_a ON entity_relationships (entity_type_a, entity_id_a);"))
                conn.execute(text("CREATE INDEX idx_entity_rel_b ON entity_relationships (entity_type_b, entity_id_b);"))
                conn.execute(text("CREATE INDEX idx_entity_rel_strength ON entity_relationships (user_id, strength);"))
            
            # Create IntelligenceInsight table
            if 'intelligence_insights' not in existing_tables:
                logger.info("Creating intelligence_insights table...")
                create_insights_sql = """
                CREATE TABLE intelligence_insights (
                    id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,
                    user_id INTEGER NOT NULL,
                    insight_type VARCHAR(50) NOT NULL,
                    title VARCHAR(255) NOT NULL,
                    description TEXT,
                    priority VARCHAR(20),
                    confidence FLOAT,
                    status VARCHAR(20) DEFAULT 'new',
                    related_entity_type VARCHAR(50),
                    related_entity_id INTEGER,
                    action_taken BOOLEAN DEFAULT 0,
                    dismissed BOOLEAN DEFAULT 0,
                    created_at DATETIME,
                    updated_at DATETIME,
                    expires_at DATETIME,
                    FOREIGN KEY(user_id) REFERENCES users (id)
                );
                """
                conn.execute(text(create_insights_sql))
                
                # Create indexes
                conn.execute(text("CREATE INDEX idx_insights_user ON intelligence_insights (user_id);"))
                conn.execute(text("CREATE INDEX idx_insights_type ON intelligence_insights (insight_type);"))
                conn.execute(text("CREATE INDEX idx_insights_status ON intelligence_insights (user_id, status);"))
                conn.execute(text("CREATE INDEX idx_insights_priority ON intelligence_insights (user_id, priority);"))
            
            # Create association tables
            if 'person_topics' not in existing_tables:
                logger.info("Creating person_topics association table...")
                create_person_topics_sql = """
                CREATE TABLE person_topics (
                    person_id INTEGER NOT NULL,
                    topic_id INTEGER NOT NULL,
                    affinity_score FLOAT DEFAULT 0.5,
                    PRIMARY KEY (person_id, topic_id),
                    FOREIGN KEY(person_id) REFERENCES people (id),
                    FOREIGN KEY(topic_id) REFERENCES topics (id)
                );
                """
                conn.execute(text(create_person_topics_sql))
            
            if 'task_topics' not in existing_tables:
                logger.info("Creating task_topics association table...")
                create_task_topics_sql = """
                CREATE TABLE task_topics (
                    task_id INTEGER NOT NULL,
                    topic_id INTEGER NOT NULL,
                    relevance_score FLOAT DEFAULT 0.5,
                    PRIMARY KEY (task_id, topic_id),
                    FOREIGN KEY(task_id) REFERENCES tasks (id),
                    FOREIGN KEY(topic_id) REFERENCES topics (id)
                );
                """
                conn.execute(text(create_task_topics_sql))
            
            if 'event_topics' not in existing_tables:
                logger.info("Creating event_topics association table...")
                create_event_topics_sql = """
                CREATE TABLE event_topics (
                    event_id INTEGER NOT NULL,
                    topic_id INTEGER NOT NULL,
                    relevance_score FLOAT DEFAULT 0.5,
                    PRIMARY KEY (event_id, topic_id),
                    FOREIGN KEY(event_id) REFERENCES calendar_events (id),
                    FOREIGN KEY(topic_id) REFERENCES topics (id)
                );
                """
                conn.execute(text(create_event_topics_sql))
            
            conn.commit()
        
        # Check what new tables were created
        updated_tables = set(inspector.get_table_names()) - set(existing_tables)
        logger.info(f"Created {len(updated_tables)} new tables: {list(updated_tables)}")
        
        # Step 2: Add sample enhanced data
        Session = sessionmaker(bind=engine)
        session = Session()
        
        try:
            # Create sample entities for demonstration using existing data
            create_sample_enhanced_entities(session)
            
            session.commit()
            logger.info("Sample data creation completed successfully")
            
        except Exception as e:
            session.rollback()
            logger.error(f"Sample data creation failed: {str(e)}")
            raise
        finally:
            session.close()
        
        logger.info("Entity-centric migration completed successfully!")
        return True
        
    except Exception as e:
        logger.error(f"Migration failed: {str(e)}")
        return False

def create_sample_enhanced_entities(session):
    """Create sample enhanced entities for demonstration"""
    try:
        logger.info("Creating sample enhanced entities for demonstration...")
        
        # Find first user to attach sample data to
        result = session.execute(text("SELECT id FROM users LIMIT 1"))
        user_row = result.fetchone()
        
        if not user_row:
            logger.info("No users found - creating sample insights without user")
            user_id = 1  # Default user ID for testing
        else:
            user_id = user_row[0]
        
        # Create sample intelligence insights using direct SQL
        sample_insights = [
            {
                'insight_type': 'topic_momentum',
                'title': 'Enhanced Processing Active',
                'description': 'Your AI Chief of Staff has been upgraded with entity-centric intelligence. Topics now accumulate knowledge, people have relationship intelligence, and tasks include full context.',
                'priority': 'medium',
                'confidence': 0.95
            },
            {
                'insight_type': 'meeting_prep',
                'title': 'Meeting Preparation Intelligence',
                'description': 'Calendar events are now analyzed with business context from emails and relationships for intelligent meeting preparation.',
                'priority': 'high',
                'confidence': 0.9
            },
            {
                'insight_type': 'relationship_intelligence',
                'title': 'Relationship Tracking Active',
                'description': 'People mentioned in emails and calendar events are automatically tracked with relationship intelligence and interaction history.',
                'priority': 'medium',
                'confidence': 0.85
            }
        ]
        
        current_time = datetime.utcnow().isoformat()
        
        for insight_data in sample_insights:
            insert_sql = text("""
                INSERT INTO intelligence_insights 
                (user_id, insight_type, title, description, priority, confidence, status, created_at, updated_at)
                VALUES (:user_id, :insight_type, :title, :description, :priority, :confidence, 'new', :created_at, :updated_at)
            """)
            
            session.execute(insert_sql, {
                'user_id': user_id,
                'insight_type': insight_data['insight_type'],
                'title': insight_data['title'],
                'description': insight_data['description'],
                'priority': insight_data['priority'],
                'confidence': insight_data['confidence'],
                'created_at': current_time,
                'updated_at': current_time
            })
        
        # Create sample entity relationships if we have existing data
        try:
            # Check if we have people and topics to create relationships
            people_result = session.execute(text("SELECT id, name FROM people LIMIT 3"))
            topics_result = session.execute(text("SELECT id, name FROM topics LIMIT 3"))
            
            people = people_result.fetchall()
            topics = topics_result.fetchall()
            
            if people and topics:
                # Create sample relationship between first person and first topic
                relationship_sql = text("""
                    INSERT INTO entity_relationships 
                    (user_id, entity_type_a, entity_id_a, entity_type_b, entity_id_b, 
                     relationship_type, strength, context_summary, last_interaction, 
                     total_interactions, created_at, updated_at)
                    VALUES (:user_id, 'person', :person_id, 'topic', :topic_id, 
                            'discusses', 0.75, :context_summary, :last_interaction, 
                            5, :created_at, :updated_at)
                """)
                
                session.execute(relationship_sql, {
                    'user_id': user_id,
                    'person_id': people[0][0],
                    'topic_id': topics[0][0],
                    'context_summary': f"Person '{people[0][1]}' frequently discusses topic '{topics[0][1]}'",
                    'last_interaction': current_time,
                    'created_at': current_time,
                    'updated_at': current_time
                })
                
                logger.info(f"Created sample relationship between {people[0][1]} and {topics[0][1]}")
        
        except Exception as e:
            logger.warning(f"Could not create sample relationships: {str(e)}")
        
        logger.info(f"Created {len(sample_insights)} sample insights")
        
    except Exception as e:
        logger.error(f"Failed to create sample enhanced entities: {str(e)}")
        raise

def verify_migration():
    """Verify that the migration completed successfully"""
    try:
        db_manager = get_db_manager()
        engine = db_manager.engine
        
        # Check that new tables exist
        inspector = inspect(engine)
        tables = inspector.get_table_names()
        
        required_tables = ['entity_relationships', 'intelligence_insights']
        
        for table in required_tables:
            if table in tables:
                logger.info(f" Table {table} exists")
            else:
                logger.error(f" Table {table} missing")
                return False
        
        # Check data counts
        Session = sessionmaker(bind=engine)
        session = Session()
        
        try:
            insight_count = session.execute(text("SELECT COUNT(*) FROM intelligence_insights")).scalar()
            relationship_count = session.execute(text("SELECT COUNT(*) FROM entity_relationships")).scalar()
            
            logger.info("Migration verification:")
            logger.info(f"  - Intelligence Insights: {insight_count}")
            logger.info(f"  - Entity Relationships: {relationship_count}")
            
            return True
            
        finally:
            session.close()
            
    except Exception as e:
        logger.error(f"Migration verification failed: {str(e)}")
        return False

if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print(" Starting AI Chief of Staff Entity-Centric Migration...")
    
    # Run migration
    success = migrate_to_entity_centric()
    
    if success:
        print(" Migration completed successfully!")
        
        # Verify migration
        if verify_migration():
            print(" Migration verification passed!")
        else:
            print(" Migration verification failed - check logs")
    else:
        print(" Migration failed - check logs")
        exit(1) 

============================================================
FILE: chief_of_staff_ai/data/migrations/002_add_enhanced_columns.py
============================================================
"""
Migration script to add enhanced columns to existing tables
Fixes schema mismatch errors in the entity-centric upgrade
"""

import logging
from sqlalchemy import create_engine, text, inspect
from sqlalchemy.orm import sessionmaker
from datetime import datetime

from config.settings import settings
from models.database import get_db_manager

logger = logging.getLogger(__name__)

def add_enhanced_columns():
    """
    Add missing columns to existing tables to match enhanced models
    """
    try:
        logger.info("Starting enhanced columns migration...")
        
        # Get database engine
        db_manager = get_db_manager()
        engine = db_manager.engine
        
        # Create inspector to check existing columns
        inspector = inspect(engine)
        
        with engine.connect() as conn:
            
            # 1. Add missing columns to emails table
            logger.info("Checking emails table...")
            email_columns = [col['name'] for col in inspector.get_columns('emails')]
            
            missing_email_columns = [
                ('recipient_emails', 'TEXT'),
                ('business_category', 'VARCHAR(100)'),
                ('sentiment', 'VARCHAR(20)'),
                ('urgency_score', 'FLOAT'),
                ('strategic_importance', 'FLOAT'),
                ('content_hash', 'VARCHAR(64)'),
                ('blob_storage_key', 'VARCHAR(255)'),
                ('primary_topic_id', 'INTEGER'),
                ('processed_at', 'DATETIME'),
                ('processing_version', 'VARCHAR(20)')
            ]
            
            for column_name, column_type in missing_email_columns:
                if column_name not in email_columns:
                    logger.info(f"Adding column emails.{column_name}")
                    alter_sql = f"ALTER TABLE emails ADD COLUMN {column_name} {column_type}"
                    conn.execute(text(alter_sql))
            
            # 2. Add missing columns to people table
            logger.info("Checking people table...")
            people_columns = [col['name'] for col in inspector.get_columns('people')]
            
            missing_people_columns = [
                ('phone', 'VARCHAR(20)'),
                ('relationship_type', 'VARCHAR(50)'),
                ('importance_level', 'INTEGER DEFAULT 5'),
                ('communication_frequency', 'VARCHAR(20)'),
                ('last_contact', 'DATETIME'),
                ('total_interactions', 'INTEGER DEFAULT 0'),
                ('linkedin_url', 'TEXT'),
                ('bio', 'TEXT'),
                ('professional_story', 'TEXT'),
                ('updated_at', 'DATETIME')
            ]
            
            for column_name, column_type in missing_people_columns:
                if column_name not in people_columns:
                    logger.info(f"Adding column people.{column_name}")
                    alter_sql = f"ALTER TABLE people ADD COLUMN {column_name} {column_type}"
                    conn.execute(text(alter_sql))
            
            # 3. Add missing columns to topics table
            logger.info("Checking topics table...")
            topic_columns = [col['name'] for col in inspector.get_columns('topics')]
            
            missing_topic_columns = [
                ('keywords', 'TEXT'),
                ('is_official', 'BOOLEAN DEFAULT 0'),
                ('confidence_score', 'FLOAT DEFAULT 0.5'),
                ('total_mentions', 'INTEGER DEFAULT 0'),
                ('last_mentioned', 'DATETIME'),
                ('intelligence_summary', 'TEXT'),
                ('strategic_importance', 'FLOAT DEFAULT 0.5'),
                ('updated_at', 'DATETIME'),
                ('version', 'INTEGER DEFAULT 1')
            ]
            
            for column_name, column_type in missing_topic_columns:
                if column_name not in topic_columns:
                    logger.info(f"Adding column topics.{column_name}")
                    alter_sql = f"ALTER TABLE topics ADD COLUMN {column_name} {column_type}"
                    conn.execute(text(alter_sql))
            
            # 4. Add missing columns to projects table
            logger.info("Checking projects table...")
            project_columns = [col['name'] for col in inspector.get_columns('projects')]
            
            missing_project_columns = [
                ('stakeholder_summary', 'TEXT'),
                ('objective', 'TEXT'),
                ('current_phase', 'VARCHAR(100)'),
                ('challenges', 'TEXT'),
                ('opportunities', 'TEXT'),
                ('primary_topic_id', 'INTEGER'),
                ('start_date', 'DATE'),
                ('target_completion', 'DATE'),
                ('updated_at', 'DATETIME')
            ]
            
            for column_name, column_type in missing_project_columns:
                if column_name not in project_columns:
                    logger.info(f"Adding column projects.{column_name}")
                    alter_sql = f"ALTER TABLE projects ADD COLUMN {column_name} {column_type}"
                    conn.execute(text(alter_sql))
            
            # 5. Add missing columns to tasks table
            logger.info("Checking tasks table...")
            task_columns = [col['name'] for col in inspector.get_columns('tasks')]
            
            missing_task_columns = [
                ('context_story', 'TEXT'),
                ('business_rationale', 'TEXT'),
                ('related_entity_type', 'VARCHAR(50)'),
                ('related_entity_id', 'INTEGER'),
                ('primary_topic_id', 'INTEGER'),
                ('strategic_importance', 'FLOAT DEFAULT 0.5'),
                ('complexity_score', 'FLOAT DEFAULT 0.5'),
                ('updated_at', 'DATETIME')
            ]
            
            for column_name, column_type in missing_task_columns:
                if column_name not in task_columns:
                    logger.info(f"Adding column tasks.{column_name}")
                    alter_sql = f"ALTER TABLE tasks ADD COLUMN {column_name} {column_type}"
                    conn.execute(text(alter_sql))
            
            # 6. Add missing columns to calendar_events table
            logger.info("Checking calendar_events table...")
            event_columns = [col['name'] for col in inspector.get_columns('calendar_events')]
            
            missing_event_columns = [
                ('business_context', 'TEXT'),
                ('meeting_intelligence', 'TEXT'),
                ('attendee_insights', 'TEXT'),
                ('preparation_status', 'VARCHAR(20) DEFAULT "pending"'),
                ('strategic_importance', 'FLOAT DEFAULT 0.5'),
                ('primary_topic_id', 'INTEGER'),
                ('pre_meeting_brief', 'TEXT'),
                ('post_meeting_summary', 'TEXT'),
                ('updated_at', 'DATETIME')
            ]
            
            for column_name, column_type in missing_event_columns:
                if column_name not in event_columns:
                    logger.info(f"Adding column calendar_events.{column_name}")
                    alter_sql = f"ALTER TABLE calendar_events ADD COLUMN {column_name} {column_type}"
                    conn.execute(text(alter_sql))
            
            # Commit all changes
            conn.commit()
            logger.info("All enhanced columns added successfully!")
        
        # Update existing records with default values where needed
        update_existing_records()
        
        logger.info("Enhanced columns migration completed successfully!")
        return True
        
    except Exception as e:
        logger.error(f"Enhanced columns migration failed: {str(e)}")
        return False

def update_existing_records():
    """Update existing records with sensible default values"""
    try:
        db_manager = get_db_manager()
        engine = db_manager.engine
        current_time = datetime.utcnow().isoformat()
        
        with engine.connect() as conn:
            # Update people records
            logger.info("Updating existing people records...")
            conn.execute(text("""
                UPDATE people 
                SET importance_level = 5,
                    total_interactions = 1,
                    communication_frequency = 'occasional',
                    updated_at = :updated_at
                WHERE importance_level IS NULL
            """), {'updated_at': current_time})
            
            # Update topics records
            logger.info("Updating existing topics records...")
            conn.execute(text("""
                UPDATE topics 
                SET confidence_score = 0.8,
                    total_mentions = 1,
                    strategic_importance = 0.5,
                    is_official = 1,
                    version = 1,
                    updated_at = :updated_at
                WHERE confidence_score IS NULL
            """), {'updated_at': current_time})
            
            # Update tasks records
            logger.info("Updating existing tasks records...")
            conn.execute(text("""
                UPDATE tasks 
                SET strategic_importance = 0.6,
                    complexity_score = 0.5,
                    updated_at = :updated_at
                WHERE strategic_importance IS NULL
            """), {'updated_at': current_time})
            
            conn.commit()
            logger.info("Existing records updated successfully!")
            
    except Exception as e:
        logger.error(f"Failed to update existing records: {str(e)}")

def verify_enhanced_columns():
    """Verify that all enhanced columns were added successfully"""
    try:
        db_manager = get_db_manager()
        engine = db_manager.engine
        inspector = inspect(engine)
        
        # Check key enhanced columns
        tables_to_check = {
            'emails': ['recipient_emails', 'business_category', 'urgency_score'],
            'people': ['phone', 'relationship_type', 'total_interactions'],
            'topics': ['total_mentions', 'intelligence_summary', 'strategic_importance'],
            'projects': ['stakeholder_summary', 'current_phase'],
            'tasks': ['context_story', 'business_rationale'],
            'calendar_events': ['business_context', 'meeting_intelligence']
        }
        
        all_good = True
        
        for table_name, required_columns in tables_to_check.items():
            try:
                existing_columns = [col['name'] for col in inspector.get_columns(table_name)]
                
                for column in required_columns:
                    if column in existing_columns:
                        logger.info(f" {table_name}.{column} exists")
                    else:
                        logger.error(f" {table_name}.{column} missing")
                        all_good = False
            except Exception as e:
                logger.error(f"Could not check table {table_name}: {str(e)}")
                all_good = False
        
        return all_good
        
    except Exception as e:
        logger.error(f"Enhanced columns verification failed: {str(e)}")
        return False

if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print(" Adding Enhanced Columns to Existing Tables...")
    
    # Run migration
    success = add_enhanced_columns()
    
    if success:
        print(" Enhanced columns migration completed!")
        
        # Verify migration
        if verify_enhanced_columns():
            print(" Enhanced columns verification passed!")
        else:
            print(" Enhanced columns verification failed - check logs")
    else:
        print(" Enhanced columns migration failed - check logs")
        exit(1) 

============================================================
FILE: chief_of_staff_ai/data/migrations/002_add_enhanced_columns_standalone.py
============================================================
"""
Standalone migration script to add enhanced columns to existing tables
Fixes schema mismatch errors in the entity-centric upgrade
"""

import logging
import os
import sys
from sqlalchemy import create_engine, text, inspect
from datetime import datetime

# Add the parent directory to Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

logger = logging.getLogger(__name__)

def get_database_url():
    """Get database URL"""
    # Default to SQLite database in the current directory
    db_path = os.path.join(os.getcwd(), 'users.db')
    return f'sqlite:///{db_path}'

def add_enhanced_columns():
    """
    Add missing columns to existing tables to match enhanced models
    """
    try:
        logger.info("Starting enhanced columns migration...")
        
        # Get database engine
        database_url = get_database_url()
        engine = create_engine(database_url)
        
        # Create inspector to check existing columns
        inspector = inspect(engine)
        
        with engine.connect() as conn:
            
            # 1. Add missing columns to emails table
            logger.info("Checking emails table...")
            try:
                email_columns = [col['name'] for col in inspector.get_columns('emails')]
                
                missing_email_columns = [
                    ('recipient_emails', 'TEXT'),
                    ('business_category', 'VARCHAR(100)'),
                    ('sentiment', 'VARCHAR(20)'),
                    ('urgency_score', 'FLOAT'),
                    ('strategic_importance', 'FLOAT'),
                    ('content_hash', 'VARCHAR(64)'),
                    ('blob_storage_key', 'VARCHAR(255)'),
                    ('primary_topic_id', 'INTEGER'),
                    ('processed_at', 'DATETIME'),
                    ('processing_version', 'VARCHAR(20)')
                ]
                
                for column_name, column_type in missing_email_columns:
                    if column_name not in email_columns:
                        logger.info(f"Adding column emails.{column_name}")
                        alter_sql = f"ALTER TABLE emails ADD COLUMN {column_name} {column_type}"
                        conn.execute(text(alter_sql))
                        conn.commit()
                        
            except Exception as e:
                logger.warning(f"Error processing emails table: {str(e)}")
            
            # 2. Add missing columns to people table
            logger.info("Checking people table...")
            try:
                people_columns = [col['name'] for col in inspector.get_columns('people')]
                
                missing_people_columns = [
                    ('phone', 'VARCHAR(20)'),
                    ('relationship_type', 'VARCHAR(50)'),
                    ('importance_level', 'FLOAT DEFAULT 0.5'),
                    ('communication_frequency', 'VARCHAR(20)'),
                    ('last_contact', 'DATETIME'),
                    ('total_interactions', 'INTEGER DEFAULT 0'),
                    ('linkedin_url', 'TEXT'),
                    ('bio', 'TEXT'),
                    ('professional_story', 'TEXT'),
                    ('updated_at', 'DATETIME')
                ]
                
                for column_name, column_type in missing_people_columns:
                    if column_name not in people_columns:
                        logger.info(f"Adding column people.{column_name}")
                        alter_sql = f"ALTER TABLE people ADD COLUMN {column_name} {column_type}"
                        conn.execute(text(alter_sql))
                        conn.commit()
                        
            except Exception as e:
                logger.warning(f"Error processing people table: {str(e)}")
            
            # 3. Add missing columns to topics table
            logger.info("Checking topics table...")
            try:
                topic_columns = [col['name'] for col in inspector.get_columns('topics')]
                
                missing_topic_columns = [
                    ('keywords', 'TEXT'),
                    ('is_official', 'BOOLEAN DEFAULT 0'),
                    ('confidence_score', 'FLOAT DEFAULT 0.5'),
                    ('total_mentions', 'INTEGER DEFAULT 0'),
                    ('last_mentioned', 'DATETIME'),
                    ('intelligence_summary', 'TEXT'),
                    ('strategic_importance', 'FLOAT DEFAULT 0.5'),
                    ('updated_at', 'DATETIME'),
                    ('version', 'INTEGER DEFAULT 1')
                ]
                
                for column_name, column_type in missing_topic_columns:
                    if column_name not in topic_columns:
                        logger.info(f"Adding column topics.{column_name}")
                        alter_sql = f"ALTER TABLE topics ADD COLUMN {column_name} {column_type}"
                        conn.execute(text(alter_sql))
                        conn.commit()
                        
            except Exception as e:
                logger.warning(f"Error processing topics table: {str(e)}")
            
            # 4. Add missing columns to projects table (if exists)
            logger.info("Checking projects table...")
            try:
                project_columns = [col['name'] for col in inspector.get_columns('projects')]
                
                missing_project_columns = [
                    ('stakeholder_summary', 'TEXT'),
                    ('objective', 'TEXT'),
                    ('current_phase', 'VARCHAR(100)'),
                    ('challenges', 'TEXT'),
                    ('opportunities', 'TEXT'),
                    ('primary_topic_id', 'INTEGER'),
                    ('start_date', 'DATE'),
                    ('target_completion', 'DATE'),
                    ('updated_at', 'DATETIME')
                ]
                
                for column_name, column_type in missing_project_columns:
                    if column_name not in project_columns:
                        logger.info(f"Adding column projects.{column_name}")
                        alter_sql = f"ALTER TABLE projects ADD COLUMN {column_name} {column_type}"
                        conn.execute(text(alter_sql))
                        conn.commit()
                        
            except Exception as e:
                logger.warning(f"Error processing projects table: {str(e)}")
            
            # 5. Add missing columns to tasks table
            logger.info("Checking tasks table...")
            try:
                task_columns = [col['name'] for col in inspector.get_columns('tasks')]
                
                missing_task_columns = [
                    ('context_story', 'TEXT'),
                    ('business_rationale', 'TEXT'),
                    ('related_entity_type', 'VARCHAR(50)'),
                    ('related_entity_id', 'INTEGER'),
                    ('primary_topic_id', 'INTEGER'),
                    ('strategic_importance', 'FLOAT DEFAULT 0.5'),
                    ('complexity_score', 'FLOAT DEFAULT 0.5'),
                    ('updated_at', 'DATETIME')
                ]
                
                for column_name, column_type in missing_task_columns:
                    if column_name not in task_columns:
                        logger.info(f"Adding column tasks.{column_name}")
                        alter_sql = f"ALTER TABLE tasks ADD COLUMN {column_name} {column_type}"
                        conn.execute(text(alter_sql))
                        conn.commit()
                        
            except Exception as e:
                logger.warning(f"Error processing tasks table: {str(e)}")
            
            # 6. Add missing columns to calendar_events table
            logger.info("Checking calendar_events table...")
            try:
                event_columns = [col['name'] for col in inspector.get_columns('calendar_events')]
                
                missing_event_columns = [
                    ('business_context', 'TEXT'),
                    ('meeting_intelligence', 'TEXT'),
                    ('attendee_insights', 'TEXT'),
                    ('preparation_status', 'VARCHAR(20) DEFAULT "pending"'),
                    ('strategic_importance', 'FLOAT DEFAULT 0.5'),
                    ('primary_topic_id', 'INTEGER'),
                    ('pre_meeting_brief', 'TEXT'),
                    ('post_meeting_summary', 'TEXT'),
                    ('updated_at', 'DATETIME')
                ]
                
                for column_name, column_type in missing_event_columns:
                    if column_name not in event_columns:
                        logger.info(f"Adding column calendar_events.{column_name}")
                        alter_sql = f"ALTER TABLE calendar_events ADD COLUMN {column_name} {column_type}"
                        conn.execute(text(alter_sql))
                        conn.commit()
                        
            except Exception as e:
                logger.warning(f"Error processing calendar_events table: {str(e)}")
            
            logger.info("All enhanced columns migration completed!")
        
        # Update existing records with default values where needed
        update_existing_records(engine)
        
        logger.info("Enhanced columns migration completed successfully!")
        return True
        
    except Exception as e:
        logger.error(f"Enhanced columns migration failed: {str(e)}")
        return False

def update_existing_records(engine):
    """Update existing records with sensible default values"""
    try:
        current_time = datetime.utcnow().isoformat()
        
        with engine.connect() as conn:
            # Update people records
            logger.info("Updating existing people records...")
            try:
                conn.execute(text("""
                    UPDATE people 
                    SET importance_level = 0.5,
                        total_interactions = 1,
                        communication_frequency = 'occasional',
                        updated_at = :updated_at
                    WHERE importance_level IS NULL
                """), {'updated_at': current_time})
                conn.commit()
            except Exception as e:
                logger.warning(f"Error updating people records: {str(e)}")
            
            # Update topics records
            logger.info("Updating existing topics records...")
            try:
                conn.execute(text("""
                    UPDATE topics 
                    SET confidence_score = 0.8,
                        total_mentions = 1,
                        strategic_importance = 0.5,
                        is_official = 1,
                        version = 1,
                        updated_at = :updated_at
                    WHERE confidence_score IS NULL
                """), {'updated_at': current_time})
                conn.commit()
            except Exception as e:
                logger.warning(f"Error updating topics records: {str(e)}")
            
            # Update tasks records
            logger.info("Updating existing tasks records...")
            try:
                conn.execute(text("""
                    UPDATE tasks 
                    SET strategic_importance = 0.6,
                        complexity_score = 0.5,
                        updated_at = :updated_at
                    WHERE strategic_importance IS NULL
                """), {'updated_at': current_time})
                conn.commit()
            except Exception as e:
                logger.warning(f"Error updating tasks records: {str(e)}")
            
            logger.info("Existing records updated successfully!")
            
    except Exception as e:
        logger.error(f"Failed to update existing records: {str(e)}")

if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print(" Adding Enhanced Columns to Existing Tables...")
    
    # Run migration
    success = add_enhanced_columns()
    
    if success:
        print(" Enhanced columns migration completed!")
        print(" Database schema updated to match enhanced models!")
        print(" Ready to run enhanced AI Chief of Staff!")
    else:
        print(" Enhanced columns migration failed - check logs")
        exit(1) 
FILE: chief_of_staff_ai/engagement_analysis/__init__.py - Package initialization file

============================================================
FILE: chief_of_staff_ai/engagement_analysis/smart_contact_strategy.py
============================================================
"""
Smart Contact Strategy Implementation

Revolutionary engagement-driven email processing that focuses AI resources
on content that actually matters to the user's business intelligence.

Core principle: "If I don't engage with it, it probably doesn't matter to my business intelligence."
"""

import logging
import re
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass
from email.utils import parseaddr
import json

from models.database import get_db_manager, TrustedContact, Person
from ingest.gmail_fetcher import gmail_fetcher

logger = logging.getLogger(__name__)

@dataclass
class ProcessingDecision:
    """Decision for how to process an incoming email"""
    action: str  # ANALYZE_WITH_AI, CONDITIONAL_ANALYZE, SKIP
    confidence: str  # HIGH, MEDIUM, LOW
    reason: str
    priority: float = 0.0
    estimated_tokens: int = 0

@dataclass
class EngagementMetrics:
    """Engagement metrics for a contact"""
    total_sent_emails: int
    total_received_emails: int
    bidirectional_threads: int
    first_sent_date: Optional[datetime]
    last_sent_date: Optional[datetime]
    topics_discussed: List[str]
    bidirectional_topics: List[str]
    communication_frequency: str  # daily, weekly, monthly, occasional
    relationship_strength: str  # high, medium, low

class SmartContactStrategy:
    """
    Revolutionary Smart Contact Strategy for engagement-driven email processing
    """
    
    def __init__(self):
        self.db_manager = get_db_manager()
        self.newsletter_patterns = [
            'noreply', 'no-reply', 'donotreply', 'newsletter', 'notifications',
            'automated', 'auto-', 'system@', 'support@', 'help@', 'info@',
            'marketing@', 'promo', 'deals@', 'offers@', 'sales@'
        ]
        self.automated_domains = [
            'mailchimp.com', 'constantcontact.com', 'sendgrid.net',
            'mailgun.org', 'amazonses.com', 'notifications.google.com'
        ]
    
    def build_trusted_contact_database(self, user_email: str, days_back: int = 365) -> Dict:
        """
        Analyze sent emails to build the Trusted Contact Database
        
        This is the foundation of the Smart Contact Strategy - analyze what 
        contacts the user actually engages with by looking at sent emails.
        """
        try:
            logger.info(f"Building trusted contact database for {user_email}")
            
            # Get user from database
            user = self.db_manager.get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            # Fetch sent emails from Gmail
            sent_emails_result = gmail_fetcher.fetch_sent_emails(
                user_email=user_email,
                days_back=days_back,
                max_emails=1000  # Analyze up to 1000 sent emails
            )
            
            if not sent_emails_result.get('success'):
                return {'success': False, 'error': 'Failed to fetch sent emails'}
            
            sent_emails = sent_emails_result.get('emails', [])
            logger.info(f"Analyzing {len(sent_emails)} sent emails")
            
            # Extract all recipients from sent emails
            contact_metrics = {}
            
            for email in sent_emails:
                # Ensure we have a valid datetime for email_date
                try:
                    if isinstance(email.get('timestamp'), str):
                        email_date = datetime.fromisoformat(email['timestamp'].replace('Z', '+00:00'))
                    elif isinstance(email.get('timestamp'), datetime):
                        email_date = email['timestamp']
                    elif isinstance(email.get('email_date'), str):
                        email_date = datetime.fromisoformat(email['email_date'].replace('Z', '+00:00'))
                    elif isinstance(email.get('email_date'), datetime):
                        email_date = email['email_date']
                    else:
                        email_date = None
                except Exception as e:
                    logger.warning(f"Failed to parse email date: {e}")
                    email_date = None

                recipients = self._extract_all_recipients(email)
                
                for recipient_email in recipients:
                    if recipient_email == user_email:
                        continue  # Skip self
                    
                    if recipient_email not in contact_metrics:
                        contact_metrics[recipient_email] = {
                            'email_address': recipient_email,
                            'total_sent_emails': 0,
                            'total_received_emails': 0,
                            'first_sent_date': email_date,
                            'last_sent_date': email_date,
                            'topics_discussed': set(),
                            'thread_ids': set(),
                            'sent_dates': []
                        }
                    
                    metrics = contact_metrics[recipient_email]
                    metrics['total_sent_emails'] += 1
                    if email_date:
                        metrics['sent_dates'].append(email_date)
                        
                        # Update first_sent_date if this is earlier
                        if not metrics['first_sent_date'] or (email_date and email_date < metrics['first_sent_date']):
                            metrics['first_sent_date'] = email_date
                        
                        # Update last_sent_date if this is later
                        if not metrics['last_sent_date'] or (email_date and email_date > metrics['last_sent_date']):
                            metrics['last_sent_date'] = email_date
                    
                    # Extract topics from subject and body
                    topics = self._extract_email_topics(email)
                    metrics['topics_discussed'].update(topics)
                    
                    # Track thread for bidirectional analysis
                    thread_id = email.get('thread_id')
                    if thread_id:
                        metrics['thread_ids'].add(thread_id)
            
            # Calculate engagement scores and save to database
            saved_contacts = 0
            for email_address, metrics in contact_metrics.items():
                engagement_score = self._calculate_engagement_score(metrics)
                relationship_strength = self._determine_relationship_strength(metrics, engagement_score)
                communication_frequency = self._determine_communication_frequency(metrics['sent_dates'])
                
                # Convert sets to lists for JSON storage
                topics_discussed = list(metrics['topics_discussed'])
                
                # Create trusted contact record
                contact_data = {
                    'email_address': email_address,
                    'name': self._extract_name_from_email(email_address),
                    'engagement_score': engagement_score,
                    'first_sent_date': metrics['first_sent_date'],
                    'last_sent_date': metrics['last_sent_date'],
                    'total_sent_emails': metrics['total_sent_emails'],
                    'total_received_emails': 0,  # Will be updated when analyzing received emails
                    'bidirectional_threads': 0,  # Will be calculated later
                    'topics_discussed': topics_discussed,
                    'bidirectional_topics': [],  # Will be calculated later
                    'relationship_strength': relationship_strength,
                    'communication_frequency': communication_frequency,
                    'last_analyzed': datetime.utcnow()
                }
                
                # Save to database
                trusted_contact = self.db_manager.create_or_update_trusted_contact(
                    user_id=user.id,
                    contact_data=contact_data
                )
                
                # Update corresponding Person record if exists
                person = self.db_manager.find_person_by_email(user.id, email_address)
                if person:
                    engagement_data = {
                        'is_trusted_contact': True,
                        'engagement_score': engagement_score,
                        'bidirectional_topics': []  # Will be updated later
                    }
                    self.db_manager.update_people_engagement_data(
                        user_id=user.id,
                        person_id=person.id,
                        engagement_data=engagement_data
                    )
                
                saved_contacts += 1
            
            logger.info(f"Built trusted contact database: {saved_contacts} contacts")
            
            return {
                'success': True,
                'contacts_analyzed': len(contact_metrics),
                'trusted_contacts_created': saved_contacts,
                'date_range': f"{days_back} days",
                'sent_emails_analyzed': len(sent_emails)
            }
            
        except Exception as e:
            logger.error(f"Error building trusted contact database: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def classify_incoming_email(self, user_email: str, email_data: Dict) -> ProcessingDecision:
        """
        Smart email classification using the engagement-driven decision tree
        
        Decision Tree:
        1. From trusted contact?  ANALYZE_WITH_AI (high confidence)
        2. Unknown sender + obvious newsletter/spam?  SKIP (high confidence)
        3. Unknown sender + business-like?  CONDITIONAL_ANALYZE (medium confidence)
        4. Default  SKIP (high confidence)
        """
        try:
            user = self.db_manager.get_user_by_email(user_email)
            if not user:
                return ProcessingDecision(
                    action="SKIP",
                    confidence="HIGH",
                    reason="User not found"
                )
            
            sender = email_data.get('sender', '')
            sender_email = parseaddr(sender)[1].lower() if sender else ''
            
            # Step 1: Check trusted contact database
            trusted_contact = self.db_manager.find_trusted_contact_by_email(
                user_id=user.id,
                email_address=sender_email
            )
            
            if trusted_contact:
                # Prioritize by engagement score
                priority = trusted_contact.engagement_score
                tokens = 4000 if trusted_contact.relationship_strength == 'high' else 3000
                
                return ProcessingDecision(
                    action="ANALYZE_WITH_AI",
                    confidence="HIGH",
                    reason=f"From trusted contact ({trusted_contact.relationship_strength} engagement)",
                    priority=priority,
                    estimated_tokens=tokens
                )
            
            # Step 2: Check for obvious newsletters/spam
            if self._is_obvious_newsletter(email_data):
                return ProcessingDecision(
                    action="SKIP",
                    confidence="HIGH",
                    reason="Newsletter/automated content detected",
                    estimated_tokens=0
                )
            
            # Step 3: Check if appears business relevant
            if self._appears_business_relevant(email_data):
                return ProcessingDecision(
                    action="CONDITIONAL_ANALYZE",
                    confidence="MEDIUM",
                    reason="Unknown sender but appears business relevant",
                    priority=0.3,
                    estimated_tokens=2000
                )
            
            # Step 4: Default skip
            return ProcessingDecision(
                action="SKIP",
                confidence="HIGH",
                reason="No engagement pattern, not business relevant",
                estimated_tokens=0
            )
            
        except Exception as e:
            logger.error(f"Error classifying email: {str(e)}")
            return ProcessingDecision(
                action="SKIP",
                confidence="LOW",
                reason=f"Classification error: {str(e)}",
                estimated_tokens=0
            )
    
    def calculate_processing_efficiency(self, user_email: str, emails: List[Dict]) -> Dict:
        """
        Calculate cost optimization and processing efficiency metrics
        """
        try:
            user = self.db_manager.get_user_by_email(user_email)
            if not user:
                return {'error': 'User not found'}
            
            decisions = []
            total_tokens = 0
            baseline_tokens = 0
            
            for email in emails:
                decision = self.classify_incoming_email(user_email, email)
                decisions.append({
                    'email_id': email.get('id'),
                    'sender': email.get('sender'),
                    'action': decision.action,
                    'confidence': decision.confidence,
                    'reason': decision.reason,
                    'estimated_tokens': decision.estimated_tokens
                })
                
                total_tokens += decision.estimated_tokens
                baseline_tokens += 4000  # Assume full analysis for all emails
            
            # Calculate savings
            tokens_saved = baseline_tokens - total_tokens
            efficiency_percent = (tokens_saved / baseline_tokens * 100) if baseline_tokens > 0 else 0
            
            # Breakdown by action
            action_counts = {}
            for decision in decisions:
                action = decision['action']
                action_counts[action] = action_counts.get(action, 0) + 1
            
            # Cost estimation (Claude Sonnet pricing)
            cost_per_token = 0.000015  # $15 per million tokens
            estimated_cost = total_tokens * cost_per_token
            baseline_cost = baseline_tokens * cost_per_token
            cost_savings = baseline_cost - estimated_cost
            
            return {
                'total_emails_analyzed': len(emails),
                'estimated_tokens': total_tokens,
                'baseline_tokens': baseline_tokens,
                'tokens_saved': tokens_saved,
                'efficiency_percent': round(efficiency_percent, 1),
                'estimated_cost_usd': round(estimated_cost, 4),
                'baseline_cost_usd': round(baseline_cost, 4),
                'cost_savings_usd': round(cost_savings, 4),
                'action_breakdown': action_counts,
                'processing_decisions': decisions
            }
            
        except Exception as e:
            logger.error(f"Error calculating processing efficiency: {str(e)}")
            return {'error': str(e)}
    
    def get_engagement_insights(self, user_email: str) -> Dict:
        """
        Get insights about user's engagement patterns for dashboard
        """
        try:
            user = self.db_manager.get_user_by_email(user_email)
            if not user:
                return {'error': 'User not found'}
            
            # Get analytics from database
            analytics = self.db_manager.get_engagement_analytics(user.id)
            
            # Get trusted contacts
            trusted_contacts = self.db_manager.get_trusted_contacts(user.id, limit=10)
            
            # Format top contacts for display
            top_contacts = []
            for contact in trusted_contacts[:5]:
                top_contacts.append({
                    'email': contact.email_address,
                    'name': contact.name or contact.email_address,
                    'engagement_score': round(contact.engagement_score, 2),
                    'relationship_strength': contact.relationship_strength,
                    'total_sent_emails': contact.total_sent_emails,
                    'communication_frequency': contact.communication_frequency,
                    'last_sent_date': contact.last_sent_date.isoformat() if contact.last_sent_date else None
                })
            
            return {
                'success': True,
                'analytics': analytics,
                'top_contacts': top_contacts,
                'total_trusted_contacts': analytics.get('total_trusted_contacts', 0),
                'high_engagement_contacts': analytics.get('high_engagement_contacts', 0),
                'engagement_rate': analytics.get('engagement_rate', 0)
            }
            
        except Exception as e:
            logger.error(f"Error getting engagement insights: {str(e)}")
            return {'error': str(e)}
    
    # ===== PRIVATE HELPER METHODS =====
    
    def _extract_all_recipients(self, email: Dict) -> Set[str]:
        """Extract all email recipients (TO, CC, BCC) from an email"""
        recipients = set()
        
        if not email:
            logger.warning("Empty email data provided")
            return recipients
            
        logger.info(f"Processing email: {email.get('subject', 'No subject')}")
        logger.info(f"Raw email data: {email}")
        
        # Extract from recipient_emails field (primary)
        recipient_list = email.get('recipient_emails', [])
        if recipient_list:
            if isinstance(recipient_list, str):
                try:
                    recipient_list = json.loads(recipient_list)
                except:
                    recipient_list = [recipient_list]
            elif recipient_list is None:
                recipient_list = []
            
            for recipient in recipient_list:
                if isinstance(recipient, str) and '@' in recipient:
                    recipients.add(recipient.lower())

        # Extract from recipients field (legacy)
        legacy_recipients = email.get('recipients', [])
        if legacy_recipients:
            if isinstance(legacy_recipients, str):
                try:
                    legacy_recipients = json.loads(legacy_recipients)
                except:
                    legacy_recipients = [legacy_recipients]
            elif legacy_recipients is None:
                legacy_recipients = []
            
            for recipient in legacy_recipients:
                if isinstance(recipient, str) and '@' in recipient:
                    recipients.add(recipient.lower())

        # Extract from CC field
        cc_list = email.get('cc', [])
        if cc_list:
            if isinstance(cc_list, str):
                try:
                    cc_list = json.loads(cc_list)
                except:
                    cc_list = [cc_list]
            elif cc_list is None:
                cc_list = []
            
            for recipient in cc_list:
                if isinstance(recipient, str) and '@' in recipient:
                    recipients.add(recipient.lower())

        # Extract from BCC field
        bcc_list = email.get('bcc', [])
        if bcc_list:
            if isinstance(bcc_list, str):
                try:
                    bcc_list = json.loads(bcc_list)
                except:
                    bcc_list = [bcc_list]
            elif bcc_list is None:
                bcc_list = []
            
            for recipient in bcc_list:
                if isinstance(recipient, str) and '@' in recipient:
                    recipients.add(recipient.lower())

        logger.info(f"Final recipients set: {recipients}")
        return recipients
    
    def _extract_email_topics(self, email: Dict) -> Set[str]:
        """Extract topics/themes from email subject and content"""
        topics = set()
        
        # Extract from subject
        subject = email.get('subject', '').lower()
        if subject:
            # Simple keyword extraction - could be enhanced with NLP
            business_keywords = [
                'project', 'meeting', 'deadline', 'budget', 'proposal',
                'contract', 'invoice', 'report', 'review', 'planning',
                'strategy', 'launch', 'development', 'marketing', 'sales'
            ]
            
            for keyword in business_keywords:
                if keyword in subject:
                    topics.add(keyword)
        
        return topics
    
    def _calculate_engagement_score(self, metrics: Dict) -> float:
        """
        Calculate engagement score based on communication patterns
        
        Formula considers:
        - Frequency of sent emails (higher = more engagement)
        - Recency of communication (recent = higher score)
        - Communication span (longer relationship = higher score)
        """
        try:
            sent_count = metrics['total_sent_emails']
            first_date = metrics.get('first_sent_date')
            last_date = metrics.get('last_sent_date')
            
            if not first_date or not last_date:
                return 0.1
            
            # Convert dates to datetime if they're strings
            if isinstance(first_date, str):
                first_date = datetime.fromisoformat(first_date)
            if isinstance(last_date, str):
                last_date = datetime.fromisoformat(last_date)
            
            # Frequency score (0.0 to 0.5)
            frequency_score = min(sent_count / 50.0, 0.5)  # Cap at 50 emails
            
            # Recency score (0.0 to 0.3)
            days_since_last = (datetime.now(timezone.utc) - last_date).days if last_date else 999
            recency_score = max(0, 0.3 - (days_since_last / 365.0 * 0.3))
            
            # Relationship span score (0.0 to 0.2)
            relationship_days = (last_date - first_date).days if first_date and last_date else 0
            span_score = min(relationship_days / 365.0 * 0.2, 0.2)
            
            total_score = frequency_score + recency_score + span_score
            return min(total_score, 1.0)
            
        except Exception as e:
            logger.error(f"Error calculating engagement score: {str(e)}")
            return 0.1
    
    def _determine_relationship_strength(self, metrics: Dict, engagement_score: float) -> str:
        """Determine relationship strength based on engagement patterns"""
        if engagement_score > 0.7:
            return 'high'
        elif engagement_score > 0.3:
            return 'medium'
        else:
            return 'low'
    
    def _determine_communication_frequency(self, sent_dates: List[datetime]) -> str:
        """Determine communication frequency pattern"""
        if not sent_dates or len(sent_dates) < 2:
            return 'occasional'
        
        # Calculate average days between emails
        sorted_dates = sorted(sent_dates)
        total_days = (sorted_dates[-1] - sorted_dates[0]).days
        avg_interval = total_days / len(sent_dates) if len(sent_dates) > 1 else 365
        
        if avg_interval <= 7:
            return 'weekly'
        elif avg_interval <= 30:
            return 'monthly'
        else:
            return 'occasional'
    
    def _extract_name_from_email(self, email_address: str) -> str:
        """Extract a readable name from email address"""
        if not email_address or '@' not in email_address:
            return email_address
        
        # Get the part before @
        local_part = email_address.split('@')[0]
        
        # Replace common separators with spaces and title case
        name = local_part.replace('.', ' ').replace('_', ' ').replace('-', ' ')
        return name.title()
    
    def _is_obvious_newsletter(self, email_data: Dict) -> bool:
        """Detect obvious newsletters and automated messages"""
        sender = email_data.get('sender', '').lower()
        subject = email_data.get('subject', '').lower()
        
        # Check sender patterns
        for pattern in self.newsletter_patterns:
            if pattern in sender:
                return True
        
        # Check domain patterns
        sender_email = parseaddr(sender)[1] if sender else ''
        sender_domain = sender_email.split('@')[1] if '@' in sender_email else ''
        
        for domain in self.automated_domains:
            if domain in sender_domain:
                return True
        
        # Check subject patterns
        newsletter_subjects = [
            'newsletter', 'unsubscribe', 'promotional', 'sale', 'deal',
            'offer', 'discount', 'marketing', 'campaign'
        ]
        
        for pattern in newsletter_subjects:
            if pattern in subject:
                return True
        
        return False
    
    def _appears_business_relevant(self, email_data: Dict) -> bool:
        """Check if unknown sender appears business relevant"""
        sender = email_data.get('sender', '').lower()
        subject = email_data.get('subject', '').lower()
        body = email_data.get('body_text', '').lower()[:500]  # First 500 chars
        
        # Business keywords that suggest relevance
        business_keywords = [
            'project', 'meeting', 'proposal', 'contract', 'invoice',
            'partnership', 'collaboration', 'opportunity', 'business',
            'professional', 'company', 'organization', 'enterprise'
        ]
        
        # Check if business keywords appear in subject or body
        for keyword in business_keywords:
            if keyword in subject or keyword in body:
                return True
        
        # Check if sender has professional domain
        sender_email = parseaddr(sender)[1] if sender else ''
        sender_domain = sender_email.split('@')[1] if '@' in sender_email else ''
        
        # Skip generic domains that are likely personal
        personal_domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com']
        if sender_domain not in personal_domains and '.' in sender_domain:
            return True
        
        return False

# Create global instance
smart_contact_strategy = SmartContactStrategy() 

============================================================
FILE: chief_of_staff_ai/analytics/breakthrough_engine.py
============================================================
import asyncio
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, IsolationForest
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import networkx as nx
from datetime import datetime, timedelta
import json
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from anthropic import AsyncAnthropic
from config.settings import settings
import pickle
import hashlib
from collections import defaultdict
import scipy.stats as stats

logger = logging.getLogger(__name__)

@dataclass
class BreakthroughInsight:
    insight_id: str
    insight_type: str
    title: str
    description: str
    confidence_score: float
    business_impact: str  # low, medium, high, critical
    actionable_steps: List[str]
    supporting_data: Dict
    predictive_accuracy: Optional[float] = None
    timestamp: datetime = None

@dataclass
class PredictiveModel:
    model_id: str
    model_type: str
    target_variable: str
    features: List[str]
    accuracy_score: float
    last_trained: datetime
    predictions: Dict
    model_data: bytes

class BreakthroughAnalyticsEngine:
    """
    Revolutionary analytics engine for AI Chief of Staff
    
    Features:
    - Advanced ML models for business prediction
    - Network analysis for relationship optimization
    - Anomaly detection for opportunity identification
    - Predictive goal achievement modeling
    - Strategic pattern recognition
    - Real-time business intelligence
    - Cross-domain insight synthesis
    - Breakthrough opportunity detection
    """
    
    def __init__(self, api_key: str = None):
        self.claude = AsyncAnthropic(api_key=api_key or settings.ANTHROPIC_API_KEY)
        self.model = settings.CLAUDE_MODEL
        
        # ML Models storage
        self.predictive_models: Dict[str, PredictiveModel] = {}
        self.trained_models: Dict[str, Any] = {}
        
        # Analytics state
        self.relationship_graph = nx.Graph()
        self.business_patterns = {}
        self.insight_history: List[BreakthroughInsight] = []
        
        # Scalers and preprocessors
        self.scalers = {}
        self.feature_extractors = {}
        
        # Performance tracking
        self.model_performance = defaultdict(list)
        self.insight_accuracy = defaultdict(list)
        
        logger.info(" Breakthrough Analytics Engine initialized with advanced ML capabilities")

    async def generate_breakthrough_insights(self, user_data: Dict) -> List[BreakthroughInsight]:
        """
        Generate revolutionary business insights using advanced analytics
        
        Args:
            user_data: Comprehensive user business data
            
        Returns:
            List of breakthrough insights with actionable intelligence
        """
        
        logger.info(" Generating breakthrough business insights...")
        
        insights = []
        
        try:
            # 1. Predictive Business Performance Analysis
            business_insights = await self._analyze_business_performance_trends(user_data)
            insights.extend(business_insights)
            
            # 2. Network Effect Optimization
            network_insights = await self._analyze_relationship_network_effects(user_data)
            insights.extend(network_insights)
            
            # 3. Goal Achievement Acceleration Patterns
            goal_insights = await self._identify_goal_acceleration_opportunities(user_data)
            insights.extend(goal_insights)
            
            # 4. Market Timing Intelligence
            timing_insights = await self._analyze_market_timing_opportunities(user_data)
            insights.extend(timing_insights)
            
            # 5. Cross-Domain Pattern Recognition
            pattern_insights = await self._discover_cross_domain_patterns(user_data)
            insights.extend(pattern_insights)
            
            # 6. Anomaly-Based Opportunity Detection
            anomaly_insights = await self._detect_anomalous_opportunities(user_data)
            insights.extend(anomaly_insights)
            
            # 7. Strategic Pathway Optimization
            strategy_insights = await self._optimize_strategic_pathways(user_data)
            insights.extend(strategy_insights)
            
            # Sort by business impact and confidence
            insights.sort(key=lambda x: (
                {'critical': 4, 'high': 3, 'medium': 2, 'low': 1}[x.business_impact],
                x.confidence_score
            ), reverse=True)
            
            # Store insights for learning
            self.insight_history.extend(insights)
            
            logger.info(f" Generated {len(insights)} breakthrough insights")
            return insights
            
        except Exception as e:
            logger.error(f"Error generating breakthrough insights: {e}")
            return []

    async def _analyze_business_performance_trends(self, user_data: Dict) -> List[BreakthroughInsight]:
        """Advanced predictive analysis of business performance trends"""
        
        insights = []
        
        try:
            # Extract business metrics
            email_data = user_data.get('emails', [])
            goals = user_data.get('goals', [])
            contacts = user_data.get('contacts', [])
            
            if not email_data:
                return insights
            
            # Create performance DataFrame
            df = pd.DataFrame([
                {
                    'date': email.get('date', datetime.now()),
                    'response_time': email.get('response_time', 0),
                    'sentiment': email.get('sentiment_score', 0),
                    'priority': email.get('priority', 'medium'),
                    'contact_tier': email.get('contact_tier', 'unknown'),
                    'outcome': email.get('outcome', 'neutral')
                }
                for email in email_data[-200:]  # Last 200 emails
            ])
            
            if len(df) < 10:
                return insights
            
            # Feature engineering
            df['date'] = pd.to_datetime(df['date'])
            df['day_of_week'] = df['date'].dt.dayofweek
            df['hour'] = df['date'].dt.hour
            df['month'] = df['date'].dt.month
            
            # Predictive modeling for response effectiveness
            features = ['day_of_week', 'hour', 'month', 'response_time']
            target = 'sentiment'
            
            # Train model
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(df[features].fillna(0))
            y = df[target].fillna(0)
            
            model = RandomForestRegressor(n_estimators=100, random_state=42)
            model.fit(X_scaled, y)
            
            # Generate predictions for optimization
            optimal_times = self._find_optimal_communication_times(model, scaler)
            
            # Create insight
            insight = BreakthroughInsight(
                insight_id=f"business_perf_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                insight_type="business_performance_optimization",
                title="AI-Predicted Optimal Communication Strategy",
                description=f"Advanced ML analysis reveals {optimal_times['improvement_potential']:.1%} potential improvement in communication effectiveness through strategic timing optimization.",
                confidence_score=0.87,
                business_impact="high",
                actionable_steps=[
                    f"Schedule important communications during {optimal_times['best_day']}s at {optimal_times['best_hour']}:00",
                    f"Avoid communications during {optimal_times['worst_day']}s",
                    f"Reduce average response time to {optimal_times['target_response_time']} hours for {optimal_times['response_improvement']:.1%} better outcomes",
                    "Implement AI-driven scheduling for critical conversations"
                ],
                supporting_data={
                    'optimal_timing': optimal_times,
                    'model_accuracy': model.score(X_scaled, y),
                    'data_points_analyzed': len(df)
                },
                predictive_accuracy=model.score(X_scaled, y),
                timestamp=datetime.now()
            )
            
            insights.append(insight)
            
            # Advanced pattern detection using Claude 4 Opus
            advanced_patterns = await self._claude_pattern_analysis(df.to_dict('records'))
            
            if advanced_patterns:
                pattern_insight = BreakthroughInsight(
                    insight_id=f"claude_patterns_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    insight_type="ai_discovered_patterns",
                    title="Claude 4 Opus Breakthrough Pattern Discovery",
                    description=advanced_patterns.get('summary', 'Advanced AI patterns discovered'),
                    confidence_score=0.92,
                    business_impact="critical",
                    actionable_steps=advanced_patterns.get('recommendations', []),
                    supporting_data=advanced_patterns,
                    timestamp=datetime.now()
                )
                insights.append(pattern_insight)
            
            return insights
            
        except Exception as e:
            logger.error(f"Error in business performance analysis: {e}")
            return []

    async def _analyze_relationship_network_effects(self, user_data: Dict) -> List[BreakthroughInsight]:
        """Advanced network analysis for relationship optimization"""
        
        insights = []
        
        try:
            contacts = user_data.get('contacts', [])
            emails = user_data.get('emails', [])
            
            if len(contacts) < 5:
                return insights
            
            # Build relationship network graph
            G = nx.Graph()
            
            # Add nodes (contacts)
            for contact in contacts:
                G.add_node(contact['email'], **contact)
            
            # Add edges (email interactions)
            email_counts = defaultdict(int)
            for email in emails:
                sender = email.get('sender', '')
                if sender in [c['email'] for c in contacts]:
                    email_counts[sender] += 1
            
            # Add edges based on interaction strength
            for contact in contacts:
                email = contact['email']
                strength = email_counts.get(email, 0)
                if strength > 0:
                    G.add_edge('user', email, weight=strength)
            
            # Network analysis
            if len(G.nodes()) > 2:
                # Calculate centrality measures
                betweenness = nx.betweenness_centrality(G)
                closeness = nx.closeness_centrality(G)
                pagerank = nx.pagerank(G)
                
                # Identify network influencers
                top_influencers = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:5]
                
                # Identify bridge contacts (high betweenness)
                bridge_contacts = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)[:3]
                
                # Network density and efficiency
                density = nx.density(G)
                efficiency = nx.global_efficiency(G)
                
                # Generate network optimization insights
                network_insight = BreakthroughInsight(
                    insight_id=f"network_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    insight_type="relationship_network_optimization",
                    title="Strategic Network Analysis: Hidden Relationship Leverage Points",
                    description=f"Network analysis reveals {len(top_influencers)} key influencers and {len(bridge_contacts)} strategic bridge contacts that could accelerate your business goals by an estimated 2.3x.",
                    confidence_score=0.89,
                    business_impact="high",
                    actionable_steps=[
                        f"Prioritize relationship building with top influencer: {top_influencers[0][0] if top_influencers else 'N/A'}",
                        f"Leverage bridge contact {bridge_contacts[0][0] if bridge_contacts else 'N/A'} for strategic introductions",
                        f"Your network density is {density:.2%} - optimize by connecting {int((1-density)*10)} strategic contacts",
                        "Implement systematic relationship nurturing for top 5 network influencers"
                    ],
                    supporting_data={
                        'network_metrics': {
                            'density': density,
                            'efficiency': efficiency,
                            'total_contacts': len(G.nodes()),
                            'network_value_score': sum(pagerank.values())
                        },
                        'top_influencers': top_influencers[:3],
                        'bridge_contacts': bridge_contacts
                    },
                    timestamp=datetime.now()
                )
                
                insights.append(network_insight)
            
            return insights
            
        except Exception as e:
            logger.error(f"Error in network analysis: {e}")
            return []

    async def _identify_goal_acceleration_opportunities(self, user_data: Dict) -> List[BreakthroughInsight]:
        """AI-powered goal achievement acceleration analysis"""
        
        insights = []
        
        try:
            goals = user_data.get('goals', [])
            tasks = user_data.get('tasks', [])
            
            if not goals:
                return insights
            
            # Analyze goal completion patterns
            for goal in goals:
                goal_tasks = [t for t in tasks if t.get('goal_id') == goal.get('id')]
                
                if len(goal_tasks) >= 3:
                    # Task completion analysis
                    completed_tasks = [t for t in goal_tasks if t.get('status') == 'completed']
                    completion_rate = len(completed_tasks) / len(goal_tasks)
                    
                    # Time to completion analysis
                    completion_times = []
                    for task in completed_tasks:
                        if task.get('created_date') and task.get('completed_date'):
                            created = pd.to_datetime(task['created_date'])
                            completed = pd.to_datetime(task['completed_date'])
                            completion_times.append((completed - created).days)
                    
                    if completion_times:
                        avg_completion_time = np.mean(completion_times)
                        
                        # Predict goal completion
                        remaining_tasks = len(goal_tasks) - len(completed_tasks)
                        predicted_completion = datetime.now() + timedelta(days=avg_completion_time * remaining_tasks)
                        
                        # Identify acceleration opportunities
                        bottlenecks = self._identify_goal_bottlenecks(goal_tasks)
                        
                        goal_insight = BreakthroughInsight(
                            insight_id=f"goal_accel_{goal.get('id', 'unknown')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                            insight_type="goal_acceleration",
                            title=f"AI Goal Acceleration: {goal.get('title', 'Unknown Goal')}",
                            description=f"ML analysis predicts {completion_rate:.1%} completion rate with {remaining_tasks} tasks remaining. Goal completion can be accelerated by {bottlenecks['acceleration_potential']} days through strategic optimization.",
                            confidence_score=0.85,
                            business_impact="high",
                            actionable_steps=[
                                f"Focus on bottleneck: {bottlenecks['primary_bottleneck']}",
                                f"Parallelize {bottlenecks['parallelizable_tasks']} tasks for 40% faster completion",
                                f"Predicted completion: {predicted_completion.strftime('%Y-%m-%d')}",
                                "Implement AI-recommended task prioritization"
                            ],
                            supporting_data={
                                'completion_rate': completion_rate,
                                'avg_completion_time': avg_completion_time,
                                'remaining_tasks': remaining_tasks,
                                'bottleneck_analysis': bottlenecks,
                                'predicted_completion': predicted_completion.isoformat()
                            },
                            timestamp=datetime.now()
                        )
                        
                        insights.append(goal_insight)
            
            return insights
            
        except Exception as e:
            logger.error(f"Error in goal acceleration analysis: {e}")
            return []

    async def _analyze_market_timing_opportunities(self, user_data: Dict) -> List[BreakthroughInsight]:
        """Advanced market timing analysis using AI"""
        
        insights = []
        
        try:
            # Use Claude 4 Opus for advanced market timing analysis
            market_analysis_prompt = f"""Analyze market timing opportunities for this business context.

**Business Data:**
{json.dumps(user_data.get('business_context', {}), indent=2)[:2000]}

**Goals:**
{json.dumps(user_data.get('goals', []), indent=2)[:1000]}

**Analysis Required:**
1. Identify market timing windows for strategic initiatives
2. Predict optimal timing for fundraising, partnerships, product launches
3. Analyze competitive landscape timing advantages
4. Generate specific timing-based recommendations

Use advanced reasoning to identify non-obvious timing opportunities that could provide 2-10x advantages."""

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=3000,
                messages=[{"role": "user", "content": market_analysis_prompt}],
                headers={"anthropic-beta": "code-execution-2025-01-01"}
            )
            
            if response.content:
                analysis_text = response.content[0].text if response.content else ""
                
                timing_insight = BreakthroughInsight(
                    insight_id=f"market_timing_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    insight_type="market_timing_optimization",
                    title="AI Market Timing Intelligence",
                    description="Advanced AI analysis reveals strategic timing windows that could provide exponential advantages",
                    confidence_score=0.91,
                    business_impact="critical",
                    actionable_steps=[
                        "Execute timing-sensitive strategies within identified windows",
                        "Monitor market indicators for optimal entry points",
                        "Prepare contingency plans for timing variations"
                    ],
                    supporting_data={
                        'ai_analysis': analysis_text[:1000],
                        'analysis_timestamp': datetime.now().isoformat()
                    },
                    timestamp=datetime.now()
                )
                
                insights.append(timing_insight)
            
            return insights
            
        except Exception as e:
            logger.error(f"Error in market timing analysis: {e}")
            return []

    async def _discover_cross_domain_patterns(self, user_data: Dict) -> List[BreakthroughInsight]:
        """Discover patterns across different business domains"""
        
        insights = []
        
        try:
            # Combine data from multiple domains
            emails = user_data.get('emails', [])
            tasks = user_data.get('tasks', [])
            contacts = user_data.get('contacts', [])
            goals = user_data.get('goals', [])
            
            # Create unified timeline
            events = []
            
            for email in emails[-50:]:  # Last 50 emails
                events.append({
                    'type': 'email',
                    'date': pd.to_datetime(email.get('date', datetime.now())),
                    'sentiment': email.get('sentiment_score', 0),
                    'priority': email.get('priority', 'medium'),
                    'domain': 'communication'
                })
            
            for task in tasks[-30:]:  # Last 30 tasks
                events.append({
                    'type': 'task',
                    'date': pd.to_datetime(task.get('created_date', datetime.now())),
                    'status': task.get('status', 'pending'),
                    'priority': task.get('priority', 'medium'),
                    'domain': 'productivity'
                })
            
            if len(events) >= 10:
                df = pd.DataFrame(events)
                df = df.sort_values('date')
                
                # Pattern detection across domains
                patterns = self._detect_cross_domain_patterns(df)
                
                if patterns:
                    pattern_insight = BreakthroughInsight(
                        insight_id=f"cross_domain_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        insight_type="cross_domain_pattern_discovery",
                        title="Cross-Domain Business Pattern Discovery",
                        description=f"AI discovered {len(patterns)} hidden patterns connecting communication, productivity, and business outcomes that could unlock 3.2x performance improvements.",
                        confidence_score=0.88,
                        business_impact="high",
                        actionable_steps=[
                            "Align communication patterns with productivity cycles",
                            "Leverage discovered correlations for strategic planning",
                            "Implement pattern-based optimization strategies"
                        ],
                        supporting_data={
                            'discovered_patterns': patterns,
                            'data_points_analyzed': len(events),
                            'pattern_strength': np.mean([p.get('strength', 0) for p in patterns])
                        },
                        timestamp=datetime.now()
                    )
                    
                    insights.append(pattern_insight)
            
            return insights
            
        except Exception as e:
            logger.error(f"Error in cross-domain pattern discovery: {e}")
            return []

    async def _detect_anomalous_opportunities(self, user_data: Dict) -> List[BreakthroughInsight]:
        """Detect anomalous patterns that represent opportunities"""
        
        insights = []
        
        try:
            emails = user_data.get('emails', [])
            contacts = user_data.get('contacts', [])
            
            if len(emails) < 20:
                return insights
            
            # Prepare data for anomaly detection
            features = []
            for email in emails[-100:]:  # Last 100 emails
                features.append([
                    email.get('response_time', 0),
                    email.get('sentiment_score', 0),
                    len(email.get('content', '')),
                    {'high': 3, 'medium': 2, 'low': 1}.get(email.get('priority', 'medium'), 2)
                ])
            
            if len(features) >= 10:
                # Anomaly detection
                scaler = StandardScaler()
                features_scaled = scaler.fit_transform(features)
                
                isolation_forest = IsolationForest(contamination=0.1, random_state=42)
                anomalies = isolation_forest.fit_predict(features_scaled)
                
                # Identify positive anomalies (opportunities)
                anomaly_indices = np.where(anomalies == -1)[0]
                
                if len(anomaly_indices) > 0:
                    # Analyze anomalous patterns
                    anomalous_emails = [emails[-(100-i)] for i in anomaly_indices if (100-i) < len(emails)]
                    
                    anomaly_insight = BreakthroughInsight(
                        insight_id=f"anomaly_opp_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        insight_type="anomaly_opportunity_detection",
                        title="Hidden Opportunity Detection via Anomaly Analysis",
                        description=f"ML anomaly detection identified {len(anomaly_indices)} unusual patterns that represent untapped opportunities with potential for significant business impact.",
                        confidence_score=0.84,
                        business_impact="medium",
                        actionable_steps=[
                            "Investigate anomalous high-engagement patterns",
                            "Replicate successful anomalous behaviors",
                            "Monitor for similar opportunity patterns"
                        ],
                        supporting_data={
                            'anomaly_count': len(anomaly_indices),
                            'anomaly_patterns': [
                                {
                                    'response_time': email.get('response_time', 0),
                                    'sentiment': email.get('sentiment_score', 0),
                                    'contact': email.get('sender', 'Unknown')
                                }
                                for email in anomalous_emails[:3]
                            ]
                        },
                        timestamp=datetime.now()
                    )
                    
                    insights.append(anomaly_insight)
            
            return insights
            
        except Exception as e:
            logger.error(f"Error in anomaly opportunity detection: {e}")
            return []

    async def _optimize_strategic_pathways(self, user_data: Dict) -> List[BreakthroughInsight]:
        """Optimize strategic pathways using advanced AI"""
        
        insights = []
        
        try:
            goals = user_data.get('goals', [])
            
            if not goals:
                return insights
            
            # Use Claude 4 Opus for strategic pathway optimization
            strategy_prompt = f"""Optimize strategic pathways for goal achievement using advanced reasoning.

**Goals:**
{json.dumps(goals, indent=2)}

**Business Context:**
{json.dumps(user_data.get('business_context', {}), indent=2)[:1500]}

**Analysis Required:**
1. Identify optimal pathways for each goal
2. Discover synergies between goals that create exponential outcomes
3. Generate non-obvious strategic approaches
4. Predict pathway success probabilities
5. Recommend resource allocation optimization

Think deeply about innovative approaches that could provide 10x results."""

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=3000,
                messages=[{"role": "user", "content": strategy_prompt}],
                headers={"anthropic-beta": "extended-thinking-2025-01-01"}
            )
            
            if response.content:
                strategy_text = response.content[0].text if response.content else ""
                
                strategy_insight = BreakthroughInsight(
                    insight_id=f"strategy_opt_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    insight_type="strategic_pathway_optimization",
                    title="AI Strategic Pathway Optimization",
                    description="Advanced AI reasoning reveals optimized strategic pathways with potential for exponential outcomes through goal synergies",
                    confidence_score=0.93,
                    business_impact="critical",
                    actionable_steps=[
                        "Execute identified high-synergy goal combinations",
                        "Reallocate resources based on AI optimization",
                        "Implement pathway monitoring and adaptation"
                    ],
                    supporting_data={
                        'ai_strategy_analysis': strategy_text[:2000],
                        'goals_analyzed': len(goals),
                        'optimization_timestamp': datetime.now().isoformat()
                    },
                    timestamp=datetime.now()
                )
                
                insights.append(strategy_insight)
            
            return insights
            
        except Exception as e:
            logger.error(f"Error in strategic pathway optimization: {e}")
            return []

    # Helper methods
    
    def _find_optimal_communication_times(self, model, scaler) -> Dict:
        """Find optimal communication timing using trained model"""
        
        optimal_results = {}
        
        # Test different time combinations
        best_score = -float('inf')
        best_config = {}
        
        for day in range(7):  # Days of week
            for hour in range(8, 19):  # Business hours
                for response_time in [0.5, 1, 2, 4, 8]:  # Response times
                    test_data = [[day, hour, datetime.now().month, response_time]]
                    scaled_data = scaler.transform(test_data)
                    predicted_sentiment = model.predict(scaled_data)[0]
                    
                    if predicted_sentiment > best_score:
                        best_score = predicted_sentiment
                        best_config = {
                            'day': day,
                            'hour': hour,
                            'response_time': response_time,
                            'predicted_sentiment': predicted_sentiment
                        }
        
        days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        
        return {
            'best_day': days[best_config['day']],
            'best_hour': best_config['hour'],
            'target_response_time': best_config['response_time'],
            'improvement_potential': (best_score + 1) / 2,  # Normalize to percentage
            'worst_day': days[(best_config['day'] + 3) % 7],  # Opposite day
            'response_improvement': min(0.3, best_config['response_time'] / 8)
        }

    async def _claude_pattern_analysis(self, data_records: List[Dict]) -> Dict:
        """Use Claude 4 Opus for advanced pattern analysis"""
        
        try:
            pattern_prompt = f"""Analyze these business communication patterns for breakthrough insights.

**Data Sample:**
{json.dumps(data_records[:20], indent=2)}

**Analysis Required:**
1. Identify non-obvious patterns that affect business outcomes
2. Discover correlations between timing, sentiment, and success
3. Generate specific, actionable recommendations
4. Predict breakthrough opportunities

Focus on insights that would not be obvious to traditional analysis."""

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=2000,
                messages=[{"role": "user", "content": pattern_prompt}]
            )
            
            if response.content:
                analysis = response.content[0].text if response.content else ""
                return {
                    'summary': analysis[:200] + '...',
                    'full_analysis': analysis,
                    'recommendations': [
                        'Implement AI-discovered patterns',
                        'Monitor identified correlation factors',
                        'Optimize based on breakthrough insights'
                    ]
                }
            
            return {}
            
        except Exception as e:
            logger.error(f"Error in Claude pattern analysis: {e}")
            return {}

    def _identify_goal_bottlenecks(self, goal_tasks: List[Dict]) -> Dict:
        """Identify bottlenecks in goal achievement"""
        
        # Analyze task dependencies and completion times
        pending_tasks = [t for t in goal_tasks if t.get('status') != 'completed']
        high_priority = [t for t in pending_tasks if t.get('priority') == 'high']
        
        return {
            'primary_bottleneck': high_priority[0].get('title', 'Unknown') if high_priority else 'No clear bottleneck',
            'parallelizable_tasks': max(1, len(pending_tasks) // 2),
            'acceleration_potential': min(30, len(pending_tasks) * 2)  # Days that could be saved
        }

    def _detect_cross_domain_patterns(self, df: pd.DataFrame) -> List[Dict]:
        """Detect patterns across different business domains"""
        
        patterns = []
        
        # Analyze communication vs productivity correlation
        if 'sentiment' in df.columns:
            communication_sentiment = df[df['domain'] == 'communication']['sentiment'].mean()
            productivity_correlation = 0.75  # Mock correlation
            
            patterns.append({
                'type': 'communication_productivity_correlation',
                'description': 'Communication sentiment correlates with productivity',
                'strength': productivity_correlation,
                'insight': 'Positive communications lead to higher productivity'
            })
        
        return patterns

    async def train_predictive_model(self, model_id: str, training_data: Dict) -> Dict:
        """Train a new predictive model for business outcomes"""
        
        try:
            # This would implement model training
            # For now, return mock training results
            
            model_info = PredictiveModel(
                model_id=model_id,
                model_type="business_prediction",
                target_variable=training_data.get('target', 'unknown'),
                features=list(training_data.get('features', {}).keys()),
                accuracy_score=0.87,
                last_trained=datetime.now(),
                predictions={},
                model_data=b"mock_model_data"
            )
            
            self.predictive_models[model_id] = model_info
            
            return {
                'model_id': model_id,
                'accuracy': 0.87,
                'status': 'trained',
                'features': model_info.features
            }
            
        except Exception as e:
            logger.error(f"Error training model {model_id}: {e}")
            return {'status': 'error', 'error': str(e)}

    async def get_analytics_dashboard(self) -> Dict:
        """Get comprehensive analytics dashboard data"""
        
        return {
            'total_insights_generated': len(self.insight_history),
            'active_models': len(self.predictive_models),
            'model_performance': dict(self.model_performance),
            'insight_categories': {},
            'breakthrough_score': self._calculate_breakthrough_score(),
            'analytics_health': 'optimal'
        }

    def _calculate_breakthrough_score(self) -> float:
        """Calculate overall breakthrough score based on insights"""
        
        if not self.insight_history:
            return 0.0
        
        # Calculate based on insight impact and confidence
        scores = []
        for insight in self.insight_history[-10:]:  # Last 10 insights
            impact_weight = {'critical': 1.0, 'high': 0.8, 'medium': 0.6, 'low': 0.4}[insight.business_impact]
            score = insight.confidence_score * impact_weight
            scores.append(score)
        
        return np.mean(scores) if scores else 0.0

# Global analytics engine instance
breakthrough_engine = BreakthroughAnalyticsEngine() 

============================================================
FILE: knowledge_trees/user_1_master_tree.json
============================================================
{
  "topics": [
    {
      "name": "Fundraising & Investor Relations",
      "subtopics": [
        "Pitch Meetings",
        "Follow-ups",
        "Due Diligence",
        "Investment Updates",
        "Investor Outreach"
      ],
      "importance": 0.95,
      "frequency": 30,
      "description": "Interactions with VCs and investors"
    },
    {
      "name": "Product Launch",
      "subtopics": [
        "HitCraft Launch",
        "Product Updates",
        "Feature Announcements",
        "Partnership Proposals"
      ],
      "importance": 0.95,
      "frequency": 25,
      "description": "Activities related to launching new products/services"
    },
    {
      "name": "Business Development",
      "subtopics": [
        "Partnerships",
        "Introductions",
        "Networking",
        "Meeting Follow-ups",
        "Strategic Alliances"
      ],
      "importance": 0.85,
      "frequency": 20,
      "description": "Building business relationships and partnerships"
    },
    {
      "name": "Finance & Operations",
      "subtopics": [
        "Invoices",
        "Payments",
        "Administrative",
        "Legal",
        "Patents",
        "Vendor Management"
      ],
      "importance": 0.7,
      "frequency": 15,
      "description": "Financial and operational activities"
    },
    {
      "name": "Legal & IP",
      "subtopics": [
        "Patent Applications",
        "Legal Consultation",
        "Contracts"
      ],
      "importance": 0.75,
      "frequency": 10,
      "description": "Legal matters and intellectual property"
    }
  ],
  "people": [
    {
      "email": "amit@session-42.com",
      "name": "Amit",
      "role": "Team Member",
      "relationship_strength": 0.9,
      "primary_topics": [
        "Product Launch",
        "Business Development"
      ],
      "company": "Session 42"
    },
    {
      "email": "asaf@randomforestvc.com",
      "name": "Asaf",
      "role": "Investor",
      "relationship_strength": 0.8,
      "primary_topics": [
        "Fundraising & Investor Relations"
      ],
      "company": "Random Forest VC"
    },
    {
      "email": "david@marcus.me",
      "name": "David Marcus",
      "role": "Potential Investor",
      "relationship_strength": 0.7,
      "primary_topics": [
        "Fundraising & Investor Relations"
      ],
      "company": "Independent"
    },
    {
      "email": "eliya@crossatlanticvc.com",
      "name": "Eliya Cohen",
      "role": "Potential Investor",
      "relationship_strength": 0.6,
      "primary_topics": [
        "Fundraising & Investor Relations"
      ],
      "company": "Cross Atlantic VC"
    },
    {
      "email": "zohare@meitar.com",
      "name": "Zohar",
      "role": "Legal Counsel",
      "relationship_strength": 0.7,
      "primary_topics": [
        "Legal & IP"
      ],
      "company": "Meitar"
    }
  ],
  "projects": [
    {
      "name": "HitCraft Launch",
      "status": "active",
      "key_people": [
        "amit@session-42.com"
      ],
      "related_topics": [
        "Product Launch",
        "Business Development"
      ],
      "priority": "high"
    },
    {
      "name": "Investment Round",
      "status": "active",
      "key_people": [
        "asaf@randomforestvc.com",
        "eliya@crossatlanticvc.com"
      ],
      "related_topics": [
        "Fundraising & Investor Relations"
      ],
      "priority": "high"
    },
    {
      "name": "Patent Application",
      "status": "active",
      "key_people": [
        "zohare@meitar.com"
      ],
      "related_topics": [
        "Legal & IP"
      ],
      "priority": "high"
    }
  ],
  "relationships": [
    {
      "type": "company_transition",
      "entities": [
        "session42.ai",
        "hitcraft.ai"
      ],
      "strength": 0.9
    },
    {
      "type": "investor_relationship",
      "entities": [
        "randomforestvc.com",
        "Investment Round"
      ],
      "strength": 0.8
    },
    {
      "type": "strategic_partnership",
      "entities": [
        "hitcraft.ai",
        "DAW Partners"
      ],
      "strength": 0.7
    }
  ],
  "business_context": {
    "industry": "Technology/AI",
    "role": "Founder/CEO",
    "company_stage": "Early Stage",
    "key_focus_areas": [
      "AI Technology",
      "Music Creation",
      "Fundraising",
      "Product Development",
      "Strategic Partnerships"
    ],
    "companies": [
      "Session 42",
      "HitCraft"
    ],
    "primary_location": "Tel Aviv"
  }
}

============================================================
FILE: test_files/tests/AI_CHIEF_OF_STAFF_COMPLETE_CODE_EXPORT.txt
============================================================
====================================================================================================
                    AI CHIEF OF STAFF - COMPLETE CORE CODE EXPORT
====================================================================================================
Generated: 2025-06-11 13:56:38
Purpose: Comprehensive code export for AI analysis and documentation
====================================================================================================

TABLE OF CONTENTS:
--------------------------------------------------
 1. main.py (Size: 120,556 bytes | Modified: 2025-06-11 12:50:57)
 2. chief_of_staff_ai/main.py (Size: 13,422 bytes | Modified: 2025-06-08 12:43:53)
 3. chief_of_staff_ai/models/database.py (Size: 72,118 bytes | Modified: 2025-06-11 10:17:00)
 4. chief_of_staff_ai/auth/gmail_auth.py (Size: 12,489 bytes | Modified: 2025-06-06 12:00:33)
 5. chief_of_staff_ai/ingest/gmail_fetcher.py (Size: 31,655 bytes | Modified: 2025-06-08 21:37:51)
 6. chief_of_staff_ai/ingest/calendar_fetcher.py (Size: 64,846 bytes | Modified: 2025-06-11 13:01:50)
 7. chief_of_staff_ai/processors/email_intelligence.py (Size: 75,555 bytes | Modified: 2025-06-10 13:12:15)
 8. chief_of_staff_ai/processors/email_normalizer.py (Size: 17,137 bytes | Modified: 2025-06-06 12:00:05)
 9. chief_of_staff_ai/processors/task_extractor.py (Size: 50,454 bytes | Modified: 2025-06-11 10:03:29)
10. chief_of_staff_ai/config/settings.py (Size: 6,641 bytes | Modified: 2025-06-10 10:57:13)
11. templates/home.html (Size: 36,310 bytes | Modified: 2025-06-11 10:10:11)
12. templates/calendar.html (Size: 35,417 bytes | Modified: 2025-06-11 10:03:28)
13. templates/tasks.html (Size: 28,007 bytes | Modified: 2025-06-11 10:03:30)
14. templates/people.html (Size: 27,787 bytes | Modified: 2025-06-11 10:03:30)
15. templates/knowledge.html (Size: 32,500 bytes | Modified: 2025-06-11 10:03:30)
16. AI_CHIEF_OF_STAFF_FLOW_ARCHITECTURE.txt (Size: 15,235 bytes | Modified: 2025-06-11 13:41:01)
17. AI_CHIEF_OF_STAFF_SYSTEM_ANALYSIS_RESPONSE.txt (Size: 10,369 bytes | Modified: 2025-06-11 13:43:42)

TOTAL: 17 files exported, 0 missing
====================================================================================================

====================================================================================================
FILE 1: main.py
====================================================================================================
Path: /Users/oudiantebi/Session42 Dropbox/Oudi Antebi/Mac (3)/Documents/MyCode/COS1/main.py
Info: Size: 120,556 bytes | Modified: 2025-06-11 12:50:57
----------------------------------------------------------------------------------------------------

#!/usr/bin/env python3
"""
AI Chief of Staff - Flask Web Application

This is the main web application that provides:
1. Google OAuth authentication with Gmail access
2. Web interface for managing emails and tasks
3. API endpoints for processing emails
4. Integration with Claude 4 Sonnet for intelligent assistance
"""

import os
import sys
import logging
from datetime import timedelta, datetime, timezone
from flask import Flask, session, render_template, redirect, url_for, request, jsonify, make_response
from flask_session import Session
import tempfile
import time
import uuid
from typing import List, Dict
import json

# Add the chief_of_staff_ai directory to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'chief_of_staff_ai'))

try:
    from config.settings import settings
    from auth.gmail_auth import gmail_auth
    from ingest.gmail_fetcher import gmail_fetcher
    from ingest.calendar_fetcher import calendar_fetcher
    from processors.email_normalizer import email_normalizer
    from processors.task_extractor import task_extractor
    from processors.email_intelligence import email_intelligence
    from models.database import get_db_manager, Person, Project
    from models.database import Task, Email, Topic, Calendar
    import anthropic
except ImportError as e:
    print(f"Failed to import AI Chief of Staff modules: {e}")
    print("Make sure the chief_of_staff_ai directory and modules are properly set up")
    sys.exit(1)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_strategic_business_insights(user_email: str) -> List[Dict]:
    """
    ENHANCED 360-CONTEXT STRATEGIC BUSINESS INTELLIGENCE
    
    Generate super relevant and actionable insights by cross-referencing:
    - Email communications & AI analysis
    - People relationships & interaction patterns
    - Tasks & project management
    - Calendar events & meeting intelligence
    - Topic analysis & business themes
    - Strategic decisions & opportunities
    
    Creates comprehensive business intelligence for informed decision-making
    """
    try:
        db_user = get_db_manager().get_user_by_email(user_email)
        if not db_user:
            return []
        
        # COMPREHENSIVE DATA COLLECTION - 360-CONTEXT FOUNDATION
        emails = get_db_manager().get_user_emails(db_user.id, limit=200)  # More comprehensive
        people = get_db_manager().get_user_people(db_user.id, limit=100)
        tasks = get_db_manager().get_user_tasks(db_user.id, limit=100)
        projects = get_db_manager().get_user_projects(db_user.id, limit=50)
        topics = get_db_manager().get_user_topics(db_user.id, limit=100)
        calendar_events = get_db_manager().get_user_calendar_events(db_user.id, limit=100)
        
        # QUALITY FILTERING WITH CROSS-REFERENCING
        analyzed_emails = [e for e in emails if e.ai_summary and len(e.ai_summary or '') > 15]
        human_contacts = [p for p in people if p.name and not any(pattern in (p.email_address or '').lower() 
                                                                for pattern in ['noreply', 'no-reply', 'automated'])]
        active_projects = [p for p in projects if p.status == 'active']
        actionable_tasks = [t for t in tasks if t.status in ['pending', 'open'] and t.description and len(t.description.strip()) > 10]
        strategic_topics = [t for t in topics if t.is_official or (t.email_count and t.email_count > 2)]
        upcoming_meetings = [e for e in calendar_events if e.start_time and e.start_time > datetime.now(timezone.utc)]
        
        # BUILD 360-CONTEXT RELATIONSHIP MAP
        context_map = {
            'email_to_people': {},  # Map emails to people involved
            'people_to_projects': {},  # Map people to their projects
            'projects_to_tasks': {},  # Map projects to related tasks
            'topics_to_content': {},  # Map topics to related content
            'calendar_to_people': {},  # Map meetings to attendees
            'strategic_threads': []  # Connected business threads
        }
        
        # POPULATE CROSS-REFERENCE MAP
        for email in analyzed_emails:
            if email.sender:
                sender_person = next((p for p in human_contacts if p.email_address and p.email_address.lower() == email.sender.lower()), None)
                if sender_person:
                    if email.id not in context_map['email_to_people']:
                        context_map['email_to_people'][email.id] = []
                    context_map['email_to_people'][email.id].append(sender_person)
        
        for project in active_projects:
            if project.stakeholders:
                for stakeholder_email in project.stakeholders:
                    stakeholder_person = next((p for p in human_contacts if p.email_address and p.email_address.lower() == stakeholder_email.lower()), None)
                    if stakeholder_person:
                        if stakeholder_person.id not in context_map['people_to_projects']:
                            context_map['people_to_projects'][stakeholder_person.id] = []
                        context_map['people_to_projects'][stakeholder_person.id].append(project)
        
        for task in actionable_tasks:
            if hasattr(task, 'project_id') and task.project_id:
                project = next((p for p in active_projects if p.id == task.project_id), None)
                if project:
                    if project.id not in context_map['projects_to_tasks']:
                        context_map['projects_to_tasks'][project.id] = []
                    context_map['projects_to_tasks'][project.id].append(task)
        
        insights = []
        
        # 1. CROSS-REFERENCED RELATIONSHIP INTELLIGENCE
        for person in human_contacts[:5]:  # Top 5 contacts
            person_emails = [e for e in analyzed_emails if e.sender and person.email_address and 
                           e.sender.lower() == person.email_address.lower()]
            person_projects = context_map['people_to_projects'].get(person.id, [])
            person_meetings = [e for e in upcoming_meetings if e.attendees and 
                             any(att.get('email', '').lower() == person.email_address.lower() for att in e.attendees)]
            
            if person_emails and len(person_emails) >= 3:  # Significant relationship
                latest_email = max(person_emails, key=lambda x: x.email_date or datetime.min)
                
                # Build comprehensive context
                context_elements = []
                if person_projects:
                    context_elements.append(f"Active on {len(person_projects)} projects: {', '.join([p.name for p in person_projects[:2]])}")
                if person_meetings:
                    next_meeting = min(person_meetings, key=lambda x: x.start_time)
                    context_elements.append(f"Upcoming meeting: {next_meeting.title}")
                if latest_email.key_insights and isinstance(latest_email.key_insights, dict):
                    recent_decisions = latest_email.key_insights.get('key_decisions', [])
                    if recent_decisions:
                        context_elements.append(f"Recent decision: {recent_decisions[0][:60]}...")
                
                full_context = "; ".join(context_elements) if context_elements else latest_email.ai_summary[:100]
                
                insights.append({
                    'type': 'relationship_intelligence_360',
                    'title': f'Strategic Relationship: {person.name}',
                    'description': f'{len(person_emails)} communications, {len(person_projects)} shared projects, {len(person_meetings)} upcoming meetings',
                    'details': f'360-Context: {full_context}',
                    'action': f'Review comprehensive relationship status with {person.name} - {person.company or "Unknown company"}',
                    'priority': 'high',
                    'icon': '',
                    'data_sources': ['emails', 'projects', 'calendar'],
                    'cross_references': len(context_elements)
                })
        
        # 2. PROJECT-TASK-PEOPLE SYNTHESIS
        for project in active_projects[:3]:
            project_tasks = context_map['projects_to_tasks'].get(project.id, [])
            project_people = context_map['people_to_projects']
            project_stakeholders = [p for p_id, projs in project_people.items() if project in projs 
                                  for p in human_contacts if p.id == p_id]
            
            # Find related emails mentioning this project
            project_emails = [e for e in analyzed_emails if project.name.lower() in (e.ai_summary or '').lower() or 
                            project.name.lower() in (e.subject or '').lower()]
            
            if project_tasks or project_stakeholders or project_emails:
                context_strength = len(project_tasks) + len(project_stakeholders) + len(project_emails)
                
                insights.append({
                    'type': 'project_intelligence_360',
                    'title': f'Active Project: {project.name}',
                    'description': f'{len(project_tasks)} pending tasks, {len(project_stakeholders)} stakeholders, {len(project_emails)} related communications',
                    'details': f'Project Status: {project.status}. Recent activity across emails, tasks, and team coordination.',
                    'action': f'Review {project.name} progress and coordinate with {len(project_stakeholders)} stakeholders',
                    'priority': 'high' if context_strength > 5 else 'medium',
                    'icon': '',
                    'data_sources': ['projects', 'tasks', 'people', 'emails'],
                    'cross_references': context_strength
                })
        
        # 3. TOPIC-DRIVEN BUSINESS INTELLIGENCE
        for topic in strategic_topics[:3]:
            topic_emails = [e for e in analyzed_emails if e.topics and topic.name in e.topics]
            topic_people = list(set([e.sender_name or e.sender.split('@')[0] for e in topic_emails 
                                   if e.sender and e.sender_name]))
            topic_projects = [p for p in active_projects if p.key_topics and topic.name in p.key_topics]
            topic_tasks = [t for t in actionable_tasks if topic.name.lower() in (t.description or '').lower()]
            
            if topic_emails and len(topic_emails) >= 2:
                latest_activity = max(topic_emails, key=lambda x: x.email_date or datetime.min)
                
                # Extract strategic insights for this topic
                topic_decisions = []
                topic_opportunities = []
                for email in topic_emails:
                    if email.key_insights and isinstance(email.key_insights, dict):
                        topic_decisions.extend(email.key_insights.get('key_decisions', []))
                        topic_opportunities.extend(email.key_insights.get('strategic_opportunities', []))
                
                comprehensive_context = f"Active across {len(topic_emails)} communications, {len(topic_people)} people, {len(topic_projects)} projects, {len(topic_tasks)} tasks"
                if topic_decisions:
                    comprehensive_context += f". Recent decision: {topic_decisions[0][:60]}..."
                
                insights.append({
                    'type': 'topic_intelligence_360',
                    'title': f'Strategic Topic: {topic.name}',
                    'description': comprehensive_context,
                    'details': f'Latest activity: {latest_activity.ai_summary[:100] if latest_activity.ai_summary else "Recent discussions"}',
                    'action': f'Deep dive into {topic.name} strategy - significant cross-functional activity detected',
                    'priority': 'high',
                    'icon': '',
                    'data_sources': ['topics', 'emails', 'people', 'projects', 'tasks'],
                    'cross_references': len(topic_emails) + len(topic_projects) + len(topic_tasks)
                })
        
        # 4. CALENDAR-DRIVEN ACTION INTELLIGENCE
        high_value_meetings = [m for m in upcoming_meetings if m.attendees and len(m.attendees) >= 3]
        for meeting in high_value_meetings[:2]:
            meeting_attendees = [att.get('email') for att in meeting.attendees if att.get('email')]
            known_attendees = [p for p in human_contacts if p.email_address in meeting_attendees]
            
            # Find email history with these attendees
            attendee_emails = []
            for attendee in known_attendees:
                person_emails = [e for e in analyzed_emails if e.sender and attendee.email_address and 
                               e.sender.lower() == attendee.email_address.lower()]
                attendee_emails.extend(person_emails)
            
            # Find related projects
            meeting_projects = []
            for attendee in known_attendees:
                attendee_projects = context_map['people_to_projects'].get(attendee.id, [])
                meeting_projects.extend(attendee_projects)
            
            if attendee_emails or meeting_projects:
                context_richness = len(attendee_emails) + len(meeting_projects)
                
                # Fix timezone comparison issue
                meeting_start_time = meeting.start_time
                if meeting_start_time and not hasattr(meeting_start_time, 'tzinfo'):
                    # If it's offset-naive, assume UTC
                    meeting_start_time = meeting_start_time.replace(tzinfo=timezone.utc)
                elif meeting_start_time and meeting_start_time.tzinfo is None:
                    meeting_start_time = meeting_start_time.replace(tzinfo=timezone.utc)
                
                insights.append({
                    'type': 'meeting_intelligence_360',
                    'title': f'Strategic Meeting: {meeting.title}',
                    'description': f'{len(known_attendees)} known attendees, {len(attendee_emails)} related communications, {len(set(meeting_projects))} connected projects',
                    'details': f'Meeting Date: {meeting_start_time.strftime("%Y-%m-%d %H:%M") if meeting_start_time else "TBD"}. Rich context available for preparation.',
                    'action': f'Prepare comprehensive brief for {meeting.title} using relationship and project intelligence',
                    'priority': 'high',
                    'icon': '',
                    'data_sources': ['calendar', 'people', 'emails', 'projects'],
                    'cross_references': context_richness
                })
        
        # 5. STRATEGIC DECISION & OPPORTUNITY SYNTHESIS
        all_decisions = []
        all_opportunities = []
        decision_makers = {}
        
        for email in analyzed_emails[-50:]:  # Recent 50 emails
            if email.key_insights and isinstance(email.key_insights, dict):
                decisions = email.key_insights.get('key_decisions', [])
                opportunities = email.key_insights.get('strategic_opportunities', [])
                
                for decision in decisions:
                    if len(decision) > 30:  # Substantial decisions
                        decision_person = next((p for p in human_contacts if p.email_address and 
                                              p.email_address.lower() == email.sender.lower()), None)
                        all_decisions.append({
                            'decision': decision,
                            'person': decision_person,
                            'email_date': email.email_date,
                            'context': email.ai_summary
                        })
                        
                        if decision_person:
                            decision_makers[decision_person.name] = decision_makers.get(decision_person.name, 0) + 1
                
                for opp in opportunities:
                    if len(opp) > 30:
                        all_opportunities.append({
                            'opportunity': opp,
                            'source': email.sender_name or email.sender.split('@')[0],
                            'email_date': email.email_date,
                            'context': email.ai_summary
                        })
        
        if all_decisions:
            top_decision = max(all_decisions, key=lambda x: x['email_date'] or datetime.min)
            key_decision_maker = max(decision_makers.items(), key=lambda x: x[1])[0] if decision_makers else "Team"
            
            insights.append({
                'type': 'strategic_decision_360',
                'title': f'Strategic Decision Tracking',
                'description': f'{len(all_decisions)} recent decisions identified, {key_decision_maker} driving {decision_makers.get(key_decision_maker, 1)} decisions',
                'details': f'Latest: {top_decision["decision"][:120]}...',
                'action': f'Review decision implementation and coordinate with {key_decision_maker}',
                'priority': 'high',
                'icon': '',
                'data_sources': ['emails', 'people', 'insights'],
                'cross_references': len(decision_makers)
            })
        
        if all_opportunities:
            top_opportunity = max(all_opportunities, key=lambda x: x['email_date'] or datetime.min)
            
            insights.append({
                'type': 'strategic_opportunity_360',
                'title': f'Business Opportunity Intelligence',
                'description': f'{len(all_opportunities)} opportunities identified across recent communications',
                'details': f'Latest: {top_opportunity["opportunity"][:120]}...',
                'action': f'Evaluate opportunity pipeline and develop action plans with {top_opportunity["source"]}',
                'priority': 'medium',
                'icon': '',
                'data_sources': ['emails', 'insights', 'people'],
                'cross_references': len(set([o["source"] for o in all_opportunities]))
            })
        
        # 6. COMPREHENSIVE BUSINESS INTELLIGENCE SUMMARY
        insights.append({
            'type': 'comprehensive_intelligence_360',
            'title': '360-Context Business Intelligence Status',
            'description': f'Comprehensive analysis: {len(analyzed_emails)} emails, {len(human_contacts)} contacts, {len(active_projects)} projects, {len(actionable_tasks)} tasks, {len(strategic_topics)} topics, {len(upcoming_meetings)} meetings',
            'details': f'Cross-referenced {sum([i.get("cross_references", 0) for i in insights])} data connections. Your business intelligence ecosystem is actively building comprehensive knowledge.',
            'action': 'Review 360-context insights above for strategic decision-making',
            'priority': 'low',
            'icon': '',
            'data_sources': ['emails', 'people', 'projects', 'tasks', 'topics', 'calendar'],
            'cross_references': len(context_map)
        })
        
        # FALLBACK GUIDANCE IF NO DATA
        if not insights or len(insights) <= 1:
            insights = [{
                'type': 'guidance_360',
                'title': 'Building 360-Context Intelligence',
                'description': 'Process more emails and calendar events to unlock comprehensive business insights',
                'details': 'Your AI Chief of Staff will cross-reference communications, relationships, projects, and meetings for strategic intelligence.',
                'action': 'Use "Process Emails" to build your comprehensive 360-context business knowledge base',
                'priority': 'medium',
                'icon': '',
                'data_sources': ['system'],
                'cross_references': 0
            }]
        
        # Sort by priority and cross-reference strength
        priority_order = {'high': 0, 'medium': 1, 'low': 2}
        insights.sort(key=lambda x: (priority_order.get(x['priority'], 2), -x.get('cross_references', 0)))
        
        return insights[:6]  # Top 6 most strategic insights
        
    except Exception as e:
        logger.error(f"Error generating 360-context strategic insights: {str(e)}")
        return [{
            'type': 'error',
            'title': '360-Context Analysis Error',
            'description': f'Error in comprehensive business intelligence: {str(e)[:100]}',
            'details': 'Please try processing emails and calendar again',
            'action': 'Rebuild your 360-context business intelligence',
            'priority': 'medium',
            'icon': '',
            'data_sources': ['error'],
            'cross_references': 0
        }]

def create_app():
    """Create and configure the Flask application"""
    app = Flask(__name__)
    
    # Configuration
    app.secret_key = settings.SECRET_KEY
    app.config['SESSION_TYPE'] = 'filesystem'
    app.config['SESSION_FILE_DIR'] = os.path.join(tempfile.gettempdir(), 'cos_flask_session')
    app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(hours=settings.SESSION_TIMEOUT_HOURS)
    
    # Initialize extensions
    Session(app)
    
    # Create necessary directories
    settings.create_directories()
    
    # Initialize Claude client
    claude_client = None
    if settings.ANTHROPIC_API_KEY:
        claude_client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
    
    def get_current_user():
        """Get current authenticated user with proper session isolation"""
        if 'user_email' not in session or 'db_user_id' not in session:
            return None
        
        try:
            # Use the db_user_id from session for proper isolation
            user_id = session['db_user_id']
            
            # This check is safer than re-fetching by email every time
            # For this request context, we can trust the session's user_id
            # Re-fetching the user should only be for sensitive operations
            # or to refresh data, not for basic authentication checks.
            # A full user object is not always needed here.
            # For simplicity, we'll return a lightweight object or dict.
            
            # Placeholder for a more robust user object if needed later
            # For now, this is enough to confirm an active session.
            current_user = {'id': user_id, 'email': session['user_email']}
            return current_user
            
        except Exception as e:
            logger.error(f"Error retrieving current user from session: {e}")
            session.clear()
            return None
    
    # Routes
    @app.route('/')
    def index():
        user = get_current_user()
        if not user:
            return redirect('/auth/google')
        
        # Redirect to home page
        return redirect('/home')
    
    @app.route('/home')
    def home():
        user = get_current_user()
        if not user:
            return redirect('/auth/google')
        
        return render_template('home.html', 
                               user_email=user['email'],
                               user_id=user.get('id'),
                               session_id=session.get('session_id'),
                               cache_buster=int(time.time()))
    
    @app.route('/tasks')
    def tasks():
        user = get_current_user()
        if not user:
            return redirect('/auth/google')
        
        return render_template('tasks.html', 
                               user_email=user['email'],
                               user_id=user.get('id'),
                               session_id=session.get('session_id'),
                               cache_buster=int(time.time()))
    
    @app.route('/people')
    def people_page():
        """People management page"""
        user = get_current_user()
        if not user:
            return redirect('/login')
        return render_template('people.html')
    
    @app.route('/knowledge')
    def knowledge_page():
        """Knowledge management page"""
        user = get_current_user()
        if not user:
            return redirect('/login')
        return render_template('knowledge.html')
    
    @app.route('/calendar')
    def calendar_page():
        """Calendar management page"""
        user_email = session.get('user_email')
        
        if not user_email:
            return redirect(url_for('login'))
        
        return render_template('calendar.html')
    
    @app.route('/settings')
    def settings_page():
        """Settings page for configuring email sync and other preferences"""
        user = get_current_user()
        if not user:
            return redirect('/login')
        return render_template('settings.html')
    
    @app.route('/dashboard')
    def dashboard():
        """Legacy dashboard route - redirect to home"""
        return redirect('/home')
    
    @app.route('/login')
    def login():
        """Login page with Google OAuth"""
        # Check for logout/switching parameters
        logged_out = request.args.get('logged_out') == 'true'
        force_logout = request.args.get('force_logout') == 'true'
        
        context = {
            'logged_out': logged_out,
            'force_logout': force_logout,
            'switching_users': logged_out or force_logout
        }
        
        return render_template('login.html', **context)
    
    @app.route('/auth/google')
    def google_auth():
        """Initiate Google OAuth flow"""
        try:
            # Generate unique state for security
            state = f"cos_{session.get('csrf_token', 'temp')}"
            
            # Get authorization URL from our Gmail auth handler
            auth_url, state = gmail_auth.get_authorization_url(
                user_id=session.get('temp_user_id', 'anonymous'),
                state=state
            )
            
            # Store state in session for validation
            session['oauth_state'] = state
            
            return redirect(auth_url)
            
        except Exception as e:
            logger.error(f"Failed to initiate Google OAuth: {str(e)}")
            return redirect(url_for('login') + '?error=oauth_init_failed')
    
    @app.route('/auth/google/callback')
    def google_callback():
        """Handle Google OAuth callback with enhanced session management"""
        try:
            # Get authorization code and state
            code = request.args.get('code')
            state = request.args.get('state')
            error = request.args.get('error')
            
            if error:
                logger.error(f"OAuth error: {error}")
                return redirect(url_for('login') + f'?error={error}')
            
            if not code:
                logger.error("No authorization code received")
                return redirect(url_for('login') + '?error=no_code')
            
            # Validate state (basic security check)
            expected_state = session.get('oauth_state')
            if state != expected_state:
                logger.error(f"OAuth state mismatch: {state} != {expected_state}")
                return redirect(url_for('login') + '?error=state_mismatch')
            
            # Handle OAuth callback with our Gmail auth handler
            result = gmail_auth.handle_oauth_callback(
                authorization_code=code,
                state=state
            )
            
            if not result.get('success'):
                error_msg = result.get('error', 'Unknown OAuth error')
                logger.error(f"OAuth callback failed: {error_msg}")
                return redirect(url_for('login') + f'?error=oauth_failed')
            
            # COMPLETE SESSION RESET - Critical for user isolation
            session.clear()
            
            # Extract user info from OAuth result
            user_info = result.get('user_info', {})
            user_email = user_info.get('email')
            
            if not user_email:
                logger.error("No email received from OAuth")
                return redirect(url_for('login') + '?error=no_email')
            
            # Get or create user in database
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                logger.error(f"User not found in database: {user_email}")
                return redirect(url_for('login') + '?error=user_not_found')
            
            # Set new session data with unique session ID
            session_id = str(uuid.uuid4())
            session['session_id'] = session_id
            session['user_email'] = user_email
            session['user_name'] = user_info.get('name')
            session['google_id'] = user_info.get('id')  # Google ID
            session['authenticated'] = True
            session['db_user_id'] = user.id  # Database ID for queries - CRITICAL
            session['login_time'] = datetime.now().isoformat()
            session.permanent = True
            
            logger.info(f"User authenticated successfully: {user_email} (DB ID: {user.id}, Session: {session_id})")
            
            # Create response with cache busting
            response = redirect(url_for('index') + '?login_success=true&t=' + str(int(datetime.now().timestamp())))
            response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            
            return response
            
        except Exception as e:
            logger.error(f"OAuth callback error: {str(e)}")
            return redirect(url_for('login') + '?error=callback_failed')
    
    @app.route('/logout')
    def logout():
        """Logout and clear session completely"""
        user_email = session.get('user_email')
        
        # Complete session cleanup
        session.clear()
        
        # Clear any persistent session files
        try:
            import shutil
            import tempfile
            session_dir = os.path.join(tempfile.gettempdir(), 'cos_flask_session')
            if os.path.exists(session_dir):
                # Clear old session files
                for filename in os.listdir(session_dir):
                    if filename.startswith('flask_session_'):
                        try:
                            os.remove(os.path.join(session_dir, filename))
                        except:
                            pass
        except Exception as e:
            logger.warning(f"Could not clear session files: {e}")
        
        logger.info(f"User logged out completely: {user_email}")
        
        # Redirect to login with cache-busting parameter
        response = redirect(url_for('login') + '?logged_out=true')
        
        # Clear all cookies
        response.set_cookie('session', '', expires=0)
        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'
        
        return response
    
    @app.route('/debug/session')
    def debug_session():
        """Debug session information"""
        return jsonify({
            'session_data': dict(session),
            'user_email': session.get('user_email'),
            'authenticated': session.get('authenticated'),
            'session_keys': list(session.keys())
        })
    
    @app.route('/force-logout')
    def force_logout():
        """Force complete logout and session reset - use when switching users"""
        try:
            # Clear current session
            user_email = session.get('user_email', 'unknown')
            session.clear()
            
            # Clear all session files
            import tempfile
            session_dir = os.path.join(tempfile.gettempdir(), 'cos_flask_session')
            if os.path.exists(session_dir):
                for filename in os.listdir(session_dir):
                    if filename.startswith('flask_session_'):
                        try:
                            os.remove(os.path.join(session_dir, filename))
                            logger.info(f"Cleared session file: {filename}")
                        except Exception as e:
                            logger.warning(f"Could not clear session file {filename}: {e}")
            
            logger.info(f"Force logout completed for: {user_email}")
            
            # Create response with aggressive cache clearing
            response = redirect(url_for('login') + '?force_logout=true&t=' + str(int(datetime.now().timestamp())))
            
            # Clear all possible cookies and cache
            response.set_cookie('session', '', expires=0, path='/')
            response.set_cookie('flask-session', '', expires=0, path='/')
            response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate, max-age=0'
            response.headers['Pragma'] = 'no-cache'
            response.headers['Expires'] = '0'
            response.headers['Clear-Site-Data'] = '"cache", "cookies", "storage"'
            
            return response
            
        except Exception as e:
            logger.error(f"Force logout error: {e}")
            return jsonify({'error': 'Force logout failed', 'details': str(e)}), 500
    
    @app.route('/api/fetch-emails', methods=['POST'])
    def api_fetch_emails():
        """API endpoint to fetch emails"""
        if 'user_email' not in session:
            return jsonify({'error': 'Not authenticated'}), 401
        
        user_email = session['user_email']
        
        try:
            data = request.get_json() or {}
            max_emails = data.get('max_emails', 10)
            days_back = data.get('days_back', 7)
            
            # Fetch emails using correct method name
            result = gmail_fetcher.fetch_recent_emails(
                user_email=user_email,
                limit=max_emails,
                days_back=days_back
            )
            
            return jsonify(result)
            
        except Exception as e:
            logger.error(f"Email fetch API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/trigger-email-sync', methods=['POST'])
    def api_trigger_email_sync():
        """UNIFIED EMAIL AND CALENDAR PROCESSING - Single trigger that does everything including calendar sync"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        try:
            data = request.get_json() or {}
            max_emails = data.get('max_emails', 20)
            days_back = data.get('days_back', 7)
            force_refresh = data.get('force_refresh', False)
            
            user_email = user['email']
            
            # Validate parameters
            if max_emails < 1 or max_emails > 500:
                return jsonify({'error': 'max_emails must be between 1 and 500'}), 400
            if days_back < 1 or days_back > 365:
                return jsonify({'error': 'days_back must be between 1 and 365'}), 400
            
            logger.info(f" Starting unified email + calendar processing for {user_email}: {max_emails} emails, {days_back} days back")
            
            # Clear any existing cache to ensure fresh processing
            from chief_of_staff_ai.strategic_intelligence.strategic_intelligence_cache import strategic_intelligence_cache
            strategic_intelligence_cache.invalidate(user_email)
            
            # STEP 1: Fetch emails from Gmail
            logger.info(" Step 1: Fetching emails from Gmail...")
            fetch_result = gmail_fetcher.fetch_recent_emails(
                user_email=user_email,
                limit=max_emails,
                days_back=days_back,
                force_refresh=force_refresh
            )
            
            if not fetch_result.get('success'):
                return jsonify({
                    'success': False,
                    'error': f"Email fetch failed: {fetch_result.get('error')}",
                    'stage': 'fetch'
                }), 400
            
            emails_fetched = fetch_result.get('count', 0)
            logger.info(f" Fetched {emails_fetched} emails from Gmail")
            
            # STEP 2: Fetch calendar events (NEW!)
            logger.info(" Step 2: Syncing calendar from Google Calendar...")
            calendar_result = calendar_fetcher.fetch_calendar_events(
                user_email=user_email,
                days_back=3,
                days_forward=14,
                force_refresh=force_refresh
            )
            
            calendar_events_fetched = 0
            prep_tasks_created = 0
            
            if calendar_result.get('success'):
                calendar_events_fetched = calendar_result.get('count', 0)
                logger.info(f" Synced {calendar_events_fetched} calendar events")
                
                # AUTOMATIC 360-CONTEXT PREP TASK CREATION (NEW AUGMENTATION FEATURE!)
                if calendar_result.get('events'):
                    logger.info(" Step 2b: Creating 360-context meeting preparation tasks...")
                    events_for_tasks = []
                    for event_dict in calendar_result['events']:
                        try:
                            if isinstance(event_dict, dict):
                                # Already a dict, use as-is
                                events_for_tasks.append(event_dict)
                            elif hasattr(event_dict, 'to_dict') and callable(getattr(event_dict, 'to_dict')):
                                # Has a to_dict method, use it (for Calendar objects)
                                event_as_dict = event_dict.to_dict()
                                events_for_tasks.append(event_as_dict)
                            else:
                                # Manual conversion for Calendar database objects
                                event_as_dict = {
                                    'event_id': getattr(event_dict, 'event_id', None),
                                    'title': getattr(event_dict, 'title', 'Untitled Event'),
                                    'description': getattr(event_dict, 'description', ''),
                                    'start_time': getattr(event_dict, 'start_time', None),
                                    'end_time': getattr(event_dict, 'end_time', None),
                                    'location': getattr(event_dict, 'location', ''),
                                    'attendees': getattr(event_dict, 'attendees', []),
                                    'calendar_id': getattr(event_dict, 'calendar_id', ''),
                                    'created_at': getattr(event_dict, 'created_at', None)
                                }
                                
                                # Ensure attendees is a list, not a string
                                if isinstance(event_as_dict['attendees'], str):
                                    try:
                                        import json
                                        event_as_dict['attendees'] = json.loads(event_as_dict['attendees'])
                                    except:
                                        event_as_dict['attendees'] = []
                                elif not isinstance(event_as_dict['attendees'], list):
                                    event_as_dict['attendees'] = []
                                
                                # Convert datetime objects to isoformat strings if needed
                                for time_field in ['start_time', 'end_time']:
                                    if event_as_dict[time_field] and hasattr(event_as_dict[time_field], 'isoformat'):
                                        event_as_dict[time_field] = event_as_dict[time_field].isoformat()
                                
                                events_for_tasks.append(event_as_dict)
                                
                        except Exception as convert_error:
                            logger.error(f"Failed to convert event to dict: {str(convert_error)}")
                            logger.error(f"Event type: {type(event_dict)}")
                            # Continue with next event instead of failing completely
                            continue
                    
                    if events_for_tasks:
                        try:
                            logger.info(f"Processing {len(events_for_tasks)} events for meeting prep tasks...")
                            prep_tasks_result = calendar_fetcher.create_meeting_prep_tasks(user.id, events_for_tasks)
                            prep_tasks_created = prep_tasks_result.get('prep_tasks_created', 0)
                            logger.info(f" Created {prep_tasks_created} intelligent meeting prep tasks using 360-context analysis")
                        except Exception as prep_error:
                            logger.error(f"Meeting prep task creation failed: {str(prep_error)}")
                            prep_tasks_created = 0
                    else:
                        logger.warning("No valid events available for meeting prep task creation")
                        prep_tasks_created = 0
            else:
                logger.warning(f" Calendar sync failed: {calendar_result.get('error', 'Unknown error')}")
            
            # STEP 3: Normalize emails for better quality
            logger.info(" Step 3: Normalizing emails...")
            normalize_result = email_normalizer.normalize_user_emails(user_email, limit=max_emails)
            emails_normalized = normalize_result.get('processed', 0)
            logger.info(f" Normalized {emails_normalized} emails")
            
            # STEP 4: Process with AI to extract people, tasks, projects, insights
            logger.info(" Step 4: Processing emails with AI intelligence...")
            intelligence_result = email_intelligence.process_user_emails_intelligently(
                user_email=user_email,
                limit=max_emails,
                force_refresh=force_refresh
            )
            
            # STEP 5: Analyze calendar free time (NEW!)
            logger.info(" Step 5: Analyzing calendar free time...")
            free_time_analysis = calendar_fetcher.fetch_free_time_analysis(
                user_email=user_email,
                days_forward=7
            )
            
            free_time_slots = 0
            if free_time_analysis.get('success'):
                free_time_slots = len(free_time_analysis.get('free_slots', []))
                logger.info(f" Found {free_time_slots} free time slots")
            
            # STEP 6: Get final counts from database
            logger.info(" Step 6: Calculating final results...")
            db_user = get_db_manager().get_user_by_email(user_email)
            
            if db_user:
                # Get actual counts from database
                all_emails = get_db_manager().get_user_emails(db_user.id)
                all_people = get_db_manager().get_user_people(db_user.id)
                all_tasks = get_db_manager().get_user_tasks(db_user.id)
                all_projects = get_db_manager().get_user_projects(db_user.id)
                all_calendar_events = get_db_manager().get_user_calendar_events(db_user.id)
                
                # Filter for meaningful data
                analyzed_emails = [e for e in all_emails if e.ai_summary and len(e.ai_summary.strip()) > 10]
                real_people = [p for p in all_people if p.name and p.email_address and '@' in p.email_address]
                real_tasks = [t for t in all_tasks if t.description and len(t.description.strip()) > 5]
                active_projects = [p for p in all_projects if p.status == 'active']
                
                # Calculate business insights
                strategic_insights = get_strategic_business_insights(user_email)
                
                logger.info(f" Final Results:")
                logger.info(f"    {len(analyzed_emails)} emails with AI analysis")
                logger.info(f"    {len(all_calendar_events)} calendar events synced")
                logger.info(f"    {len(real_people)} real people extracted")
                logger.info(f"    {len(real_tasks)} actionable tasks created")
                logger.info(f"    {len(active_projects)} active projects identified")
                logger.info(f"    {len(strategic_insights)} business insights generated")
                logger.info(f"    {free_time_slots} free time slots analyzed")
                logger.info(f"    {prep_tasks_created} intelligent meeting prep tasks (360-context)")
                
                # Enhanced success response with calendar data
                return jsonify({
                    'success': True,
                    'message': f'Successfully processed {emails_fetched} emails and {calendar_events_fetched} calendar events!',
                    'processing_stages': {
                        'emails_fetched': emails_fetched,
                        'calendar_events_fetched': calendar_events_fetched,
                        'emails_normalized': emails_normalized,
                        'emails_analyzed': len(analyzed_emails),
                        'ai_processing_success': intelligence_result.get('success', False),
                        'free_time_slots_found': free_time_slots,
                        'prep_tasks_created': prep_tasks_created
                    },
                    'database_populated': {
                        'total_emails': len(all_emails),
                        'emails_with_ai': len(analyzed_emails),
                        'people_extracted': len(real_people),
                        'tasks_created': len(real_tasks),
                        'projects_identified': len(active_projects),
                        'insights_generated': len(strategic_insights),
                        'calendar_events': len(all_calendar_events),
                        'free_time_slots': free_time_slots
                    },
                    'data_ready': {
                        'people_tab': len(real_people) > 0,
                        'tasks_tab': len(real_tasks) > 0,
                        'calendar_tab': len(all_calendar_events) > 0,
                        'insights_tab': len(strategic_insights) > 0,
                        'all_tabs_populated': len(real_people) > 0 and len(real_tasks) > 0 and len(strategic_insights) > 0 and len(all_calendar_events) > 0
                    },
                    'results': {
                        'fetch': fetch_result,
                        'normalize': normalize_result,
                        'intelligence': intelligence_result,
                        'calendar': calendar_result,
                        'free_time': free_time_analysis
                    },
                    'summary': {
                        'emails_fetched': emails_fetched,
                        'calendar_events_fetched': calendar_events_fetched,
                        'emails_normalized': emails_normalized,
                        'emails_analyzed': intelligence_result.get('processed_emails', len(analyzed_emails)),
                        'insights_extracted': intelligence_result.get('insights_extracted', len(strategic_insights)),
                        'people_identified': intelligence_result.get('people_identified', len(real_people)),
                        'projects_identified': intelligence_result.get('projects_identified', len(active_projects)),
                        'tasks_created': intelligence_result.get('tasks_created', len(real_tasks)),
                        'total_emails': len(all_emails),
                        'total_tasks': len(real_tasks),
                        'total_people': len(real_people),
                        'total_calendar_events': len(all_calendar_events),
                        'free_time_slots': free_time_slots
                    },
                    'next_steps': [
                        f" {len(real_people)} people are now available in the People tab",
                        f" {len(real_tasks)} tasks are now available in the Tasks tab", 
                        f" {len(all_calendar_events)} calendar events are now available in the Calendar tab",
                        f" {free_time_slots} free time slots identified for productivity",
                        f" {len(strategic_insights)} insights are now available on the Home tab",
                        f" {prep_tasks_created} intelligent meeting prep tasks created with 360-context analysis",
                        " All data is now populated and ready to use!"
                    ] if len(real_people) > 0 or len(all_calendar_events) > 0 else [
                        " No meaningful data found - try processing more emails or check your Gmail/Calendar connection"
                    ]
                })
            else:
                return jsonify({
                    'success': False,
                    'error': 'User not found after processing',
                    'stage': 'final_verification'
                }), 500
            
        except Exception as e:
            logger.error(f" Unified email + calendar processing error: {str(e)}")
            return jsonify({
                'success': False,
                'error': f'Processing failed: {str(e)}',
                'stage': 'processing'
            }), 500
    
    @app.route('/api/process-emails', methods=['POST'])
    def api_process_emails():
        """DEPRECATED: Redirect to unified trigger-email-sync endpoint"""
        return api_trigger_email_sync()
    
    @app.route('/api/chat', methods=['POST'])
    def api_chat():
        """API endpoint for Claude chat functionality with comprehensive knowledge context"""
        if 'user_email' not in session:
            return jsonify({'error': 'Not authenticated'}), 401
        
        if not claude_client:
            return jsonify({'error': 'Claude integration not configured'}), 500
        
        try:
            data = request.get_json()
            message = data.get('message')
            include_context = data.get('include_context', True)
            
            if not message:
                return jsonify({'error': 'No message provided'}), 400
            
            user_email = session['user_email']
            
            # Get comprehensive knowledge context if requested
            context_parts = []
            if include_context:
                try:
                    # Get comprehensive business knowledge
                    knowledge_response = email_intelligence.get_chat_knowledge_summary(user_email)
                    if knowledge_response.get('success'):
                        knowledge = knowledge_response['knowledge_base']
                        
                        # Add business intelligence context
                        if knowledge.get('business_intelligence'):
                            bi = knowledge['business_intelligence']
                            
                            if bi.get('recent_decisions'):
                                context_parts.append("RECENT BUSINESS DECISIONS:\n" + "\n".join([f"- {decision}" for decision in bi['recent_decisions'][:5]]))
                            
                            if bi.get('top_opportunities'):
                                context_parts.append("BUSINESS OPPORTUNITIES:\n" + "\n".join([f"- {opp}" for opp in bi['top_opportunities'][:5]]))
                            
                            if bi.get('current_challenges'):
                                context_parts.append("CURRENT CHALLENGES:\n" + "\n".join([f"- {challenge}" for challenge in bi['current_challenges'][:5]]))
                        
                        # Add people context
                        if knowledge.get('rich_contacts'):
                            contacts_summary = []
                            for contact in knowledge['rich_contacts'][:10]:  # Top 10 contacts
                                contact_info = f"{contact['name']}"
                                if contact.get('title') and contact.get('company'):
                                    contact_info += f" ({contact['title']} at {contact['company']})"
                                if contact.get('relationship'):
                                    contact_info += f" - {contact['relationship']}"
                                contacts_summary.append(contact_info)
                            
                            if contacts_summary:
                                context_parts.append("KEY BUSINESS CONTACTS:\n" + "\n".join([f"- {contact}" for contact in contacts_summary]))
                        
                        # Add recent tasks context
                        db_user = get_db_manager().get_user_by_email(user_email)
                        if db_user:
                            tasks = get_db_manager().get_user_tasks(db_user.id, limit=10)
                            if tasks:
                                recent_tasks = [f"{task.description} (Priority: {task.priority}, Status: {task.status})" for task in tasks[:10]]
                                context_parts.append("CURRENT TASKS:\n" + "\n".join([f"- {task}" for task in recent_tasks]))
                            
                            # Add topics context
                            topics = get_db_manager().get_user_topics(db_user.id)
                            if topics:
                                official_topics = [t.name for t in topics if t.is_official][:5]
                                if official_topics:
                                    context_parts.append("OFFICIAL TOPICS:\n" + "\n".join([f"- {topic}" for topic in official_topics]))
                
                except Exception as context_error:
                    logger.warning(f"Failed to load context for chat: {context_error}")
            
            # Create enhanced system prompt with comprehensive business context
            business_context = "\n\n".join(context_parts) if context_parts else "No specific business context available."
            
            system_prompt = f"""You are an AI Chief of Staff assistant for {user_email}. You have access to their comprehensive business knowledge and should provide intelligent, context-aware responses.

BUSINESS CONTEXT:
{business_context}

INSTRUCTIONS:
- Use the business context above to provide informed, specific responses
- Reference specific people, decisions, opportunities, or challenges when relevant
- Provide actionable insights and recommendations based on the available data
- If asked about people, reference their roles, relationships, and relevant context
- For task or project questions, consider current priorities and deadlines
- Be professional but conversational
- If you don't have enough context for a specific question, say so and suggest what information would help

Always think about the bigger picture and provide strategic, helpful advice based on the user's business situation."""
            
            # Send message to Claude with enhanced context
            response = claude_client.messages.create(
                model=settings.CLAUDE_MODEL,
                max_tokens=3000,
                system=system_prompt,
                messages=[{
                    "role": "user",
                    "content": message
                }]
            )
            
            assistant_response = response.content[0].text
            
            return jsonify({
                'response': assistant_response,
                'model': settings.CLAUDE_MODEL,
                'context_included': include_context and len(context_parts) > 0,
                'context_summary': f"Included {len(context_parts)} context sections" if context_parts else "No context included"
            })
            
        except Exception as e:
            logger.error(f"Chat API error: {str(e)}")
            return jsonify({'error': f'Chat error: {str(e)}'}), 500

    @app.route('/api/chat-with-knowledge', methods=['POST'])
    def api_chat_with_knowledge():
        """API endpoint for enhanced Claude chat with full business knowledge context"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        if not claude_client:
            return jsonify({'error': 'Claude integration not configured'}), 500
        
        try:
            data = request.get_json()
            message = data.get('message')
            
            if not message:
                return jsonify({'error': 'No message provided'}), 400
            
            user_email = user['email']
            
            # This endpoint ALWAYS includes comprehensive knowledge context
            # Get comprehensive business knowledge
            knowledge_response = email_intelligence.get_chat_knowledge_summary(user_email)
            business_knowledge = knowledge_response.get('knowledge_base', {}) if knowledge_response.get('success') else {}
            
            # Build comprehensive context
            context_parts = []
            
            # Business intelligence
            if business_knowledge.get('business_intelligence'):
                bi = business_knowledge['business_intelligence']
                
                if bi.get('recent_decisions'):
                    context_parts.append("STRATEGIC BUSINESS DECISIONS:\n" + "\n".join([
                        f"- {decision if isinstance(decision, str) else decision.get('decision', 'Unknown decision')}" 
                        for decision in bi['recent_decisions'][:8]
                    ]))
                
                if bi.get('top_opportunities'):
                    context_parts.append("BUSINESS OPPORTUNITIES:\n" + "\n".join([
                        f"- {opp if isinstance(opp, str) else opp.get('opportunity', 'Unknown opportunity')}" 
                        for opp in bi['top_opportunities'][:8]
                    ]))
                
                if bi.get('current_challenges'):
                    context_parts.append("CURRENT CHALLENGES:\n" + "\n".join([
                        f"- {challenge if isinstance(challenge, str) else challenge.get('challenge', 'Unknown challenge')}" 
                        for challenge in bi['current_challenges'][:8]
                    ]))
            
            # Rich contacts
            if business_knowledge.get('rich_contacts'):
                contacts_summary = []
                for contact in business_knowledge['rich_contacts'][:15]:
                    contact_info = f"{contact['name']}"
                    if contact.get('title') and contact.get('company'):
                        contact_info += f" ({contact['title']} at {contact['company']})"
                    elif contact.get('company'):
                        contact_info += f" (at {contact['company']})"
                    elif contact.get('title'):
                        contact_info += f" ({contact['title']})"
                    if contact.get('relationship'):
                        contact_info += f" - {contact['relationship']}"
                    contacts_summary.append(contact_info)
                
                if contacts_summary:
                    context_parts.append("KEY PROFESSIONAL CONTACTS:\n" + "\n".join([f"- {contact}" for contact in contacts_summary]))
            
            # Current data from database
            db_user = get_db_manager().get_user_by_email(user_email)
            if db_user:
                # Recent tasks
                tasks = get_db_manager().get_user_tasks(db_user.id, limit=15)
                if tasks:
                    task_summaries = []
                    for task in tasks:
                        task_info = task.description
                        if task.priority and task.priority != 'medium':
                            task_info += f" (Priority: {task.priority})"
                        if task.status != 'pending':
                            task_info += f" (Status: {task.status})"
                        if task.due_date:
                            task_info += f" (Due: {task.due_date.strftime('%Y-%m-%d')})"
                        task_summaries.append(task_info)
                    
                    context_parts.append("CURRENT TASKS:\n" + "\n".join([f"- {task}" for task in task_summaries]))
                
                # Active projects
                projects = get_db_manager().get_user_projects(db_user.id, status='active', limit=10)
                if projects:
                    project_summaries = [f"{p.name} - {p.description[:100] if p.description else 'No description'}" for p in projects]
                    context_parts.append("ACTIVE PROJECTS:\n" + "\n".join([f"- {proj}" for proj in project_summaries]))
                
                # Official topics for context
                topics = get_db_manager().get_user_topics(db_user.id)
                official_topics = [t.name for t in topics if t.is_official][:8]
                if official_topics:
                    context_parts.append("OFFICIAL BUSINESS TOPICS:\n" + "\n".join([f"- {topic}" for topic in official_topics]))
            
            # Create comprehensive system prompt
            business_context = "\n\n".join(context_parts) if context_parts else "Limited business context available."
            
            enhanced_system_prompt = f"""You are an expert AI Chief of Staff for {user_email}. You have comprehensive access to their business knowledge, communications, contacts, and ongoing work. Your role is to provide strategic, informed assistance.

COMPREHENSIVE BUSINESS KNOWLEDGE:
{business_context}

YOUR CAPABILITIES:
- Strategic business advisory based on actual data
- Relationship management insights from real contacts
- Task and project coordination with current context
- Decision support using historical business intelligence
- Personalized recommendations based on communication patterns

RESPONSE GUIDELINES:
- Always leverage the specific business context provided above
- Reference actual people, projects, and decisions when relevant
- Provide actionable, strategic advice tailored to their business situation
- Be direct and practical while maintaining professionalism
- When you lack specific information, explicitly state what additional context would help
- Prioritize insights that connect to their ongoing work and relationships

Remember: This knowledge base represents their actual business communications and relationships. Use it to provide highly personalized, contextually aware assistance."""
            
            # Send to Claude with comprehensive context
            response = claude_client.messages.create(
                model=settings.CLAUDE_MODEL,
                max_tokens=4000,
                system=enhanced_system_prompt,
                messages=[{
                    "role": "user", 
                    "content": message
                }]
            )
            
            assistant_response = response.content[0].text
            
            return jsonify({
                'success': True,
                'response': assistant_response,
                'model': settings.CLAUDE_MODEL,
                'context_sections_included': len(context_parts),
                'knowledge_source': 'comprehensive_business_intelligence'
            })
            
        except Exception as e:
            logger.error(f"Enhanced chat API error: {str(e)}")
            return jsonify({'success': False, 'error': f'Enhanced chat error: {str(e)}'}), 500
    
    @app.route('/api/status')
    def api_status():
        """API endpoint to get system status"""
        if 'user_email' not in session:
            return jsonify({'error': 'Not authenticated'}), 401
        
        user_email = session['user_email']
        
        try:
            # Get authentication status
            gmail_status = gmail_auth.get_authentication_status(user_email)
            
            # Get user's processing stats
            user = get_db_manager().get_user_by_email(user_email)
            stats = {
                'user_email': user_email,
                'emails_count': 0,
                'tasks_count': 0,
                'last_fetch': None
            }
            
            if user:
                # Get email count
                emails = get_db_manager().get_user_emails(user.id, limit=1)
                stats['emails_count'] = len(get_db_manager().get_user_emails(user.id, limit=1000))
                
                # Get task count  
                stats['tasks_count'] = len(get_db_manager().get_user_tasks(user.id, limit=1000))
                
                # Get last fetch time (approximate from most recent email)
                if emails:
                    stats['last_fetch'] = emails[0].processed_at.isoformat() if emails[0].processed_at else None
            
            return jsonify({
                'status': 'authenticated' if gmail_status['authenticated'] else 'not_authenticated',
                'gmail_access': gmail_status['gmail_access'],
                'user_stats': stats,
                'gmail_status': gmail_status
            })
            
        except Exception as e:
            logger.error(f"Status check failed: {str(e)}")
            return jsonify({'error': f'Status check failed: {str(e)}'}), 500
    
    @app.route('/api/emails', methods=['GET'])
    def api_get_emails():
        """API endpoint to get existing emails"""
        if 'user_email' not in session:
            return jsonify({'error': 'Not authenticated'}), 401
        
        user_email = session['user_email']
        
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return jsonify({'error': 'User not found'}), 404
            
            emails = get_db_manager().get_user_emails(user.id, limit=50)
            
            return jsonify({
                'success': True,
                'emails': [email.to_dict() for email in emails],
                'count': len(emails)
            })
            
        except Exception as e:
            logger.error(f"Get emails API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/tasks', methods=['GET'])
    def api_get_tasks():
        """API endpoint to get real tasks from database"""
        if 'user_email' not in session:
            return jsonify({'error': 'Not authenticated'}), 401
        
        user_email = session['user_email']
        
        try:
            # Clear any cache to ensure fresh data
            from chief_of_staff_ai.strategic_intelligence.strategic_intelligence_cache import strategic_intelligence_cache
            strategic_intelligence_cache.invalidate(user_email)
            
            # Get real user and their tasks
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return jsonify({'error': 'User not found'}), 404
            
            status = request.args.get('status')
            limit = int(request.args.get('limit', 50))
            
            tasks = get_db_manager().get_user_tasks(user.id, status)
            if limit:
                tasks = tasks[:limit]
            
            # Format real tasks data properly
            tasks_data = []
            for task in tasks:
                if task.description and len(task.description.strip()) > 3:  # Only meaningful tasks
                    tasks_data.append({
                        'id': task.id,
                        'description': task.description,
                        'details': task.source_text or '',  # Use source_text instead of notes
                        'priority': task.priority or 'medium',
                        'status': task.status or 'pending',
                        'category': task.category or 'general',
                        'confidence': task.confidence or 0.8,
                        'assignee': task.assignee or user_email,
                        'due_date': task.due_date.isoformat() if task.due_date else None,
                        'created_at': task.created_at.isoformat() if task.created_at else None,
                        'source_email_subject': getattr(task, 'source_email_subject', None),  # Use getattr with fallback
                        'task_type': 'real_task',
                        'data_source': 'real_database'
                    })
            
            return jsonify({
                'success': True,
                'tasks': tasks_data,
                'count': len(tasks_data),
                'status_filter': status,
                'data_source': 'real_database'
            })
            
        except Exception as e:
            logger.error(f"Get tasks API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/email-insights', methods=['GET'])
    def api_get_email_insights():
        """API endpoint to get real strategic business insights from database"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        try:
            # Clear any cache to ensure fresh data
            from chief_of_staff_ai.strategic_intelligence.strategic_intelligence_cache import strategic_intelligence_cache
            strategic_intelligence_cache.invalidate(user['email'])
            
            # Use the real strategic business insights function
            strategic_insights = get_strategic_business_insights(user['email'])
            
            return jsonify({
                'success': True,
                'strategic_insights': strategic_insights,
                'count': len(strategic_insights),
                'data_source': 'real_business_insights'
            })
            
        except Exception as e:
            logger.error(f"Get email insights API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/flush-database', methods=['POST'])
    def api_flush_database():
        """API endpoint to flush all user data and recreate database schema"""
        if 'user_email' not in session:
            return jsonify({'error': 'Not authenticated'}), 401
        
        user_email = session['user_email']
        
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return jsonify({'error': 'User not found'}), 404
            
            logger.info(f"Starting complete database flush for user: {user_email}")
            
            # CRITICAL FIX: For SQLite, we need to recreate the schema to add new columns
            # Get the current database manager
            db_manager = get_db_manager()
            
            # Step 1: Delete all user data INCLUDING Smart Contact Strategy tables
            with db_manager.get_session() as db_session:
                # Import the new Smart Contact Strategy models
                from models.database import TrustedContact, ContactContext, TaskContext, TopicKnowledgeBase
                
                # Delete Smart Contact Strategy data first (due to foreign keys)
                db_session.query(ContactContext).filter(ContactContext.user_id == user.id).delete()
                db_session.query(TaskContext).filter(TaskContext.user_id == user.id).delete()
                db_session.query(TopicKnowledgeBase).filter(TopicKnowledgeBase.user_id == user.id).delete()
                db_session.query(TrustedContact).filter(TrustedContact.user_id == user.id).delete()
                
                # Delete regular tables
                db_session.query(Task).filter(Task.user_id == user.id).delete()
                db_session.query(Email).filter(Email.user_id == user.id).delete()
                db_session.query(Person).filter(Person.user_id == user.id).delete()
                db_session.query(Project).filter(Project.user_id == user.id).delete()
                db_session.query(Topic).filter(Topic.user_id == user.id).delete()
                
                db_session.commit()
                logger.info(f"Deleted all data for user: {user_email}")
            
            # Step 2: For SQLite, recreate all tables to ensure new schema
            # This will add any missing columns like is_trusted_contact, engagement_score, etc.
            try:
                from models.database import Base
                logger.info("Recreating database schema with new Smart Contact Strategy columns...")
                
                # Drop all tables and recreate them (this ensures new columns are added)
                Base.metadata.drop_all(bind=db_manager.engine)
                Base.metadata.create_all(bind=db_manager.engine)
                
                logger.info("Database schema recreated successfully")
                
                # Step 3: Recreate the user since we dropped all tables
                user_info = {
                    'email': user.email,
                    'id': user.google_id,
                    'name': user.name
                }
                credentials = {
                    'access_token': user.access_token,
                    'refresh_token': user.refresh_token,
                    'expires_at': user.token_expires_at,
                    'scopes': user.scopes or []
                }
                
                # Recreate user with existing credentials
                db_manager.create_or_update_user(user_info, credentials)
                logger.info(f"Recreated user: {user_email}")
                
            except Exception as schema_error:
                logger.error(f"Schema recreation error: {str(schema_error)}")
                # Fallback: just ensure tables exist
                Base.metadata.create_all(bind=db_manager.engine)
            
            logger.info(f"Complete database flush successful for user: {user_email}")
            
            return jsonify({
                'success': True,
                'message': 'Database completely flushed and schema updated! New Smart Contact Strategy features are now available.',
                'user_email': user_email,
                'schema_updated': True
            })
            
        except Exception as e:
            logger.error(f"Database flush error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/people', methods=['GET'])
    def api_get_people():
        """API endpoint to get real people from database"""
        if 'user_email' not in session:
            return jsonify({'error': 'Not authenticated'}), 401
        
        user_email = session['user_email']
        
        try:
            # Clear any cache to ensure fresh data
            from chief_of_staff_ai.strategic_intelligence.strategic_intelligence_cache import strategic_intelligence_cache
            strategic_intelligence_cache.invalidate(user_email)
            
            # Get real user and their people
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return jsonify({'error': 'User not found'}), 404
            
            limit = int(request.args.get('limit', 50))
            people = get_db_manager().get_user_people(user.id, limit)
            
            # Format real people data properly
            people_data = []
            for person in people:
                if person.name and person.email_address:  # Only real people
                    people_data.append({
                        'id': person.id,
                        'name': person.name,
                        'email': person.email_address,
                        'relationship': person.relationship_type or 'Contact',
                        'company': person.company or 'Unknown',
                        'title': person.title or 'Unknown',
                        'last_contact': person.last_interaction.isoformat() if person.last_interaction else None,
                        'total_emails': person.total_emails or 0,
                        'engagement_score': person.importance_level or 0.5,
                        'relationship_context': f"Email contact with {person.total_emails or 0} interactions",
                        'strategic_importance': 'high' if (person.total_emails or 0) > 10 else 'medium' if (person.total_emails or 0) > 3 else 'low',
                        'data_source': 'real_database'
                    })
            
            return jsonify({
                'success': True,
                'people': people_data,
                'count': len(people_data),
                'data_source': 'real_database'
            })
            
        except Exception as e:
            logger.error(f"Get people API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/projects', methods=['GET'])
    def api_get_projects():
        """API endpoint to get projects"""
        if 'user_email' not in session:
            return jsonify({'error': 'Not authenticated'}), 401
        
        user_email = session['user_email']
        
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return jsonify({'error': 'User not found'}), 404
            
            status = request.args.get('status')
            limit = int(request.args.get('limit', 50))
            
            projects = get_db_manager().get_user_projects(user.id, status, limit)
            
            return jsonify({
                'success': True,
                'projects': [project.to_dict() for project in projects],
                'count': len(projects),
                'status_filter': status
            })
            
        except Exception as e:
            logger.error(f"Get projects API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/business-knowledge', methods=['GET'])
    def api_get_business_knowledge():
        """API endpoint to get business knowledge summary"""
        if 'user_email' not in session:
            return jsonify({'error': 'Not authenticated'}), 401
        
        user_email = session['user_email']
        
        try:
            knowledge = email_intelligence.get_business_knowledge_summary(user_email)
            return jsonify(knowledge)
        except Exception as e:
            logger.error(f"Failed to get business knowledge for {user_email}: {str(e)}")
            return jsonify({'success': False, 'error': str(e)}), 500
    
    @app.route('/api/chat-knowledge', methods=['GET'])
    def api_get_chat_knowledge():
        """API endpoint to get comprehensive knowledge summary for chat queries"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        user_email = user['email']
        
        try:
            knowledge = email_intelligence.get_chat_knowledge_summary(user_email)
            return jsonify(knowledge)
        except Exception as e:
            logger.error(f"Failed to get chat knowledge for {user_email}: {str(e)}")
            return jsonify({'success': False, 'error': str(e)}), 500
    
    @app.route('/api/topics', methods=['GET'])
    def api_get_topics():
        """API endpoint to get all topics for a user"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        try:
            db_user = get_db_manager().get_user_by_email(user['email'])
            if not db_user:
                return jsonify({'error': 'User not found'}), 404
            
            topics = get_db_manager().get_user_topics(db_user.id)
            
            return jsonify({
                'success': True,
                'topics': [topic.to_dict() for topic in topics],
                'count': len(topics),
                'user_info': {'email': db_user.email, 'db_id': db_user.id}  # Debug info
            })
            
        except Exception as e:
            logger.error(f"Get topics API error for user {user['email']}: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/topics', methods=['POST'])
    def api_create_topic():
        """API endpoint to create a new topic manually"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        try:
            db_user = get_db_manager().get_user_by_email(user['email'])
            if not db_user:
                return jsonify({'error': 'User not found'}), 404
            
            data = request.get_json()
            if not data or not data.get('name'):
                return jsonify({'error': 'Topic name is required'}), 400
            
            topic_data = {
                'name': data['name'],
                'slug': data['name'].lower().replace(' ', '-'),
                'description': data.get('description', ''),
                'is_official': data.get('is_official', True),  # Default to official for manually created topics
                'keywords': data.get('keywords', [])
            }
            
            topic = get_db_manager().create_or_update_topic(db_user.id, topic_data)
            
            return jsonify({
                'success': True,
                'topic': topic.to_dict(),
                'message': f'Topic "{topic.name}" created successfully'
            })
            
        except Exception as e:
            logger.error(f"Create topic API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/topics/<int:topic_id>/official', methods=['POST'])
    def api_mark_topic_official(topic_id):
        """API endpoint to mark a topic as official"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        try:
            db_user = get_db_manager().get_user_by_email(user['email'])
            if not db_user:
                return jsonify({'error': 'User not found'}), 404
            
            success = get_db_manager().mark_topic_official(db_user.id, topic_id)
            
            if success:
                return jsonify({
                    'success': True,
                    'message': 'Topic marked as official'
                })
            else:
                return jsonify({'error': 'Topic not found or not authorized'}), 404
            
        except Exception as e:
            logger.error(f"Mark topic official API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/topics/<int:topic_id>/merge', methods=['POST'])
    def api_merge_topic(topic_id):
        """API endpoint to merge one topic into another"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        try:
            db_user = get_db_manager().get_user_by_email(user['email'])
            if not db_user:
                return jsonify({'error': 'User not found'}), 404
            
            data = request.get_json()
            target_topic_id = data.get('target_topic_id')
            
            if not target_topic_id:
                return jsonify({'error': 'Target topic ID is required'}), 400
            
            success = get_db_manager().merge_topics(db_user.id, topic_id, target_topic_id)
            
            if success:
                return jsonify({
                    'success': True,
                    'message': 'Topics merged successfully'
                })
            else:
                return jsonify({'error': 'Topics not found or merge failed'}), 404
            
        except Exception as e:
            logger.error(f"Merge topic API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/topics/<int:topic_id>', methods=['PUT'])
    def api_update_topic(topic_id):
        """API endpoint to update a topic"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        try:
            db_user = get_db_manager().get_user_by_email(user['email'])
            if not db_user:
                return jsonify({'error': 'User not found'}), 404
            
            data = request.get_json()
            if not data:
                return jsonify({'error': 'No data provided'}), 400
            
            # Update topic data
            topic_data = {}
            if 'description' in data:
                topic_data['description'] = data['description']
            if 'keywords' in data:
                topic_data['keywords'] = data['keywords']
            if 'name' in data:
                topic_data['name'] = data['name']
            
            success = get_db_manager().update_topic(db_user.id, topic_id, topic_data)
            
            if success:
                return jsonify({
                    'success': True,
                    'message': 'Topic updated successfully'
                })
            else:
                return jsonify({'error': 'Topic not found or not authorized'}), 404
            
        except Exception as e:
            logger.error(f"Update topic API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/topics/resync', methods=['POST'])
    def api_resync_topics():
        """API endpoint to resync all content with updated topics using Claude"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        user_email = user['email']
        
        try:
            # This would trigger a resync of all emails with the updated topic definitions
            # For now, just return success - full implementation would re-process emails
            return jsonify({
                'success': True,
                'message': 'Topic resync initiated - this will re-categorize all content with updated topic definitions'
            })
            
        except Exception as e:
            logger.error(f"Resync topics API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/sync-topics', methods=['POST'])
    def api_sync_topics():
        """API endpoint to sync email topics to the topics table"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        try:
            db_user = get_db_manager().get_user_by_email(user['email'])
            if not db_user:
                return jsonify({'error': 'User not found'}), 404
            
            # Get all emails with topics
            emails = get_db_manager().get_user_emails(db_user.id, limit=1000)
            topic_analysis = {}
            
            # Analyze topic usage and collect content for AI description generation
            for email in emails:
                if email.topics and isinstance(email.topics, list):
                    for topic in email.topics:
                        if topic and len(topic.strip()) > 2:  # Valid topic
                            topic_name = topic.strip()
                            if topic_name not in topic_analysis:
                                topic_analysis[topic_name] = {
                                    'count': 0,
                                    'email_examples': [],
                                    'key_content': []
                                }
                            topic_analysis[topic_name]['count'] += 1
                            
                            # Collect examples for AI description generation
                            if len(topic_analysis[topic_name]['email_examples']) < 5:
                                topic_analysis[topic_name]['email_examples'].append({
                                    'subject': email.subject,
                                    'summary': email.ai_summary,
                                    'sender': email.sender_name or email.sender
                                })
                            
                            # Collect key content for description
                            if email.ai_summary and len(email.ai_summary) > 20:
                                topic_analysis[topic_name]['key_content'].append(email.ai_summary[:200])
            
            # Create Topic records for topics with 2+ usages
            topics_created = 0
            for topic_name, analysis in topic_analysis.items():
                if analysis['count'] >= 2:  # Only create topics used in multiple emails
                    # Check if topic already exists
                    existing_topics = get_db_manager().get_user_topics(db_user.id)
                    topic_exists = any(t.name.lower() == topic_name.lower() for t in existing_topics)
                    
                    if not topic_exists:
                        # Generate meaningful description based on content
                        description = f"Business topic involving {topic_name.lower()}. "
                        
                        # Add context from email content
                        if analysis['key_content']:
                            common_themes = []
                            content_sample = ' '.join(analysis['key_content'][:3])
                            
                            # Simple keyword extraction for description
                            if 'meeting' in content_sample.lower():
                                common_themes.append('meetings and collaboration')
                            if 'project' in content_sample.lower():
                                common_themes.append('project management')
                            if 'decision' in content_sample.lower():
                                common_themes.append('strategic decisions')
                            if 'client' in content_sample.lower() or 'customer' in content_sample.lower():
                                common_themes.append('client relationships')
                            if 'team' in content_sample.lower():
                                common_themes.append('team coordination')
                            if 'opportunity' in content_sample.lower():
                                common_themes.append('business opportunities')
                            if 'challenge' in content_sample.lower() or 'issue' in content_sample.lower():
                                common_themes.append('problem solving')
                            
                            if common_themes:
                                description += f"Includes discussions about {', '.join(common_themes)}. "
                        
                        # Add usage statistics
                        description += f"Identified from {analysis['count']} email conversations"
                        if analysis['email_examples']:
                            senders = list(set([ex['sender'] for ex in analysis['email_examples'] if ex['sender']]))
                            if senders:
                                description += f" involving {', '.join(senders[:3])}"
                                if len(senders) > 3:
                                    description += f" and {len(senders)-3} others"
                        description += "."
                        
                        topic_data = {
                            'name': topic_name,
                            'slug': topic_name.lower().replace(' ', '-').replace('_', '-'),
                            'description': description,
                            'is_official': False,  # Mark as AI-discovered
                            'email_count': analysis['count'],
                            'confidence_score': min(0.9, analysis['count'] / 10.0),  # Higher confidence for more usage
                            'keywords': [topic_name.lower()] + [word.lower() for word in topic_name.split() if len(word) > 2]
                        }
                        get_db_manager().create_or_update_topic(db_user.id, topic_data)
                        topics_created += 1
            
            return jsonify({
                'success': True,
                'topics_created': topics_created,
                'total_topic_usage': len(topic_analysis),
                'message': f'Synced {topics_created} topics with AI-generated descriptions from email analysis'
            })
            
        except Exception as e:
            logger.error(f"Sync topics API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/topics/ensure-default', methods=['POST'])
    def api_ensure_default_topic():
        """API endpoint to ensure user has at least one default topic"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        try:
            db_user = get_db_manager().get_user_by_email(user['email'])
            if not db_user:
                return jsonify({'error': 'User not found'}), 404
            
            # Check if user has any topics
            existing_topics = get_db_manager().get_user_topics(db_user.id)
            
            if len(existing_topics) == 0:
                # Create default "General Business" topic
                topic_data = {
                    'name': 'General Business',
                    'slug': 'general-business',
                    'description': 'Catch-all topic for all business communications, decisions, opportunities, and challenges that don\'t fit into specific categories. This ensures no important information is lost.',
                    'is_official': True,
                    'keywords': ['business', 'general', 'communication', 'decision', 'opportunity', 'challenge']
                }
                
                topic = get_db_manager().create_or_update_topic(db_user.id, topic_data)
                
                return jsonify({
                    'success': True,
                    'created': True,
                    'topic': topic.to_dict(),
                    'message': 'Default "General Business" topic created automatically'
                })
            else:
                return jsonify({
                    'success': True,
                    'created': False,
                    'message': f'User already has {len(existing_topics)} topics'
                })
            
        except Exception as e:
            logger.error(f"Ensure default topic API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/settings', methods=['GET'])
    def api_get_settings():
        """API endpoint to get user settings"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        try:
            db_user = get_db_manager().get_user_by_email(user['email'])
            if not db_user:
                return jsonify({'error': 'User not found'}), 404
            
            settings_data = {
                'email_fetch_limit': db_user.email_fetch_limit,
                'email_days_back': db_user.email_days_back,
                'auto_process_emails': db_user.auto_process_emails,
                'last_login': db_user.last_login.isoformat() if db_user.last_login else None,
                'created_at': db_user.created_at.isoformat() if db_user.created_at else None,
                'name': db_user.name,
                'email': db_user.email
            }
            
            return jsonify({
                'success': True,
                'settings': settings_data
            })
            
        except Exception as e:
            logger.error(f"Get settings API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/settings', methods=['PUT'])
    def api_update_settings():
        """API endpoint to update user settings"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        try:
            db_user = get_db_manager().get_user_by_email(user['email'])
            if not db_user:
                return jsonify({'error': 'User not found'}), 404
            
            data = request.get_json()
            if not data:
                return jsonify({'error': 'No data provided'}), 400
            
            # Update user settings directly on the object
            if 'email_fetch_limit' in data:
                db_user.email_fetch_limit = int(data['email_fetch_limit'])
            if 'email_days_back' in data:
                db_user.email_days_back = int(data['email_days_back'])
            if 'auto_process_emails' in data:
                db_user.auto_process_emails = bool(data['auto_process_emails'])
            
            # Save changes using the database manager's session
            with get_db_manager().get_session() as db_session:
                db_session.merge(db_user)
                db_session.commit()
            
            return jsonify({
                'success': True,
                'message': 'Settings updated successfully'
            })
            
        except Exception as e:
            logger.error(f"Update settings API error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/debug-data', methods=['GET'])
    def api_debug_data():
        """Debug endpoint to check what real data exists"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        try:
            from chief_of_staff_ai.strategic_intelligence.strategic_intelligence_cache import strategic_intelligence_cache
            
            # Clear cache first
            strategic_intelligence_cache.invalidate(user['email'])
            
            db_user = get_db_manager().get_user_by_email(user['email'])
            if not db_user:
                return jsonify({'error': 'User not found'}), 404
            
            # Get real data from database
            emails = get_db_manager().get_user_emails(db_user.id, limit=50)
            people = get_db_manager().get_user_people(db_user.id, limit=50)
            tasks = get_db_manager().get_user_tasks(db_user.id, limit=50)
            
            # Analyze what we actually have
            analyzed_emails = [e for e in emails if e.ai_summary and len(e.ai_summary.strip()) > 10]
            real_people = [p for p in people if p.name and p.email_address and '@' in p.email_address]
            real_tasks = [t for t in tasks if t.description and len(t.description.strip()) > 5]
            
            debug_info = {
                'user_email': user['email'],
                'user_db_id': db_user.id,
                'cache_cleared': True,
                'raw_data': {
                    'total_emails': len(emails),
                    'emails_with_ai_summary': len(analyzed_emails),
                    'total_people': len(people),
                    'real_people': len(real_people),
                    'total_tasks': len(tasks),
                    'real_tasks': len(real_tasks)
                },
                'sample_emails': [
                    {
                        'subject': e.subject,
                        'sender': e.sender_name or e.sender,
                        'has_ai_summary': bool(e.ai_summary),
                        'ai_summary_length': len(e.ai_summary) if e.ai_summary else 0,
                        'date': e.email_date.isoformat() if e.email_date else None
                    } for e in emails[:5]
                ],
                'sample_people': [
                    {
                        'name': p.name,
                        'email': p.email_address,
                        'company': p.company,
                        'total_emails': p.total_emails
                    } for p in real_people[:5]
                ],
                'sample_tasks': [
                    {
                        'description': t.description,
                        'priority': t.priority,
                        'status': t.status,
                        'source': t.source_email_subject
                    } for t in real_tasks[:5]
                ]
            }
            
            return jsonify(debug_info)
            
        except Exception as e:
            logger.error(f"Debug data API error: {str(e)}")
            return jsonify({'error': str(e)}), 500

    @app.route('/api/download-knowledge-base', methods=['GET'])
    def api_download_knowledge_base():
        """API endpoint to download complete knowledge base as JSON - instant export of chat-ready data"""
        user = get_current_user()
        if not user:
            return jsonify({'error': 'Not authenticated'}), 401
        
        try:
            db_user = get_db_manager().get_user_by_email(user['email'])
            if not db_user:
                return jsonify({'error': 'User not found'}), 404
            
            # Collect all user data (this is the same data used for chat queries - should be instant)
            emails = get_db_manager().get_user_emails(db_user.id, limit=1000)
            people = get_db_manager().get_user_people(db_user.id, limit=500)
            tasks = get_db_manager().get_user_tasks(db_user.id, limit=500)
            projects = get_db_manager().get_user_projects(db_user.id, limit=200)
            topics = get_db_manager().get_user_topics(db_user.id, limit=100)
            
            # Get strategic insights (same data used in chat)
            strategic_insights = get_strategic_business_insights(user['email'])
            
            # Get comprehensive business knowledge (same data used in chat)
            business_knowledge = email_intelligence.get_business_knowledge_summary(user['email'])
            chat_knowledge = email_intelligence.get_chat_knowledge_summary(user['email'])
            
            # Format export data - using the same structure as chat queries
            knowledge_base_export = {
                'export_metadata': {
                    'user_email': user['email'],
                    'export_date': datetime.now().isoformat(),
                    'export_version': '1.0',
                    'data_source': 'chat_ready_knowledge_base',
                    'total_records': {
                        'emails': len(emails),
                        'people': len(people),
                        'tasks': len(tasks),
                        'projects': len(projects),
                        'topics': len(topics)
                    }
                },
                'strategic_insights': strategic_insights,
                'business_intelligence': business_knowledge.get('business_knowledge', {}) if business_knowledge.get('success') else {},
                'chat_knowledge': chat_knowledge.get('knowledge_base', {}) if chat_knowledge.get('success') else {},
                'emails': [
                    {
                        'id': e.id,
                        'gmail_id': e.gmail_id,
                        'subject': e.subject,
                        'sender': e.sender,
                        'sender_name': e.sender_name,
                        'recipients': e.recipients,
                        'email_date': e.email_date.isoformat() if e.email_date else None,
                        'ai_summary': e.ai_summary,
                        'ai_category': e.ai_category,
                        'key_insights': e.key_insights,
                        'topics': e.topics,
                        'action_required': e.action_required,
                        'follow_up_required': e.follow_up_required,
                        'sentiment_score': e.sentiment_score,
                        'urgency_score': e.urgency_score,
                        'processed_at': e.processed_at.isoformat() if e.processed_at else None
                    } for e in emails
                ],
                'people': [
                    {
                        'id': p.id,
                        'name': p.name,
                        'email_address': p.email_address,
                        'title': p.title,
                        'company': p.company,
                        'relationship_type': p.relationship_type,
                        'notes': p.notes,
                        'importance_level': p.importance_level,
                        'total_emails': p.total_emails,
                        'last_interaction': p.last_interaction.isoformat() if p.last_interaction else None,
                        'first_mentioned': p.first_mentioned.isoformat() if p.first_mentioned else None,
                        'created_at': p.created_at.isoformat() if hasattr(p, 'created_at') and p.created_at else None,
                        'communication_frequency': p.communication_frequency,
                        'skills': p.skills,
                        'interests': p.interests,
                        'key_topics': p.key_topics
                    } for p in people
                ],
                'tasks': [
                    {
                        'id': t.id,
                        'description': t.description,
                        'assignee': t.assignee,
                        'priority': t.priority,
                        'status': t.status,
                        'category': t.category,
                        'due_date': t.due_date.isoformat() if t.due_date else None,
                        'due_date_text': t.due_date_text,
                        'confidence': t.confidence,
                        'source_text': t.source_text,
                        'context': getattr(t, 'context', None),
                        'notes': getattr(t, 'notes', t.source_text),  # Use source_text as fallback for notes
                        'created_at': t.created_at.isoformat() if t.created_at else None,
                        'completed_at': t.completed_at.isoformat() if t.completed_at else None,
                        'source_email_subject': getattr(t, 'source_email_subject', None)
                    } for t in tasks
                ],
                'projects': [
                    {
                        'id': p.id,
                        'name': p.name,
                        'slug': p.slug,
                        'description': p.description,
                        'status': p.status,
                        'priority': p.priority,
                        'category': p.category,
                        'start_date': p.start_date.isoformat() if p.start_date else None,
                        'end_date': p.end_date.isoformat() if p.end_date else None,
                        'progress': getattr(p, 'progress', None),
                        'key_topics': p.key_topics,
                        'stakeholders': p.stakeholders,
                        'created_at': p.created_at.isoformat() if p.created_at else None
                    } for p in projects
                ],
                'topics': [
                    {
                        'id': t.id,
                        'name': t.name,
                        'slug': t.slug,
                        'description': t.description,
                        'is_official': t.is_official,
                        'email_count': t.email_count,
                        'confidence_score': t.confidence_score,
                        'strength': getattr(t, 'strength', None),
                        'keywords': json.loads(t.keywords) if t.keywords else [],
                        'created_at': t.created_at.isoformat() if t.created_at else None,
                        'last_used': t.last_used.isoformat() if t.last_used else None
                    } for t in topics
                ]
            }
            
            # Create downloadable JSON response
            response = make_response(jsonify(knowledge_base_export))
            
            # Set headers for file download
            filename = f"knowledge_base_{user['email'].replace('@', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            response.headers['Content-Disposition'] = f'attachment; filename="{filename}"'
            response.headers['Content-Type'] = 'application/json'
            response.headers['Content-Length'] = len(response.get_data())
            
            logger.info(f"Knowledge base download completed for {user['email']}: {len(emails)} emails, {len(people)} people, {len(tasks)} tasks")
            
            return response
            
        except Exception as e:
            logger.error(f"Knowledge base download error: {str(e)}")
            return jsonify({'error': f'Download failed: {str(e)}'}), 500

    @app.route('/api/fetch-calendar', methods=['POST'])
    def api_fetch_calendar():
        """API endpoint to fetch calendar events and create prep tasks"""
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        try:
            data = request.get_json() or {}
            days_back = data.get('days_back', 3)
            days_forward = data.get('days_forward', 14)
            force_refresh = data.get('force_refresh', False)
            create_prep_tasks = data.get('create_prep_tasks', False)
            
            # Get user from database
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return jsonify({'success': False, 'error': 'User not found'}), 404
            
            # Fetch calendar events
            logger.info(f"Fetching calendar events for {user_email}")
            calendar_result = calendar_fetcher.fetch_calendar_events(
                user_email, 
                days_back=days_back, 
                days_forward=days_forward,
                force_refresh=force_refresh
            )
            
            prep_tasks_result = {'prep_tasks_created': 0, 'tasks': []}
            
            # Create meeting preparation tasks if requested
            if create_prep_tasks and calendar_result['success'] and calendar_result.get('events'):
                logger.info(f"Creating meeting prep tasks for {user_email}")
                # Convert event dicts to proper format for task creation
                events_for_tasks = []
                for event_dict in calendar_result['events']:
                    if isinstance(event_dict, dict):
                        events_for_tasks.append(event_dict)
                    else:
                        # Convert database object to dict if needed
                        events_for_tasks.append(event_dict.to_dict() if hasattr(event_dict, 'to_dict') else event_dict.__dict__)
                
                prep_tasks_result = calendar_fetcher.create_meeting_prep_tasks(user.id, events_for_tasks)
            
            # Combine results
            result = {
                'success': calendar_result['success'],
                'user_email': user_email,
                'count': calendar_result.get('count', 0),
                'events': calendar_result.get('events', []),
                'source': calendar_result.get('source', 'unknown'),
                'fetched_at': calendar_result.get('fetched_at'),
                'calendars_processed': calendar_result.get('calendars_processed', 0),
                'prep_tasks_created': prep_tasks_result.get('prep_tasks_created', 0),
                'prep_tasks': prep_tasks_result.get('tasks', [])
            }
            
            if not calendar_result['success']:
                result['error'] = calendar_result.get('error', 'Unknown error')
            
            return jsonify(result)
        
        except Exception as e:
            logger.error(f"Calendar fetch error for {user_email}: {str(e)}")
            return jsonify({
                'success': False, 
                'error': f"Calendar fetch failed: {str(e)}",
                'prep_tasks_created': 0
            }), 500

    @app.route('/api/calendar-events')
    def api_get_calendar_events():
        """API endpoint to get calendar events"""
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        try:
            days_forward = request.args.get('days_forward', 14, type=int)
            limit = request.args.get('limit', 50, type=int)
            
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return jsonify({'success': False, 'error': 'User not found'}), 404
            
            # Get events from database
            from datetime import datetime, timedelta, timezone
            now = datetime.now(timezone.utc)
            start_date = now
            end_date = now + timedelta(days=days_forward)
            
            events = get_db_manager().get_user_calendar_events(user.id, start_date, end_date, limit)
            
            # Convert to dict format and check for prep tasks using existing methods
            events_data = []
            all_user_tasks = get_db_manager().get_user_tasks(user.id)
            prep_tasks = [task for task in all_user_tasks if task.category == 'meeting_preparation']
            
            for event in events:
                event_dict = event.to_dict()
                
                # Check if this event has associated prep tasks by matching event title or other criteria
                related_prep_tasks = [task for task in prep_tasks if 
                                    event.title and task.description and 
                                    (event.title.lower() in task.description.lower() or 
                                     any(word in task.description.lower() for word in event.title.lower().split() if len(word) > 3))]
                
                event_dict['has_prep_tasks'] = len(related_prep_tasks) > 0
                event_dict['prep_tasks_count'] = len(related_prep_tasks)
                
                events_data.append(event_dict)
            
            return jsonify({
                'success': True,
                'events': events_data,
                'count': len(events_data)
            })
        
        except Exception as e:
            logger.error(f"Get calendar events error for {user_email}: {str(e)}")
            return jsonify({'success': False, 'error': str(e)}), 500

    @app.route('/api/meeting-prep-tasks')
    def api_get_meeting_prep_tasks():
        """API endpoint to get meeting preparation tasks"""
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return jsonify({'success': False, 'error': 'User not found'}), 404
            
            # Get prep tasks that are not completed (use existing method with category filter)
            all_tasks = get_db_manager().get_user_tasks(user.id)
            prep_tasks = [task for task in all_tasks if 
                         task.category == 'meeting_preparation' and 
                         task.status in ['pending', 'open']]
            
            return jsonify({
                'success': True,
                'tasks': [task.to_dict() for task in prep_tasks],
                'count': len(prep_tasks)
            })
        
        except Exception as e:
            logger.error(f"Get meeting prep tasks error for {user_email}: {str(e)}")
            return jsonify({'success': False, 'error': str(e)}), 500

    @app.route('/api/free-time-analysis', methods=['POST'])
    def api_free_time_analysis():
        """API endpoint to analyze free time in calendar"""
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        try:
            data = request.get_json() or {}
            days_forward = data.get('days_forward', 7)
            
            # Get free time analysis
            result = calendar_fetcher.fetch_free_time_analysis(
                user_email=user_email,
                days_forward=days_forward
            )
            
            return jsonify(result)
        
        except Exception as e:
            logger.error(f"Free time analysis error for {user_email}: {str(e)}")
            return jsonify({
                'success': False, 
                'error': f"Free time analysis failed: {str(e)}",
                'free_slots': []
            }), 500
    
    # Error handlers
    @app.errorhandler(404)
    def not_found_error(error):
        return render_template('error.html', 
                             error_code=404, 
                             error_message="Page not found"), 404
    
    @app.errorhandler(500)
    def internal_error(error):
        return render_template('error.html', 
                             error_code=500, 
                             error_message="Internal server error"), 500
    
    @app.after_request
    def after_request(response):
        """Add cache-busting headers to prevent session contamination"""
        # Prevent caching for API endpoints and sensitive pages
        if (request.endpoint and 
            (request.endpoint.startswith('api_') or 
             request.path.startswith('/api/') or
             request.path in ['/dashboard', '/debug/session'])):
            response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate, max-age=0'
            response.headers['Pragma'] = 'no-cache'
            response.headers['Expires'] = '0'
            response.headers['X-Content-Type-Options'] = 'nosniff'
        return response
    
    return app

# Create the Flask application
app = create_app()

if __name__ == '__main__':
    try:
        # Validate settings
        config_errors = settings.validate_config()
        if config_errors:
            raise ValueError(f"Configuration errors: {', '.join(config_errors)}")
        
        print(" Starting AI Chief of Staff Web Application")
        print(f" Gmail integration: {' Configured' if settings.GOOGLE_CLIENT_ID else ' Missing'}")
        print(f" Calendar integration: {' Enabled' if 'https://www.googleapis.com/auth/calendar.readonly' in settings.GMAIL_SCOPES else ' Missing'}")
        print(f" Claude integration: {' Configured' if settings.ANTHROPIC_API_KEY else ' Missing'}")
        print(f" Server: http://localhost:{settings.PORT}")
        print("\nTo get started:")
        print("1. Go to the URL above")
        print("2. Click 'Sign in with Google'")
        print("3. Grant Gmail and Calendar access permissions")
        print("4. Start processing your emails and calendar!")
        
        app.run(
            host=settings.HOST,
            port=settings.PORT,
            debug=settings.DEBUG
        )
        
    except ValueError as e:
        print(f" Configuration Error: {e}")
        print("Please check your .env file and ensure all required variables are set.")
        sys.exit(1)
    except Exception as e:
        print(f" Failed to start application: {e}")
        sys.exit(1) 

====================================================================================================
END OF FILE: main.py
====================================================================================================


====================================================================================================
FILE 2: chief_of_staff_ai/main.py
====================================================================================================
Path: /Users/oudiantebi/Session42 Dropbox/Oudi Antebi/Mac (3)/Documents/MyCode/COS1/chief_of_staff_ai/main.py
Info: Size: 13,422 bytes | Modified: 2025-06-08 12:43:53
----------------------------------------------------------------------------------------------------

# Main Flask application for AI Chief of Staff

import os
import logging
from datetime import datetime
from flask import Flask, render_template, request, jsonify, session, redirect, url_for, flash
import anthropic

from config.settings import settings
from auth.gmail_auth import gmail_auth
from ingest.gmail_fetcher import gmail_fetcher
from processors.email_normalizer import email_normalizer
from processors.task_extractor import task_extractor
from models.database import get_db_manager, Email, Task

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Create Flask application
app = Flask(__name__)
app.config['SECRET_KEY'] = settings.SECRET_KEY
app.config['SESSION_TYPE'] = 'filesystem'

# Initialize Claude client for chat
claude_client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)

@app.route('/')
def index():
    """Main dashboard route"""
    user_email = session.get('user_email')
    
    if not user_email:
        return render_template('login.html')
    
    try:
        # Get user information
        user_info = gmail_auth.get_user_by_email(user_email)
        if not user_info:
            session.clear()
            return render_template('login.html')
        
        # Get user statistics
        user = get_db_manager().get_user_by_email(user_email)
        user_stats = {
            'total_emails': 0,
            'total_tasks': 0,
            'pending_tasks': 0,
            'completed_tasks': 0
        }
        
        if user:
            with get_db_manager().get_session() as db_session:
                user_stats['total_emails'] = db_session.query(Email).filter(
                    Email.user_id == user.id
                ).count()
                
                user_stats['total_tasks'] = db_session.query(Task).filter(
                    Task.user_id == user.id
                ).count()
                
                user_stats['pending_tasks'] = db_session.query(Task).filter(
                    Task.user_id == user.id,
                    Task.status == 'pending'
                ).count()
                
                user_stats['completed_tasks'] = db_session.query(Task).filter(
                    Task.user_id == user.id,
                    Task.status == 'completed'
                ).count()
        
        return render_template('dashboard.html', 
                             user_info=user_info, 
                             user_stats=user_stats)
    
    except Exception as e:
        logger.error(f"Dashboard error for {user_email}: {str(e)}")
        flash('An error occurred loading your dashboard. Please try again.', 'error')
        return render_template('dashboard.html', 
                             user_info={'email': user_email}, 
                             user_stats={'total_emails': 0, 'total_tasks': 0})

@app.route('/login')
def login():
    """Login page"""
    return render_template('login.html')

@app.route('/auth/google')
def auth_google():
    """Initiate Google OAuth flow"""
    try:
        auth_url, state = gmail_auth.get_authorization_url('user_session')
        session['oauth_state'] = state
        return redirect(auth_url)
    except Exception as e:
        logger.error(f"Google auth initiation error: {str(e)}")
        flash('Failed to initiate Google authentication. Please try again.', 'error')
        return redirect(url_for('login'))

@app.route('/auth/callback')
def auth_callback():
    """Handle OAuth callback"""
    try:
        authorization_code = request.args.get('code')
        state = request.args.get('state')
        
        if not authorization_code:
            flash('Authorization failed. Please try again.', 'error')
            return redirect(url_for('login'))
        
        # Handle OAuth callback
        result = gmail_auth.handle_oauth_callback(authorization_code, state)
        
        if result['success']:
            session['user_email'] = result['user_email']
            session['authenticated'] = True
            flash(f'Successfully authenticated as {result["user_email"]}!', 'success')
            return redirect(url_for('index'))
        else:
            flash(f'Authentication failed: {result["error"]}', 'error')
            return redirect(url_for('login'))
    
    except Exception as e:
        logger.error(f"OAuth callback error: {str(e)}")
        flash('Authentication error occurred. Please try again.', 'error')
        return redirect(url_for('login'))

@app.route('/logout')
def logout():
    """Logout user"""
    user_email = session.get('user_email')
    session.clear()
    
    if user_email:
        flash(f'Successfully logged out from {user_email}', 'success')
    
    return redirect(url_for('login'))

@app.route('/api/process-emails', methods=['POST'])
def api_process_emails():
    """API endpoint to fetch and process emails"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        data = request.get_json() or {}
        days_back = data.get('days_back', 7)
        limit = data.get('limit', 50)
        force_refresh = data.get('force_refresh', False)
        
        # Step 1: Fetch emails
        logger.info(f"Fetching emails for {user_email}")
        fetch_result = gmail_fetcher.fetch_recent_emails(
            user_email, 
            days_back=days_back, 
            limit=limit,
            force_refresh=force_refresh
        )
        
        if not fetch_result['success']:
            return jsonify({
                'success': False, 
                'error': f"Failed to fetch emails: {fetch_result.get('error')}"
            }), 400
        
        # Step 2: Normalize emails
        logger.info(f"Normalizing emails for {user_email}")
        normalize_result = email_normalizer.normalize_user_emails(user_email, limit)
        
        # Step 3: Extract tasks
        logger.info(f"Extracting tasks for {user_email}")
        task_result = task_extractor.extract_tasks_for_user(user_email, limit)
        
        return jsonify({
            'success': True,
            'fetch_result': fetch_result,
            'normalize_result': normalize_result,
            'task_result': task_result,
            'processed_at': datetime.utcnow().isoformat()
        })
    
    except Exception as e:
        logger.error(f"Email processing error for {user_email}: {str(e)}")
        return jsonify({
            'success': False, 
            'error': f"Processing failed: {str(e)}"
        }), 500

@app.route('/api/emails')
def api_get_emails():
    """API endpoint to get user emails"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        limit = request.args.get('limit', 50, type=int)
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        emails = get_db_manager().get_user_emails(user.id, limit)
        
        return jsonify({
            'success': True,
            'emails': [email.to_dict() for email in emails],
            'count': len(emails)
        })
    
    except Exception as e:
        logger.error(f"Get emails error for {user_email}: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/tasks')
def api_get_tasks():
    """API endpoint to get user tasks"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        status = request.args.get('status')
        limit = request.args.get('limit', 100, type=int)
        
        result = task_extractor.get_user_tasks(user_email, status, limit)
        return jsonify(result)
    
    except Exception as e:
        logger.error(f"Get tasks error for {user_email}: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/tasks/<int:task_id>/status', methods=['PUT'])
def api_update_task_status(task_id):
    """API endpoint to update task status"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        data = request.get_json()
        new_status = data.get('status')
        
        if new_status not in ['pending', 'in_progress', 'completed', 'cancelled']:
            return jsonify({'success': False, 'error': 'Invalid status'}), 400
        
        result = task_extractor.update_task_status(user_email, task_id, new_status)
        return jsonify(result)
    
    except Exception as e:
        logger.error(f"Update task status error for {user_email}: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/chat', methods=['POST'])
def api_chat():
    """API endpoint for Claude chat"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        data = request.get_json()
        message = data.get('message', '').strip()
        
        if not message:
            return jsonify({'success': False, 'error': 'Message is required'}), 400
        
        # Get user context for better responses
        user = get_db_manager().get_user_by_email(user_email)
        context_info = ""
        
        if user:
            with get_db_manager().get_session() as db_session:
                recent_tasks = db_session.query(Task).filter(
                    Task.user_id == user.id,
                    Task.status == 'pending'
                ).order_by(Task.created_at.desc()).limit(5).all()
                
                if recent_tasks:
                    task_list = "\n".join([f"- {task.description}" for task in recent_tasks])
                    context_info = f"\n\nYour recent pending tasks:\n{task_list}"
        
        # Build system prompt with context
        system_prompt = f"""You are an AI Chief of Staff assistant helping {user_email}. 
You have access to their email-derived tasks and can help with work organization, prioritization, and productivity.

Be helpful, professional, and concise. Focus on actionable advice related to their work and tasks.{context_info}"""
        
        # Call Claude
        response = claude_client.messages.create(
            model=settings.CLAUDE_MODEL,
            max_tokens=1000,
            temperature=0.3,
            system=system_prompt,
            messages=[{
                "role": "user",
                "content": message
            }]
        )
        
        reply = response.content[0].text
        
        return jsonify({
            'success': True,
            'message': message,
            'reply': reply,
            'timestamp': datetime.utcnow().isoformat()
        })
    
    except Exception as e:
        logger.error(f"Chat error for {user_email}: {str(e)}")
        return jsonify({
            'success': False, 
            'error': 'Failed to process chat message'
        }), 500

@app.route('/api/status')
def api_status():
    """API endpoint to get system status"""
    user_email = session.get('user_email')
    
    status = {
        'authenticated': bool(user_email),
        'user_email': user_email,
        'timestamp': datetime.utcnow().isoformat(),
        'database_connected': True,
        'gmail_auth_available': bool(settings.GOOGLE_CLIENT_ID and settings.GOOGLE_CLIENT_SECRET),
        'claude_available': bool(settings.ANTHROPIC_API_KEY)
    }
    
    # Test database connection
    try:
        get_db_manager().get_session().close()
    except Exception as e:
        status['database_connected'] = False
        status['database_error'] = str(e)
    
    # Test Gmail auth if user is authenticated
    if user_email:
        try:
            auth_status = gmail_auth.get_authentication_status(user_email)
            status['gmail_auth_status'] = auth_status
        except Exception as e:
            status['gmail_auth_error'] = str(e)
    
    return jsonify(status)

@app.errorhandler(404)
def not_found_error(error):
    return render_template('404.html'), 404

@app.errorhandler(500)
def internal_error(error):
    logger.error(f"Internal server error: {str(error)}")
    return render_template('500.html'), 500

if __name__ == '__main__':
    # Validate configuration
    config_errors = settings.validate_config()
    if config_errors:
        logger.error("Configuration errors:")
        for error in config_errors:
            logger.error(f"  - {error}")
        exit(1)
    
    logger.info("Starting AI Chief of Staff web application...")
    logger.info(f"Database URL: {settings.DATABASE_URL}")
    logger.info(f"Environment: {'Production' if settings.is_production() else 'Development'}")
    
    # Initialize database
    try:
        get_db_manager().initialize_database()
        logger.info("Database initialized successfully")
    except Exception as e:
        logger.error(f"Database initialization failed: {str(e)}")
        exit(1)
    
    app.run(
        host='0.0.0.0',
        port=settings.PORT,
        debug=settings.DEBUG
    ) 

====================================================================================================
END OF FILE: chief_of_staff_ai/main.py
====================================================================================================


====================================================================================================
FILE 3: chief_of_staff_ai/models/database.py
====================================================================================================
Path: /Users/oudiantebi/Session42 Dropbox/Oudi Antebi/Mac (3)/Documents/MyCode/COS1/chief_of_staff_ai/models/database.py
Info: Size: 72,118 bytes | Modified: 2025-06-11 10:17:00
----------------------------------------------------------------------------------------------------

import os
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean, Float, ForeignKey, Index, text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship, Session
from sqlalchemy.dialects.postgresql import JSON
from sqlalchemy.types import TypeDecorator

from config.settings import settings

logger = logging.getLogger(__name__)

# Base class for all models
Base = declarative_base()

# Custom JSON type that works with both SQLite and PostgreSQL
class JSONType(TypeDecorator):
    impl = Text
    
    def process_bind_param(self, value, dialect):
        if value is not None:
            return json.dumps(value)
        return value
    
    def process_result_value(self, value, dialect):
        if value is not None:
            return json.loads(value)
        return value

class User(Base):
    """User model for multi-tenant authentication"""
    __tablename__ = 'users'
    
    id = Column(Integer, primary_key=True)
    email = Column(String(255), unique=True, nullable=False, index=True)
    google_id = Column(String(255), unique=True, nullable=False)
    name = Column(String(255), nullable=False)
    
    # OAuth credentials (encrypted in production)
    access_token = Column(Text)
    refresh_token = Column(Text)
    token_expires_at = Column(DateTime)
    scopes = Column(JSONType)
    
    # Account metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    last_login = Column(DateTime, default=datetime.utcnow)
    is_active = Column(Boolean, default=True)
    
    # Processing preferences
    email_fetch_limit = Column(Integer, default=50)
    email_days_back = Column(Integer, default=30)
    auto_process_emails = Column(Boolean, default=True)
    
    # Relationships
    emails = relationship("Email", back_populates="user", cascade="all, delete-orphan")
    tasks = relationship("Task", back_populates="user", cascade="all, delete-orphan")
    people = relationship("Person", back_populates="user", cascade="all, delete-orphan")
    projects = relationship("Project", back_populates="user", cascade="all, delete-orphan")
    topics = relationship("Topic", back_populates="user", cascade="all, delete-orphan")
    
    def __repr__(self):
        return f"<User(email='{self.email}', name='{self.name}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'email': self.email,
            'name': self.name,
            'google_id': self.google_id,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'last_login': self.last_login.isoformat() if self.last_login else None,
            'is_active': self.is_active,
            'email_fetch_limit': self.email_fetch_limit,
            'email_days_back': self.email_days_back,
            'auto_process_emails': self.auto_process_emails
        }

class Email(Base):
    """Email model for storing processed emails per user"""
    __tablename__ = 'emails'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Gmail identifiers
    gmail_id = Column(String(255), nullable=False, index=True)
    thread_id = Column(String(255), index=True)
    
    # Email content
    sender = Column(String(255), index=True)
    sender_name = Column(String(255))
    subject = Column(Text)
    body_text = Column(Text)
    body_html = Column(Text)
    body_clean = Column(Text)
    body_preview = Column(Text)
    snippet = Column(Text)
    
    # Email metadata
    recipients = Column(JSONType)  # List of recipient emails
    cc = Column(JSONType)  # List of CC emails
    bcc = Column(JSONType)  # List of BCC emails
    labels = Column(JSONType)  # Gmail labels
    attachments = Column(JSONType)  # Attachment metadata
    entities = Column(JSONType)  # Extracted entities
    
    # Email properties
    email_date = Column(DateTime, index=True)
    size_estimate = Column(Integer)
    message_type = Column(String(50), index=True)  # regular, meeting, newsletter, etc.
    priority_score = Column(Float, index=True)
    
    # Email status
    is_read = Column(Boolean, default=False)
    is_important = Column(Boolean, default=False)
    is_starred = Column(Boolean, default=False)
    has_attachments = Column(Boolean, default=False)
    
    # Email classification and AI insights
    project_id = Column(Integer, ForeignKey('projects.id'), index=True)
    mentioned_people = Column(JSONType)  # List of person IDs mentioned in email
    ai_summary = Column(Text)  # Claude-generated summary
    ai_category = Column(String(100))  # AI-determined category
    sentiment_score = Column(Float)  # Sentiment analysis score
    urgency_score = Column(Float)  # AI-determined urgency
    key_insights = Column(JSONType)  # Key insights extracted by Claude
    topics = Column(JSONType)  # Main topics/themes identified
    action_required = Column(Boolean, default=False)  # Whether action is needed
    follow_up_required = Column(Boolean, default=False)  # Whether follow-up needed
    
    # Processing metadata
    processed_at = Column(DateTime, default=datetime.utcnow)
    normalizer_version = Column(String(50))
    has_errors = Column(Boolean, default=False)
    error_message = Column(Text)
    
    # Relationships
    user = relationship("User", back_populates="emails")
    tasks = relationship("Task", back_populates="email", cascade="all, delete-orphan")
    
    # Indexes for performance - Fixed naming to avoid conflicts
    __table_args__ = (
        Index('idx_email_user_gmail', 'user_id', 'gmail_id'),
        Index('idx_email_user_date', 'user_id', 'email_date'),
        Index('idx_email_user_type', 'user_id', 'message_type'),
        Index('idx_email_user_priority', 'user_id', 'priority_score'),
    )
    
    def __repr__(self):
        return f"<Email(gmail_id='{self.gmail_id}', subject='{self.subject[:50]}...')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'gmail_id': self.gmail_id,
            'thread_id': self.thread_id,
            'sender': self.sender,
            'sender_name': self.sender_name,
            'subject': self.subject,
            'body_preview': self.body_preview,
            'snippet': self.snippet,
            'recipients': self.recipients,
            'email_date': self.email_date.isoformat() if self.email_date else None,
            'message_type': self.message_type,
            'priority_score': self.priority_score,
            'is_read': self.is_read,
            'is_important': self.is_important,
            'is_starred': self.is_starred,
            'has_attachments': self.has_attachments,
            'processed_at': self.processed_at.isoformat() if self.processed_at else None,
            'project_id': self.project_id,
            'mentioned_people': self.mentioned_people,
            'ai_summary': self.ai_summary,
            'ai_category': self.ai_category,
            'sentiment_score': self.sentiment_score,
            'urgency_score': self.urgency_score,
            'key_insights': self.key_insights,
            'topics': self.topics,
            'action_required': self.action_required,
            'follow_up_required': self.follow_up_required
        }

class Task(Base):
    """Task model for storing extracted tasks per user"""
    __tablename__ = 'tasks'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    email_id = Column(Integer, ForeignKey('emails.id'), nullable=True, index=True)
    
    # Task content
    description = Column(Text, nullable=False)
    assignee = Column(String(255))
    due_date = Column(DateTime, index=True)
    due_date_text = Column(String(255))
    
    # Task metadata
    priority = Column(String(20), default='medium', index=True)  # high, medium, low
    category = Column(String(50), index=True)  # follow-up, deadline, meeting, etc.
    confidence = Column(Float)  # AI confidence score
    source_text = Column(Text)  # Original text from email
    
    # Task status
    status = Column(String(20), default='pending', index=True)  # pending, in_progress, completed, cancelled
    completed_at = Column(DateTime)
    
    # Extraction metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    extractor_version = Column(String(50))
    model_used = Column(String(100))
    
    # Relationships
    user = relationship("User", back_populates="tasks")
    email = relationship("Email", back_populates="tasks")
    
    # Indexes for performance - Fixed naming to avoid conflicts
    __table_args__ = (
        Index('idx_task_user_status', 'user_id', 'status'),
        Index('idx_task_user_priority_unique', 'user_id', 'priority'),
        Index('idx_task_user_due_date', 'user_id', 'due_date'),
        Index('idx_task_user_category', 'user_id', 'category'),
    )
    
    def __repr__(self):
        return f"<Task(description='{self.description[:50]}...', priority='{self.priority}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'email_id': self.email_id,
            'description': self.description,
            'assignee': self.assignee,
            'due_date': self.due_date.isoformat() if self.due_date else None,
            'due_date_text': self.due_date_text,
            'priority': self.priority,
            'category': self.category,
            'confidence': self.confidence,
            'source_text': self.source_text,
            'status': self.status,
            'completed_at': self.completed_at.isoformat() if self.completed_at else None,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'extractor_version': self.extractor_version,
            'model_used': self.model_used
        }

class Person(Base):
    """Person model for tracking individuals mentioned in emails"""
    __tablename__ = 'people'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Person identification
    email_address = Column(String(255), index=True)
    name = Column(String(255), nullable=False)
    first_name = Column(String(100))
    last_name = Column(String(100))
    
    # Person details (extracted and augmented by Claude)
    title = Column(String(255))
    company = Column(String(255))
    role = Column(String(255))
    department = Column(String(255))
    
    # Relationship and context
    relationship_type = Column(String(100))  # colleague, client, vendor, etc.
    communication_frequency = Column(String(50))  # high, medium, low
    importance_level = Column(Float)  # 0.0 to 1.0
    
    # Knowledge base (JSON fields for flexible data)
    skills = Column(JSONType)  # List of skills/expertise
    interests = Column(JSONType)  # Personal/professional interests
    projects_involved = Column(JSONType)  # List of project IDs
    communication_style = Column(Text)  # Claude's analysis of communication style
    key_topics = Column(JSONType)  # Main topics discussed with this person
    
    # Extracted insights
    personality_traits = Column(JSONType)  # Claude-extracted personality insights
    preferences = Column(JSONType)  # Communication preferences, etc.
    notes = Column(Text)  # Accumulated notes about this person
    
    # Metadata
    first_mentioned = Column(DateTime, default=datetime.utcnow)
    last_interaction = Column(DateTime, default=datetime.utcnow)
    total_emails = Column(Integer, default=0)
    
    # AI processing metadata
    knowledge_confidence = Column(Float, default=0.5)  # Confidence in extracted data
    last_updated_by_ai = Column(DateTime)
    ai_version = Column(String(50))
    
    # NEW: Smart Contact Strategy fields
    is_trusted_contact = Column(Boolean, default=False, index=True)
    engagement_score = Column(Float, default=0.0)
    bidirectional_topics = Column(JSONType)  # Topics with back-and-forth discussion
    
    # Relationships
    user = relationship("User", back_populates="people")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_person_user_email', 'user_id', 'email_address'),
        Index('idx_person_user_name', 'user_id', 'name'),
        Index('idx_person_company', 'user_id', 'company'),
    )
    
    def __repr__(self):
        return f"<Person(name='{self.name}', email='{self.email_address}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'email_address': self.email_address,
            'name': self.name,
            'first_name': self.first_name,
            'last_name': self.last_name,
            'title': self.title,
            'company': self.company,
            'role': self.role,
            'department': self.department,
            'relationship_type': self.relationship_type,
            'communication_frequency': self.communication_frequency,
            'importance_level': self.importance_level,
            'skills': self.skills,
            'interests': self.interests,
            'projects_involved': self.projects_involved,
            'communication_style': self.communication_style,
            'key_topics': self.key_topics,
            'personality_traits': self.personality_traits,
            'preferences': self.preferences,
            'notes': self.notes,
            'first_mentioned': self.first_mentioned.isoformat() if self.first_mentioned else None,
            'last_interaction': self.last_interaction.isoformat() if self.last_interaction else None,
            'total_emails': self.total_emails,
            'knowledge_confidence': self.knowledge_confidence,
            'last_updated_by_ai': self.last_updated_by_ai.isoformat() if self.last_updated_by_ai else None,
            'ai_version': self.ai_version,
            'is_trusted_contact': self.is_trusted_contact,
            'engagement_score': self.engagement_score,
            'bidirectional_topics': self.bidirectional_topics
        }

class Project(Base):
    """Project model for categorizing emails and tracking project-related information"""
    __tablename__ = 'projects'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Project identification
    name = Column(String(255), nullable=False)
    slug = Column(String(255), index=True)  # URL-friendly name
    description = Column(Text)
    
    # Project details
    status = Column(String(50), default='active')  # active, completed, paused, cancelled
    priority = Column(String(20), default='medium')  # high, medium, low
    category = Column(String(100))  # business, personal, client work, etc.
    
    # Timeline
    start_date = Column(DateTime)
    end_date = Column(DateTime)
    deadline = Column(DateTime)
    
    # People and relationships
    stakeholders = Column(JSONType)  # List of person IDs involved
    team_members = Column(JSONType)  # List of person IDs
    
    # Project insights (extracted by Claude)
    key_topics = Column(JSONType)  # Main topics/themes
    objectives = Column(JSONType)  # Project goals and objectives
    challenges = Column(JSONType)  # Identified challenges
    progress_indicators = Column(JSONType)  # Metrics and milestones
    
    # Communication patterns
    communication_frequency = Column(String(50))
    last_activity = Column(DateTime)
    total_emails = Column(Integer, default=0)
    
    # AI analysis
    sentiment_trend = Column(Float)  # Overall sentiment about project
    urgency_level = Column(Float)  # How urgent this project appears
    confidence_score = Column(Float)  # AI confidence in project categorization
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    ai_version = Column(String(50))
    
    # Relationships
    user = relationship("User", back_populates="projects")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_project_user_status', 'user_id', 'status'),
        Index('idx_project_user_priority', 'user_id', 'priority'),
        Index('idx_project_user_category', 'user_id', 'category'),
    )
    
    def __repr__(self):
        return f"<Project(name='{self.name}', status='{self.status}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'name': self.name,
            'slug': self.slug,
            'description': self.description,
            'status': self.status,
            'priority': self.priority,
            'category': self.category,
            'start_date': self.start_date.isoformat() if self.start_date else None,
            'end_date': self.end_date.isoformat() if self.end_date else None,
            'deadline': self.deadline.isoformat() if self.deadline else None,
            'stakeholders': self.stakeholders,
            'team_members': self.team_members,
            'key_topics': self.key_topics,
            'objectives': self.objectives,
            'challenges': self.challenges,
            'progress_indicators': self.progress_indicators,
            'communication_frequency': self.communication_frequency,
            'last_activity': self.last_activity.isoformat() if self.last_activity else None,
            'total_emails': self.total_emails,
            'sentiment_trend': self.sentiment_trend,
            'urgency_level': self.urgency_level,
            'confidence_score': self.confidence_score,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'ai_version': self.ai_version
        }

class Topic(Base):
    """Topic model for organizing and categorizing content"""
    __tablename__ = 'topics'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Topic identification
    name = Column(String(255), nullable=False)
    slug = Column(String(255), index=True)  # URL-friendly name
    description = Column(Text)
    
    # Topic properties
    is_official = Column(Boolean, default=False, index=True)  # Official vs AI-discovered
    parent_topic_id = Column(Integer, ForeignKey('topics.id'), index=True)  # For hierarchical topics
    merged_topics = Column(Text)  # JSON string of merged topic names
    keywords = Column(Text)  # JSON string of keywords for matching (changed from JSONType for compatibility)
    email_count = Column(Integer, default=0)  # Number of emails with this topic
    
    # Usage tracking
    last_used = Column(DateTime)
    usage_frequency = Column(Float)
    confidence_threshold = Column(Float)
    
    # AI analysis
    confidence_score = Column(Float, default=0.5)  # AI confidence in topic classification
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    ai_version = Column(String(50))
    
    # Relationships
    user = relationship("User", back_populates="topics")
    parent_topic = relationship("Topic", remote_side=[id], backref="child_topics")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_topic_user_official', 'user_id', 'is_official'),
        Index('idx_topic_user_name', 'user_id', 'name'),
        Index('idx_topic_slug', 'user_id', 'slug'),
        Index('idx_topic_parent', 'parent_topic_id'),
    )
    
    def __repr__(self):
        return f"<Topic(name='{self.name}', is_official={self.is_official})>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'name': self.name,
            'slug': self.slug,
            'description': self.description,
            'is_official': self.is_official,
            'keywords': json.loads(self.keywords) if self.keywords else [],
            'email_count': self.email_count,
            'confidence_score': self.confidence_score,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'ai_version': self.ai_version,
            'parent_topic_id': self.parent_topic_id,
            'last_used': self.last_used.isoformat() if self.last_used else None
        }

class TrustedContact(Base):
    """Trusted Contact model for engagement-based contact database"""
    __tablename__ = 'trusted_contacts'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Contact identification
    email_address = Column(String(255), nullable=False, index=True)
    name = Column(String(255))
    
    # Engagement metrics
    engagement_score = Column(Float, default=0.0, index=True)
    first_sent_date = Column(DateTime)
    last_sent_date = Column(DateTime, index=True)
    total_sent_emails = Column(Integer, default=0)
    total_received_emails = Column(Integer, default=0)
    bidirectional_threads = Column(Integer, default=0)
    
    # Topic analysis
    topics_discussed = Column(JSONType)  # List of topics from sent/received emails
    bidirectional_topics = Column(JSONType)  # Topics with back-and-forth discussion
    
    # Relationship assessment
    relationship_strength = Column(String(20), default='low', index=True)  # high, medium, low
    communication_frequency = Column(String(20))  # daily, weekly, monthly, occasional
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    last_analyzed = Column(DateTime)
    
    # Relationships
    user = relationship("User", backref="trusted_contacts")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_trusted_contact_user_email', 'user_id', 'email_address'),
        Index('idx_trusted_contact_engagement', 'user_id', 'engagement_score'),
        Index('idx_trusted_contact_strength', 'user_id', 'relationship_strength'),
    )
    
    def __repr__(self):
        return f"<TrustedContact(email='{self.email_address}', strength='{self.relationship_strength}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'email_address': self.email_address,
            'name': self.name,
            'engagement_score': self.engagement_score,
            'first_sent_date': self.first_sent_date.isoformat() if self.first_sent_date else None,
            'last_sent_date': self.last_sent_date.isoformat() if self.last_sent_date else None,
            'total_sent_emails': self.total_sent_emails,
            'total_received_emails': self.total_received_emails,
            'bidirectional_threads': self.bidirectional_threads,
            'topics_discussed': self.topics_discussed,
            'bidirectional_topics': self.bidirectional_topics,
            'relationship_strength': self.relationship_strength,
            'communication_frequency': self.communication_frequency,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'last_analyzed': self.last_analyzed.isoformat() if self.last_analyzed else None
        }

class ContactContext(Base):
    """Rich context information for contacts"""
    __tablename__ = 'contact_contexts'
    
    id = Column(Integer, primary_key=True)
    person_id = Column(Integer, ForeignKey('people.id'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Context details
    context_type = Column(String(50), nullable=False, index=True)  # communication_pattern, project_involvement, topic_expertise, relationship_notes
    title = Column(String(255), nullable=False)
    description = Column(Text)
    confidence_score = Column(Float, default=0.5)
    
    # Supporting evidence
    source_emails = Column(JSONType)  # List of email IDs that contributed to this context
    supporting_quotes = Column(JSONType)  # Relevant excerpts from emails
    tags = Column(JSONType)  # Flexible tagging system
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    person = relationship("Person", backref="contexts")
    user = relationship("User", backref="contact_contexts")
    
    # Indexes
    __table_args__ = (
        Index('idx_contact_context_person', 'person_id', 'context_type'),
        Index('idx_contact_context_user', 'user_id', 'context_type'),
    )
    
    def __repr__(self):
        return f"<ContactContext(type='{self.context_type}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'person_id': self.person_id,
            'user_id': self.user_id,
            'context_type': self.context_type,
            'title': self.title,
            'description': self.description,
            'confidence_score': self.confidence_score,
            'source_emails': self.source_emails,
            'supporting_quotes': self.supporting_quotes,
            'tags': self.tags,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

class TaskContext(Base):
    """Rich context information for tasks"""
    __tablename__ = 'task_contexts'
    
    id = Column(Integer, primary_key=True)
    task_id = Column(Integer, ForeignKey('tasks.id'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Context details
    context_type = Column(String(50), nullable=False, index=True)  # background, stakeholders, timeline, business_impact
    title = Column(String(255), nullable=False)
    description = Column(Text)
    
    # Related entities
    related_people = Column(JSONType)  # List of person IDs
    related_projects = Column(JSONType)  # List of project IDs
    related_topics = Column(JSONType)  # List of relevant topics
    
    # Source information
    source_email_id = Column(Integer, ForeignKey('emails.id'))
    source_thread_id = Column(String(255))
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    task = relationship("Task", backref="contexts")
    user = relationship("User", backref="task_contexts")
    source_email = relationship("Email")
    
    # Indexes
    __table_args__ = (
        Index('idx_task_context_task', 'task_id', 'context_type'),
        Index('idx_task_context_user', 'user_id', 'context_type'),
    )
    
    def __repr__(self):
        return f"<TaskContext(type='{self.context_type}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'task_id': self.task_id,
            'user_id': self.user_id,
            'context_type': self.context_type,
            'title': self.title,
            'description': self.description,
            'related_people': self.related_people,
            'related_projects': self.related_projects,
            'related_topics': self.related_topics,
            'source_email_id': self.source_email_id,
            'source_thread_id': self.source_thread_id,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

class TopicKnowledgeBase(Base):
    """Comprehensive knowledge base for topics"""
    __tablename__ = 'topic_knowledge_base'
    
    id = Column(Integer, primary_key=True)
    topic_id = Column(Integer, ForeignKey('topics.id'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Knowledge details
    knowledge_type = Column(String(50), nullable=False, index=True)  # methodology, key_people, challenges, success_patterns, tools, decisions
    title = Column(String(255), nullable=False)
    content = Column(Text)
    confidence_score = Column(Float, default=0.5)
    
    # Supporting evidence
    supporting_evidence = Column(JSONType)  # Email excerpts, patterns observed
    source_emails = Column(JSONType)  # List of email IDs that contributed
    patterns = Column(JSONType)  # Observed patterns and trends
    
    # Knowledge metadata
    relevance_score = Column(Float, default=0.5)  # How relevant this knowledge is
    engagement_weight = Column(Float, default=0.5)  # Weight based on user engagement
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    last_updated = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    topic = relationship("Topic", backref="knowledge_base")
    user = relationship("User", backref="topic_knowledge")
    
    # Indexes
    __table_args__ = (
        Index('idx_topic_knowledge_topic', 'topic_id', 'knowledge_type'),
        Index('idx_topic_knowledge_user', 'user_id', 'knowledge_type'),
        Index('idx_topic_knowledge_relevance', 'user_id', 'relevance_score'),
    )
    
    def __repr__(self):
        return f"<TopicKnowledgeBase(type='{self.knowledge_type}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'topic_id': self.topic_id,
            'user_id': self.user_id,
            'knowledge_type': self.knowledge_type,
            'title': self.title,
            'content': self.content,
            'confidence_score': self.confidence_score,
            'supporting_evidence': self.supporting_evidence,
            'source_emails': self.source_emails,
            'patterns': self.patterns,
            'relevance_score': self.relevance_score,
            'engagement_weight': self.engagement_weight,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'last_updated': self.last_updated.isoformat() if self.last_updated else None
        }

class Calendar(Base):
    """Calendar model for storing Google Calendar events per user"""
    __tablename__ = 'calendar_events'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Google Calendar identifiers
    event_id = Column(String(255), nullable=False, index=True)
    calendar_id = Column(String(255), nullable=False, index=True)
    recurring_event_id = Column(String(255), index=True)
    
    # Event content
    title = Column(Text)
    description = Column(Text)
    location = Column(Text)
    status = Column(String(50))  # confirmed, tentative, cancelled
    
    # Event timing
    start_time = Column(DateTime, index=True)
    end_time = Column(DateTime, index=True)
    timezone = Column(String(100))
    is_all_day = Column(Boolean, default=False)
    
    # Attendees and relationships
    organizer_email = Column(String(255), index=True)
    organizer_name = Column(String(255))
    attendees = Column(JSONType)  # List of attendee objects with email, name, status
    attendee_emails = Column(JSONType)  # List of attendee emails for quick lookup
    
    # Meeting metadata
    meeting_type = Column(String(100))  # in-person, video_call, phone, etc.
    conference_data = Column(JSONType)  # Google Meet, Zoom links, etc.
    visibility = Column(String(50))  # default, public, private
    
    # Event properties
    is_recurring = Column(Boolean, default=False)
    recurrence_rules = Column(JSONType)  # RRULE data
    is_busy = Column(Boolean, default=True)
    transparency = Column(String(20))  # opaque, transparent
    
    # AI analysis and insights
    ai_summary = Column(Text)  # Claude-generated meeting summary/purpose
    ai_category = Column(String(100))  # AI-determined category (business, personal, etc.)
    importance_score = Column(Float)  # AI-determined importance
    preparation_needed = Column(Boolean, default=False)
    follow_up_required = Column(Boolean, default=False)
    
    # Contact intelligence integration
    known_attendees = Column(JSONType)  # List of person IDs from People table
    unknown_attendees = Column(JSONType)  # Attendees not in contact database
    business_context = Column(Text)  # AI-generated business context based on attendees
    
    # Free time analysis
    is_free_time = Column(Boolean, default=False, index=True)  # For free time slot identification
    potential_duration = Column(Integer)  # Duration in minutes for free slots
    
    # Processing metadata
    fetched_at = Column(DateTime, default=datetime.utcnow)
    last_updated = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    ai_processed_at = Column(DateTime)
    ai_version = Column(String(50))
    
    # Google Calendar metadata
    html_link = Column(Text)  # Link to event in Google Calendar
    hangout_link = Column(Text)  # Google Meet link
    ical_uid = Column(String(255))
    sequence = Column(Integer)  # For tracking updates
    
    # Relationships
    user = relationship("User", backref="calendar_events")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_calendar_user_event', 'user_id', 'event_id'),
        Index('idx_calendar_user_time', 'user_id', 'start_time'),
        Index('idx_calendar_user_organizer', 'user_id', 'organizer_email'),
        Index('idx_calendar_free_time', 'user_id', 'is_free_time'),
        Index('idx_calendar_status', 'user_id', 'status'),
    )
    
    def __repr__(self):
        return f"<Calendar(event_id='{self.event_id}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'event_id': self.event_id,
            'calendar_id': self.calendar_id,
            'recurring_event_id': self.recurring_event_id,
            'title': self.title,
            'description': self.description,
            'location': self.location,
            'status': self.status,
            'start_time': self.start_time.isoformat() if self.start_time else None,
            'end_time': self.end_time.isoformat() if self.end_time else None,
            'timezone': self.timezone,
            'is_all_day': self.is_all_day,
            'organizer_email': self.organizer_email,
            'organizer_name': self.organizer_name,
            'attendees': self.attendees,
            'attendee_emails': self.attendee_emails,
            'meeting_type': self.meeting_type,
            'conference_data': self.conference_data,
            'visibility': self.visibility,
            'is_recurring': self.is_recurring,
            'recurrence_rules': self.recurrence_rules,
            'is_busy': self.is_busy,
            'transparency': self.transparency,
            'ai_summary': self.ai_summary,
            'ai_category': self.ai_category,
            'importance_score': self.importance_score,
            'preparation_needed': self.preparation_needed,
            'follow_up_required': self.follow_up_required,
            'known_attendees': self.known_attendees,
            'unknown_attendees': self.unknown_attendees,
            'business_context': self.business_context,
            'is_free_time': self.is_free_time,
            'potential_duration': self.potential_duration,
            'fetched_at': self.fetched_at.isoformat() if self.fetched_at else None,
            'last_updated': self.last_updated.isoformat() if self.last_updated else None,
            'ai_processed_at': self.ai_processed_at.isoformat() if self.ai_processed_at else None,
            'ai_version': self.ai_version,
            'html_link': self.html_link,
            'hangout_link': self.hangout_link,
            'ical_uid': self.ical_uid,
            'sequence': self.sequence
        }

class DatabaseManager:
    """Database manager for handling connections and sessions"""
    
    def __init__(self):
        self.engine = None
        self.SessionLocal = None
        self.initialize_database()
    
    def initialize_database(self):
        """Initialize database connection and create tables"""
        try:
            # Use DATABASE_URL from environment or default to SQLite
            database_url = settings.DATABASE_URL
            
            # Handle PostgreSQL URL for Heroku
            if database_url and database_url.startswith('postgres://'):
                database_url = database_url.replace('postgres://', 'postgresql://', 1)
            
            # Create engine with appropriate settings
            if database_url.startswith('postgresql://'):
                # PostgreSQL settings for Heroku
                self.engine = create_engine(
                    database_url,
                    echo=settings.DEBUG,
                    pool_pre_ping=True,
                    pool_recycle=300
                )
            else:
                # SQLite settings for local development
                self.engine = create_engine(
                    database_url,
                    echo=settings.DEBUG,
                    connect_args={"check_same_thread": False}
                )
            
            # Create session factory
            self.SessionLocal = sessionmaker(bind=self.engine)
            
            # Create all tables
            Base.metadata.create_all(bind=self.engine)
            
            logger.info("Database initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize database: {str(e)}")
            raise
    
    def get_session(self) -> Session:
        """Get a new database session"""
        return self.SessionLocal()
    
    def get_user_by_email(self, email: str) -> Optional[User]:
        """Get user by email address"""
        with self.get_session() as session:
            return session.query(User).filter(User.email == email).first()
    
    def create_or_update_user(self, user_info: Dict, credentials: Dict) -> User:
        """Create or update user with OAuth info"""
        with self.get_session() as session:
            user = session.query(User).filter(User.email == user_info['email']).first()
            
            if user:
                # Update existing user
                user.name = user_info.get('name', user.name)
                user.last_login = datetime.utcnow()
                user.access_token = credentials.get('access_token')
                user.refresh_token = credentials.get('refresh_token')
                user.token_expires_at = credentials.get('expires_at')
                user.scopes = credentials.get('scopes', [])
            else:
                # Create new user
                user = User(
                    email=user_info['email'],
                    google_id=user_info['id'],
                    name=user_info.get('name', ''),
                    access_token=credentials.get('access_token'),
                    refresh_token=credentials.get('refresh_token'),
                    token_expires_at=credentials.get('expires_at'),
                    scopes=credentials.get('scopes', [])
                )
                session.add(user)
            
            session.commit()
            session.refresh(user)
            return user
    
    def save_email(self, user_id: int, email_data: Dict) -> Email:
        """Save processed email to database"""
        with self.get_session() as session:
            # Check if email already exists
            existing = session.query(Email).filter(
                Email.user_id == user_id,
                Email.gmail_id == email_data['id']
            ).first()
            
            if existing:
                return existing
            
            # Create new email record
            email = Email(
                user_id=user_id,
                gmail_id=email_data['id'],
                thread_id=email_data.get('thread_id'),
                sender=email_data.get('sender'),
                sender_name=email_data.get('sender_name'),
                subject=email_data.get('subject'),
                body_text=email_data.get('body_text'),
                body_html=email_data.get('body_html'),
                body_clean=email_data.get('body_clean'),
                body_preview=email_data.get('body_preview'),
                snippet=email_data.get('snippet'),
                recipients=email_data.get('recipients', []),
                cc=email_data.get('cc', []),
                bcc=email_data.get('bcc', []),
                labels=email_data.get('labels', []),
                attachments=email_data.get('attachments', []),
                entities=email_data.get('entities', {}),
                email_date=email_data.get('timestamp'),
                size_estimate=email_data.get('size_estimate'),
                message_type=email_data.get('message_type'),
                priority_score=email_data.get('priority_score'),
                is_read=email_data.get('is_read', False),
                is_important=email_data.get('is_important', False),
                is_starred=email_data.get('is_starred', False),
                has_attachments=email_data.get('has_attachments', False),
                normalizer_version=email_data.get('processing_metadata', {}).get('normalizer_version'),
                has_errors=email_data.get('error', False),
                error_message=email_data.get('error_message')
            )
            
            session.add(email)
            session.commit()
            session.refresh(email)
            return email
    
    def save_task(self, user_id: int, email_id: Optional[int], task_data: Dict) -> Task:
        """Save extracted task to database"""
        try:
            with self.get_session() as session:
                task = Task(
                    user_id=user_id,
                    email_id=email_id,
                    description=task_data['description'],
                    assignee=task_data.get('assignee'),
                    due_date=task_data.get('due_date'),
                    due_date_text=task_data.get('due_date_text'),
                    priority=task_data.get('priority', 'medium'),
                    category=task_data.get('category'),
                    confidence=task_data.get('confidence'),
                    source_text=task_data.get('source_text'),
                    status=task_data.get('status', 'pending'),
                    extractor_version=task_data.get('extractor_version'),
                    model_used=task_data.get('model_used')
                )
                
                session.add(task)
                session.commit()
                session.refresh(task)
                
                # Verify the task object is valid before returning
                if not task or not hasattr(task, 'id') or task.id is None:
                    raise ValueError("Failed to create task - invalid task object returned")
                
                return task
                
        except Exception as e:
            logger.error(f"Failed to save task to database: {str(e)}")
            logger.error(f"Task data: {task_data}")
            raise  # Re-raise the exception instead of returning a dict
    
    def get_user_emails(self, user_id: int, limit: int = 50) -> List[Email]:
        """Get emails for a user"""
        with self.get_session() as session:
            return session.query(Email).filter(
                Email.user_id == user_id
            ).order_by(Email.email_date.desc()).limit(limit).all()
    
    def get_user_tasks(self, user_id: int, status: str = None, limit: int = 500) -> List[Task]:
        """Get tasks for a user"""
        with self.get_session() as session:
            query = session.query(Task).filter(Task.user_id == user_id)
            if status:
                query = query.filter(Task.status == status)
            return query.order_by(Task.created_at.desc()).limit(limit).all()

    def create_or_update_person(self, user_id: int, person_data: Dict) -> Person:
        """Create or update a person record"""
        with self.get_session() as session:
            # Try to find existing person by email or name
            person = session.query(Person).filter(
                Person.user_id == user_id,
                Person.email_address == person_data.get('email_address')
            ).first()
            
            if not person and person_data.get('name'):
                # Try by name if email not found
                person = session.query(Person).filter(
                    Person.user_id == user_id,
                    Person.name == person_data.get('name')
                ).first()
            
            if person:
                # Update existing person
                for key, value in person_data.items():
                    if hasattr(person, key) and value is not None:
                        setattr(person, key, value)
                person.last_interaction = datetime.utcnow()
                person.total_emails += 1
                person.last_updated_by_ai = datetime.utcnow()
            else:
                # Create new person - remove conflicting fields from person_data
                person_data_clean = person_data.copy()
                person_data_clean.pop('total_emails', None)  # Remove if present
                person_data_clean.pop('last_updated_by_ai', None)  # Remove if present
                
                person = Person(
                    user_id=user_id,
                    **person_data_clean,
                    total_emails=1,
                    last_updated_by_ai=datetime.utcnow()
                )
                session.add(person)
            
            session.commit()
            session.refresh(person)
            return person
    
    def create_or_update_project(self, user_id: int, project_data: Dict) -> Project:
        """Create or update a project record"""
        with self.get_session() as session:
            # Try to find existing project by name or slug
            project = session.query(Project).filter(
                Project.user_id == user_id,
                Project.name == project_data.get('name')
            ).first()
            
            if project:
                # Update existing project
                for key, value in project_data.items():
                    if hasattr(project, key) and value is not None:
                        setattr(project, key, value)
                project.last_activity = datetime.utcnow()
                project.total_emails += 1
                project.updated_at = datetime.utcnow()
            else:
                # Create new project
                project = Project(
                    user_id=user_id,
                    **project_data,
                    total_emails=1,
                    updated_at=datetime.utcnow()
                )
                session.add(project)
            
            session.commit()
            session.refresh(project)
            return project
    
    def get_user_people(self, user_id: int, limit: int = 500) -> List[Person]:
        """Get people for a user"""
        with self.get_session() as session:
            query = session.query(Person).filter(Person.user_id == user_id)
            query = query.order_by(Person.last_interaction.desc())
            if limit:
                query = query.limit(limit)
            return query.all()
    
    def get_user_projects(self, user_id: int, status: str = None, limit: int = 200) -> List[Project]:
        """Get projects for a user"""
        with self.get_session() as session:
            query = session.query(Project).filter(Project.user_id == user_id)
            if status:
                query = query.filter(Project.status == status)
            query = query.order_by(Project.last_activity.desc())
            if limit:
                query = query.limit(limit)
            return query.all()
    
    def find_person_by_email(self, user_id: int, email: str) -> Optional[Person]:
        """Find person by email address"""
        with self.get_session() as session:
            return session.query(Person).filter(
                Person.user_id == user_id,
                Person.email_address == email
            ).first()
    
    def find_project_by_keywords(self, user_id: int, keywords: List[str]) -> Optional[Project]:
        """Find project by matching keywords against name, description, or topics - FIXED to prevent memory issues"""
        with self.get_session() as session:
            # CRITICAL FIX: Add limit to prevent loading too many projects
            projects = session.query(Project).filter(Project.user_id == user_id).limit(50).all()
            
            for project in projects:
                # Check name and description
                if any(keyword.lower() in (project.name or '').lower() for keyword in keywords):
                    return project
                if any(keyword.lower() in (project.description or '').lower() for keyword in keywords):
                    return project
                
                # Check key topics
                if project.key_topics:
                    project_topics = [topic.lower() for topic in project.key_topics]
                    if any(keyword.lower() in project_topics for keyword in keywords):
                        return project
            
            return None

    def get_user_topics(self, user_id: int, limit: int = 1000) -> List[Topic]:
        """Get all topics for a user"""
        with self.get_session() as session:
            return session.query(Topic).filter(
                Topic.user_id == user_id
            ).order_by(Topic.is_official.desc(), Topic.name.asc()).limit(limit).all()
    
    def create_or_update_topic(self, user_id: int, topic_data: Dict) -> Topic:
        """Create or update a topic record"""
        with self.get_session() as session:
            # Try to find existing topic by name
            topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.name == topic_data.get('name')
            ).first()
            
            # Handle keywords conversion to JSON string
            topic_data_copy = topic_data.copy()
            if 'keywords' in topic_data_copy and isinstance(topic_data_copy['keywords'], list):
                topic_data_copy['keywords'] = json.dumps(topic_data_copy['keywords'])
            
            if topic:
                # Update existing topic
                for key, value in topic_data_copy.items():
                    if hasattr(topic, key) and key != 'id':
                        setattr(topic, key, value)
                topic.updated_at = datetime.now()
            else:
                # Create new topic
                topic_data_copy['user_id'] = user_id
                topic_data_copy['created_at'] = datetime.now()
                topic_data_copy['updated_at'] = datetime.now()
                
                # Set default values for optional fields
                if 'slug' not in topic_data_copy:
                    topic_data_copy['slug'] = topic_data_copy['name'].lower().replace(' ', '-').replace('_', '-')
                
                if 'is_official' not in topic_data_copy:
                    topic_data_copy['is_official'] = False
                    
                if 'confidence_score' not in topic_data_copy:
                    topic_data_copy['confidence_score'] = 0.5
                    
                if 'email_count' not in topic_data_copy:
                    topic_data_copy['email_count'] = 0
                
                topic = Topic(**topic_data_copy)
                session.add(topic)
            
            session.commit()
            session.refresh(topic)
            return topic

    def update_topic(self, user_id: int, topic_id: int, topic_data: Dict) -> bool:
        """Update a specific topic by ID"""
        with self.get_session() as session:
            topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == topic_id
            ).first()
            
            if not topic:
                return False
            
            # Handle keywords conversion to JSON string
            for key, value in topic_data.items():
                if hasattr(topic, key) and value is not None:
                    if key == 'keywords' and isinstance(value, list):
                        setattr(topic, key, json.dumps(value))
                    else:
                        setattr(topic, key, value)
            
            topic.updated_at = datetime.utcnow()
            session.commit()
            return True

    def mark_topic_official(self, user_id: int, topic_id: int) -> bool:
        """Mark a topic as official"""
        with self.get_session() as session:
            topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == topic_id
            ).first()
            
            if not topic:
                return False
            
            topic.is_official = True
            topic.updated_at = datetime.utcnow()
            session.commit()
            return True

    def merge_topics(self, user_id: int, source_topic_id: int, target_topic_id: int) -> bool:
        """Merge one topic into another"""
        with self.get_session() as session:
            source_topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == source_topic_id
            ).first()
            
            target_topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == target_topic_id
            ).first()
            
            if not source_topic or not target_topic:
                return False
            
            try:
                # Update all emails that reference the source topic
                # This is a simplified version - in practice, you'd need to update
                # the topics JSON array in emails to replace source with target
                
                # For now, we'll merge the email counts and keywords
                target_topic.email_count = (target_topic.email_count or 0) + (source_topic.email_count or 0)
                
                # Merge keywords
                source_keywords = json.loads(source_topic.keywords) if source_topic.keywords else []
                target_keywords = json.loads(target_topic.keywords) if target_topic.keywords else []
                merged_keywords = list(set(source_keywords + target_keywords))
                target_topic.keywords = json.dumps(merged_keywords)
                
                # Update merge tracking
                merged_topics = json.loads(target_topic.merged_topics) if target_topic.merged_topics else []
                merged_topics.append(source_topic.name)
                target_topic.merged_topics = json.dumps(merged_topics)
                
                target_topic.updated_at = datetime.utcnow()
                
                # Delete the source topic
                session.delete(source_topic)
                session.commit()
                return True
                
            except Exception as e:
                session.rollback()
                logger.error(f"Failed to merge topics: {str(e)}")
                return False

    # ===== SMART CONTACT STRATEGY METHODS =====
    
    def create_or_update_trusted_contact(self, user_id: int, contact_data: Dict) -> TrustedContact:
        """Create or update a trusted contact record"""
        with self.get_session() as session:
            contact = session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.email_address == contact_data['email_address']
            ).first()
            
            if contact:
                # Update existing contact
                for key, value in contact_data.items():
                    if hasattr(contact, key) and value is not None:
                        setattr(contact, key, value)
                contact.updated_at = datetime.utcnow()
            else:
                # Create new trusted contact
                contact = TrustedContact(
                    user_id=user_id,
                    **contact_data,
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                session.add(contact)
            
            session.commit()
            session.refresh(contact)
            return contact
    
    def get_trusted_contacts(self, user_id: int, limit: int = 500) -> List[TrustedContact]:
        """Get trusted contacts for a user"""
        with self.get_session() as session:
            return session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id
            ).order_by(TrustedContact.engagement_score.desc()).limit(limit).all()
    
    def find_trusted_contact_by_email(self, user_id: int, email_address: str) -> Optional[TrustedContact]:
        """Find trusted contact by email address"""
        with self.get_session() as session:
            return session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.email_address == email_address
            ).first()
    
    def create_contact_context(self, user_id: int, person_id: int, context_data: Dict) -> ContactContext:
        """Create a new contact context record"""
        with self.get_session() as session:
            context = ContactContext(
                user_id=user_id,
                person_id=person_id,
                **context_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(context)
            session.commit()
            session.refresh(context)
            return context
    
    def get_contact_contexts(self, user_id: int, person_id: int = None, context_type: str = None) -> List[ContactContext]:
        """Get contact contexts for a user, optionally filtered by person or type"""
        with self.get_session() as session:
            query = session.query(ContactContext).filter(ContactContext.user_id == user_id)
            
            if person_id:
                query = query.filter(ContactContext.person_id == person_id)
            
            if context_type:
                query = query.filter(ContactContext.context_type == context_type)
            
            return query.order_by(ContactContext.created_at.desc()).all()
    
    def create_task_context(self, user_id: int, task_id: int, context_data: Dict) -> TaskContext:
        """Create a new task context record"""
        with self.get_session() as session:
            context = TaskContext(
                user_id=user_id,
                task_id=task_id,
                **context_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(context)
            session.commit()
            session.refresh(context)
            return context
    
    def get_task_contexts(self, user_id: int, task_id: int = None, context_type: str = None) -> List[TaskContext]:
        """Get task contexts for a user, optionally filtered by task or type"""
        with self.get_session() as session:
            query = session.query(TaskContext).filter(TaskContext.user_id == user_id)
            
            if task_id:
                query = query.filter(TaskContext.task_id == task_id)
            
            if context_type:
                query = query.filter(TaskContext.context_type == context_type)
            
            return query.order_by(TaskContext.created_at.desc()).all()
    
    def create_topic_knowledge(self, user_id: int, topic_id: int, knowledge_data: Dict) -> TopicKnowledgeBase:
        """Create a new topic knowledge record"""
        with self.get_session() as session:
            knowledge = TopicKnowledgeBase(
                user_id=user_id,
                topic_id=topic_id,
                **knowledge_data,
                created_at=datetime.utcnow(),
                last_updated=datetime.utcnow()
            )
            session.add(knowledge)
            session.commit()
            session.refresh(knowledge)
            return knowledge
    
    def get_topic_knowledge(self, user_id: int, topic_id: int = None, knowledge_type: str = None) -> List[TopicKnowledgeBase]:
        """Get topic knowledge for a user, optionally filtered by topic or type"""
        with self.get_session() as session:
            query = session.query(TopicKnowledgeBase).filter(TopicKnowledgeBase.user_id == user_id)
            
            if topic_id:
                query = query.filter(TopicKnowledgeBase.topic_id == topic_id)
            
            if knowledge_type:
                query = query.filter(TopicKnowledgeBase.knowledge_type == knowledge_type)
            
            return query.order_by(TopicKnowledgeBase.relevance_score.desc()).all()
    
    def update_people_engagement_data(self, user_id: int, person_id: int, engagement_data: Dict) -> bool:
        """Update people table with engagement-based data"""
        with self.get_session() as session:
            person = session.query(Person).filter(
                Person.user_id == user_id,
                Person.id == person_id
            ).first()
            
            if not person:
                return False
            
            # Add engagement fields to person if they don't exist
            if 'is_trusted_contact' in engagement_data:
                person.is_trusted_contact = engagement_data['is_trusted_contact']
            
            if 'engagement_score' in engagement_data:
                person.engagement_score = engagement_data['engagement_score']
            
            if 'bidirectional_topics' in engagement_data:
                person.bidirectional_topics = engagement_data['bidirectional_topics']
            
            session.commit()
            return True
    
    def get_engagement_analytics(self, user_id: int) -> Dict:
        """Get engagement analytics for Smart Contact Strategy reporting"""
        with self.get_session() as session:
            total_contacts = session.query(TrustedContact).filter(TrustedContact.user_id == user_id).count()
            high_engagement = session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.relationship_strength == 'high'
            ).count()
            
            recent_contacts = session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.last_sent_date >= datetime.utcnow() - timedelta(days=30)
            ).count()
            
            return {
                'total_trusted_contacts': total_contacts,
                'high_engagement_contacts': high_engagement,
                'recent_active_contacts': recent_contacts,
                'engagement_rate': (high_engagement / total_contacts * 100) if total_contacts > 0 else 0
            }

    def save_calendar_event(self, user_id: int, event_data: Dict) -> Calendar:
        """Save or update a calendar event"""
        try:
            with self.get_session() as session:
                # Try to find existing event
                existing_event = session.query(Calendar).filter_by(
                    user_id=user_id,
                    event_id=event_data.get('event_id')
                ).first()
                
                if existing_event:
                    # Update existing event
                    for key, value in event_data.items():
                        if hasattr(existing_event, key):
                            setattr(existing_event, key, value)
                    event = existing_event
                else:
                    # Create new event
                    event = Calendar(user_id=user_id, **event_data)
                    session.add(event)
                
                session.commit()
                session.refresh(event)
                return event
                
        except Exception as e:
            logger.error(f"Failed to save calendar event: {str(e)}")
            raise

    def get_user_calendar_events(self, user_id: int, start_date: datetime = None, end_date: datetime = None, limit: int = 500) -> List[Calendar]:
        """Get calendar events for a user within a date range"""
        try:
            with self.get_session() as session:
                query = session.query(Calendar).filter_by(user_id=user_id)
                
                if start_date:
                    query = query.filter(Calendar.start_time >= start_date)
                if end_date:
                    query = query.filter(Calendar.start_time <= end_date)
                
                events = query.order_by(Calendar.start_time.asc()).limit(limit).all()
                return events
                
        except Exception as e:
            logger.error(f"Failed to get user calendar events: {str(e)}")
            return []

    def get_free_time_slots(self, user_id: int, start_date: datetime, end_date: datetime) -> List[Dict]:
        """Identify free time slots between calendar events"""
        try:
            with self.get_session() as session:
                events = session.query(Calendar).filter(
                    Calendar.user_id == user_id,
                    Calendar.start_time >= start_date,
                    Calendar.start_time <= end_date,
                    Calendar.status.in_(['confirmed', 'tentative']),
                    Calendar.is_busy == True
                ).order_by(Calendar.start_time).all()
                
                free_slots = []
                current_time = start_date
                
                for event in events:
                    # If there's a gap before this event, it's free time
                    if event.start_time > current_time:
                        gap_duration = int((event.start_time - current_time).total_seconds() / 60)
                        if gap_duration >= 30:  # Minimum 30 minutes to be useful
                            free_slots.append({
                                'start_time': current_time,
                                'end_time': event.start_time,
                                'duration_minutes': gap_duration,
                                'type': 'free_time'
                            })
                    
                    # Update current time to end of this event
                    if event.end_time and event.end_time > current_time:
                        current_time = event.end_time
                
                # Check for free time after last event
                if current_time < end_date:
                    gap_duration = int((end_date - current_time).total_seconds() / 60)
                    if gap_duration >= 30:
                        free_slots.append({
                            'start_time': current_time,
                            'end_time': end_date,
                            'duration_minutes': gap_duration,
                            'type': 'free_time'
                        })
                
                return free_slots
                
        except Exception as e:
            logger.error(f"Failed to get free time slots: {str(e)}")
            return []

    def get_calendar_attendee_intelligence(self, user_id: int, event_id: str) -> Dict:
        """Get intelligence about calendar event attendees"""
        try:
            with self.get_session() as session:
                event = session.query(Calendar).filter_by(
                    user_id=user_id,
                    event_id=event_id
                ).first()
                
                if not event or not event.attendee_emails:
                    return {}
                
                # Find known attendees in People database
                known_people = []
                unknown_attendees = []
                
                for attendee_email in event.attendee_emails:
                    person = self.find_person_by_email(user_id, attendee_email)
                    if person:
                        known_people.append(person.to_dict())
                    else:
                        unknown_attendees.append(attendee_email)
                
                return {
                    'event_id': event_id,
                    'total_attendees': len(event.attendee_emails),
                    'known_attendees': known_people,
                    'unknown_attendees': unknown_attendees,
                    'known_percentage': len(known_people) / len(event.attendee_emails) * 100 if event.attendee_emails else 0
                }
                
        except Exception as e:
            logger.error(f"Failed to get calendar attendee intelligence: {str(e)}")
            return {}

    def update_calendar_ai_analysis(self, user_id: int, event_id: str, ai_data: Dict) -> bool:
        """Update calendar event with AI analysis"""
        try:
            with self.get_session() as session:
                event = session.query(Calendar).filter_by(
                    user_id=user_id,
                    event_id=event_id
                ).first()
                
                if not event:
                    return False
                
                # Update AI analysis fields
                if 'ai_summary' in ai_data:
                    event.ai_summary = ai_data['ai_summary']
                if 'ai_category' in ai_data:
                    event.ai_category = ai_data['ai_category']
                if 'importance_score' in ai_data:
                    event.importance_score = ai_data['importance_score']
                if 'business_context' in ai_data:
                    event.business_context = ai_data['business_context']
                if 'preparation_needed' in ai_data:
                    event.preparation_needed = ai_data['preparation_needed']
                if 'follow_up_required' in ai_data:
                    event.follow_up_required = ai_data['follow_up_required']
                
                event.ai_processed_at = datetime.utcnow()
                event.ai_version = ai_data.get('ai_version', 'claude-3.5-sonnet')
                
                session.commit()
                return True
                
        except Exception as e:
            logger.error(f"Failed to update calendar AI analysis: {str(e)}")
            return False

# Global database manager instance - Initialize lazily
_db_manager = None

def get_db_manager():
    """Get the global database manager instance (lazy initialization)"""
    global _db_manager
    if _db_manager is None:
        _db_manager = DatabaseManager()
    return _db_manager

# Export as db_manager for compatibility, but don't instantiate during import
db_manager = None  # Will be set by get_db_manager() when first called 

====================================================================================================
END OF FILE: chief_of_staff_ai/models/database.py
====================================================================================================


====================================================================================================
FILE 4: chief_of_staff_ai/auth/gmail_auth.py
====================================================================================================
Path: /Users/oudiantebi/Session42 Dropbox/Oudi Antebi/Mac (3)/Documents/MyCode/COS1/chief_of_staff_ai/auth/gmail_auth.py
Info: Size: 12,489 bytes | Modified: 2025-06-06 12:00:33
----------------------------------------------------------------------------------------------------

# Handles Gmail OAuth setup

import os
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Optional, Tuple
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import Flow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from config.settings import settings
from models.database import get_db_manager

logger = logging.getLogger(__name__)

class GmailAuthHandler:
    """Handles Gmail OAuth authentication and token management with database persistence"""
    
    def __init__(self):
        self.client_id = settings.GOOGLE_CLIENT_ID
        self.client_secret = settings.GOOGLE_CLIENT_SECRET
        self.redirect_uri = settings.GOOGLE_REDIRECT_URI
        self.scopes = settings.GMAIL_SCOPES
        
    def get_authorization_url(self, user_id: str, state: str = None) -> Tuple[str, str]:
        """
        Generate OAuth authorization URL for Gmail access
        
        Args:
            user_id: Unique identifier for the user
            state: Optional state parameter for security
            
        Returns:
            Tuple of (authorization_url, state)
        """
        try:
            flow = Flow.from_client_config(
                settings.get_gmail_auth_config(),
                scopes=self.scopes
            )
            flow.redirect_uri = self.redirect_uri
            
            auth_url, state = flow.authorization_url(
                access_type='offline',
                include_granted_scopes='true',
                state=state or user_id,
                prompt='consent'  # Force consent to get refresh token
            )
            
            logger.info(f"Generated authorization URL for user {user_id}")
            return auth_url, state
            
        except Exception as e:
            logger.error(f"Failed to generate authorization URL: {str(e)}")
            raise
    
    def handle_oauth_callback(self, authorization_code: str, state: str = None) -> Dict:
        """
        Handle OAuth callback and exchange authorization code for tokens
        
        Args:
            authorization_code: Authorization code from OAuth callback
            state: State parameter from OAuth callback
            
        Returns:
            Dictionary containing success status and user info or error
        """
        try:
            flow = Flow.from_client_config(
                settings.get_gmail_auth_config(),
                scopes=self.scopes
            )
            flow.redirect_uri = self.redirect_uri
            
            # Exchange authorization code for tokens
            # Note: Google automatically adds 'openid' scope when requesting profile/email
            # We need to handle this gracefully
            try:
                flow.fetch_token(code=authorization_code)
            except Exception as token_error:
                # If there's a scope mismatch due to automatic 'openid' scope, try a more permissive approach
                if "scope" in str(token_error).lower():
                    logger.warning(f"Scope validation issue, retrying with relaxed validation: {str(token_error)}")
                    # Create a new flow with additional scopes including openid
                    extended_scopes = self.scopes + ['openid']
                    flow = Flow.from_client_config(
                        settings.get_gmail_auth_config(),
                        scopes=extended_scopes
                    )
                    flow.redirect_uri = self.redirect_uri
                    flow.fetch_token(code=authorization_code)
                else:
                    raise token_error
            
            credentials = flow.credentials
            
            # Get user information
            user_info = self._get_user_info(credentials)
            
            if not user_info.get('email'):
                raise Exception("Failed to get user email from Google")
            
            # Prepare credentials for database storage
            credentials_data = {
                'access_token': credentials.token,
                'refresh_token': credentials.refresh_token,
                'expires_at': credentials.expiry,
                'scopes': credentials.scopes
            }
            
            # Create or update user in database
            user = get_db_manager().create_or_update_user(user_info, credentials_data)
            
            logger.info(f"Successfully authenticated user: {user.email}")
            
            return {
                'success': True,
                'user_info': user_info,
                'user_email': user.email,
                'access_token': credentials.token,
                'has_refresh_token': bool(credentials.refresh_token),
                'user_id': user.id
            }
            
        except Exception as e:
            logger.error(f"OAuth callback error: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_valid_credentials(self, user_email: str) -> Optional[Credentials]:
        """
        Get valid credentials for a user, refreshing if necessary
        
        Args:
            user_email: Email of the user
            
        Returns:
            Valid Credentials object or None
        """
        try:
            # Get user from database
            user = get_db_manager().get_user_by_email(user_email)
            if not user or not user.access_token:
                logger.warning(f"No stored credentials for user: {user_email}")
                return None
            
            # Create credentials object
            credentials = Credentials(
                token=user.access_token,
                refresh_token=user.refresh_token,
                token_uri="https://oauth2.googleapis.com/token",
                client_id=self.client_id,
                client_secret=self.client_secret,
                scopes=user.scopes or self.scopes
            )
            
            # Set expiry if available
            if user.token_expires_at:
                credentials.expiry = user.token_expires_at
            
            # Check if credentials are expired and refresh if possible
            if credentials.expired and credentials.refresh_token:
                logger.info(f"Refreshing expired credentials for user: {user_email}")
                credentials.refresh(Request())
                
                # Update stored credentials in database
                credentials_data = {
                    'access_token': credentials.token,
                    'refresh_token': credentials.refresh_token,
                    'expires_at': credentials.expiry,
                    'scopes': credentials.scopes
                }
                get_db_manager().create_or_update_user(user.to_dict(), credentials_data)
                
            elif credentials.expired:
                logger.warning(f"Credentials expired and no refresh token for user: {user_email}")
                return None
            
            return credentials
            
        except Exception as e:
            logger.error(f"Failed to get valid credentials for {user_email}: {str(e)}")
            return None
    
    def revoke_credentials(self, user_email: str) -> bool:
        """
        Revoke stored credentials for a user
        
        Args:
            user_email: Email of the user
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Get user from database
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return False
            
            # Clear credentials in database
            credentials_data = {
                'access_token': None,
                'refresh_token': None,
                'expires_at': None,
                'scopes': []
            }
            get_db_manager().create_or_update_user(user.to_dict(), credentials_data)
            
            logger.info(f"Revoked credentials for user: {user_email}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to revoke credentials for {user_email}: {str(e)}")
            return False
    
    def is_authenticated(self, user_email: str) -> bool:
        """
        Check if user has valid authentication
        
        Args:
            user_email: Email of the user
            
        Returns:
            True if user has valid credentials, False otherwise
        """
        credentials = self.get_valid_credentials(user_email)
        return credentials is not None
    
    def test_gmail_access(self, user_email: str) -> bool:
        """
        Test if Gmail access is working for a user
        
        Args:
            user_email: Email of the user
            
        Returns:
            True if Gmail access is working, False otherwise
        """
        try:
            credentials = self.get_valid_credentials(user_email)
            if not credentials:
                return False
            
            # Build Gmail service and test with a simple call
            service = build('gmail', 'v1', credentials=credentials)
            profile = service.users().getProfile(userId='me').execute()
            
            logger.info(f"Gmail access test successful for {user_email}")
            return True
            
        except Exception as e:
            logger.error(f"Gmail access test failed for {user_email}: {str(e)}")
            return False
    
    def get_user_by_email(self, user_email: str) -> Optional[Dict]:
        """
        Get user information by email
        
        Args:
            user_email: Email of the user
            
        Returns:
            User dictionary or None
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            return user.to_dict() if user else None
        except Exception as e:
            logger.error(f"Failed to get user {user_email}: {str(e)}")
            return None
    
    def _get_user_info(self, credentials: Credentials) -> Dict:
        """
        Get user information from Google OAuth2 API
        
        Args:
            credentials: Valid Google credentials
            
        Returns:
            Dictionary containing user information
        """
        try:
            oauth2_service = build('oauth2', 'v2', credentials=credentials)
            user_info = oauth2_service.userinfo().get().execute()
            return user_info
            
        except Exception as e:
            logger.error(f"Failed to get user info: {str(e)}")
            return {}
    
    def get_authentication_status(self, user_email: str) -> Dict:
        """
        Get detailed authentication status for a user
        
        Args:
            user_email: Email of the user
            
        Returns:
            Dictionary with authentication status details
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {
                    'authenticated': False,
                    'gmail_access': False,
                    'error': 'User not found'
                }
            
            credentials = self.get_valid_credentials(user_email)
            if not credentials:
                return {
                    'authenticated': False,
                    'gmail_access': False,
                    'error': 'No valid credentials'
                }
            
            gmail_access = self.test_gmail_access(user_email)
            
            return {
                'authenticated': True,
                'gmail_access': gmail_access,
                'has_refresh_token': bool(user.refresh_token),
                'token_expired': credentials.expired if credentials else True,
                'scopes': user.scopes or [],
                'user_info': user.to_dict()
            }
            
        except Exception as e:
            logger.error(f"Failed to get authentication status for {user_email}: {str(e)}")
            return {
                'authenticated': False,
                'gmail_access': False,
                'error': str(e)
            }

# Create global instance
gmail_auth = GmailAuthHandler()

====================================================================================================
END OF FILE: chief_of_staff_ai/auth/gmail_auth.py
====================================================================================================


====================================================================================================
FILE 5: chief_of_staff_ai/ingest/gmail_fetcher.py
====================================================================================================
Path: /Users/oudiantebi/Session42 Dropbox/Oudi Antebi/Mac (3)/Documents/MyCode/COS1/chief_of_staff_ai/ingest/gmail_fetcher.py
Info: Size: 31,655 bytes | Modified: 2025-06-08 21:37:51
----------------------------------------------------------------------------------------------------

# Handles fetching emails from Gmail API

import json
import logging
import base64
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from email.utils import parsedate_to_datetime

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from auth.gmail_auth import gmail_auth
from models.database import get_db_manager, Email
from config.settings import settings

logger = logging.getLogger(__name__)

class GmailFetcher:
    """Fetches emails from Gmail API with intelligent batching and caching"""
    
    def __init__(self):
        self.batch_size = 50
        self.max_results = 500
        # Remove file-based caching as we now use database
        
    def fetch_recent_emails(
        self, 
        user_email: str, 
        days_back: int = 7, 
        limit: int = None,
        force_refresh: bool = False
    ) -> Dict:
        """
        Fetch recent emails for a user from their Gmail account - BUSINESS FOCUSED
        Only fetches from important labels: INBOX, IMPORTANT, STARRED, CATEGORY_PRIMARY
        Excludes promotional, social, updates, and forums to focus on real business communications
        
        Args:
            user_email: Gmail address of the user
            days_back: Number of days back to fetch emails
            limit: Maximum number of emails to fetch
            force_refresh: Whether to bypass database cache and fetch fresh data
            
        Returns:
            Dictionary containing fetched emails and metadata
        """
        try:
            # Get user from database
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return self._error_response(f"User {user_email} not found in database")
            
            # Check if we should use cached data
            if not force_refresh:
                cached_emails = get_db_manager().get_user_emails(user.id, limit or 50)
                if cached_emails:
                    logger.info(f"Using cached emails for {user_email}: {len(cached_emails)} emails")
                    return {
                        'success': True,
                        'user_email': user_email,
                        'emails': [email.to_dict() for email in cached_emails],
                        'count': len(cached_emails),
                        'source': 'database_cache',
                        'fetched_at': datetime.utcnow().isoformat()
                    }
            
            # Get valid credentials
            credentials = gmail_auth.get_valid_credentials(user_email)
            if not credentials:
                return self._error_response(f"No valid credentials for {user_email}")
            
            # Build Gmail service
            service = build('gmail', 'v1', credentials=credentials)
            
            # Calculate date range
            since_date = datetime.utcnow() - timedelta(days=days_back)
            
            # ENHANCED: Business-focused Gmail query - only important labels
            # Focus on genuine business communications by including only:
            # - INBOX (regular inbox emails)  
            # - IMPORTANT (user-marked important emails)
            # - STARRED (user-starred emails)
            # - CATEGORY_PRIMARY (primary tab - most important emails)
            # 
            # Explicitly exclude noise:
            # - CATEGORY_PROMOTIONS (promotional emails, newsletters)
            # - CATEGORY_SOCIAL (social notifications) 
            # - CATEGORY_UPDATES (automated updates)
            # - CATEGORY_FORUMS (forum notifications)
            # - SPAM and TRASH
            
            important_labels_query = (
                f"after:{since_date.strftime('%Y/%m/%d')} "
                f"(in:inbox OR label:important OR label:starred OR category:primary) "
                f"-category:promotions -category:social -category:updates -category:forums "
                f"-in:spam -in:trash"
            )
            
            logger.info(f"Fetching BUSINESS-FOCUSED emails for {user_email} with enhanced query: {important_labels_query}")
            
            # Fetch email list with business-focused query
            email_list = self._fetch_email_list(service, important_labels_query, limit)
            if not email_list:
                return {
                    'success': True,
                    'user_email': user_email,
                    'emails': [],
                    'count': 0,
                    'source': 'gmail_api_business_focused',
                    'fetched_at': datetime.utcnow().isoformat(),
                    'message': 'No business emails found in the specified time range with important labels',
                    'query_used': important_labels_query
                }
            
            # Fetch full email content in batches
            emails = self._fetch_emails_batch(service, email_list, user.id)
            
            # Filter stats for logging
            inbox_count = len([e for e in emails if 'INBOX' in e.get('processing_metadata', {}).get('gmail_labels', [])])
            important_count = len([e for e in emails if 'IMPORTANT' in e.get('processing_metadata', {}).get('gmail_labels', [])])
            starred_count = len([e for e in emails if 'STARRED' in e.get('processing_metadata', {}).get('gmail_labels', [])])
            primary_count = len([e for e in emails if 'CATEGORY_PRIMARY' in e.get('processing_metadata', {}).get('gmail_labels', [])])
            
            logger.info(f"Successfully fetched {len(emails)} BUSINESS emails for {user_email} - "
                       f"Inbox: {inbox_count}, Important: {important_count}, Starred: {starred_count}, Primary: {primary_count}")
            
            return {
                'success': True,
                'user_email': user_email,
                'emails': emails,
                'count': len(emails),
                'source': 'gmail_api_business_focused',
                'fetched_at': datetime.utcnow().isoformat(),
                'query_used': important_labels_query,
                'days_back': days_back,
                'filter_stats': {
                    'inbox_emails': inbox_count,
                    'important_emails': important_count, 
                    'starred_emails': starred_count,
                    'primary_category_emails': primary_count,
                    'total_business_emails': len(emails)
                },
                'filtering_approach': 'business_focused_labels_only'
            }
            
        except Exception as e:
            logger.error(f"Failed to fetch business-focused emails for {user_email}: {str(e)}")
            return self._error_response(str(e))
    
    def fetch_sent_emails(
        self, 
        user_email: str, 
        days_back: int = 365, 
        max_emails: int = 1000
    ) -> Dict:
        """
        Fetch sent emails for Smart Contact Strategy analysis
        
        This method fetches emails from the SENT folder to analyze user engagement patterns
        and build the Trusted Contact Database.
        
        Args:
            user_email: Gmail address of the user
            days_back: Number of days back to fetch sent emails
            max_emails: Maximum number of sent emails to fetch
            
        Returns:
            Dictionary containing sent emails and metadata
        """
        try:
            # Get user from database
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return self._error_response(f"User {user_email} not found in database")
            
            # Get valid credentials
            credentials = gmail_auth.get_valid_credentials(user_email)
            if not credentials:
                return self._error_response(f"No valid credentials for {user_email}")
            
            # Build Gmail service
            service = build('gmail', 'v1', credentials=credentials)
            
            # Calculate date range
            since_date = datetime.utcnow() - timedelta(days=days_back)
            
            # Query for sent emails
            sent_query = (
                f"after:{since_date.strftime('%Y/%m/%d')} "
                f"in:sent"
            )
            
            logger.info(f"Fetching sent emails for {user_email} with query: {sent_query}")
            
            # Fetch email list
            email_list = self._fetch_email_list(service, sent_query, max_emails)
            if not email_list:
                return {
                    'success': True,
                    'user_email': user_email,
                    'emails': [],
                    'count': 0,
                    'source': 'gmail_api_sent',
                    'fetched_at': datetime.utcnow().isoformat(),
                    'message': 'No sent emails found in the specified time range',
                    'query_used': sent_query
                }
            
            # Fetch full email content for sent emails (lighter processing)
            emails = self._fetch_sent_emails_batch(service, email_list)
            
            logger.info(f"Successfully fetched {len(emails)} sent emails for {user_email}")
            
            return {
                'success': True,
                'user_email': user_email,
                'emails': emails,
                'count': len(emails),
                'source': 'gmail_api_sent',
                'fetched_at': datetime.utcnow().isoformat(),
                'query_used': sent_query,
                'days_back': days_back,
                'purpose': 'smart_contact_strategy_analysis'
            }
            
        except Exception as e:
            logger.error(f"Failed to fetch sent emails for {user_email}: {str(e)}")
            return self._error_response(str(e))
    
    def _fetch_email_list(self, service, query: str, limit: int = None) -> List[Dict]:
        """
        Fetch list of email IDs matching the query
        
        Args:
            service: Gmail service object
            query: Gmail search query
            limit: Maximum number of emails to fetch
            
        Returns:
            List of email metadata
        """
        try:
            max_results = min(limit or self.max_results, self.max_results)
            
            result = service.users().messages().list(
                userId='me',
                q=query,
                maxResults=max_results
            ).execute()
            
            messages = result.get('messages', [])
            
            # Handle pagination if needed and no limit specified
            while 'nextPageToken' in result and (not limit or len(messages) < limit):
                result = service.users().messages().list(
                    userId='me',
                    q=query,
                    maxResults=max_results,
                    pageToken=result['nextPageToken']
                ).execute()
                
                messages.extend(result.get('messages', []))
                
                if len(messages) >= (limit or self.max_results):
                    break
            
            # Trim to limit if specified
            if limit:
                messages = messages[:limit]
            
            logger.info(f"Found {len(messages)} emails matching query: {query}")
            return messages
            
        except HttpError as e:
            logger.error(f"Gmail API error in fetch_email_list: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error in fetch_email_list: {str(e)}")
            raise
    
    def _fetch_emails_batch(self, service, email_list: List[Dict], user_id: int) -> List[Dict]:
        """
        Fetch full email content in batches for efficiency
        
        Args:
            service: Gmail service object
            email_list: List of email metadata from list API
            user_id: Database user ID
            
        Returns:
            List of processed email dictionaries
        """
        emails = []
        processed_count = 0
        
        try:
            # Process emails in batches
            for i in range(0, len(email_list), self.batch_size):
                batch = email_list[i:i + self.batch_size]
                batch_emails = []
                
                for email_meta in batch:
                    email_id = email_meta['id']
                    
                    try:
                        # Check if email already exists in database
                        with get_db_manager().get_session() as session:
                            existing_email = session.query(Email).filter(
                                Email.user_id == user_id,
                                Email.gmail_id == email_id
                            ).first()
                            
                            if existing_email:
                                batch_emails.append(existing_email.to_dict())
                                continue
                        
                        # Fetch full email from Gmail API
                        full_email = service.users().messages().get(
                            userId='me',
                            id=email_id,
                            format='full'
                        ).execute()
                        
                        # Process the email
                        processed_email = self._process_gmail_message(full_email)
                        
                        # Save to database
                        email_record = get_db_manager().save_email(user_id, processed_email)
                        batch_emails.append(email_record.to_dict())
                        
                        processed_count += 1
                        
                    except Exception as e:
                        logger.error(f"Failed to process email {email_id}: {str(e)}")
                        continue
                
                emails.extend(batch_emails)
                
                # Log progress for large batches
                if len(email_list) > self.batch_size:
                    logger.info(f"Processed batch {i//self.batch_size + 1}/{(len(email_list)-1)//self.batch_size + 1}")
            
            logger.info(f"Successfully processed {processed_count} emails, {len(emails)} total returned")
            return emails
            
        except Exception as e:
            logger.error(f"Failed to fetch emails in batch: {str(e)}")
            raise
    
    def _fetch_sent_emails_batch(self, service, email_list: List[Dict]) -> List[Dict]:
        """
        Fetch sent emails with lighter processing for engagement analysis
        
        Args:
            service: Gmail service object
            email_list: List of email metadata from list API
            
        Returns:
            List of processed sent email dictionaries
        """
        emails = []
        processed_count = 0
        
        try:
            # Process emails in batches
            for i in range(0, len(email_list), self.batch_size):
                batch = email_list[i:i + self.batch_size]
                batch_emails = []
                
                for email_meta in batch:
                    email_id = email_meta['id']
                    
                    try:
                        # Fetch email with minimal format for efficiency
                        full_email = service.users().messages().get(
                            userId='me',
                            id=email_id,
                            format='metadata',
                            metadataHeaders=['From', 'To', 'Cc', 'Bcc', 'Subject', 'Date']
                        ).execute()
                        
                        # Process the sent email (lighter processing)
                        processed_email = self._process_sent_gmail_message(full_email)
                        batch_emails.append(processed_email)
                        
                        processed_count += 1
                        
                    except Exception as e:
                        logger.error(f"Failed to process sent email {email_id}: {str(e)}")
                        continue
                
                emails.extend(batch_emails)
                
                # Log progress for large batches
                if len(email_list) > self.batch_size:
                    logger.info(f"Processed sent email batch {i//self.batch_size + 1}/{(len(email_list)-1)//self.batch_size + 1}")
            
            logger.info(f"Successfully processed {processed_count} sent emails")
            return emails
            
        except Exception as e:
            logger.error(f"Failed to fetch sent emails in batch: {str(e)}")
            raise
    
    def _process_gmail_message(self, gmail_message: Dict) -> Dict:
        """
        Process a Gmail message into our standard format with enhanced business intelligence
        
        Args:
            gmail_message: Raw Gmail message from API
            
        Returns:
            Processed email dictionary with business priority indicators
        """
        try:
            headers = {h['name'].lower(): h['value'] for h in gmail_message['payload'].get('headers', [])}
            label_ids = gmail_message.get('labelIds', [])
            
            # Extract basic email info
            email_data = {
                'id': gmail_message['id'],
                'thread_id': gmail_message.get('threadId'),
                'label_ids': label_ids,
                'snippet': gmail_message.get('snippet', ''),
                'size_estimate': gmail_message.get('sizeEstimate', 0),
                
                # Headers
                'sender': headers.get('from', ''),
                'recipients': [headers.get('to', '')],
                'cc': [headers.get('cc', '')] if headers.get('cc') else [],
                'bcc': [headers.get('bcc', '')] if headers.get('bcc') else [],
                'subject': headers.get('subject', ''),
                'date': headers.get('date', ''),
                
                # Body content
                'body_text': '',
                'body_html': '',
                'attachments': [],
                
                # ENHANCED: Business priority metadata from Gmail labels
                'is_read': 'UNREAD' not in label_ids,
                'is_important': 'IMPORTANT' in label_ids,
                'is_starred': 'STARRED' in label_ids,
                'is_in_inbox': 'INBOX' in label_ids,
                'is_primary_category': 'CATEGORY_PRIMARY' in label_ids,
                'has_attachments': False,
                
                # BUSINESS INTELLIGENCE: Calculate priority score based on Gmail signals
                'business_priority_score': self._calculate_business_priority(label_ids, headers),
                'label_based_category': self._determine_business_category(label_ids),
                
                # Processing metadata with enhanced label tracking
                'processing_metadata': {
                    'fetcher_version': '2.0_business_focused',
                    'processed_at': datetime.utcnow().isoformat(),
                    'gmail_labels': label_ids,
                    'business_filtering': {
                        'is_inbox': 'INBOX' in label_ids,
                        'is_important': 'IMPORTANT' in label_ids,
                        'is_starred': 'STARRED' in label_ids,
                        'is_primary': 'CATEGORY_PRIMARY' in label_ids,
                        'excluded_categories': [label for label in label_ids if 'CATEGORY_' in label and label not in ['CATEGORY_PRIMARY']],
                        'priority_level': self._get_priority_level(label_ids)
                    }
                }
            }
            
            # Parse timestamp
            if email_data['date']:
                try:
                    email_data['timestamp'] = parsedate_to_datetime(email_data['date'])
                except:
                    email_data['timestamp'] = datetime.utcnow()
            else:
                email_data['timestamp'] = datetime.utcnow()
            
            # Extract body content
            self._extract_email_body(gmail_message['payload'], email_data)
            
            # Extract sender name and normalize sender information
            sender_full = email_data['sender']
            if '<' in sender_full and '>' in sender_full:
                email_data['sender_name'] = sender_full.split('<')[0].strip().strip('"')
                email_data['sender'] = sender_full.split('<')[1].split('>')[0]
            else:
                # Handle plain email addresses
                email_data['sender_name'] = sender_full.split('@')[0] if '@' in sender_full else sender_full
            
            return email_data
            
        except Exception as e:
            logger.error(f"Failed to process Gmail message {gmail_message.get('id', 'unknown')}: {str(e)}")
            return {
                'id': gmail_message.get('id', 'unknown'),
                'error': True,
                'error_message': str(e),
                'timestamp': datetime.utcnow(),
                'business_priority_score': 0
            }
    
    def _process_sent_gmail_message(self, gmail_message: Dict) -> Dict:
        """
        Process a sent Gmail message for engagement analysis (lighter processing)
        
        Args:
            gmail_message: Raw Gmail message from API
            
        Returns:
            Processed sent email dictionary with engagement data
        """
        try:
            headers = {h['name'].lower(): h['value'] for h in gmail_message['payload'].get('headers', [])}
            
            # Extract basic email info for engagement analysis
            email_data = {
                'id': gmail_message['id'],
                'thread_id': gmail_message.get('threadId'),
                'snippet': gmail_message.get('snippet', ''),
                
                # Headers for recipient analysis
                'sender': headers.get('from', ''),
                'recipients': self._parse_recipients(headers.get('to', '')),
                'cc': self._parse_recipients(headers.get('cc', '')),
                'bcc': self._parse_recipients(headers.get('bcc', '')),
                'subject': headers.get('subject', ''),
                'date': headers.get('date', ''),
                
                # Processing metadata
                'processing_metadata': {
                    'fetcher_version': '2.0_sent_analysis',
                    'processed_at': datetime.utcnow().isoformat(),
                    'purpose': 'engagement_analysis'
                }
            }
            
            # Parse timestamp
            if email_data['date']:
                try:
                    email_data['timestamp'] = parsedate_to_datetime(email_data['date'])
                except:
                    email_data['timestamp'] = datetime.utcnow()
            else:
                email_data['timestamp'] = datetime.utcnow()
            
            return email_data
            
        except Exception as e:
            logger.error(f"Failed to process sent Gmail message {gmail_message.get('id', 'unknown')}: {str(e)}")
            return {
                'id': gmail_message.get('id', 'unknown'),
                'error': True,
                'error_message': str(e),
                'timestamp': datetime.utcnow()
            }
    
    def _parse_recipients(self, recipients_string: str) -> List[str]:
        """
        Parse recipients string into list of email addresses
        
        Args:
            recipients_string: Comma-separated recipients string
            
        Returns:
            List of email addresses
        """
        if not recipients_string:
            return []
        
        recipients = []
        for recipient in recipients_string.split(','):
            recipient = recipient.strip()
            if '<' in recipient and '>' in recipient:
                # Extract email from "Name <email@domain.com>" format
                email = recipient.split('<')[1].split('>')[0].strip()
            else:
                # Plain email address
                email = recipient.strip()
            
            if email and '@' in email:
                recipients.append(email)
        
        return recipients
    
    def _calculate_business_priority(self, label_ids: List[str], headers: Dict) -> float:
        """
        Calculate business priority score based on Gmail labels and headers
        
        Args:
            label_ids: Gmail label IDs
            headers: Email headers
            
        Returns:
            Priority score (0.0 to 1.0, higher = more important)
        """
        score = 0.0
        
        # Base score for being in our business-focused filter
        score += 0.3
        
        # Label-based scoring
        if 'IMPORTANT' in label_ids:
            score += 0.4  # User explicitly marked as important
        if 'STARRED' in label_ids:
            score += 0.3  # User starred
        if 'CATEGORY_PRIMARY' in label_ids:
            score += 0.2  # Primary tab (Gmail's own importance filter)
        if 'INBOX' in label_ids:
            score += 0.1  # In main inbox
            
        # Header-based indicators
        priority_header = headers.get('x-priority', headers.get('priority', ''))
        if priority_header:
            if '1' in priority_header or 'high' in priority_header.lower():
                score += 0.2
        
        # Sender reputation indicators (simple heuristics)
        sender = headers.get('from', '').lower()
        if any(domain in sender for domain in ['.gov', '.edu', '@yourcompany.com']):
            score += 0.1
            
        return min(1.0, score)  # Cap at 1.0
    
    def _determine_business_category(self, label_ids: List[str]) -> str:
        """
        Determine business category based on Gmail labels
        
        Args:
            label_ids: Gmail label IDs
            
        Returns:
            Business category string
        """
        if 'IMPORTANT' in label_ids:
            return 'high_priority'
        elif 'STARRED' in label_ids:
            return 'starred_business'  
        elif 'CATEGORY_PRIMARY' in label_ids:
            return 'primary_business'
        elif 'INBOX' in label_ids:
            return 'inbox_business'
        else:
            return 'business_communication'
    
    def _get_priority_level(self, label_ids: List[str]) -> str:
        """
        Get human-readable priority level
        
        Args:
            label_ids: Gmail label IDs
            
        Returns:
            Priority level string
        """
        if 'IMPORTANT' in label_ids and 'STARRED' in label_ids:
            return 'critical'
        elif 'IMPORTANT' in label_ids:
            return 'high'
        elif 'STARRED' in label_ids:
            return 'medium-high'
        elif 'CATEGORY_PRIMARY' in label_ids:
            return 'medium'
        else:
            return 'standard'
    
