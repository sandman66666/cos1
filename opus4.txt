trusted_contacts(self, user_id: int, limit: int = 500) -> List[TrustedContact]:
        """Get trusted contacts for a user"""
        with self.get_session() as session:
            return session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id
            ).order_by(TrustedContact.engagement_score.desc()).limit(limit).all()
    
    def find_trusted_contact_by_email(self, user_id: int, email_address: str) -> Optional[TrustedContact]:
        """Find trusted contact by email address"""
        with self.get_session() as session:
            return session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.email_address == email_address
            ).first()
    
    def create_contact_context(self, user_id: int, person_id: int, context_data: Dict) -> ContactContext:
        """Create a new contact context record"""
        with self.get_session() as session:
            context = ContactContext(
                user_id=user_id,
                person_id=person_id,
                **context_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(context)
            session.commit()
            session.refresh(context)
            return context
    
    def get_contact_contexts(self, user_id: int, person_id: int = None, context_type: str = None) -> List[ContactContext]:
        """Get contact contexts for a user, optionally filtered by person or type"""
        with self.get_session() as session:
            query = session.query(ContactContext).filter(ContactContext.user_id == user_id)
            
            if person_id:
                query = query.filter(ContactContext.person_id == person_id)
            
            if context_type:
                query = query.filter(ContactContext.context_type == context_type)
            
            return query.order_by(ContactContext.created_at.desc()).all()
    
    def create_task_context(self, user_id: int, task_id: int, context_data: Dict) -> TaskContext:
        """Create a new task context record"""
        with self.get_session() as session:
            context = TaskContext(
                user_id=user_id,
                task_id=task_id,
                **context_data,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(context)
            session.commit()
            session.refresh(context)
            return context
    
    def get_task_contexts(self, user_id: int, task_id: int = None, context_type: str = None) -> List[TaskContext]:
        """Get task contexts for a user, optionally filtered by task or type"""
        with self.get_session() as session:
            query = session.query(TaskContext).filter(TaskContext.user_id == user_id)
            
            if task_id:
                query = query.filter(TaskContext.task_id == task_id)
            
            if context_type:
                query = query.filter(TaskContext.context_type == context_type)
            
            return query.order_by(TaskContext.created_at.desc()).all()
    
    def create_topic_knowledge(self, user_id: int, topic_id: int, knowledge_data: Dict) -> TopicKnowledgeBase:
        """Create a new topic knowledge record"""
        with self.get_session() as session:
            knowledge = TopicKnowledgeBase(
                user_id=user_id,
                topic_id=topic_id,
                **knowledge_data,
                created_at=datetime.utcnow(),
                last_updated=datetime.utcnow()
            )
            session.add(knowledge)
            session.commit()
            session.refresh(knowledge)
            return knowledge
    
    def get_topic_knowledge(self, user_id: int, topic_id: int = None, knowledge_type: str = None) -> List[TopicKnowledgeBase]:
        """Get topic knowledge for a user, optionally filtered by topic or type"""
        with self.get_session() as session:
            query = session.query(TopicKnowledgeBase).filter(TopicKnowledgeBase.user_id == user_id)
            
            if topic_id:
                query = query.filter(TopicKnowledgeBase.topic_id == topic_id)
            
            if knowledge_type:
                query = query.filter(TopicKnowledgeBase.knowledge_type == knowledge_type)
            
            return query.order_by(TopicKnowledgeBase.relevance_score.desc()).all()
    
    def update_people_engagement_data(self, user_id: int, person_id: int, engagement_data: Dict) -> bool:
        """Update people table with engagement-based data"""
        with self.get_session() as session:
            person = session.query(Person).filter(
                Person.user_id == user_id,
                Person.id == person_id
            ).first()
            
            if not person:
                return False
            
            # Add engagement fields to person if they don't exist
            if 'is_trusted_contact' in engagement_data:
                person.is_trusted_contact = engagement_data['is_trusted_contact']
            
            if 'engagement_score' in engagement_data:
                person.engagement_score = engagement_data['engagement_score']
            
            if 'bidirectional_topics' in engagement_data:
                person.bidirectional_topics = engagement_data['bidirectional_topics']
            
            session.commit()
            return True
    
    def get_engagement_analytics(self, user_id: int) -> Dict:
        """Get engagement analytics for Smart Contact Strategy reporting"""
        with self.get_session() as session:
            total_contacts = session.query(TrustedContact).filter(TrustedContact.user_id == user_id).count()
            high_engagement = session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.relationship_strength == 'high'
            ).count()
            
            recent_contacts = session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.last_sent_date >= datetime.utcnow() - timedelta(days=30)
            ).count()
            
            return {
                'total_trusted_contacts': total_contacts,
                'high_engagement_contacts': high_engagement,
                'recent_active_contacts': recent_contacts,
                'engagement_rate': (high_engagement / total_contacts * 100) if total_contacts > 0 else 0
            }

    def save_calendar_event(self, user_id: int, event_data: Dict) -> Calendar:
        """Save or update a calendar event"""
        try:
            with self.get_session() as session:
                # Try to find existing event
                existing_event = session.query(Calendar).filter_by(
                    user_id=user_id,
                    event_id=event_data.get('event_id')
                ).first()
                
                if existing_event:
                    # Update existing event
                    for key, value in event_data.items():
                        if hasattr(existing_event, key):
                            setattr(existing_event, key, value)
                    event = existing_event
                else:
                    # Create new event
                    event = Calendar(user_id=user_id, **event_data)
                    session.add(event)
                
                session.commit()
                session.refresh(event)
                return event
                
        except Exception as e:
            logger.error(f"Failed to save calendar event: {str(e)}")
            raise

    def get_user_calendar_events(self, user_id: int, start_date: datetime = None, end_date: datetime = None, limit: int = 500) -> List[Calendar]:
        """Get calendar events for a user within a date range"""
        try:
            with self.get_session() as session:
                query = session.query(Calendar).filter_by(user_id=user_id)
                
                if start_date:
                    query = query.filter(Calendar.start_time >= start_date)
                if end_date:
                    query = query.filter(Calendar.start_time <= end_date)
                
                events = query.order_by(Calendar.start_time.asc()).limit(limit).all()
                return events
                
        except Exception as e:
            logger.error(f"Failed to get user calendar events: {str(e)}")
            return []

    def get_free_time_slots(self, user_id: int, start_date: datetime, end_date: datetime) -> List[Dict]:
        """Identify free time slots between calendar events"""
        try:
            with self.get_session() as session:
                events = session.query(Calendar).filter(
                    Calendar.user_id == user_id,
                    Calendar.start_time >= start_date,
                    Calendar.start_time <= end_date,
                    Calendar.status.in_(['confirmed', 'tentative']),
                    Calendar.is_busy == True
                ).order_by(Calendar.start_time).all()
                
                free_slots = []
                current_time = start_date
                
                for event in events:
                    # If there's a gap before this event, it's free time
                    if event.start_time > current_time:
                        gap_duration = int((event.start_time - current_time).total_seconds() / 60)
                        if gap_duration >= 30:  # Minimum 30 minutes to be useful
                            free_slots.append({
                                'start_time': current_time,
                                'end_time': event.start_time,
                                'duration_minutes': gap_duration,
                                'type': 'free_time'
                            })
                    
                    # Update current time to end of this event
                    if event.end_time and event.end_time > current_time:
                        current_time = event.end_time
                
                # Check for free time after last event
                if current_time < end_date:
                    gap_duration = int((end_date - current_time).total_seconds() / 60)
                    if gap_duration >= 30:
                        free_slots.append({
                            'start_time': current_time,
                            'end_time': end_date,
                            'duration_minutes': gap_duration,
                            'type': 'free_time'
                        })
                
                return free_slots
                
        except Exception as e:
            logger.error(f"Failed to get free time slots: {str(e)}")
            return []

    def get_calendar_attendee_intelligence(self, user_id: int, event_id: str) -> Dict:
        """Get intelligence about calendar event attendees"""
        try:
            with self.get_session() as session:
                event = session.query(Calendar).filter_by(
                    user_id=user_id,
                    event_id=event_id
                ).first()
                
                if not event or not event.attendee_emails:
                    return {}
                
                # Find known attendees in People database
                known_people = []
                unknown_attendees = []
                
                for attendee_email in event.attendee_emails:
                    person = self.find_person_by_email(user_id, attendee_email)
                    if person:
                        known_people.append(person.to_dict())
                    else:
                        unknown_attendees.append(attendee_email)
                
                return {
                    'event_id': event_id,
                    'total_attendees': len(event.attendee_emails),
                    'known_attendees': known_people,
                    'unknown_attendees': unknown_attendees,
                    'known_percentage': len(known_people) / len(event.attendee_emails) * 100 if event.attendee_emails else 0
                }
                
        except Exception as e:
            logger.error(f"Failed to get calendar attendee intelligence: {str(e)}")
            return {}

    def update_calendar_ai_analysis(self, user_id: int, event_id: str, ai_data: Dict) -> bool:
        """Update calendar event with AI analysis"""
        try:
            with self.get_session() as session:
                event = session.query(Calendar).filter_by(
                    user_id=user_id,
                    event_id=event_id
                ).first()
                
                if not event:
                    return False
                
                # Update AI analysis fields
                if 'ai_summary' in ai_data:
                    event.ai_summary = ai_data['ai_summary']
                if 'ai_category' in ai_data:
                    event.ai_category = ai_data['ai_category']
                if 'importance_score' in ai_data:
                    event.importance_score = ai_data['importance_score']
                if 'business_context' in ai_data:
                    event.business_context = ai_data['business_context']
                if 'preparation_needed' in ai_data:
                    event.preparation_needed = ai_data['preparation_needed']
                if 'follow_up_required' in ai_data:
                    event.follow_up_required = ai_data['follow_up_required']
                
                event.ai_processed_at = datetime.utcnow()
                event.ai_version = ai_data.get('ai_version', 'claude-3.5-sonnet')
                
                session.commit()
                return True
                
        except Exception as e:
            logger.error(f"Failed to update calendar AI analysis: {str(e)}")
            return False

# Global database manager instance - Initialize lazily
_db_manager = None

def get_db_manager():
    """Get the global database manager instance (lazy initialization)"""
    global _db_manager
    if _db_manager is None:
        _db_manager = DatabaseManager()
    return _db_manager

# Export as db_manager for compatibility, but don't instantiate during import
db_manager = None  # Will be set by get_db_manager() when first called 
FILE: archive/backup_files/v1_original/models/__init__.py - Package initialization file

============================================================
FILE: archive/backup_files/v1_original/processors/email_normalizer.py
============================================================
# Normalizes raw Gmail data into clean format

import re
import logging
from datetime import datetime
from typing import Dict, List, Optional
from html import unescape
from bs4 import BeautifulSoup

from models.database import get_db_manager, Email

logger = logging.getLogger(__name__)

class EmailNormalizer:
    """Normalizes emails into clean, standardized format with entity extraction"""
    
    def __init__(self):
        self.version = "1.0"
        
    def normalize_user_emails(self, user_email: str, limit: int = None) -> Dict:
        """
        Normalize all emails for a user that haven't been normalized yet
        
        Args:
            user_email: Email of the user
            limit: Maximum number of emails to process
            
        Returns:
            Dictionary with normalization results
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            # Get emails that need normalization
            with get_db_manager().get_session() as session:
                emails = session.query(Email).filter(
                    Email.user_id == user.id,
                    Email.body_clean.is_(None)  # Not normalized yet
                ).limit(limit or 100).all()
            
            if not emails:
                logger.info(f"No emails to normalize for {user_email}")
                return {
                    'success': True,
                    'user_email': user_email,
                    'processed': 0,
                    'message': 'No emails need normalization'
                }
            
            processed_count = 0
            error_count = 0
            
            for email in emails:
                try:
                    # Convert database email to dict for processing
                    email_dict = {
                        'id': email.gmail_id,
                        'subject': email.subject,
                        'body_text': email.body_text,
                        'body_html': email.body_html,
                        'sender': email.sender,
                        'sender_name': email.sender_name,
                        'snippet': email.snippet,
                        'timestamp': email.email_date
                    }
                    
                    # Normalize the email
                    normalized = self.normalize_email(email_dict)
                    
                    # Update the database record
                    with get_db_manager().get_session() as session:
                        email_record = session.query(Email).filter(
                            Email.user_id == user.id,
                            Email.gmail_id == email.gmail_id
                        ).first()
                        
                        if email_record:
                            email_record.body_clean = normalized.get('body_clean')
                            email_record.body_preview = normalized.get('body_preview')
                            email_record.entities = normalized.get('entities', {})
                            email_record.message_type = normalized.get('message_type')
                            email_record.priority_score = normalized.get('priority_score')
                            email_record.normalizer_version = self.version
                            
                            session.commit()
                            processed_count += 1
                    
                except Exception as e:
                    logger.error(f"Failed to normalize email {email.gmail_id}: {str(e)}")
                    error_count += 1
                    continue
            
            logger.info(f"Normalized {processed_count} emails for {user_email} ({error_count} errors)")
            
            return {
                'success': True,
                'user_email': user_email,
                'processed': processed_count,
                'errors': error_count,
                'normalizer_version': self.version
            }
            
        except Exception as e:
            logger.error(f"Failed to normalize emails for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def normalize_email(self, email_data: Dict) -> Dict:
        """
        Normalize a single email into clean format
        
        Args:
            email_data: Raw email data dictionary
            
        Returns:
            Normalized email data
        """
        try:
            # Start with original data
            normalized = email_data.copy()
            
            # Clean and extract body content
            body_clean = self._extract_clean_body(email_data)
            normalized['body_clean'] = body_clean
            
            # Create preview (first 300 chars)
            normalized['body_preview'] = self._create_preview(body_clean)
            
            # Extract entities
            normalized['entities'] = self._extract_entities(email_data, body_clean)
            
            # Determine message type
            normalized['message_type'] = self._classify_message_type(email_data, body_clean)
            
            # Calculate priority score
            normalized['priority_score'] = self._calculate_priority_score(email_data, body_clean)
            
            # Add processing metadata
            normalized['processing_metadata'] = {
                'normalizer_version': self.version,
                'normalized_at': datetime.utcnow().isoformat(),
                'body_length': len(body_clean) if body_clean else 0
            }
            
            return normalized
            
        except Exception as e:
            logger.error(f"Failed to normalize email {email_data.get('id', 'unknown')}: {str(e)}")
            return {
                **email_data,
                'normalization_error': str(e),
                'processing_metadata': {
                    'normalizer_version': self.version,
                    'normalized_at': datetime.utcnow().isoformat(),
                    'error': True
                }
            }
    
    def _extract_clean_body(self, email_data: Dict) -> str:
        """
        Extract clean text from email body
        
        Args:
            email_data: Email data dictionary
            
        Returns:
            Clean body text
        """
        try:
            body_text = email_data.get('body_text', '')
            body_html = email_data.get('body_html', '')
            
            # Prefer HTML if available, fallback to text
            if body_html:
                # Parse HTML and extract text
                soup = BeautifulSoup(body_html, 'html.parser')
                
                # Remove script and style elements
                for script in soup(['script', 'style']):
                    script.decompose()
                
                # Get text and clean it
                text = soup.get_text()
                
                # Break into lines and remove leading/trailing spaces
                lines = (line.strip() for line in text.splitlines())
                
                # Break multi-headlines into a line each
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                
                # Drop blank lines
                clean_text = '\n'.join(chunk for chunk in chunks if chunk)
                
            elif body_text:
                clean_text = body_text
                
            else:
                # Fallback to snippet
                clean_text = email_data.get('snippet', '')
            
            if not clean_text:
                return ''
                
            # Remove quoted text (replies/forwards)
            clean_text = self._remove_quoted_text(clean_text)
            
            # Remove excessive whitespace
            clean_text = re.sub(r'\n\s*\n', '\n\n', clean_text)
            clean_text = re.sub(r' +', ' ', clean_text)
            
            # Decode HTML entities
            clean_text = unescape(clean_text)
            
            return clean_text.strip()
            
        except Exception as e:
            logger.error(f"Failed to extract clean body: {str(e)}")
            return email_data.get('snippet', '')
    
    def _remove_quoted_text(self, text: str) -> str:
        """
        Remove quoted text from emails (replies/forwards)
        
        Args:
            text: Email body text
            
        Returns:
            Text with quoted sections removed
        """
        try:
            # Common quote patterns
            quote_patterns = [
                r'On .* wrote:.*',
                r'From:.*\nSent:.*\nTo:.*\nSubject:.*',
                r'-----Original Message-----.*',
                r'> .*',  # Lines starting with >
                r'________________________________.*',  # Outlook separator
                r'From: .*<.*>.*',
                r'Sent from my .*',
                r'\n\n.*On.*\d{4}.*at.*\d{1,2}:\d{2}.*wrote:'
            ]
            
            cleaned_text = text
            
            for pattern in quote_patterns:
                cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.DOTALL | re.IGNORECASE)
            
            # Remove excessive newlines created by quote removal
            cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text)
            
            return cleaned_text.strip()
            
        except Exception as e:
            logger.error(f"Failed to remove quoted text: {str(e)}")
            return text
    
    def _create_preview(self, body_text: str) -> str:
        """
        Create a preview of the email body
        
        Args:
            body_text: Clean email body text
            
        Returns:
            Preview text (first 300 characters)
        """
        if not body_text:
            return ''
        
        # Take first 300 characters
        preview = body_text[:300]
        
        # If we cut in the middle of a word, cut to last complete word
        if len(body_text) > 300:
            last_space = preview.rfind(' ')
            if last_space > 250:  # Only if we have a reasonable amount of text
                preview = preview[:last_space] + '...'
            else:
                preview += '...'
        
        return preview
    
    def _extract_entities(self, email_data: Dict, body_text: str) -> Dict:
        """
        Extract entities from email content
        
        Args:
            email_data: Email data dictionary
            body_text: Clean email body text
            
        Returns:
            Dictionary of extracted entities
        """
        try:
            entities = {
                'people': [],
                'companies': [],
                'dates': [],
                'times': [],
                'urls': [],
                'emails': [],
                'phone_numbers': [],
                'amounts': []
            }
            
            # Extract email addresses
            email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
            entities['emails'] = list(set(re.findall(email_pattern, body_text)))
            
            # Extract URLs
            url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
            entities['urls'] = list(set(re.findall(url_pattern, body_text)))
            
            # Extract phone numbers (US format)
            phone_pattern = r'\b(?:\+?1[-.\s]?)?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})\b'
            phone_matches = re.findall(phone_pattern, body_text)
            entities['phone_numbers'] = ['-'.join(match) for match in phone_matches]
            
            # Extract dates (simple patterns)
            date_patterns = [
                r'\b\d{1,2}/\d{1,2}/\d{4}\b',
                r'\b\d{1,2}-\d{1,2}-\d{4}\b',
                r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2},? \d{4}\b'
            ]
            for pattern in date_patterns:
                entities['dates'].extend(re.findall(pattern, body_text, re.IGNORECASE))
            
            # Extract times
            time_pattern = r'\b\d{1,2}:\d{2}(?:\s?[AP]M)?\b'
            entities['times'] = list(set(re.findall(time_pattern, body_text, re.IGNORECASE)))
            
            # Extract monetary amounts
            amount_pattern = r'\$\d{1,3}(?:,\d{3})*(?:\.\d{2})?'
            entities['amounts'] = list(set(re.findall(amount_pattern, body_text)))
            
            # Remove empty lists and duplicates
            for key in entities:
                entities[key] = list(set(entities[key])) if entities[key] else []
            
            return entities
            
        except Exception as e:
            logger.error(f"Failed to extract entities: {str(e)}")
            return {}
    
    def _classify_message_type(self, email_data: Dict, body_text: str) -> str:
        """
        Classify the type of email message
        
        Args:
            email_data: Email data dictionary
            body_text: Clean email body text
            
        Returns:
            Message type classification
        """
        try:
            subject = email_data.get('subject', '').lower()
            body_lower = body_text.lower() if body_text else ''
            sender = email_data.get('sender', '').lower()
            
            # Meeting/Calendar invites
            meeting_keywords = ['meeting', 'call', 'zoom', 'teams', 'webex', 'conference', 'invite', 'calendar']
            if any(keyword in subject for keyword in meeting_keywords):
                return 'meeting'
            
            # Automated/System emails
            system_domains = ['noreply', 'no-reply', 'donotreply', 'mailer-daemon', 'bounce']
            if any(domain in sender for domain in system_domains):
                return 'automated'
            
            # Newsletters/Marketing
            newsletter_keywords = ['unsubscribe', 'newsletter', 'marketing', 'promotional']
            if any(keyword in body_lower for keyword in newsletter_keywords):
                return 'newsletter'
            
            # Action required
            action_keywords = ['urgent', 'asap', 'deadline', 'required', 'please review', 'action needed']
            if any(keyword in subject for keyword in action_keywords):
                return 'action_required'
            
            # FYI/Information
            fyi_keywords = ['fyi', 'for your information', 'heads up', 'update', 'status']
            if any(keyword in subject for keyword in fyi_keywords):
                return 'informational'
            
            # Default to regular
            return 'regular'
            
        except Exception as e:
            logger.error(f"Failed to classify message type: {str(e)}")
            return 'regular'
    
    def _calculate_priority_score(self, email_data: Dict, body_text: str) -> float:
        """
        Calculate priority score for email (0.0 to 1.0)
        
        Args:
            email_data: Email data dictionary
            body_text: Clean email body text
            
        Returns:
            Priority score between 0.0 and 1.0
        """
        try:
            score = 0.5  # Base score
            
            subject = email_data.get('subject', '').lower()
            body_lower = body_text.lower() if body_text else ''
            
            # High priority keywords
            urgent_keywords = ['urgent', 'asap', 'emergency', 'critical', 'deadline']
            for keyword in urgent_keywords:
                if keyword in subject or keyword in body_lower:
                    score += 0.2
            
            # Medium priority keywords
            important_keywords = ['important', 'priority', 'please review', 'action needed']
            for keyword in important_keywords:
                if keyword in subject or keyword in body_lower:
                    score += 0.1
            
            # Questions increase priority slightly
            if '?' in subject or '?' in body_text:
                score += 0.05
            
            # Direct communication (personal emails)
            if '@' in email_data.get('sender', '') and 'noreply' not in email_data.get('sender', ''):
                score += 0.1
            
            # Reduce score for automated emails
            automated_keywords = ['unsubscribe', 'automated', 'noreply', 'notification']
            for keyword in automated_keywords:
                if keyword in email_data.get('sender', '').lower():
                    score -= 0.2
            
            # Ensure score is between 0.0 and 1.0
            return max(0.0, min(1.0, score))
            
        except Exception as e:
            logger.error(f"Failed to calculate priority score: {str(e)}")
            return 0.5

# Create global instance
email_normalizer = EmailNormalizer()

============================================================
FILE: archive/backup_files/v1_original/processors/task_extractor.py
============================================================
# Extract actionable tasks from emails using Claude 4 Sonnet

import json
import logging
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional
import re
from dateutil import parser
import anthropic

from config.settings import settings
from models.database import get_db_manager, Email, Task

logger = logging.getLogger(__name__)

class TaskExtractor:
    """Extracts actionable tasks from emails using Claude 4 Sonnet"""
    
    def __init__(self):
        self.client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        self.model = "claude-3-5-sonnet-20241022"
        self.version = "1.0"
        
    def extract_tasks_for_user(self, user_email: str, limit: int = None, force_refresh: bool = False) -> Dict:
        """
        ENHANCED 360-CONTEXT TASK EXTRACTION
        
        Extract tasks with comprehensive business intelligence by cross-referencing:
        - Email communications & AI analysis
        - People relationships & interaction patterns
        - Project context & status
        - Calendar events & meeting intelligence
        - Topic analysis & business themes
        - Strategic decisions & opportunities
        
        Creates super relevant and actionable tasks with full business context
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            # COMPREHENSIVE BUSINESS CONTEXT COLLECTION
            business_context = self._get_360_business_context(user.id)
            
            # Get normalized emails that need task extraction
            with get_db_manager().get_session() as session:
                query = session.query(Email).filter(
                    Email.user_id == user.id,
                    Email.body_clean.isnot(None)  # Already normalized
                )
                
                if not force_refresh:
                    # Only process emails that don't have tasks yet
                    query = query.filter(~session.query(Task).filter(
                        Task.email_id == Email.id
                    ).exists())
                
                emails = query.limit(limit or 50).all()
            
            if not emails:
                logger.info(f"No emails to process for 360-context task extraction for {user_email}")
                return {
                    'success': True,
                    'user_email': user_email,
                    'processed_emails': 0,
                    'extracted_tasks': 0,
                    'message': 'No emails need 360-context task extraction'
                }
            
            processed_emails = 0
            total_tasks = 0
            error_count = 0
            context_enhanced_tasks = 0
            
            for email in emails:
                try:
                    # Convert database email to dict for processing
                    email_dict = {
                        'id': email.gmail_id,
                        'subject': email.subject,
                        'sender': email.sender,
                        'sender_name': email.sender_name,
                        'body_clean': email.body_clean,
                        'body_preview': email.body_preview,
                        'timestamp': email.email_date,
                        'message_type': email.message_type,
                        'priority_score': email.priority_score,
                        'ai_summary': email.ai_summary,
                        'key_insights': email.key_insights,
                        'topics': email.topics
                    }
                    
                    # ENHANCED EXTRACTION with 360-context
                    extraction_result = self.extract_tasks_with_360_context(email_dict, business_context)
                    
                    if extraction_result['success'] and extraction_result['tasks']:
                        # Save tasks to database with enhanced context
                        for task_data in extraction_result['tasks']:
                            task_data['email_id'] = email.id
                            task_data['extractor_version'] = f"{self.version}_360_context"
                            task_data['model_used'] = self.model
                            
                            # Check if this task was context-enhanced
                            if task_data.get('context_enhanced'):
                                context_enhanced_tasks += 1
                            
                            get_db_manager().save_task(user.id, email.id, task_data)
                            total_tasks += 1
                    
                    processed_emails += 1
                    
                except Exception as e:
                    logger.error(f"Failed to extract 360-context tasks from email {email.gmail_id}: {str(e)}")
                    error_count += 1
                    continue
            
            logger.info(f"Extracted {total_tasks} tasks ({context_enhanced_tasks} context-enhanced) from {processed_emails} emails for {user_email} ({error_count} errors)")
            
            return {
                'success': True,
                'user_email': user_email,
                'processed_emails': processed_emails,
                'extracted_tasks': total_tasks,
                'context_enhanced_tasks': context_enhanced_tasks,
                'errors': error_count,
                'extractor_version': f"{self.version}_360_context"
            }
            
        except Exception as e:
            logger.error(f"Failed to extract 360-context tasks for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _get_360_business_context(self, user_id: int) -> Dict:
        """
        Collect comprehensive business intelligence context for task extraction
        """
        try:
            context = {
                'people': [],
                'projects': [],
                'topics': [],
                'calendar_events': [],
                'recent_decisions': [],
                'opportunities': [],
                'relationship_map': {},
                'project_map': {},
                'topic_keywords': {}
            }
            
            # Get business data
            people = get_db_manager().get_user_people(user_id, limit=100)
            projects = get_db_manager().get_user_projects(user_id, limit=50)
            topics = get_db_manager().get_user_topics(user_id, limit=50)
            calendar_events = get_db_manager().get_user_calendar_events(user_id, limit=50)
            emails = get_db_manager().get_user_emails(user_id, limit=100)
            
            # Process people for relationship context
            for person in people:
                if person.name and person.email_address:
                    person_info = {
                        'name': person.name,
                        'email': person.email_address,
                        'company': person.company,
                        'title': person.title,
                        'relationship': person.relationship_type,
                        'total_emails': person.total_emails or 0,
                        'importance': person.importance_level or 0.5
                    }
                    context['people'].append(person_info)
                    context['relationship_map'][person.email_address.lower()] = person_info
            
            # Process projects for context linking
            for project in projects:
                if project.name and project.status == 'active':
                    project_info = {
                        'name': project.name,
                        'description': project.description,
                        'status': project.status,
                        'priority': project.priority,
                        'stakeholders': project.stakeholders or []
                    }
                    context['projects'].append(project_info)
                    context['project_map'][project.name.lower()] = project_info
            
            # Process topics for keyword matching
            for topic in topics:
                if topic.name:
                    topic_info = {
                        'name': topic.name,
                        'description': topic.description,
                        'keywords': json.loads(topic.keywords) if topic.keywords else [],
                        'is_official': topic.is_official
                    }
                    context['topics'].append(topic_info)
                    # Build keyword map for topic detection
                    all_keywords = [topic.name.lower()] + [kw.lower() for kw in topic_info['keywords']]
                    for keyword in all_keywords:
                        if keyword not in context['topic_keywords']:
                            context['topic_keywords'][keyword] = []
                        context['topic_keywords'][keyword].append(topic_info)
            
            # Process calendar events for meeting context
            now = datetime.now(timezone.utc)
            upcoming_meetings = [e for e in calendar_events if e.start_time and e.start_time > now]
            for meeting in upcoming_meetings[:20]:  # Next 20 meetings
                meeting_info = {
                    'title': meeting.title,
                    'start_time': meeting.start_time,
                    'attendees': meeting.attendees or [],
                    'description': meeting.description
                }
                context['calendar_events'].append(meeting_info)
            
            # Extract recent decisions and opportunities from emails
            for email in emails[-30:]:  # Recent 30 emails
                if email.key_insights and isinstance(email.key_insights, dict):
                    decisions = email.key_insights.get('key_decisions', [])
                    context['recent_decisions'].extend(decisions[:2])  # Top 2 per email
                    
                    opportunities = email.key_insights.get('strategic_opportunities', [])
                    context['opportunities'].extend(opportunities[:2])  # Top 2 per email
            
            return context
            
        except Exception as e:
            logger.error(f"Failed to get 360-context for task extraction: {str(e)}")
            return {}
    
    def extract_tasks_with_360_context(self, email_data: Dict, business_context: Dict) -> Dict:
        """
        Extract actionable tasks with comprehensive 360-context intelligence
        
        Args:
            email_data: Normalized email data dictionary with AI analysis
            business_context: Comprehensive business intelligence context
            
        Returns:
            Dictionary containing extracted tasks with enhanced context
        """
        try:
            # Check if email has enough content for task extraction
            body_clean = email_data.get('body_clean', '')
            if not body_clean or len(body_clean.strip()) < 20:
                return {
                    'success': True,
                    'email_id': email_data.get('id'),
                    'tasks': [],
                    'reason': 'Email content too short for task extraction'
                }
            
            # Skip certain message types that unlikely contain tasks
            message_type = email_data.get('message_type', 'regular')
            if message_type in ['newsletter', 'automated']:
                return {
                    'success': True,
                    'email_id': email_data.get('id'),
                    'tasks': [],
                    'reason': f'Message type "{message_type}" skipped for task extraction'
                }
            
            # ANALYZE BUSINESS CONTEXT CONNECTIONS
            email_context = self._analyze_email_business_connections(email_data, business_context)
            
            # Prepare enhanced email context for Claude
            enhanced_email_context = self._prepare_360_email_context(email_data, email_context, business_context)
            
            # Call Claude for 360-context task extraction
            claude_response = self._call_claude_for_360_tasks(enhanced_email_context, email_context)
            
            if not claude_response:
                return {
                    'success': False,
                    'email_id': email_data.get('id'),
                    'error': 'Failed to get response from Claude for 360-context extraction'
                }
            
            # Parse Claude's response with context enhancement
            tasks = self._parse_claude_360_response(claude_response, email_data, email_context)
            
            # Enhance tasks with 360-context metadata
            enhanced_tasks = []
            for task in tasks:
                enhanced_task = self._enhance_task_with_360_context(task, email_data, email_context, business_context)
                enhanced_tasks.append(enhanced_task)
            
            return {
                'success': True,
                'email_id': email_data.get('id'),
                'tasks': enhanced_tasks,
                'extraction_metadata': {
                    'extracted_at': datetime.utcnow().isoformat(),
                    'extractor_version': f"{self.version}_360_context",
                    'model_used': self.model,
                    'email_priority': email_data.get('priority_score', 0.5),
                    'context_connections': email_context.get('connection_count', 0),
                    'business_intelligence_used': True
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to extract 360-context tasks from email {email_data.get('id', 'unknown')}: {str(e)}")
            return {
                'success': False,
                'email_id': email_data.get('id'),
                'error': str(e)
            }
    
    def _prepare_email_context(self, email_data: Dict) -> str:
        """
        Prepare email context for Claude task extraction
        
        Args:
            email_data: Email data dictionary
            
        Returns:
            Formatted email context string
        """
        sender = email_data.get('sender_name') or email_data.get('sender', '')
        subject = email_data.get('subject', '')
        body = email_data.get('body_clean', '')
        timestamp = email_data.get('timestamp')
        
        # Format timestamp
        if timestamp:
            try:
                if isinstance(timestamp, str):
                    timestamp = parser.parse(timestamp)
                date_str = timestamp.strftime('%Y-%m-%d %H:%M')
            except:
                date_str = 'Unknown date'
        else:
            date_str = 'Unknown date'
        
        context = f"""Email Details:
From: {sender}
Date: {date_str}
Subject: {subject}

Email Content:
{body}
"""
        return context
    
    def _call_claude_for_tasks(self, email_context: str) -> Optional[str]:
        """
        Call Claude 4 Sonnet to extract tasks from email
        
        Args:
            email_context: Formatted email context
            
        Returns:
            Claude's response or None if failed
        """
        try:
            system_prompt = """You are an expert AI assistant that extracts actionable tasks from emails. Your job is to identify specific tasks, action items, deadlines, and follow-ups from email content.

Please analyze the email and extract actionable tasks following these guidelines:

1. **Task Identification**: Look for:
   - Direct requests or assignments
   - Deadlines and due dates
   - Follow-up actions needed
   - Meetings to schedule or attend
   - Documents to review or create
   - Decisions to make
   - Items requiring response

2. **Task Details**: For each task, identify:
   - Clear description of what needs to be done
   - Who is responsible (assignee)
   - When it needs to be done (due date/deadline)
   - Priority level (high, medium, low)
   - Category (follow-up, deadline, meeting, review, etc.)

3. **Response Format**: Return a JSON array of tasks. Each task should have:
   - "description": Clear, actionable description
   - "assignee": Who should do this (if mentioned)
   - "due_date": Specific date if mentioned (YYYY-MM-DD format)
   - "due_date_text": Original due date text from email
   - "priority": high/medium/low based on urgency and importance
   - "category": type of task (follow-up, deadline, meeting, review, etc.)
   - "confidence": 0.0-1.0 confidence score
   - "source_text": Original text from email that led to this task

Return ONLY the JSON array. If no actionable tasks are found, return an empty array []."""

            user_prompt = f"""Please analyze this email and extract actionable tasks:

{email_context}

Remember to return only a JSON array of tasks, or an empty array [] if no actionable tasks are found."""

            message = self.client.messages.create(
                model=self.model,
                max_tokens=2000,
                temperature=0.1,
                system=system_prompt,
                messages=[
                    {
                        "role": "user",
                        "content": user_prompt
                    }
                ]
            )
            
            response_text = message.content[0].text.strip()
            logger.debug(f"Claude response: {response_text}")
            
            return response_text
            
        except Exception as e:
            logger.error(f"Failed to call Claude for task extraction: {str(e)}")
            return None
    
    def _parse_claude_response(self, response: str, email_data: Dict) -> List[Dict]:
        """
        Parse Claude's JSON response into task dictionaries
        
        Args:
            response: Claude's response text
            email_data: Original email data
            
        Returns:
            List of task dictionaries
        """
        try:
            # Try to find JSON in the response
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            
            if json_start == -1 or json_end == 0:
                logger.warning("No JSON array found in Claude response")
                return []
            
            json_text = response[json_start:json_end]
            
            # Parse JSON
            tasks_data = json.loads(json_text)
            
            if not isinstance(tasks_data, list):
                logger.warning("Claude response is not a JSON array")
                return []
            
            tasks = []
            for task_data in tasks_data:
                if isinstance(task_data, dict) and task_data.get('description'):
                    tasks.append(task_data)
            
            logger.info(f"Parsed {len(tasks)} tasks from Claude response")
            return tasks
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Claude JSON response: {str(e)}")
            logger.error(f"Response was: {response}")
            return []
        except Exception as e:
            logger.error(f"Failed to parse Claude response: {str(e)}")
            return []
    
    def _enhance_task(self, task: Dict, email_data: Dict) -> Dict:
        """
        Enhance task with additional metadata and processing
        
        Args:
            task: Task dictionary from Claude
            email_data: Original email data
            
        Returns:
            Enhanced task dictionary
        """
        try:
            # Start with Claude's task data
            enhanced_task = task.copy()
            
            # Parse due date if provided
            if task.get('due_date'):
                try:
                    # Try to parse various date formats
                    due_date = parser.parse(task['due_date'])
                    enhanced_task['due_date'] = due_date
                except:
                    # If parsing fails, try to extract from due_date_text
                    enhanced_task['due_date'] = self._extract_date_from_text(
                        task.get('due_date_text', '')
                    )
            elif task.get('due_date_text'):
                enhanced_task['due_date'] = self._extract_date_from_text(task['due_date_text'])
            
            # Set default values
            enhanced_task['priority'] = task.get('priority', 'medium').lower()
            enhanced_task['category'] = task.get('category', 'action_item')
            enhanced_task['confidence'] = min(1.0, max(0.0, task.get('confidence', 0.8)))
            enhanced_task['status'] = 'pending'
            
            # Determine assignee context
            if not enhanced_task.get('assignee'):
                # If no specific assignee mentioned, assume it's for the email recipient
                enhanced_task['assignee'] = 'me'
            
            # Enhance priority based on email priority and urgency
            email_priority = email_data.get('priority_score', 0.5)
            if email_priority > 0.8:
                if enhanced_task['priority'] == 'medium':
                    enhanced_task['priority'] = 'high'
            
            # Add contextual category if not specified
            if enhanced_task['category'] == 'action_item':
                enhanced_task['category'] = self._determine_category(
                    enhanced_task['description'], 
                    email_data
                )
            
            return enhanced_task
            
        except Exception as e:
            logger.error(f"Failed to enhance task: {str(e)}")
            return task
    
    def _extract_date_from_text(self, text: str) -> Optional[datetime]:
        """
        Extract date from text using various patterns
        
        Args:
            text: Text that might contain a date
            
        Returns:
            Parsed datetime or None
        """
        if not text:
            return None
        
        try:
            # Try direct parsing first
            return parser.parse(text, fuzzy=True)
        except:
            pass
        
        # Try common patterns
        patterns = [
            r'(\d{1,2}/\d{1,2}/\d{4})',
            r'(\d{1,2}-\d{1,2}-\d{4})',
            r'(\w+\s+\d{1,2},?\s+\d{4})',
            r'(next\s+\w+)',
            r'(tomorrow)',
            r'(today)',
            r'(this\s+week)',
            r'(next\s+week)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text.lower())
            if match:
                try:
                    return parser.parse(match.group(1), fuzzy=True)
                except:
                    continue
        
        return None
    
    def _determine_category(self, description: str, email_data: Dict) -> str:
        """
        Determine task category based on description and email context
        
        Args:
            description: Task description
            email_data: Email context
            
        Returns:
            Task category
        """
        description_lower = description.lower()
        subject = email_data.get('subject', '').lower()
        
        # Meeting-related tasks
        if any(keyword in description_lower for keyword in ['meeting', 'call', 'schedule', 'zoom', 'teams']):
            return 'meeting'
        
        # Review tasks
        if any(keyword in description_lower for keyword in ['review', 'check', 'look at', 'examine']):
            return 'review'
        
        # Response tasks
        if any(keyword in description_lower for keyword in ['reply', 'respond', 'answer', 'get back']):
            return 'follow-up'
        
        # Document tasks
        if any(keyword in description_lower for keyword in ['document', 'report', 'write', 'create', 'draft']):
            return 'document'
        
        # Decision tasks
        if any(keyword in description_lower for keyword in ['decide', 'choose', 'approve', 'confirm']):
            return 'decision'
        
        # Deadline tasks
        if any(keyword in description_lower for keyword in ['deadline', 'due', 'submit', 'deliver']):
            return 'deadline'
        
        return 'action_item'
    
    def get_user_tasks(self, user_email: str, status: str = None, limit: int = None) -> Dict:
        """
        Get extracted tasks for a user
        
        Args:
            user_email: Email of the user
            status: Filter by task status (pending, in_progress, completed)
            limit: Maximum number of tasks to return
            
        Returns:
            Dictionary with user tasks
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            tasks = get_db_manager().get_user_tasks(user.id, status)
            
            if limit:
                tasks = tasks[:limit]
            
            return {
                'success': True,
                'user_email': user_email,
                'tasks': [task.to_dict() for task in tasks],
                'count': len(tasks),
                'status_filter': status
            }
            
        except Exception as e:
            logger.error(f"Failed to get tasks for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def update_task_status(self, user_email: str, task_id: int, status: str) -> Dict:
        """
        Update task status
        
        Args:
            user_email: Email of the user
            task_id: ID of the task to update
            status: New status (pending, in_progress, completed, cancelled)
            
        Returns:
            Dictionary with update result
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            with get_db_manager().get_session() as session:
                task = session.query(Task).filter(
                    Task.id == task_id,
                    Task.user_id == user.id
                ).first()
                
                if not task:
                    return {'success': False, 'error': 'Task not found'}
                
                task.status = status
                task.updated_at = datetime.utcnow()
                
                if status == 'completed':
                    task.completed_at = datetime.utcnow()
                
                session.commit()
                
                return {
                    'success': True,
                    'task_id': task_id,
                    'new_status': status,
                    'updated_at': task.updated_at.isoformat()
                }
            
        except Exception as e:
            logger.error(f"Failed to update task {task_id} for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _analyze_email_business_connections(self, email_data: Dict, business_context: Dict) -> Dict:
        """
        Analyze connections between email and business intelligence context
        """
        try:
            connections = {
                'related_people': [],
                'related_projects': [],
                'related_topics': [],
                'related_meetings': [],
                'connection_count': 0,
                'context_strength': 0.0
            }
            
            sender_email = email_data.get('sender', '').lower()
            subject = (email_data.get('subject') or '').lower()
            body = (email_data.get('body_clean') or '').lower()
            ai_summary = (email_data.get('ai_summary') or '').lower()
            email_topics = email_data.get('topics') or []
            
            # Find related people
            if sender_email in business_context.get('relationship_map', {}):
                person_info = business_context['relationship_map'][sender_email]
                connections['related_people'].append(person_info)
                connections['connection_count'] += 1
                connections['context_strength'] += person_info.get('importance', 0.5)
            
            # Find related projects
            for project_name, project_info in business_context.get('project_map', {}).items():
                if (project_name in subject or project_name in body or project_name in ai_summary):
                    connections['related_projects'].append(project_info)
                    connections['connection_count'] += 1
                    connections['context_strength'] += 0.8  # High value for project connection
            
            # Find related topics
            for topic in email_topics:
                topic_lower = topic.lower()
                if topic_lower in business_context.get('topic_keywords', {}):
                    topic_infos = business_context['topic_keywords'][topic_lower]
                    connections['related_topics'].extend(topic_infos)
                    connections['connection_count'] += len(topic_infos)
                    connections['context_strength'] += 0.6 * len(topic_infos)
            
            # Find related upcoming meetings
            for meeting in business_context.get('calendar_events', []):
                meeting_attendees = meeting.get('attendees', [])
                meeting_title = meeting.get('title', '').lower()
                
                # Check if sender is in meeting attendees
                if any(att.get('email', '').lower() == sender_email for att in meeting_attendees):
                    connections['related_meetings'].append(meeting)
                    connections['connection_count'] += 1
                    connections['context_strength'] += 0.7
                
                # Check if meeting title relates to email subject/content
                if any(keyword in meeting_title for keyword in subject.split() + body.split()[:20] if len(keyword) > 3):
                    if meeting not in connections['related_meetings']:
                        connections['related_meetings'].append(meeting)
                        connections['connection_count'] += 1
                        connections['context_strength'] += 0.5
            
            # Normalize context strength
            connections['context_strength'] = min(1.0, connections['context_strength'] / max(1, connections['connection_count']))
            
            return connections
            
        except Exception as e:
            logger.error(f"Failed to analyze email business connections: {str(e)}")
            return {'related_people': [], 'related_projects': [], 'related_topics': [], 'related_meetings': [], 'connection_count': 0, 'context_strength': 0.0}
    
    def _prepare_360_email_context(self, email_data: Dict, email_context: Dict, business_context: Dict) -> str:
        """
        Prepare comprehensive email context with business intelligence for Claude
        """
        try:
            sender = email_data.get('sender_name') or email_data.get('sender', '')
            subject = email_data.get('subject', '')
            body = email_data.get('body_clean', '')
            ai_summary = email_data.get('ai_summary', '')
            timestamp = email_data.get('timestamp')
            
            # Format timestamp
            if timestamp:
                try:
                    if isinstance(timestamp, str):
                        timestamp = parser.parse(timestamp)
                    date_str = timestamp.strftime('%Y-%m-%d %H:%M')
                except:
                    date_str = 'Unknown date'
            else:
                date_str = 'Unknown date'
            
            # Build business context summary
            context_elements = []
            
            # Add people context
            if email_context['related_people']:
                people_info = []
                for person in email_context['related_people']:
                    people_info.append(f"{person['name']} ({person.get('company', 'Unknown company')}) - {person.get('total_emails', 0)} previous interactions")
                context_elements.append(f"RELATED PEOPLE: {'; '.join(people_info)}")
            
            # Add project context
            if email_context['related_projects']:
                project_info = []
                for project in email_context['related_projects']:
                    project_info.append(f"{project['name']} (Status: {project.get('status', 'Unknown')}, Priority: {project.get('priority', 'Unknown')})")
                context_elements.append(f"RELATED PROJECTS: {'; '.join(project_info)}")
            
            # Add topic context
            if email_context['related_topics']:
                topic_names = [topic['name'] for topic in email_context['related_topics'] if topic.get('is_official')]
                if topic_names:
                    context_elements.append(f"RELATED BUSINESS TOPICS: {', '.join(topic_names)}")
            
            # Add meeting context
            if email_context['related_meetings']:
                meeting_info = []
                for meeting in email_context['related_meetings']:
                    meeting_date = meeting['start_time'].strftime('%Y-%m-%d %H:%M') if meeting.get('start_time') else 'TBD'
                    meeting_info.append(f"{meeting['title']} ({meeting_date})")
                context_elements.append(f"RELATED UPCOMING MEETINGS: {'; '.join(meeting_info)}")
            
            # Add strategic insights
            if business_context.get('recent_decisions'):
                recent_decisions = business_context['recent_decisions'][:3]
                context_elements.append(f"RECENT BUSINESS DECISIONS: {'; '.join(recent_decisions)}")
            
            if business_context.get('opportunities'):
                opportunities = business_context['opportunities'][:3]
                context_elements.append(f"STRATEGIC OPPORTUNITIES: {'; '.join(opportunities)}")
            
            business_intelligence = '\n'.join(context_elements) if context_elements else "No specific business context identified."
            
            enhanced_context = f"""Email Details:
From: {sender}
Date: {date_str}
Subject: {subject}

AI Summary: {ai_summary}

Email Content:
{body}

BUSINESS INTELLIGENCE CONTEXT:
{business_intelligence}

Context Strength: {email_context.get('context_strength', 0.0):.2f} (0.0 = no context, 1.0 = highly connected)
"""
            return enhanced_context
            
        except Exception as e:
            logger.error(f"Failed to prepare 360-context email: {str(e)}")
            return self._prepare_email_context(email_data)
    
    def _call_claude_for_360_tasks(self, enhanced_email_context: str, email_context: Dict) -> Optional[str]:
        """
        Call Claude 4 Sonnet for 360-context task extraction with business intelligence
        """
        try:
            context_strength = email_context.get('context_strength', 0.0)
            connection_count = email_context.get('connection_count', 0)
            
            system_prompt = f"""You are an expert AI Chief of Staff that extracts actionable tasks from emails using comprehensive business intelligence context. You have access to the user's complete business ecosystem including relationships, projects, topics, and strategic insights.

BUSINESS INTELLIGENCE CAPABILITIES:
- Cross-reference people relationships and interaction history
- Connect tasks to active projects and strategic initiatives  
- Leverage topic analysis and business themes
- Consider upcoming meetings and calendar context
- Incorporate recent business decisions and opportunities

ENHANCED TASK EXTRACTION GUIDELINES:

1. **360-Context Task Identification**: Look for tasks that:
   - Connect to the business relationships and projects mentioned
   - Align with strategic opportunities and recent decisions
   - Prepare for upcoming meetings with related attendees
   - Advance active projects and business initiatives
   - Leverage the full business context for maximum relevance

2. **Business-Aware Task Details**: For each task, provide:
   - Clear, actionable description with business context
   - Connect to specific people, projects, or meetings when relevant
   - Priority based on business importance and relationships
   - Category that reflects business context (project_work, relationship_management, strategic_planning, etc.)
   - Due dates that consider business timing and meeting schedules

3. **Context Enhancement Indicators**: 
   - Mark tasks as "context_enhanced": true if they leverage business intelligence
   - Include "business_context" field explaining the connection
   - Add "stakeholders" field if specific people are involved
   - Include "project_connection" if tied to active projects

Current Email Context Strength: {context_strength:.2f} ({connection_count} business connections identified)

RESPONSE FORMAT: Return a JSON array of tasks. Each task should have:
- "description": Clear, actionable description with business context
- "assignee": Who should do this (considering business relationships)
- "due_date": Specific date if mentioned (YYYY-MM-DD format)
- "due_date_text": Original due date text from email
- "priority": high/medium/low (elevated if high business context)
- "category": business-aware category (project_work, relationship_management, meeting_prep, strategic_planning, etc.)
- "confidence": 0.0-1.0 confidence score (higher with business context)
- "source_text": Original text from email that led to this task
- "context_enhanced": true/false (true if business intelligence was used)
- "business_context": Explanation of business connections (if context_enhanced)
- "stakeholders": List of relevant people from business context
- "project_connection": Name of related project if applicable

Return ONLY the JSON array. If no actionable tasks are found, return an empty array []."""

            user_prompt = f"""Please analyze this email with full business intelligence context and extract actionable tasks:

{enhanced_email_context}

Focus on tasks that leverage the business context for maximum relevance and strategic value. Consider the relationships, projects, meetings, and strategic insights provided."""

            message = self.client.messages.create(
                model=self.model,
                max_tokens=3000,  # More tokens for detailed context
                temperature=0.1,
                system=system_prompt,
                messages=[
                    {
                        "role": "user",
                        "content": user_prompt
                    }
                ]
            )
            
            response_text = message.content[0].text.strip()
            logger.debug(f"Claude 360-context response: {response_text}")
            
            return response_text
            
        except Exception as e:
            logger.error(f"Failed to call Claude for 360-context task extraction: {str(e)}")
            return None
    
    def _parse_claude_360_response(self, response: str, email_data: Dict, email_context: Dict) -> List[Dict]:
        """
        Parse Claude's 360-context JSON response into enhanced task dictionaries
        """
        try:
            # Try to find JSON in the response
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            
            if json_start == -1 or json_end == 0:
                logger.warning("No JSON array found in Claude 360-context response")
                return []
            
            json_text = response[json_start:json_end]
            
            # Parse JSON
            tasks_data = json.loads(json_text)
            
            if not isinstance(tasks_data, list):
                logger.warning("Claude 360-context response is not a JSON array")
                return []
            
            tasks = []
            for task_data in tasks_data:
                if isinstance(task_data, dict) and task_data.get('description'):
                    # Validate 360-context fields
                    if task_data.get('context_enhanced') and not task_data.get('business_context'):
                        task_data['business_context'] = f"Connected to {email_context.get('connection_count', 0)} business elements"
                    
                    tasks.append(task_data)
            
            logger.info(f"Parsed {len(tasks)} 360-context tasks from Claude response")
            return tasks
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Claude 360-context JSON response: {str(e)}")
            logger.error(f"Response was: {response}")
            return []
        except Exception as e:
            logger.error(f"Failed to parse Claude 360-context response: {str(e)}")
            return []
    
    def _enhance_task_with_360_context(self, task: Dict, email_data: Dict, email_context: Dict, business_context: Dict) -> Dict:
        """
        Enhance task with comprehensive 360-context metadata and business intelligence
        """
        try:
            # Start with Claude's task data
            enhanced_task = task.copy()
            
            # Parse due date if provided
            if task.get('due_date'):
                try:
                    due_date = parser.parse(task['due_date'])
                    enhanced_task['due_date'] = due_date
                except:
                    enhanced_task['due_date'] = self._extract_date_from_text(
                        task.get('due_date_text', '')
                    )
            elif task.get('due_date_text'):
                enhanced_task['due_date'] = self._extract_date_from_text(task['due_date_text'])
            
            # Set default values with 360-context awareness
            enhanced_task['priority'] = task.get('priority', 'medium').lower()
            enhanced_task['category'] = task.get('category', 'action_item')
            enhanced_task['confidence'] = min(1.0, max(0.0, task.get('confidence', 0.8)))
            enhanced_task['status'] = 'pending'
            
            # Enhance based on business context strength
            context_strength = email_context.get('context_strength', 0.0)
            if context_strength > 0.7:  # High context strength
                if enhanced_task['priority'] == 'medium':
                    enhanced_task['priority'] = 'high'
                enhanced_task['confidence'] = min(1.0, enhanced_task['confidence'] + 0.1)
            
            # Determine assignee with business context
            if not enhanced_task.get('assignee'):
                # Check if specific people are mentioned in business context
                related_people = email_context.get('related_people', [])
                if related_people and len(related_people) == 1:
                    enhanced_task['assignee'] = related_people[0]['name']
                else:
                    enhanced_task['assignee'] = 'me'
            
            # Enhance category with business context
            if enhanced_task['category'] == 'action_item':
                enhanced_task['category'] = self._determine_360_category(
                    enhanced_task['description'], 
                    email_data,
                    email_context
                )
            
            # Add 360-context specific fields
            if task.get('context_enhanced'):
                enhanced_task['context_enhanced'] = True
                enhanced_task['business_context'] = task.get('business_context', 'Business intelligence context applied')
                enhanced_task['context_strength'] = context_strength
                enhanced_task['connection_count'] = email_context.get('connection_count', 0)
            
            # Add stakeholder information
            stakeholders = task.get('stakeholders', [])
            if not stakeholders and email_context.get('related_people'):
                stakeholders = [person['name'] for person in email_context['related_people']]
            enhanced_task['stakeholders'] = stakeholders
            
            # Add project connection
            if task.get('project_connection'):
                enhanced_task['project_connection'] = task['project_connection']
            elif email_context.get('related_projects'):
                enhanced_task['project_connection'] = email_context['related_projects'][0]['name']
            
            return enhanced_task
            
        except Exception as e:
            logger.error(f"Failed to enhance task with 360-context: {str(e)}")
            return task
    
    def _determine_360_category(self, description: str, email_data: Dict, email_context: Dict) -> str:
        """
        Determine task category with 360-context business intelligence
        """
        try:
            description_lower = description.lower()
            subject = email_data.get('subject', '').lower()
            
            # Business context-aware categorization
            if email_context.get('related_projects'):
                return 'project_work'
            
            if email_context.get('related_meetings'):
                return 'meeting_prep'
            
            if email_context.get('related_people') and len(email_context['related_people']) > 0:
                person = email_context['related_people'][0]
                if person.get('importance', 0) > 0.7:
                    return 'relationship_management'
            
            # Strategic context
            if any(keyword in description_lower for keyword in ['strategy', 'strategic', 'decision', 'opportunity']):
                return 'strategic_planning'
            
            # Default categorization with business awareness
            if any(keyword in description_lower for keyword in ['meeting', 'call', 'schedule', 'zoom', 'teams']):
                return 'meeting'
            
            if any(keyword in description_lower for keyword in ['review', 'check', 'look at', 'examine']):
                return 'review'
            
            if any(keyword in description_lower for keyword in ['reply', 'respond', 'answer', 'get back']):
                return 'follow-up'
            
            if any(keyword in description_lower for keyword in ['document', 'report', 'write', 'create', 'draft']):
                return 'document'
            
            if any(keyword in description_lower for keyword in ['decide', 'choose', 'approve', 'confirm']):
                return 'decision'
            
            if any(keyword in description_lower for keyword in ['deadline', 'due', 'submit', 'deliver']):
                return 'deadline'
            
            return 'action_item'
            
        except Exception as e:
            logger.error(f"Failed to determine 360-context category: {str(e)}")
            return 'action_item'
    
    def extract_tasks_from_email(self, email_data: Dict) -> Dict:
        """
        LEGACY METHOD: Extract actionable tasks from a single email using Claude 4 Sonnet
        This method is kept for backward compatibility but users should use extract_tasks_with_360_context
        """
        try:
            logger.warning("Using legacy task extraction - consider upgrading to 360-context extraction")
            
            # Check if email has enough content for task extraction
            body_clean = email_data.get('body_clean', '')
            if not body_clean or len(body_clean.strip()) < 20:
                return {
                    'success': True,
                    'email_id': email_data.get('id'),
                    'tasks': [],
                    'reason': 'Email content too short for task extraction'
                }
            
            # Skip certain message types that unlikely contain tasks
            message_type = email_data.get('message_type', 'regular')
            if message_type in ['newsletter', 'automated']:
                return {
                    'success': True,
                    'email_id': email_data.get('id'),
                    'tasks': [],
                    'reason': f'Message type "{message_type}" skipped for task extraction'
                }
            
            # Prepare email context for Claude
            email_context = self._prepare_email_context(email_data)
            
            # Call Claude for task extraction
            claude_response = self._call_claude_for_tasks(email_context)
            
            if not claude_response:
                return {
                    'success': False,
                    'email_id': email_data.get('id'),
                    'error': 'Failed to get response from Claude'
                }
            
            # Parse Claude's response
            tasks = self._parse_claude_response(claude_response, email_data)
            
            # Enhance tasks with additional metadata
            enhanced_tasks = []
            for task in tasks:
                enhanced_task = self._enhance_task(task, email_data)
                enhanced_tasks.append(enhanced_task)
            
            return {
                'success': True,
                'email_id': email_data.get('id'),
                'tasks': enhanced_tasks,
                'extraction_metadata': {
                    'extracted_at': datetime.utcnow().isoformat(),
                    'extractor_version': self.version,
                    'model_used': self.model,
                    'email_priority': email_data.get('priority_score', 0.5)
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to extract tasks from email {email_data.get('id', 'unknown')}: {str(e)}")
            return {
                'success': False,
                'email_id': email_data.get('id'),
                'error': str(e)
            }

# Create global instance
task_extractor = TaskExtractor()

============================================================
FILE: archive/backup_files/v1_original/processors/email_intelligence.py
============================================================
# Enhanced Email Intelligence Processor using Claude 4 Sonnet

import json
import logging
import re
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple
import anthropic
import time

from config.settings import settings
from models.database import get_db_manager, Email, Person, Project, Task, User

logger = logging.getLogger(__name__)

class EmailIntelligenceProcessor:
    """Advanced email intelligence using Claude 4 Sonnet for comprehensive understanding"""
    
    def __init__(self):
        self.client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        self.model = "claude-3-5-sonnet-20241022"
        self.version = "2.2"  # Debug version with relaxed filters
        
        # Quality filtering patterns (RELAXED FOR DEBUGGING)
        self.non_human_patterns = [
            'noreply', 'no-reply', 'donotreply', 'automated', 'newsletter',
            'unsubscribe', 'notification', 'system', 'support', 'help',
            'admin', 'contact', 'info', 'sales', 'marketing', 'hello',
            'team', 'notifications', 'alerts', 'updates', 'reports'
        ]
        
        # RELAXED quality thresholds to capture more content
        self.min_insight_length = 10  # Reduced from 15
        self.min_confidence_score = 0.4  # Reduced from 0.6 - be more inclusive
        
    def process_user_emails_intelligently(self, user_email: str, limit: int = None, force_refresh: bool = False) -> Dict:
        """
        Process user emails with Claude 4 Sonnet for high-quality business intelligence
        Enhanced with quality filtering and strategic insights
        """
        try:
            logger.info(f"Starting quality-focused email processing for {user_email}")
            
            # Get user and validate
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
                
            # Get business context for enhanced AI analysis
            user_context = self._get_user_business_context(user.id)
            
            # Get emails needing processing with quality pre-filtering
            emails_to_process = self._get_emails_needing_processing(user.id, limit or 100, force_refresh)
            
            # RELAXED: Filter for quality emails but be more inclusive
            quality_filtered_emails = self._filter_quality_emails_debug(emails_to_process, user_email)
            
            logger.info(f"Found {len(emails_to_process)} emails to process, {len(quality_filtered_emails)} passed quality filters")
            
            if not quality_filtered_emails:
                logger.warning(f"No emails passed quality filters for {user_email}")
                return {
                    'success': True,
                    'user_email': user_email,
                    'processed_emails': 0,
                    'high_quality_insights': 0,
                    'human_contacts_identified': 0,
                    'meaningful_projects': 0,
                    'actionable_tasks': 0,
                    'processor_version': self.version,
                    'debug_info': f"No emails passed filters out of {len(emails_to_process)} total emails"
                }
            
            # Limit to top quality emails for processing
            emails_to_process = quality_filtered_emails[:limit or 50]
            
            processed_count = 0
            insights_extracted = 0
            people_identified = 0
            projects_identified = 0
            tasks_created = 0
            
            for idx, email in enumerate(emails_to_process):
                try:
                    logger.info(f"Processing email {idx + 1}/{len(emails_to_process)} for {user_email}")
                    logger.debug(f"Email from: {email.sender}, subject: {email.subject}")
                    
                    # Skip if email has issues
                    if not email.body_clean and not email.snippet:
                        logger.warning(f"Skipping email {email.gmail_id} - no content")
                        continue
                    
                    # Get comprehensive email analysis from Claude with enhanced prompts
                    analysis = self._get_quality_focused_email_analysis(email, user, user_context)
                    
                    if analysis:
                        logger.debug(f"AI Analysis received for email {email.gmail_id}")
                        logger.debug(f"Strategic value score: {analysis.get('strategic_value_score', 'N/A')}")
                        logger.debug(f"Sender analysis: {analysis.get('sender_analysis', {})}")
                        logger.debug(f"People found: {len(analysis.get('people', []))}")
                        
                        if self._validate_analysis_quality_debug(analysis):
                            # Update email with insights
                            self._update_email_with_insights(email, analysis)
                            
                            # Extract and update people information (with human filtering)
                            if analysis.get('people') or analysis.get('sender_analysis'):
                                people_count = self._process_human_contacts_only_debug(user.id, analysis, email)
                                people_identified += people_count
                                logger.info(f"Extracted {people_count} people from email {email.gmail_id}")
                            
                            # Extract and update project information
                            if analysis.get('project') and self._validate_project_quality(analysis['project']):
                                project = self._process_project_insights(user.id, analysis['project'], email)
                                if project:
                                    projects_identified += 1
                                    email.project_id = project.id
                            
                            # Extract high-confidence tasks only
                            if analysis.get('tasks'):
                                tasks_count = self._process_high_quality_tasks(user.id, email.id, analysis['tasks'])
                                tasks_created += tasks_count
                            
                            insights_extracted += 1
                        else:
                            logger.info(f"Analysis for email {email.gmail_id} didn't meet quality thresholds")
                    else:
                        logger.warning(f"No analysis returned for email {email.gmail_id}")
                    
                    processed_count += 1
                    
                    # Add a small delay to prevent overwhelming the system
                    time.sleep(0.1)
                    
                except Exception as e:
                    logger.error(f"Failed to intelligently process email {email.gmail_id}: {str(e)}")
                    continue
            
            logger.info(f"Quality-focused processing: {processed_count} emails, {people_identified} people identified for {user_email}")
            
            return {
                'success': True,
                'user_email': user_email,
                'processed_emails': processed_count,
                'high_quality_insights': insights_extracted,
                'human_contacts_identified': people_identified,
                'meaningful_projects': projects_identified,
                'actionable_tasks': tasks_created,
                'processor_version': self.version,
                'debug_info': f"Processed {processed_count} emails, passed quality filters: {len(quality_filtered_emails)}"
            }
            
        except Exception as e:
            logger.error(f"Failed intelligent email processing for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _get_emails_needing_processing(self, user_id: int, limit: int, force_refresh: bool) -> List[Email]:
        """Get emails that need Claude analysis (generic filter)"""
        with get_db_manager().get_session() as session:
            query = session.query(Email).filter(
                Email.user_id == user_id,
                Email.body_clean.isnot(None)
            )
            
            if not force_refresh:
                query = query.filter(Email.ai_summary.is_(None))
            
            # Detach from session before returning to avoid issues
            emails = query.order_by(Email.email_date.desc()).limit(limit).all()
            session.expunge_all()
            return emails

    def _filter_unreplied_emails(self, emails: List[Email], user_email: str) -> List[Email]:
        """Filter a list of emails to find ones that are likely unreplied"""
        unreplied = []
        for email in emails:
            # If email is from the user themselves, skip
            if email.sender and user_email.lower() in email.sender.lower():
                continue

            # If email contains certain patterns suggesting it's automated, skip
            automated_patterns = [
                'noreply', 'no-reply', 'donotreply', 'automated', 'newsletter',
                'unsubscribe', 'notification only', 'system generated'
            ]
            sender_lower = (email.sender or '').lower()
            subject_lower = (email.subject or '').lower()
            if any(pattern in sender_lower or pattern in subject_lower for pattern in automated_patterns):
                continue

            # Default to including emails that seem personal/business oriented
            unreplied.append(email)
        return unreplied
    
    def _is_unreplied_email(self, email: Email, user_email: str) -> bool:
        """Determine if an email is unreplied using heuristics"""
        # If email is from the user themselves, skip
        if email.sender and user_email.lower() in email.sender.lower():
            return False
        
        # If email contains certain patterns suggesting it's automated, skip
        automated_patterns = [
            'noreply', 'no-reply', 'donotreply', 'automated', 'newsletter',
            'unsubscribe', 'notification only', 'system generated'
        ]
        
        sender_lower = (email.sender or '').lower()
        subject_lower = (email.subject or '').lower()
        
        if any(pattern in sender_lower or pattern in subject_lower for pattern in automated_patterns):
            return False
        
        # If email is marked as important or has action-oriented subject, include it
        action_words = ['review', 'approve', 'sign', 'confirm', 'urgent', 'asap', 'deadline', 'meeting']
        if any(word in subject_lower for word in action_words):
            return True
        
        # Default to including emails that seem personal/business oriented
        return True
    
    def _get_quality_focused_email_analysis(self, email: Email, user, user_context: Dict) -> Optional[Dict]:
        """Get quality-focused email analysis from Claude with enhanced business context"""
        try:
            # Safety check to prevent processing very large emails
            email_content = email.body_clean or email.snippet or ""
            if len(email_content) > 10000:  # Limit email content size
                logger.warning(f"Email {email.gmail_id} too large ({len(email_content)} chars), truncating")
                email_content = email_content[:10000] + "... [truncated]"
            
            if len(email_content) < 10:  # Skip very short emails
                logger.warning(f"Email {email.gmail_id} too short, skipping AI analysis")
                return None
            
            email_context = self._prepare_enhanced_email_context(email, user)
            
            # Limit context size to prevent API issues
            if len(email_context) > 15000:
                logger.warning(f"Email context too large for {email.gmail_id}, truncating")
                email_context = email_context[:15000] + "... [truncated]"
            
            # Enhanced system prompt with business context and quality requirements
            business_context_str = self._format_business_context(user_context)
            
            system_prompt = f"""You are an expert AI Chief of Staff that provides comprehensive email analysis for business intelligence and productivity. Be INCLUSIVE and extract valuable insights from business communications.

**YOUR MISSION:**
- Extract ALL valuable business intelligence, contacts, tasks, and insights
- Be inclusive rather than restrictive - capture business value wherever it exists
- Focus on building comprehensive knowledge about professional relationships and work

**BUSINESS CONTEXT FOR {user.email}:**
{business_context_str}

**ANALYSIS REQUIREMENTS:**

1. **EMAIL SUMMARY**: Clear description of the email's business purpose and content
2. **PEOPLE EXTRACTION**: Extract ALL human contacts with professional relevance (be generous!)
   - ALWAYS extract the sender if they're a real person
   - Extract anyone mentioned by name with business context
   - Include names even with limited contact information
3. **TASK IDENTIFICATION**: Find ANY actionable items or commitments mentioned
4. **BUSINESS INSIGHTS**: Extract any strategic value, opportunities, or challenges
5. **PROJECT CONTEXT**: Identify any work initiatives or business activities
6. **TOPIC EXTRACTION**: Identify business topics, project names, company names, technologies

**INCLUSIVE EXTRACTION GUIDELINES:**
- Extract people even if limited info is available (name + context is enough)
- Include tasks with clear actionable language, even if informal
- Capture business insights at any level (strategic, operational, or tactical)
- Process emails from colleagues, clients, partners, vendors - anyone professional
- Include follow-ups, scheduling, decisions, updates, and work discussions
- Extract topics like project names, company names, technologies, business areas
- Be generous with topic extraction - include any business-relevant subjects

Return a JSON object with this structure:
{{
    "summary": "Clear description of the email's business purpose and key content",
    "strategic_value_score": 0.7,  // Be generous - most business emails have value
    "sender_analysis": {{
        "name": "Sender's actual name (extract from signature or display name)",
        "role": "Their role/title if mentioned",
        "company": "Their company if identifiable",
        "relationship": "Professional relationship context",
        "is_human_contact": true,  // Default to true for most senders
        "business_relevance": "Why this person is professionally relevant"
    }},
    "people": [
        {{
            "name": "Full name of any person mentioned",
            "email": "their_email@example.com",
            "role": "Their role if mentioned",
            "company": "Company if mentioned", 
            "relationship": "Professional context",
            "business_relevance": "Why they're mentioned/relevant",
            "mentioned_context": "How they were mentioned in the email"
        }}
    ],
    "project": {{
        "name": "Project or initiative name",
        "description": "Description of the work or project",
        "category": "business/client_work/internal/operational",
        "priority": "high/medium/low",
        "status": "active/planning/discussed",
        "business_impact": "Potential impact or value",
        "key_stakeholders": ["person1", "person2"]
    }},
    "business_insights": {{
        "key_decisions": ["Any decisions mentioned or needed"],
        "strategic_opportunities": ["Opportunities or potential business value"],
        "business_challenges": ["Challenges or issues discussed"],
        "actionable_metrics": ["Any numbers or metrics mentioned"],
        "competitive_intelligence": ["Market or competitor information"],
        "partnership_opportunities": ["Collaboration potential"]
    }},
    "tasks": [
        {{
            "description": "Clear description of the actionable item",
            "assignee": "{user.email}",
            "due_date": "2025-02-15",
            "due_date_text": "deadline mentioned in email",
            "priority": "high/medium/low",
            "category": "action_item/follow_up/meeting/review",
            "confidence": 0.8,  // Be generous with confidence scores
            "business_context": "Why this task matters",
            "success_criteria": "What completion looks like"
        }}
    ],
    "topics": ["HitCraft", "board meeting", "fundraising", "AI in music", "certification", "business development"],  // Extract: project names, company names, technologies, business areas, meeting types
    "ai_category": "business_communication/client_work/project_coordination/operational"
}}

**IMPORTANT**: Extract value from most business emails. Only skip obvious spam or completely irrelevant content. Be generous with people extraction and task identification.
"""

            user_prompt = f"""Analyze this email comprehensively for business intelligence. Extract ALL valuable people, tasks, and insights:

{email_context}

Focus on building comprehensive business knowledge. Extract people and tasks generously - capture business value wherever it exists."""

            # Add timeout and retry protection
            max_retries = 2
            for attempt in range(max_retries):
                try:
                    logger.info(f"Calling Claude API for comprehensive analysis of email {email.gmail_id}, attempt {attempt + 1}")
                    
                    message = self.client.messages.create(
                        model=self.model,
                        max_tokens=3000,
                        temperature=0.1,
                        system=system_prompt,
                        messages=[{"role": "user", "content": user_prompt}]
                    )
                    
                    response_text = message.content[0].text.strip()
                    
                    # Handle null responses (low-quality emails)
                    if response_text.lower().strip() in ['null', 'none', '{}', '']:
                        logger.info(f"Claude rejected email {email.gmail_id} as low-quality")
                        return None
                    
                    # Parse JSON response with better error handling
                    json_start = response_text.find('{')
                    json_end = response_text.rfind('}') + 1
                    
                    if json_start != -1 and json_end > json_start:
                        json_text = response_text[json_start:json_end]
                        try:
                            analysis = json.loads(json_text)
                            logger.info(f"Successfully analyzed email {email.gmail_id}")
                            return analysis
                        except json.JSONDecodeError as json_error:
                            logger.error(f"JSON parsing error for email {email.gmail_id}: {str(json_error)}")
                            if attempt < max_retries - 1:
                                time.sleep(1)  # Wait before retry
                                continue
                            return None
                    else:
                        logger.warning(f"No valid JSON found in Claude response for email {email.gmail_id}")
                        if attempt < max_retries - 1:
                            time.sleep(1)  # Wait before retry
                            continue
                        return None
                        
                except Exception as api_error:
                    logger.error(f"Claude API error for email {email.gmail_id}, attempt {attempt + 1}: {str(api_error)}")
                    if attempt < max_retries - 1:
                        time.sleep(2)  # Wait longer before retry
                        continue
                    return None
            
            logger.warning(f"Failed to analyze email {email.gmail_id} after {max_retries} attempts")
            return None
            
        except Exception as e:
            logger.error(f"Failed to get email analysis from Claude for {email.gmail_id}: {str(e)}")
            return None
    
    def _format_business_context(self, user_context: Dict) -> str:
        """Format business context for AI prompt"""
        context_parts = []
        
        if user_context.get('existing_projects'):
            context_parts.append(f"Current Projects: {', '.join(user_context['existing_projects'])}")
        
        if user_context.get('key_contacts'):
            context_parts.append(f"Key Business Contacts: {', '.join(user_context['key_contacts'][:5])}")  # Top 5
        
        if user_context.get('official_topics'):
            context_parts.append(f"Business Focus Areas: {', '.join(user_context['official_topics'])}")
        
        return '\n'.join(context_parts) if context_parts else "No existing business context available"
    
    def _validate_analysis_quality(self, analysis: Dict) -> bool:
        """Validate that the analysis meets quality standards - RELAXED VERSION"""
        try:
            # RELAXED: Check strategic value score - lowered threshold
            strategic_value = analysis.get('strategic_value_score', 0)
            if strategic_value < 0.5:  # Reduced from 0.6 to 0.5
                logger.info(f"Analysis rejected - low strategic value: {strategic_value}")
                return False
            
            # RELAXED: Check summary quality - reduced minimum length
            summary = analysis.get('summary', '')
            if len(summary) < self.min_insight_length:
                logger.info(f"Analysis rejected - summary too short: {len(summary)} chars")
                return False
            
            # RELAXED: More lenient trivial content detection
            trivial_phrases = [
                'thanks', 'thank you', 'got it', 'received', 'noted', 'okay', 'ok',
                'sounds good', 'will do', 'understood', 'acknowledged'
            ]
            
            # Only reject if it's VERY short AND contains only trivial phrases
            if any(phrase in summary.lower() for phrase in trivial_phrases) and len(summary) < 30:  # Reduced from 50
                logger.info(f"Analysis rejected - trivial content detected")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"Error validating analysis quality: {str(e)}")
            return False
    
    def _validate_project_quality(self, project_data: Dict) -> bool:
        """Validate that project data meets quality standards"""
        if not project_data or not project_data.get('name'):
            return False
        
        # Check project name is substantial
        if len(project_data['name']) < 5:
            return False
        
        # Check for meaningful description
        description = project_data.get('description', '')
        if len(description) < self.min_insight_length:
            return False
        
        return True
    
    def _process_human_contacts_only(self, user_id: int, analysis: Dict, email: Email) -> int:
        """Process people information with human contact filtering and KNOWLEDGE ACCUMULATION"""
        people_count = 0
        
        # Process sender first (with human validation and knowledge accumulation)
        sender_analysis = analysis.get('sender_analysis')
        if (sender_analysis and email.sender and 
            sender_analysis.get('is_human_contact') and
            not self._is_non_human_contact(email.sender)):
            
            # Get existing person to accumulate knowledge
            existing_person = get_db_manager().find_person_by_email(user_id, email.sender)
            
            # Accumulate notes and context over time
            existing_notes = existing_person.notes if existing_person else ""
            new_relevance = sender_analysis.get('business_relevance', '')
            
            # Combine old and new notes intelligently
            accumulated_notes = existing_notes
            if new_relevance and new_relevance not in accumulated_notes:
                if accumulated_notes:
                    accumulated_notes += f"\n\nRecent Context: {new_relevance}"
                else:
                    accumulated_notes = new_relevance
            
            person_data = {
                'email_address': email.sender,
                'name': sender_analysis.get('name', email.sender_name or email.sender),
                'role': sender_analysis.get('role') or (existing_person.role if existing_person else None),
                'company': sender_analysis.get('company') or (existing_person.company if existing_person else None),
                'relationship_type': sender_analysis.get('relationship') or (existing_person.relationship_type if existing_person else None),
                'notes': accumulated_notes,  # Accumulated knowledge
                'importance_level': max(0.8, existing_person.importance_level if existing_person else 0.8),  # Increment importance
                'ai_version': self.version,
                'total_emails': (existing_person.total_emails if existing_person else 0) + 1  # Increment email count
            }
            get_db_manager().create_or_update_person(user_id, person_data)
            people_count += 1
        
        # Process mentioned people (with human validation and accumulation)
        people_data = analysis.get('people', [])
        if isinstance(people_data, list):
            for person_info in people_data:
                if (person_info.get('email') or person_info.get('name')) and person_info.get('business_relevance'):
                    # Additional human validation
                    email_addr = person_info.get('email', '')
                    if email_addr and self._is_non_human_contact(email_addr):
                        continue
                    
                    # Get existing person to accumulate knowledge
                    existing_person = None
                    if email_addr:
                        existing_person = get_db_manager().find_person_by_email(user_id, email_addr)
                    
                    # Accumulate knowledge
                    existing_notes = existing_person.notes if existing_person else ""
                    new_relevance = person_info.get('business_relevance', '')
                    
                    accumulated_notes = existing_notes
                    if new_relevance and new_relevance not in accumulated_notes:
                        if accumulated_notes:
                            accumulated_notes += f"\n\nMentioned Context: {new_relevance}"
                        else:
                            accumulated_notes = new_relevance
                    
                    person_data = {
                        'email_address': person_info.get('email'),
                        'name': person_info['name'],
                        'role': person_info.get('role') or (existing_person.role if existing_person else None),
                        'company': person_info.get('company') or (existing_person.company if existing_person else None),
                        'relationship_type': person_info.get('relationship') or (existing_person.relationship_type if existing_person else None),
                        'notes': accumulated_notes,  # Accumulated knowledge
                        'ai_version': self.version
                    }
                    get_db_manager().create_or_update_person(user_id, person_data)
                    people_count += 1
        
        return people_count
    
    def _process_high_quality_tasks(self, user_id: int, email_id: int, tasks_data: List[Dict]) -> int:
        """Process and save actionable tasks - VERY INCLUSIVE VERSION"""
        tasks_count = 0
        
        for task_info in tasks_data:
            # Validate task quality - very permissive
            if not task_info.get('description'):
                continue
            
            # VERY INCLUSIVE: Check confidence threshold - very low threshold
            confidence = task_info.get('confidence', 0.8)  # Default to 0.8 if missing
            if confidence < 0.3:  # Very low threshold
                continue
            
            # VERY INCLUSIVE: Check description length - very permissive
            description = task_info['description']
            if len(description) < 5:  # Very short minimum
                continue
            
            task_data = {
                'description': description,
                'assignee': task_info.get('assignee'),
                'due_date': self._parse_due_date(task_info.get('due_date')),
                'due_date_text': task_info.get('due_date_text'),
                'priority': task_info.get('priority', 'medium'),
                'category': task_info.get('category', 'action_item'),
                'confidence': confidence,
                'source_text': task_info.get('success_criteria', ''),
                'context': task_info.get('business_context', ''),
                'status': 'pending',
                'extractor_version': self.version,
                'model_used': self.model
            }
            
            get_db_manager().save_task(user_id, email_id, task_data)
            tasks_count += 1
            logger.info(f"Created task: {description[:50]}...")
        
        return tasks_count
    
    def _prepare_enhanced_email_context(self, email: Email, user) -> str:
        """Prepare comprehensive email context for Claude analysis"""
        timestamp = email.email_date.strftime('%Y-%m-%d %H:%M') if email.email_date else 'Unknown'
        
        context = f"""EMAIL ANALYSIS REQUEST

Recipient: {user.email} ({user.name})
From: {email.sender_name or 'Unknown'} <{email.sender}>
Date: {timestamp}
Subject: {email.subject}

Email Content:
{email.body_clean or email.snippet}

Additional Context:
- Recipients: {', '.join(email.recipients) if email.recipients else 'Not specified'}
- Thread ID: {email.thread_id}
- Email Labels: {', '.join(email.labels) if email.labels else 'None'}
- Message Type: {email.message_type or 'Unknown'}
- Priority Score: {email.priority_score or 'Not calculated'}
"""
        return context
    
    def _update_email_with_insights(self, email: Email, analysis: Dict):
        """Update email record with Claude insights"""
        with get_db_manager().get_session() as session:
            email_record = session.query(Email).filter(Email.id == email.id).first()
            if email_record:
                email_record.ai_summary = analysis.get('summary')
                email_record.ai_category = analysis.get('ai_category')
                email_record.sentiment_score = analysis.get('sentiment_score')
                email_record.urgency_score = analysis.get('urgency_score')
                email_record.key_insights = analysis.get('business_insights')
                email_record.topics = analysis.get('topics')
                email_record.action_required = analysis.get('action_required', False)
                email_record.follow_up_required = analysis.get('follow_up_required', False)
                
                session.commit()
    
    def _process_project_insights(self, user_id: int, project_data: Dict, email: Email) -> Optional[Project]:
        """Process and update project information - SAFE VERSION"""
        if not project_data or not project_data.get('name'):
            return None
        
        try:
            project_info = {
                'name': project_data['name'],
                'slug': self._create_slug(project_data['name']),
                'description': project_data.get('description'),
                'category': project_data.get('category'),
                'priority': project_data.get('priority', 'medium'),
                'status': project_data.get('status', 'active'),
                'key_topics': project_data.get('key_topics', []),
                'stakeholders': project_data.get('stakeholders', []),
                'ai_version': self.version
            }
            
            return get_db_manager().create_or_update_project(user_id, project_info)
            
        except Exception as e:
            logger.error(f"Error processing project insights: {str(e)}")
            return None
    
    def _create_slug(self, name: str) -> str:
        """Create URL-friendly slug from name"""
        return re.sub(r'[^a-zA-Z0-9]+', '-', name.lower()).strip('-')
    
    def _parse_due_date(self, date_str: str) -> Optional[datetime]:
        """Parse due date string into datetime"""
        if not date_str:
            return None
        
        try:
            return datetime.strptime(date_str, '%Y-%m-%d')
        except:
            return None
    
    def get_business_knowledge_summary(self, user_email: str) -> Dict:
        """Get comprehensive business knowledge summary with quality synthesis"""
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            # Get all processed emails with quality filtering
            emails = get_db_manager().get_user_emails(user.id, limit=1000)
            projects = get_db_manager().get_user_projects(user.id, limit=200)
            people = get_db_manager().get_user_people(user.id, limit=500)
            
            # Filter for high-quality insights only
            quality_emails = [e for e in emails if e.ai_summary and len(e.ai_summary) > self.min_insight_length]
            human_contacts = [p for p in people if not self._is_non_human_contact(p.email_address or '')]
            substantial_projects = [p for p in projects if p.description and len(p.description) > self.min_insight_length]
            
            # Synthesize high-quality business insights
            strategic_decisions = []
            business_opportunities = []
            key_challenges = []
            competitive_insights = []
            
            for email in quality_emails:
                if email.key_insights and isinstance(email.key_insights, dict):
                    insights = email.key_insights
                    
                    # Extract strategic-level insights only
                    decisions = insights.get('key_decisions', [])
                    strategic_decisions.extend([d for d in decisions if len(d) > self.min_insight_length])
                    
                    opportunities = insights.get('strategic_opportunities', insights.get('opportunities', []))
                    business_opportunities.extend([o for o in opportunities if len(o) > self.min_insight_length])
                    
                    challenges = insights.get('business_challenges', insights.get('challenges', []))
                    key_challenges.extend([c for c in challenges if len(c) > self.min_insight_length])
                    
                    competitive = insights.get('competitive_intelligence', [])
                    competitive_insights.extend([ci for ci in competitive if len(ci) > self.min_insight_length])
            
            # Get meaningful topics
            topics = get_db_manager().get_user_topics(user.id, limit=1000)
            business_topics = [topic.name for topic in topics if topic.is_official or 
                              (topic.description and len(topic.description) > 10)]
            
            return {
                'success': True,
                'user_email': user_email,
                'business_knowledge': {
                    'summary_stats': {
                        'quality_emails_analyzed': len(quality_emails),
                        'human_contacts': len(human_contacts),
                        'substantial_projects': len(substantial_projects),
                        'strategic_insights': len(strategic_decisions) + len(business_opportunities) + len(key_challenges)
                    },
                    'strategic_intelligence': {
                        'key_decisions': self._deduplicate_and_rank(strategic_decisions)[:8],  # Top 8 strategic decisions
                        'business_opportunities': self._deduplicate_and_rank(business_opportunities)[:8],
                        'key_challenges': self._deduplicate_and_rank(key_challenges)[:8],
                        'competitive_intelligence': self._deduplicate_and_rank(competitive_insights)[:5]
                    },
                    'business_topics': business_topics[:15],  # Top 15 business topics
                    'network_intelligence': {
                        'total_human_contacts': len(human_contacts),
                        'active_projects': len([p for p in substantial_projects if p.status == 'active']),
                        'project_categories': list(set([p.category for p in substantial_projects if p.category]))
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to get business knowledge for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}

    def get_chat_knowledge_summary(self, user_email: str) -> Dict:
        """Get comprehensive knowledge summary for chat interface with enhanced context"""
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            # Get all processed data with quality filters
            emails = get_db_manager().get_user_emails(user.id, limit=1000)
            projects = get_db_manager().get_user_projects(user.id, limit=200)
            people = get_db_manager().get_user_people(user.id, limit=500)
            topics = get_db_manager().get_user_topics(user.id, limit=1000)
            
            # GET CALENDAR EVENTS FOR KNOWLEDGE BASE
            now = datetime.now(timezone.utc)
            calendar_events = get_db_manager().get_user_calendar_events(
                user.id, 
                start_date=now - timedelta(days=30),  # Past 30 days for context
                end_date=now + timedelta(days=60),    # Next 60 days for planning
                limit=200
            )
            
            # Filter for high-quality content
            quality_emails = [e for e in emails if e.ai_summary and len(e.ai_summary) > self.min_insight_length]
            human_contacts = [p for p in people if not self._is_non_human_contact(p.email_address or '') and p.name]
            
            # Compile rich contacts with enhanced professional context
            rich_contacts = []
            for person in human_contacts[:15]:  # Top 15 human contacts
                # Create rich professional story
                professional_story = self._create_professional_story(person, quality_emails)
                
                contact_info = {
                    'name': person.name,
                    'email': person.email_address,
                    'title': person.title or person.role,
                    'company': person.company,
                    'relationship': person.relationship_type,
                    'story': professional_story,
                    'total_emails': person.total_emails or 0,
                    'last_interaction': person.last_interaction.isoformat() if person.last_interaction else None,
                    'importance_score': person.importance_level or 0.5
                }
                rich_contacts.append(contact_info)
            
            # Enhanced business intelligence compilation
            business_decisions = []
            opportunities = []
            challenges = []
            
            for email in quality_emails:
                if email.key_insights and isinstance(email.key_insights, dict):
                    insights = email.key_insights
                    
                    # Enhanced insight extraction with context
                    decisions = insights.get('key_decisions', [])
                    for decision in decisions:
                        if len(decision) > self.min_insight_length:
                            business_decisions.append({
                                'decision': decision,
                                'context': email.ai_summary,
                                'sender': email.sender_name or email.sender,
                                'date': email.email_date.isoformat() if email.email_date else None
                            })
                    
                    opps = insights.get('strategic_opportunities', insights.get('opportunities', []))
                    for opp in opps:
                        if len(opp) > self.min_insight_length:
                            opportunities.append({
                                'opportunity': opp,
                                'context': email.ai_summary,
                                'source': email.sender_name or email.sender,
                                'date': email.email_date.isoformat() if email.email_date else None
                            })
                    
                    chals = insights.get('business_challenges', insights.get('challenges', []))
                    for chal in chals:
                        if len(chal) > self.min_insight_length:
                            challenges.append({
                                'challenge': chal,
                                'context': email.ai_summary,
                                'source': email.sender_name or email.sender,
                                'date': email.email_date.isoformat() if email.email_date else None
                            })
            
            # Enhanced topic knowledge with rich contexts
            topic_knowledge = {
                'all_topics': [topic.name for topic in topics if topic.is_official or 
                              (topic.description and len(topic.description) > 10)],
                'official_topics': [topic.name for topic in topics if topic.is_official],
                'topic_contexts': {}
            }
            
            for topic in topics:
                if topic.is_official or (topic.description and len(topic.description) > 10):
                    topic_emails = [email for email in quality_emails if email.topics and topic.name in email.topics]
                    contexts = []
                    for email in topic_emails[:3]:  # Top 3 emails per topic
                        if email.ai_summary:
                            contexts.append({
                                'summary': email.ai_summary,
                                'sender': email.sender_name or email.sender,
                                'date': email.email_date.isoformat() if email.email_date else None,
                                'email_subject': email.subject
                            })
                    topic_knowledge['topic_contexts'][topic.name] = contexts
            
            # Enhanced statistics
            summary_stats = {
                'total_emails_analyzed': len(quality_emails),
                'rich_contacts': len(rich_contacts),
                'business_decisions': len(business_decisions),
                'opportunities_identified': len(opportunities),
                'challenges_tracked': len(challenges),
                'active_projects': len([p for p in projects if p.status == 'active']),
                'official_topics': len([t for t in topics if t.is_official]),
                'calendar_events': len(calendar_events),
                'upcoming_meetings': len([e for e in calendar_events if e.start_time and e.start_time > now]),
                'recent_meetings': len([e for e in calendar_events if e.start_time and e.start_time < now])
            }
            
            # Process calendar intelligence for knowledge base
            calendar_intelligence = self._extract_calendar_intelligence(calendar_events, people, now)
            
            return {
                'success': True,
                'user_email': user_email,
                'knowledge_base': {
                    'summary_stats': summary_stats,
                    'rich_contacts': rich_contacts,
                    'business_intelligence': {
                        'recent_decisions': sorted(business_decisions, 
                                                 key=lambda x: x['date'] or '', reverse=True)[:8],
                        'top_opportunities': sorted(opportunities,
                                                  key=lambda x: x['date'] or '', reverse=True)[:8],
                        'current_challenges': sorted(challenges,
                                                   key=lambda x: x['date'] or '', reverse=True)[:8]
                    },
                    'topic_knowledge': topic_knowledge,
                    'projects_summary': [
                        {
                            'name': project.name,
                            'description': project.description,
                            'status': project.status,
                            'priority': project.priority,
                            'stakeholders': project.stakeholders or [],
                            'key_topics': project.key_topics or []
                        }
                        for project in projects if project.description and len(project.description) > self.min_insight_length
                    ][:10],  # Top 10 substantial projects
                    'calendar_events': calendar_events,
                    'calendar_intelligence': calendar_intelligence
                }
            }
        
        except Exception as e:
            logger.error(f"Failed to get chat knowledge for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _create_professional_story(self, person: Person, emails: List[Email]) -> str:
        """Create a rich professional story for a contact based on email interactions"""
        try:
            # Find emails from this person
            person_emails = [e for e in emails if e.sender and person.email_address and 
                           e.sender.lower() == person.email_address.lower()]
            
            if not person_emails:
                return f"Professional contact with {person.relationship_type or 'business'} relationship."
            
            # Analyze communication patterns and content
            total_emails = len(person_emails)
            recent_emails = sorted(person_emails, key=lambda x: x.email_date or datetime.min, reverse=True)[:3]
            
            # Extract key themes from their communication
            themes = []
            for email in recent_emails:
                if email.ai_summary and len(email.ai_summary) > 20:
                    themes.append(email.ai_summary)
            
            # Create professional narrative
            story_parts = []
            
            if person.company and person.title:
                story_parts.append(f"{person.title} at {person.company}")
            elif person.company:
                story_parts.append(f"Works at {person.company}")
            elif person.title:
                story_parts.append(f"{person.title}")
            
            if total_emails > 1:
                story_parts.append(f"Active correspondence with {total_emails} substantive emails")
            
            if themes:
                story_parts.append(f"Recent discussions: {'; '.join(themes[:2])}")
            
            if person.relationship_type:
                story_parts.append(f"Relationship: {person.relationship_type}")
            
            return '. '.join(story_parts) if story_parts else "Professional business contact"
            
        except Exception as e:
            logger.error(f"Error creating professional story: {str(e)}")
            return "Professional business contact"
    
    def _deduplicate_and_rank(self, items: List[str]) -> List[str]:
        """Deduplicate similar items and rank by relevance"""
        if not items:
            return []
        
        # Simple deduplication by similarity (basic approach)
        unique_items = []
        for item in items:
            # Check if this item is too similar to existing ones
            is_duplicate = False
            for existing in unique_items:
                # Simple similarity check - if 70% of words overlap, consider duplicate
                item_words = set(item.lower().split())
                existing_words = set(existing.lower().split())
                
                if len(item_words) > 0 and len(existing_words) > 0:
                    overlap = len(item_words & existing_words)
                    similarity = overlap / min(len(item_words), len(existing_words))
                    if similarity > 0.7:
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                unique_items.append(item)
        
        # Rank by length and specificity (longer, more specific items are often better)
        unique_items.sort(key=lambda x: (len(x), len(x.split())), reverse=True)
        
        return unique_items

    def _get_user_business_context(self, user_id: int) -> Dict:
        """Get existing business context to enhance AI analysis"""
        try:
            # Get existing high-quality projects
            projects = get_db_manager().get_user_projects(user_id, limit=50)
            project_context = [p.name for p in projects if p.description and len(p.description) > 20]
            
            # Get existing high-quality people
            people = get_db_manager().get_user_people(user_id, limit=100)
            people_context = [f"{p.name} ({p.role or 'Unknown role'}) at {p.company or 'Unknown company'}" 
                             for p in people if p.name and not self._is_non_human_contact(p.email_address or '')]
            
            # Get existing topics
            topics = get_db_manager().get_user_topics(user_id, limit=100)
            topic_context = [t.name for t in topics if t.is_official]
            
            return {
                'existing_projects': project_context[:10],  # Top 10 projects
                'key_contacts': people_context[:20],  # Top 20 human contacts
                'official_topics': topic_context[:15]  # Top 15 official topics
            }
        except Exception as e:
            logger.error(f"Failed to get user business context: {str(e)}")
            return {'existing_projects': [], 'key_contacts': [], 'official_topics': []}
    
    def _filter_quality_emails_debug(self, emails: List[Email], user_email: str) -> List[Email]:
        """Enhanced filtering for quality-focused email processing - DEBUG VERSION WITH RELAXED FILTERS"""
        quality_emails = []
        
        for email in emails:
            logger.debug(f"Evaluating email from {email.sender} with subject: {email.subject}")
            
            # Skip emails from the user themselves - check both email and name
            if email.sender and user_email.lower() in email.sender.lower():
                logger.debug(f"Skipping email from user themselves: {email.sender}")
                continue
            
            # Also check sender name to catch cases where user's name appears as sender
            user_name_parts = user_email.split('@')[0].lower()  # Get username part
            sender_name = (email.sender_name or '').lower()
            if (sender_name and len(user_name_parts) > 3 and 
                user_name_parts in sender_name.replace('.', '').replace('_', '')):
                logger.debug(f"Skipping email from user by name: {sender_name}")
                continue

            # RELAXED: Only skip the most obvious non-human senders
            if self._is_obviously_non_human_contact(email.sender or ''):
                logger.debug(f"Skipping obviously non-human sender: {email.sender}")
                continue
                
            # RELAXED: Skip only obvious newsletters and promotional content
            if self._is_obvious_newsletter_or_promotional(email):
                logger.debug(f"Skipping obvious newsletter/promotional content")
                continue
                
            # RELAXED: Very permissive content length - just need some content
            content = email.body_clean or email.snippet or ''
            if len(content.strip()) < 10:  # Very permissive
                logger.debug(f"Skipping email with minimal content: {len(content)} chars")
                continue
                
            # RELAXED: Only skip very obvious automated emails
            subject_lower = (email.subject or '').lower()
            automated_subjects = ['automated', 'automatic reply', 'out of office']
            if any(pattern in subject_lower for pattern in automated_subjects) and len(content) < 50:
                logger.debug(f"Skipping automated email with subject: {email.subject}")
                continue
                
            logger.debug(f"Email passed quality filters: {email.sender}")
            quality_emails.append(email)
        
        logger.info(f"Quality filtering: {len(quality_emails)} emails passed out of {len(emails)} total")
        return quality_emails

    def _is_obviously_non_human_contact(self, email_address: str) -> bool:
        """RELAXED: Only filter obviously non-human contacts - for debugging"""
        if not email_address:
            return True
            
        email_lower = email_address.lower()
        
        # Only the most obvious non-human patterns
        obvious_non_human_patterns = [
            'noreply', 'no-reply', 'donotreply', 'mailer-daemon',
            'postmaster@', 'daemon@', 'bounce@', 'automated@',
            'robot@', 'bot@'
        ]
        
        # Check against obvious non-human patterns only
        for pattern in obvious_non_human_patterns:
            if pattern in email_lower:
                logger.debug(f"Obvious non-human pattern detected: {pattern} in {email_address}")
                return True
                
        return False

    def _is_obvious_newsletter_or_promotional(self, email: Email) -> bool:
        """RELAXED: Only filter obvious newsletters - for debugging"""
        if not email:
            return True
            
        sender = (email.sender or '').lower()
        subject = (email.subject or '').lower()
        content = (email.body_clean or email.snippet or '').lower()
        
        # Only check for very obvious newsletter patterns
        obvious_newsletter_patterns = [
            'substack.com', 'mailchimp.com', 'beehiiv.com',
            'unsubscribe', 'view in browser', 'manage preferences'
        ]
        
        # Check domain patterns
        for pattern in obvious_newsletter_patterns:
            if pattern in sender or pattern in content:
                logger.debug(f"Obvious newsletter pattern detected: {pattern}")
                return True
                
        return False

    def _validate_analysis_quality_debug(self, analysis: Dict) -> bool:
        """Validate that the analysis meets quality standards - VERY INCLUSIVE VERSION"""
        try:
            # VERY INCLUSIVE: Check strategic value score - very low threshold
            strategic_value = analysis.get('strategic_value_score', 0.7)  # Default to 0.7 if missing
            if strategic_value < 0.2:  # Only reject very low value
                logger.debug(f"Analysis rejected - very low strategic value: {strategic_value}")
                return False
            
            # VERY INCLUSIVE: Check summary quality - very reduced minimum length
            summary = analysis.get('summary', '')
            if len(summary) < 3:  # Almost any summary is acceptable
                logger.debug(f"Analysis rejected - summary too short: {len(summary)} chars")
                return False
            
            logger.debug(f"Analysis passed quality validation - strategic value: {strategic_value}")
            return True
            
        except Exception as e:
            logger.error(f"Error validating analysis quality: {str(e)}")
            return True  # Default to accepting if validation fails

    def _process_human_contacts_only_debug(self, user_id: int, analysis: Dict, email: Email) -> int:
        """Process people information with RELAXED human contact filtering - DEBUG VERSION"""
        people_count = 0
        
        logger.debug(f"Processing contacts for email from {email.sender}")
        logger.debug(f"Analysis contains sender_analysis: {'sender_analysis' in analysis}")
        logger.debug(f"Analysis contains people: {'people' in analysis}")
        
        # Process sender first (with relaxed human validation)
        sender_analysis = analysis.get('sender_analysis')
        if sender_analysis and email.sender:
            logger.debug(f"Sender analysis: {sender_analysis}")
            
            # RELAXED: Don't require is_human_contact flag, infer from context
            is_human = (sender_analysis.get('is_human_contact', True) and  # Default to True
                       not self._is_obviously_non_human_contact(email.sender))
            
            if is_human:
                logger.debug(f"Processing sender as human contact: {email.sender}")
                
                # Get existing person to accumulate knowledge
                existing_person = get_db_manager().find_person_by_email(user_id, email.sender)
                
                # Accumulate notes and context over time
                existing_notes = existing_person.notes if existing_person else ""
                new_relevance = sender_analysis.get('business_relevance', '')
                
                # Combine old and new notes intelligently
                accumulated_notes = existing_notes
                if new_relevance and new_relevance not in accumulated_notes:
                    if accumulated_notes:
                        accumulated_notes += f"\n\nRecent Context: {new_relevance}"
                    else:
                        accumulated_notes = new_relevance
                
                person_data = {
                    'email_address': email.sender,
                    'name': sender_analysis.get('name', email.sender_name or email.sender),
                    'role': sender_analysis.get('role') or (existing_person.role if existing_person else None),
                    'company': sender_analysis.get('company') or (existing_person.company if existing_person else None),
                    'relationship_type': sender_analysis.get('relationship') or (existing_person.relationship_type if existing_person else None),
                    'notes': accumulated_notes,  # Accumulated knowledge
                    'importance_level': max(0.8, existing_person.importance_level if existing_person else 0.8),  # Increment importance
                    'ai_version': self.version,
                    'total_emails': (existing_person.total_emails if existing_person else 0) + 1  # Increment email count
                }
                get_db_manager().create_or_update_person(user_id, person_data)
                people_count += 1
                logger.info(f"Created/updated person: {person_data['name']} ({person_data['email_address']})")
            else:
                logger.debug(f"Sender not processed as human contact: {email.sender}")
        
        # Process mentioned people (with relaxed validation)
        people_data = analysis.get('people', [])
        if isinstance(people_data, list):
            logger.debug(f"Processing {len(people_data)} mentioned people")
            for person_info in people_data:
                if person_info.get('email') or person_info.get('name'):
                    # RELAXED: Additional human validation but more permissive
                    email_addr = person_info.get('email', '')
                    if email_addr and self._is_obviously_non_human_contact(email_addr):
                        logger.debug(f"Skipping obviously non-human mentioned person: {email_addr}")
                        continue
                    
                    # Get existing person to accumulate knowledge
                    existing_person = None
                    if email_addr:
                        existing_person = get_db_manager().find_person_by_email(user_id, email_addr)
                    
                    # Accumulate knowledge
                    existing_notes = existing_person.notes if existing_person else ""
                    new_relevance = person_info.get('business_relevance', '')
                    
                    accumulated_notes = existing_notes
                    if new_relevance and new_relevance not in accumulated_notes:
                        if accumulated_notes:
                            accumulated_notes += f"\n\nMentioned Context: {new_relevance}"
                        else:
                            accumulated_notes = new_relevance
                    
                    person_data = {
                        'email_address': person_info.get('email'),
                        'name': person_info['name'],
                        'role': person_info.get('role') or (existing_person.role if existing_person else None),
                        'company': person_info.get('company') or (existing_person.company if existing_person else None),
                        'relationship_type': person_info.get('relationship') or (existing_person.relationship_type if existing_person else None),
                        'notes': accumulated_notes,  # Accumulated knowledge
                        'ai_version': self.version
                    }
                    get_db_manager().create_or_update_person(user_id, person_data)
                    people_count += 1
                    logger.info(f"Created/updated mentioned person: {person_data['name']}")
        
        logger.info(f"Total people processed for this email: {people_count}")
        return people_count

    def _filter_quality_emails(self, emails: List[Email], user_email: str) -> List[Email]:
        """Enhanced filtering for quality-focused email processing - BUSINESS FOCUSED"""
        quality_emails = []
        
        for email in emails:
            # ENHANCED: Skip emails from the user themselves - check both email and name
            if email.sender and user_email.lower() in email.sender.lower():
                continue
            
            # ENHANCED: Also check sender name to catch cases where user's name appears as sender
            user_name_parts = user_email.split('@')[0].lower()  # Get username part
            sender_name = (email.sender_name or '').lower()
            if (sender_name and len(user_name_parts) > 3 and 
                user_name_parts in sender_name.replace('.', '').replace('_', '')):
                continue

            # Skip non-human senders
            if self._is_non_human_contact(email.sender or ''):
                continue
                
            # ENHANCED: Skip newsletters and promotional content
            if self._is_newsletter_or_promotional(email):
                continue
                
            # RELAXED: Reduced minimum content length from 50 to 25
            content = email.body_clean or email.snippet or ''
            if len(content.strip()) < 25:  # Much more permissive
                continue
                
            # RELAXED: More permissive automated subject filtering
            subject_lower = (email.subject or '').lower()
            automated_subjects = ['automatic', 'notification', 'alert']  # Removed 're:', 'fwd:', 'update', 'reminder'
            # Only skip if BOTH automated subject AND very short content
            if any(pattern in subject_lower for pattern in automated_subjects) and len(content) < 100:  # More lenient
                continue
                
            # EXPANDED: More business indicators to catch valuable emails
            business_indicators = [
                'meeting', 'project', 'proposal', 'contract', 'agreement',
                'decision', 'feedback', 'review', 'discussion', 'strategy',
                'client', 'customer', 'partner', 'collaboration', 'opportunity',
                'budget', 'funding', 'investment', 'deal', 'business',
                'follow up', 'followup', 'call', 'schedule', 'deadline',
                'urgent', 'important', 'action', 'update', 'progress',
                'team', 'work', 'development', 'launch', 'release'
            ]
            
            has_business_content = any(indicator in content.lower() or indicator in subject_lower 
                                     for indicator in business_indicators)
            
            # MORE INCLUSIVE: Accept if business content OR longer than 150 chars (reduced from 300)
            if has_business_content or len(content) > 150:
                quality_emails.append(email)
            # ADDED: Also include emails with meaningful sender names (not just email addresses)
            elif email.sender_name and len(email.sender_name) > 3 and len(content) > 50:
                quality_emails.append(email)
        
        # Sort by email date (newest first) and content length (longer = potentially more substantial)
        quality_emails.sort(key=lambda e: (e.email_date or datetime.min, len(e.body_clean or e.snippet or '')), reverse=True)
        
        return quality_emails

    def _is_newsletter_or_promotional(self, email: Email) -> bool:
        """Detect and filter out newsletters, promotional emails, and automated content"""
        if not email:
            return True
            
        sender = (email.sender or '').lower()
        subject = (email.subject or '').lower()
        content = (email.body_clean or email.snippet or '').lower()
        sender_name = (email.sender_name or '').lower()
        
        # Newsletter domains and patterns
        newsletter_domains = [
            'substack.com', 'mailchimp.com', 'constantcontact.com', 'campaign-archive.com',
            'beehiiv.com', 'ghost.org', 'medium.com', 'linkedin.com/pulse',
            'newsletter', 'mail.', 'noreply', 'no-reply', 'donotreply', 'marketing',
            'promotions', 'offers', 'deals', 'sales', 'campaigns'
        ]
        
        # Newsletter subject patterns
        newsletter_subjects = [
            'newsletter', 'weekly digest', 'daily digest', 'roundup', 'briefing',
            'this week in', 'weekly update', 'monthly update', 'startup digest',
            'tech digest', 'vc corner', 'venture capital', 'investment newsletter',
            'industry news', 'market update', 'funding round', 'startup funding'
        ]
        
        # Newsletter content patterns
        newsletter_content = [
            'unsubscribe', 'view in browser', 'manage preferences', 'update subscription',
            'forward to a friend', 'share this newsletter', 'subscriber', 'mailing list',
            'this email was sent to', 'you are receiving this', 'promotional email'
        ]
        
        # Newsletter sender name patterns
        newsletter_names = [
            'newsletter', 'digest', 'briefing', 'update', 'news', 'weekly',
            'daily', 'monthly', 'roundup', 'vc corner', 'startup', 'lenny',
            'substack', 'medium', 'ghost'
        ]
        
        # Check domain patterns
        for domain in newsletter_domains:
            if domain in sender:
                return True
        
        # Check subject patterns
        for pattern in newsletter_subjects:
            if pattern in subject:
                return True
            
        # Check content patterns
        for pattern in newsletter_content:
            if pattern in content:
                return True
        
        # Check sender name patterns
        for pattern in newsletter_names:
            if pattern in sender_name:
                return True
        
        # Additional heuristics for promotional content
        promotional_indicators = [
            'special offer', 'limited time', 'exclusive deal', 'discount',
            'sale ends', 'act now', 'don\'t miss', 'free trial', 'premium upgrade',
            'webinar invitation', 'event invitation', 'conference', 'summit'
        ]
        
        promotional_count = sum(1 for indicator in promotional_indicators 
                               if indicator in content or indicator in subject)
        
        # If multiple promotional indicators, likely promotional
        if promotional_count >= 2:
            return True
        
        # Check for mass email patterns
        mass_email_patterns = [
            'dear valued', 'dear customer', 'dear subscriber', 'dear member',
            'greetings', 'hello there', 'hi everyone', 'dear all'
        ]
        
        for pattern in mass_email_patterns:
            if pattern in content[:200]:  # Check first 200 chars
                return True
        
        return False

    def _is_non_human_contact(self, email_address: str) -> bool:
        """Determine if an email address belongs to a non-human sender - BALANCED VERSION"""
        if not email_address:
            return True
            
        email_lower = email_address.lower()
        
        # FOCUSED: Only filter obvious automation, preserve business contacts
        definite_non_human_patterns = [
            'noreply', 'no-reply', 'donotreply', 'do-not-reply', 
            'mailer-daemon', 'postmaster@', 'daemon@', 'bounce@',
            'robot@', 'bot@', 'automated@', 'system@notification',
            'newsletter@', 'digest@', 'updates@notifications'
        ]
        
        # Check against definite non-human patterns only
        for pattern in definite_non_human_patterns:
            if pattern in email_lower:
                return True
        
        # SPECIFIC: Only filter major newsletter/automation services
        automation_domains = [
            'substack.com', 'beehiiv.com', 'mailchimp.com', 'constantcontact.com',
            'campaign-archive.com', 'sendgrid.net', 'mailgun.org', 'mandrill.com'
        ]
        
        for domain in automation_domains:
            if domain in email_lower:
                return True
        
        # PRESERVE: Keep business contacts that might use standard business email patterns
        # Removed: 'admin@', 'info@', 'contact@', 'help@', 'service@', 'team@', 'hello@', 'hi@'
        # Removed: 'linkedin.com', 'facebook.com', etc. - people use these for business
                
        return False

    def _extract_calendar_intelligence(self, calendar_events: List, people: List, now: datetime) -> Dict:
        """Extract calendar intelligence from events for knowledge base"""
        try:
            upcoming_events = []
            recent_events = []
            meeting_patterns = {}
            attendee_frequency = {}
            
            # Process calendar events
            for event in calendar_events:
                if not hasattr(event, 'start_time') or not event.start_time:
                    continue
                    
                # Categorize by time
                if event.start_time > now:
                    # Upcoming events
                    upcoming_events.append({
                        'title': event.title or 'Untitled Meeting',
                        'start_time': event.start_time.isoformat() if event.start_time else None,
                        'attendees': len(event.attendee_emails or []),
                        'meeting_type': getattr(event, 'meeting_type', 'unknown')
                    })
                else:
                    # Recent events for context
                    recent_events.append({
                        'title': event.title or 'Untitled Meeting',
                        'start_time': event.start_time.isoformat() if event.start_time else None,
                        'attendees': len(event.attendee_emails or [])
                    })
                
                # Track meeting patterns
                if hasattr(event, 'meeting_type') and event.meeting_type:
                    meeting_patterns[event.meeting_type] = meeting_patterns.get(event.meeting_type, 0) + 1
                
                # Track attendee frequency for relationship intelligence
                if hasattr(event, 'attendee_emails') and event.attendee_emails:
                    for attendee_email in event.attendee_emails:
                        attendee_frequency[attendee_email] = attendee_frequency.get(attendee_email, 0) + 1
            
            # Get top attendees for relationship context
            top_attendees = sorted(attendee_frequency.items(), key=lambda x: x[1], reverse=True)[:10]
            
            # Map attendees to known people
            attendee_insights = []
            for attendee_email, meeting_count in top_attendees:
                # Find matching person in people database
                matching_person = None
                for person in people:
                    if hasattr(person, 'email_address') and person.email_address == attendee_email:
                        matching_person = person
                        break
                
                if matching_person:
                    attendee_insights.append({
                        'name': matching_person.name,
                        'email': attendee_email,
                        'meeting_frequency': meeting_count,
                        'company': getattr(matching_person, 'company', None),
                        'relationship': getattr(matching_person, 'relationship_type', None)
                    })
                else:
                    attendee_insights.append({
                        'name': attendee_email.split('@')[0],  # Use email username as fallback
                        'email': attendee_email,
                        'meeting_frequency': meeting_count,
                        'company': None,
                        'relationship': 'unknown'
                    })
            
            return {
                'upcoming_events': sorted(upcoming_events, key=lambda x: x['start_time'] or '')[:5],
                'recent_events': sorted(recent_events, key=lambda x: x['start_time'] or '', reverse=True)[:5],
                'meeting_patterns': meeting_patterns,
                'frequent_attendees': attendee_insights,
                'total_upcoming': len(upcoming_events),
                'total_recent': len(recent_events)
            }
            
        except Exception as e:
            logger.error(f"Failed to extract calendar intelligence: {str(e)}")
            return {
                'upcoming_events': [],
                'recent_events': [],
                'meeting_patterns': {},
                'frequent_attendees': [],
                'total_upcoming': 0,
                'total_recent': 0
            }

# Global instance
email_intelligence = EmailIntelligenceProcessor() 

============================================================
FILE: archive/refactor_docs/1_enhanced_DB.txt
============================================================
# Enhanced Database Models for Entity-Centric Intelligence

from sqlalchemy import Column, Integer, String, Text, DateTime, Boolean, Float, ForeignKey, Table, JSON
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime

Base = declarative_base()

# Association Tables for Many-to-Many Relationships
person_topic_association = Table(
    'person_topics',
    Base.metadata,
    Column('person_id', Integer, ForeignKey('people.id')),
    Column('topic_id', Integer, ForeignKey('topics.id')),
    Column('affinity_score', Float, default=0.5),  # How connected this person is to this topic
    Column('created_at', DateTime, default=datetime.utcnow),
    Column('last_interaction', DateTime, default=datetime.utcnow)
)

task_topic_association = Table(
    'task_topics',
    Base.metadata,
    Column('task_id', Integer, ForeignKey('tasks.id')),
    Column('topic_id', Integer, ForeignKey('topics.id')),
    Column('relevance_score', Float, default=0.5),
    Column('created_at', DateTime, default=datetime.utcnow)
)

event_topic_association = Table(
    'event_topics', 
    Base.metadata,
    Column('event_id', Integer, ForeignKey('calendar_events.id')),
    Column('topic_id', Integer, ForeignKey('topics.id')),
    Column('relevance_score', Float, default=0.5),
    Column('created_at', DateTime, default=datetime.utcnow)
)

class Topic(Base):
    """Topics as the central brain - persistent memory containers"""
    __tablename__ = 'topics'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    name = Column(String(200), nullable=False)
    description = Column(Text)
    keywords = Column(Text)  # Comma-separated for now, can be normalized later
    is_official = Column(Boolean, default=False)
    confidence_score = Column(Float, default=0.5)
    
    # Intelligence accumulation fields
    total_mentions = Column(Integer, default=0)
    last_mentioned = Column(DateTime)
    intelligence_summary = Column(Text)  # AI-generated summary of what we know about this topic
    strategic_importance = Column(Float, default=0.5)  # How important this topic is to the user
    
    # Topic evolution tracking
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    version = Column(Integer, default=1)  # Track topic evolution
    
    # Relationships - Topics as the central hub
    people = relationship("Person", secondary=person_topic_association, back_populates="topics")
    tasks = relationship("Task", secondary=task_topic_association, back_populates="topics")
    events = relationship("CalendarEvent", secondary=event_topic_association, back_populates="topics")
    
    # Direct content relationships
    emails = relationship("Email", back_populates="primary_topic")
    projects = relationship("Project", back_populates="primary_topic")

class Person(Base):
    """Enhanced Person model with relationship intelligence"""
    __tablename__ = 'people'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    name = Column(String(200), nullable=False)
    email_address = Column(String(255))
    phone = Column(String(50))
    company = Column(String(200))
    title = Column(String(200))
    
    # Relationship intelligence
    relationship_type = Column(String(100))  # colleague, client, partner, etc.
    importance_level = Column(Float, default=0.5)
    communication_frequency = Column(String(50))  # daily, weekly, monthly, etc.
    last_contact = Column(DateTime)
    total_interactions = Column(Integer, default=0)
    
    # Professional context (extracted from signatures, etc.)
    linkedin_url = Column(String(255))
    bio = Column(Text)
    professional_story = Column(Text)  # AI-generated summary of professional relationship
    
    # Intelligence accumulation
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    topics = relationship("Topic", secondary=person_topic_association, back_populates="people")
    tasks_assigned = relationship("Task", foreign_keys="Task.assignee_id", back_populates="assignee")
    tasks_mentioned = relationship("Task", foreign_keys="Task.mentioned_person_id", back_populates="mentioned_person")

class Task(Base):
    """Enhanced Task model with full context"""
    __tablename__ = 'tasks'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    description = Column(Text, nullable=False)
    context_story = Column(Text)  # WHY this task exists - the narrative context
    
    # Assignments and ownership
    assignee_id = Column(Integer, ForeignKey('people.id'))
    mentioned_person_id = Column(Integer, ForeignKey('people.id'))  # Person mentioned in task
    
    # Task metadata
    priority = Column(String(20), default='medium')
    status = Column(String(20), default='pending')
    category = Column(String(100))
    confidence = Column(Float, default=0.8)
    
    # Source tracking
    source_email_id = Column(Integer, ForeignKey('emails.id'))
    source_event_id = Column(Integer, ForeignKey('calendar_events.id'))
    
    # Temporal information
    due_date = Column(DateTime)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    completed_at = Column(DateTime)
    
    # Relationships
    topics = relationship("Topic", secondary=task_topic_association, back_populates="tasks")
    assignee = relationship("Person", foreign_keys=[assignee_id], back_populates="tasks_assigned")
    mentioned_person = relationship("Person", foreign_keys=[mentioned_person_id], back_populates="tasks_mentioned")
    source_email = relationship("Email", back_populates="generated_tasks")

class Email(Base):
    """Streamlined Email model focused on intelligence, not storage"""
    __tablename__ = 'emails'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    gmail_id = Column(String(255), unique=True, nullable=False)
    
    # Essential metadata only
    subject = Column(String(500))
    sender = Column(String(255))
    sender_name = Column(String(255))
    recipient_emails = Column(Text)  # JSON array of recipients
    email_date = Column(DateTime)
    
    # Intelligence fields
    ai_summary = Column(Text)  # Concise summary for display
    business_category = Column(String(100))  # meeting, project, decision, etc.
    sentiment = Column(String(50))
    urgency_score = Column(Float, default=0.5)
    strategic_importance = Column(Float, default=0.5)
    
    # Content storage strategy: metadata in DB, content in blob storage
    content_hash = Column(String(64))  # SHA-256 of content for deduplication
    blob_storage_key = Column(String(255))  # Reference to external content storage
    
    # Primary topic assignment (Topics as brain concept)
    primary_topic_id = Column(Integer, ForeignKey('topics.id'))
    
    # Processing metadata
    processed_at = Column(DateTime)
    processing_version = Column(String(50))
    
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    primary_topic = relationship("Topic", back_populates="emails")
    generated_tasks = relationship("Task", back_populates="source_email")

class CalendarEvent(Base):
    """Enhanced Calendar model with business intelligence"""
    __tablename__ = 'calendar_events'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    google_event_id = Column(String(255), unique=True, nullable=False)
    
    # Event basics
    title = Column(String(500))
    description = Column(Text)
    location = Column(String(500))
    start_time = Column(DateTime)
    end_time = Column(DateTime)
    
    # Business intelligence
    business_context = Column(Text)  # AI-generated context about meeting purpose
    attendee_intelligence = Column(Text)  # Summary of known attendees and relationships
    preparation_priority = Column(Float, default=0.5)  # How important prep is for this meeting
    
    # Meeting outcome tracking
    outcome_summary = Column(Text)  # Post-meeting AI analysis
    follow_up_needed = Column(Boolean, default=False)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    topics = relationship("Topic", secondary=event_topic_association, back_populates="events")

class Project(Base):
    """Projects as coherent business initiatives"""
    __tablename__ = 'projects'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    name = Column(String(200), nullable=False)
    description = Column(Text)
    status = Column(String(50), default='active')
    priority = Column(String(20), default='medium')
    
    # Project intelligence
    stakeholder_summary = Column(Text)  # AI summary of key people involved
    objective = Column(Text)
    current_phase = Column(String(100))
    challenges = Column(Text)
    opportunities = Column(Text)
    
    # Primary topic assignment
    primary_topic_id = Column(Integer, ForeignKey('topics.id'))
    
    # Timeline
    start_date = Column(DateTime)
    target_completion = Column(DateTime)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    primary_topic = relationship("Topic", back_populates="projects")

class EntityRelationship(Base):
    """Track relationships between any entities for advanced intelligence"""
    __tablename__ = 'entity_relationships'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    
    # Generic entity references
    entity_type_a = Column(String(50), nullable=False)  # person, topic, project, etc.
    entity_id_a = Column(Integer, nullable=False)
    entity_type_b = Column(String(50), nullable=False)
    entity_id_b = Column(Integer, nullable=False)
    
    # Relationship metadata
    relationship_type = Column(String(100))  # collaborates_on, leads, discusses, etc.
    strength = Column(Float, default=0.5)  # How strong this relationship is
    frequency = Column(String(50))  # How often they interact
    
    # Intelligence context
    context_summary = Column(Text)  # AI summary of this relationship
    last_interaction = Column(DateTime)
    total_interactions = Column(Integer, default=1)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

class IntelligenceInsight(Base):
    """Capture proactive insights generated by the system"""
    __tablename__ = 'intelligence_insights'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    
    # Insight metadata
    insight_type = Column(String(100), nullable=False)  # relationship_alert, opportunity, decision_needed
    title = Column(String(200), nullable=False)
    description = Column(Text, nullable=False)
    priority = Column(String(20), default='medium')
    confidence = Column(Float, default=0.8)
    
    # Entity connections
    related_entity_type = Column(String(50))  # What entity triggered this insight
    related_entity_id = Column(Integer)
    
    # User interaction
    status = Column(String(50), default='new')  # new, viewed, acted_on, dismissed
    user_feedback = Column(String(50))  # helpful, not_helpful, etc.
    
    # Temporal
    expires_at = Column(DateTime)  # Some insights are time-sensitive
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

# Migration strategy: Create these tables alongside existing ones,
# then populate with data transformation scripts

============================================================
FILE: archive/refactor_docs/7_proactive_analytics_engine.txt
============================================================
# Predictive Analytics Engine - Future Intelligence
# This transforms your system from reactive to genuinely predictive

import logging
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
from dataclasses import dataclass
import json
from collections import defaultdict, deque
import threading
import time

from processors.unified_entity_engine import entity_engine, EntityContext
from models.enhanced_models import Person, Topic, Task, CalendarEvent, Email, IntelligenceInsight
from models.database import get_db_manager

logger = logging.getLogger(__name__)

@dataclass
class PredictionResult:
    prediction_type: str
    confidence: float
    predicted_value: Any
    reasoning: str
    time_horizon: str  # short_term, medium_term, long_term
    data_points_used: int
    created_at: datetime

@dataclass
class TrendPattern:
    entity_type: str
    entity_id: int
    pattern_type: str  # growth, decline, cyclical, volatile
    strength: float
    confidence: float
    data_points: List[Tuple[datetime, float]]
    prediction: Optional[float] = None

class PredictiveAnalytics:
    """
    Advanced predictive analytics engine that learns from patterns and predicts future states.
    This is what makes your system truly intelligent - anticipating rather than just reacting.
    """
    
    def __init__(self):
        self.pattern_cache = {}
        self.prediction_cache = {}
        self.learning_models = {}
        self.pattern_detection_thread = None
        self.running = False
        
    def start(self):
        """Start the predictive analytics engine"""
        self.running = True
        self.pattern_detection_thread = threading.Thread(
            target=self._continuous_pattern_detection, 
            name="PredictiveAnalytics"
        )
        self.pattern_detection_thread.daemon = True
        self.pattern_detection_thread.start()
        logger.info("Started predictive analytics engine")
    
    def stop(self):
        """Stop the predictive analytics engine"""
        self.running = False
        if self.pattern_detection_thread:
            self.pattern_detection_thread.join(timeout=5)
        logger.info("Stopped predictive analytics engine")
    
    # =====================================================================
    # RELATIONSHIP PREDICTION METHODS
    # =====================================================================
    
    def predict_relationship_opportunities(self, user_id: int) -> List[PredictionResult]:
        """Predict relationship opportunities and networking needs"""
        predictions = []
        
        try:
            with get_db_manager().get_session() as session:
                # Get all people and their interaction patterns
                people = session.query(Person).filter(Person.user_id == user_id).all()
                
                for person in people:
                    # Predict relationship decay
                    decay_prediction = self._predict_relationship_decay(person)
                    if decay_prediction:
                        predictions.append(decay_prediction)
                    
                    # Predict optimal contact timing
                    contact_prediction = self._predict_optimal_contact_time(person)
                    if contact_prediction:
                        predictions.append(contact_prediction)
                
                # Predict networking opportunities
                network_predictions = self._predict_networking_opportunities(people)
                predictions.extend(network_predictions)
                
        except Exception as e:
            logger.error(f"Failed to predict relationship opportunities: {str(e)}")
        
        return predictions
    
    def _predict_relationship_decay(self, person: Person) -> Optional[PredictionResult]:
        """Predict if a relationship is at risk of decay"""
        if not person.last_contact or person.total_interactions < 3:
            return None
        
        days_since_contact = (datetime.utcnow() - person.last_contact).days
        importance = person.importance_level or 0.5
        
        # Calculate decay risk based on importance and recency
        expected_contact_frequency = self._calculate_expected_frequency(person)
        decay_risk = min(1.0, days_since_contact / expected_contact_frequency)
        
        if decay_risk > 0.7 and importance > 0.6:
            return PredictionResult(
                prediction_type='relationship_decay_risk',
                confidence=decay_risk,
                predicted_value=f"High risk of relationship decay with {person.name}",
                reasoning=f"No contact for {days_since_contact} days, expected frequency is {expected_contact_frequency} days",
                time_horizon='short_term',
                data_points_used=person.total_interactions,
                created_at=datetime.utcnow()
            )
        
        return None
    
    def _predict_optimal_contact_time(self, person: Person) -> Optional[PredictionResult]:
        """Predict optimal time to contact someone"""
        if not person.last_contact or person.total_interactions < 2:
            return None
        
        # Analyze historical contact patterns
        contact_pattern = self._analyze_contact_pattern(person)
        
        if contact_pattern and contact_pattern['confidence'] > 0.6:
            next_optimal = contact_pattern['next_optimal_date']
            days_until = (next_optimal - datetime.utcnow()).days
            
            if 0 <= days_until <= 7:  # Within next week
                return PredictionResult(
                    prediction_type='optimal_contact_timing',
                    confidence=contact_pattern['confidence'],
                    predicted_value=next_optimal,
                    reasoning=f"Based on historical pattern, optimal contact window approaching",
                    time_horizon='short_term',
                    data_points_used=person.total_interactions,
                    created_at=datetime.utcnow()
                )
        
        return None
    
    def _predict_networking_opportunities(self, people: List[Person]) -> List[PredictionResult]:
        """Predict networking opportunities based on relationship graph"""
        predictions = []
        
        try:
            # Build relationship graph
            relationship_graph = self._build_relationship_graph(people)
            
            # Find potential introductions
            for person_a in people:
                if (person_a.importance_level or 0) > 0.7:
                    # Find people who might benefit from meeting person_a
                    potential_matches = self._find_introduction_opportunities(
                        person_a, people, relationship_graph
                    )
                    
                    for match_person, confidence in potential_matches:
                        if confidence > 0.6:
                            predictions.append(PredictionResult(
                                prediction_type='networking_opportunity',
                                confidence=confidence,
                                predicted_value=f"Introduction opportunity: {person_a.name}  {match_person.name}",
                                reasoning=f"Complementary professional contexts and mutual benefit potential",
                                time_horizon='medium_term',
                                data_points_used=len(people),
                                created_at=datetime.utcnow()
                            ))
            
        except Exception as e:
            logger.error(f"Failed to predict networking opportunities: {str(e)}")
        
        return predictions
    
    # =====================================================================
    # TOPIC MOMENTUM PREDICTION
    # =====================================================================
    
    def predict_topic_trends(self, user_id: int) -> List[PredictionResult]:
        """Predict which topics will become important"""
        predictions = []
        
        try:
            with get_db_manager().get_session() as session:
                # Get topics with historical data
                topics = session.query(Topic).filter(
                    Topic.user_id == user_id,
                    Topic.total_mentions > 1
                ).all()
                
                for topic in topics:
                    # Analyze topic momentum
                    momentum_prediction = self._predict_topic_momentum(topic, session)
                    if momentum_prediction:
                        predictions.append(momentum_prediction)
                
                # Predict emerging topics
                emerging_predictions = self._predict_emerging_topics(user_id, session)
                predictions.extend(emerging_predictions)
                
        except Exception as e:
            logger.error(f"Failed to predict topic trends: {str(e)}")
        
        return predictions
    
    def _predict_topic_momentum(self, topic: Topic, session) -> Optional[PredictionResult]:
        """Predict if a topic will gain or lose momentum"""
        # Get historical mention pattern
        mention_history = self._get_topic_mention_history(topic, session)
        
        if len(mention_history) < 3:
            return None
        
        # Analyze trend
        trend_analysis = self._analyze_time_series_trend(mention_history)
        
        if trend_analysis['confidence'] > 0.6:
            prediction_type = 'topic_momentum_increase' if trend_analysis['direction'] > 0 else 'topic_momentum_decrease'
            
            return PredictionResult(
                prediction_type=prediction_type,
                confidence=trend_analysis['confidence'],
                predicted_value=trend_analysis['predicted_next_value'],
                reasoning=f"Topic showing {trend_analysis['direction_text']} trend over {len(mention_history)} data points",
                time_horizon='medium_term',
                data_points_used=len(mention_history),
                created_at=datetime.utcnow()
            )
        
        return None
    
    def _predict_emerging_topics(self, user_id: int, session) -> List[PredictionResult]:
        """Predict topics that will emerge based on early signals"""
        predictions = []
        
        try:
            # Look at recent emails for early topic signals
            recent_emails = session.query(Email).filter(
                Email.user_id == user_id,
                Email.email_date > datetime.utcnow() - timedelta(days=7)
            ).all()
            
            # Analyze keywords and patterns in recent communications
            keyword_patterns = self._analyze_emerging_keyword_patterns(recent_emails)
            
            for pattern in keyword_patterns:
                if pattern['emergence_score'] > 0.7:
                    predictions.append(PredictionResult(
                        prediction_type='emerging_topic',
                        confidence=pattern['emergence_score'],
                        predicted_value=pattern['keywords'],
                        reasoning=f"Early signals detected in recent communications",
                        time_horizon='short_term',
                        data_points_used=len(recent_emails),
                        created_at=datetime.utcnow()
                    ))
            
        except Exception as e:
            logger.error(f"Failed to predict emerging topics: {str(e)}")
        
        return predictions
    
    # =====================================================================
    # BUSINESS OPPORTUNITY PREDICTION
    # =====================================================================
    
    def predict_business_opportunities(self, user_id: int) -> List[PredictionResult]:
        """Predict business opportunities based on communication patterns"""
        predictions = []
        
        try:
            # Predict meeting outcomes
            meeting_predictions = self._predict_meeting_outcomes(user_id)
            predictions.extend(meeting_predictions)
            
            # Predict project opportunities
            project_predictions = self._predict_project_opportunities(user_id)
            predictions.extend(project_predictions)
            
            # Predict decision timing
            decision_predictions = self._predict_decision_timing(user_id)
            predictions.extend(decision_predictions)
            
        except Exception as e:
            logger.error(f"Failed to predict business opportunities: {str(e)}")
        
        return predictions
    
    def _predict_meeting_outcomes(self, user_id: int) -> List[PredictionResult]:
        """Predict likely outcomes of upcoming meetings"""
        predictions = []
        
        try:
            with get_db_manager().get_session() as session:
                # Get upcoming meetings
                upcoming_meetings = session.query(CalendarEvent).filter(
                    CalendarEvent.user_id == user_id,
                    CalendarEvent.start_time > datetime.utcnow(),
                    CalendarEvent.start_time < datetime.utcnow() + timedelta(days=7)
                ).all()
                
                for meeting in upcoming_meetings:
                    outcome_prediction = self._analyze_meeting_success_probability(meeting, session)
                    if outcome_prediction:
                        predictions.append(outcome_prediction)
            
        except Exception as e:
            logger.error(f"Failed to predict meeting outcomes: {str(e)}")
        
        return predictions
    
    def _predict_project_opportunities(self, user_id: int) -> List[PredictionResult]:
        """Predict potential project opportunities from communication patterns"""
        predictions = []
        
        try:
            with get_db_manager().get_session() as session:
                # Analyze recent communications for project signals
                recent_emails = session.query(Email).filter(
                    Email.user_id == user_id,
                    Email.email_date > datetime.utcnow() - timedelta(days=14),
                    Email.strategic_importance > 0.6
                ).all()
                
                project_signals = self._detect_project_formation_signals(recent_emails)
                
                for signal in project_signals:
                    if signal['probability'] > 0.6:
                        predictions.append(PredictionResult(
                            prediction_type='project_opportunity',
                            confidence=signal['probability'],
                            predicted_value=signal['description'],
                            reasoning=signal['reasoning'],
                            time_horizon='medium_term',
                            data_points_used=len(recent_emails),
                            created_at=datetime.utcnow()
                        ))
            
        except Exception as e:
            logger.error(f"Failed to predict project opportunities: {str(e)}")
        
        return predictions
    
    # =====================================================================
    # PATTERN DETECTION AND ANALYSIS
    # =====================================================================
    
    def _continuous_pattern_detection(self):
        """Continuously detect patterns in user data"""
        while self.running:
            try:
                # Get active users for pattern analysis
                active_users = self._get_active_users_for_analysis()
                
                for user_id in active_users:
                    # Detect new patterns
                    patterns = self._detect_user_patterns(user_id)
                    
                    # Update pattern cache
                    self.pattern_cache[user_id] = patterns
                    
                    # Generate predictions based on patterns
                    predictions = self._generate_pattern_based_predictions(user_id, patterns)
                    
                    # Store high-confidence predictions
                    self._store_predictions(user_id, predictions)
                
                # Sleep for analysis interval (every 2 hours)
                time.sleep(7200)
                
            except Exception as e:
                logger.error(f"Error in continuous pattern detection: {str(e)}")
                time.sleep(300)  # Sleep 5 minutes on error
    
    def _detect_user_patterns(self, user_id: int) -> Dict[str, List[TrendPattern]]:
        """Detect patterns in user's business intelligence data"""
        patterns = {
            'communication_patterns': [],
            'topic_patterns': [],
            'relationship_patterns': [],
            'temporal_patterns': []
        }
        
        try:
            with get_db_manager().get_session() as session:
                # Detect communication patterns
                comm_patterns = self._detect_communication_patterns(user_id, session)
                patterns['communication_patterns'] = comm_patterns
                
                # Detect topic evolution patterns
                topic_patterns = self._detect_topic_evolution_patterns(user_id, session)
                patterns['topic_patterns'] = topic_patterns
                
                # Detect relationship patterns
                rel_patterns = self._detect_relationship_evolution_patterns(user_id, session)
                patterns['relationship_patterns'] = rel_patterns
                
                # Detect temporal patterns (time-based behaviors)
                temporal_patterns = self._detect_temporal_patterns(user_id, session)
                patterns['temporal_patterns'] = temporal_patterns
            
        except Exception as e:
            logger.error(f"Failed to detect user patterns: {str(e)}")
        
        return patterns
    
    # =====================================================================
    # ADVANCED ANALYTICS HELPER METHODS
    # =====================================================================
    
    def _analyze_time_series_trend(self, data_points: List[Tuple[datetime, float]]) -> Dict:
        """Analyze trend in time series data"""
        if len(data_points) < 3:
            return {'confidence': 0, 'direction': 0}
        
        # Sort by time
        sorted_points = sorted(data_points, key=lambda x: x[0])
        
        # Extract values and calculate simple linear regression
        values = [point[1] for point in sorted_points]
        n = len(values)
        x = list(range(n))
        
        # Calculate slope using least squares
        x_mean = sum(x) / n
        y_mean = sum(values) / n
        
        numerator = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n))
        denominator = sum((x[i] - x_mean) ** 2 for i in range(n))
        
        if denominator == 0:
            return {'confidence': 0, 'direction': 0}
        
        slope = numerator / denominator
        
        # Calculate R-squared for confidence
        y_pred = [slope * (i - x_mean) + y_mean for i in x]
        ss_res = sum((values[i] - y_pred[i]) ** 2 for i in range(n))
        ss_tot = sum((values[i] - y_mean) ** 2 for i in range(n))
        
        r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
        
        # Predict next value
        next_value = slope * n + y_mean
        
        return {
            'confidence': max(0, min(1, r_squared)),
            'direction': 1 if slope > 0 else -1 if slope < 0 else 0,
            'direction_text': 'increasing' if slope > 0 else 'decreasing' if slope < 0 else 'stable',
            'slope': slope,
            'predicted_next_value': max(0, next_value)
        }
    
    def _calculate_expected_frequency(self, person: Person) -> int:
        """Calculate expected contact frequency for a person"""
        base_frequency = 30  # Default 30 days
        
        # Adjust based on importance
        importance_factor = (person.importance_level or 0.5)
        frequency = base_frequency * (1 - importance_factor * 0.7)
        
        # Adjust based on relationship type
        relationship_adjustments = {
            'colleague': 0.7,
            'client': 0.5,
            'partner': 0.6,
            'manager': 0.4,
            'friend': 0.8
        }
        
        rel_type = person.relationship_type or 'contact'
        adjustment = relationship_adjustments.get(rel_type.lower(), 1.0)
        
        return max(7, int(frequency * adjustment))
    
    def _analyze_contact_pattern(self, person: Person) -> Optional[Dict]:
        """Analyze historical contact pattern for a person"""
        # This would analyze email timestamps to detect patterns
        # For now, return a simplified pattern based on importance
        
        if not person.last_contact or person.total_interactions < 3:
            return None
        
        expected_freq = self._calculate_expected_frequency(person)
        days_since = (datetime.utcnow() - person.last_contact).days
        
        # Simple pattern: next optimal contact based on expected frequency
        next_optimal = person.last_contact + timedelta(days=expected_freq)
        confidence = min(1.0, person.total_interactions / 10)
        
        return {
            'next_optimal_date': next_optimal,
            'confidence': confidence,
            'pattern_type': 'regular'
        }
    
    def _build_relationship_graph(self, people: List[Person]) -> Dict:
        """Build a relationship graph for networking analysis"""
        # Simplified relationship graph based on shared topics and companies
        graph = defaultdict(list)
        
        for i, person_a in enumerate(people):
            for j, person_b in enumerate(people[i+1:], i+1):
                # Calculate relationship strength
                strength = self._calculate_relationship_strength(person_a, person_b)
                if strength > 0.3:
                    graph[person_a.id].append((person_b.id, strength))
                    graph[person_b.id].append((person_a.id, strength))
        
        return dict(graph)
    
    def _calculate_relationship_strength(self, person_a: Person, person_b: Person) -> float:
        """Calculate relationship strength between two people"""
        strength = 0.0
        
        # Same company
        if person_a.company and person_b.company and person_a.company == person_b.company:
            strength += 0.4
        
        # Similar titles
        if person_a.title and person_b.title:
            # Simple word overlap
            title_a_words = set(person_a.title.lower().split())
            title_b_words = set(person_b.title.lower().split())
            overlap = len(title_a_words & title_b_words)
            if overlap > 0:
                strength += 0.2
        
        # Shared topics (would need to implement topic relationships)
        # For now, use importance levels as proxy
        importance_similarity = 1 - abs((person_a.importance_level or 0.5) - (person_b.importance_level or 0.5))
        strength += importance_similarity * 0.2
        
        return min(1.0, strength)
    
    def _calculate_introduction_benefit(self, person_a: Person, person_b: Person) -> float:
        """Calculate potential benefit of introducing two people"""
        benefit = 0.0
        
        # High importance people are good to introduce
        avg_importance = ((person_a.importance_level or 0.5) + (person_b.importance_level or 0.5)) / 2
        benefit += avg_importance * 0.4
        
        # Complementary companies/industries
        if person_a.company and person_b.company and person_a.company != person_b.company:
            benefit += 0.3
        
        # Different but relevant titles
        if person_a.title and person_b.title:
            # Simple heuristic: different titles might be complementary
            if person_a.title != person_b.title:
                benefit += 0.2
        
        # Both are active communicators
        if (person_a.total_interactions or 0) > 5 and (person_b.total_interactions or 0) > 5:
            benefit += 0.1
        
        return min(1.0, benefit)
    
    def _get_topic_mention_history(self, topic: Topic, session) -> List[Tuple[datetime, int]]:
        """Get historical mention data for a topic"""
        # This would analyze emails over time to track topic mentions
        # For now, create a simplified history based on available data
        
        history = []
        
        # Get emails related to this topic (simplified)
        related_emails = session.query(Email).filter(
            Email.user_id == topic.user_id,
            Email.primary_topic_id == topic.id
        ).order_by(Email.email_date).all()
        
        # Group by week and count mentions
        weekly_counts = defaultdict(int)
        for email in related_emails:
            if email.email_date:
                week_start = email.email_date.replace(hour=0, minute=0, second=0, microsecond=0)
                week_start = week_start - timedelta(days=week_start.weekday())
                weekly_counts[week_start] += 1
        
        # Convert to time series
        for week, count in sorted(weekly_counts.items()):
            history.append((week, count))
        
        return history
    
    def _analyze_emerging_keyword_patterns(self, recent_emails: List[Email]) -> List[Dict]:
        """Analyze recent emails for emerging keyword patterns"""
        patterns = []
        
        # Extract keywords from recent emails
        keyword_frequency = defaultdict(int)
        email_count = len(recent_emails)
        
        for email in recent_emails:
            if email.ai_summary:
                # Simple keyword extraction (would be more sophisticated in production)
                words = email.ai_summary.lower().split()
                # Filter for meaningful business terms
                business_words = [
                    word for word in words 
                    if len(word) > 4 and word.isalpha() and word not in [
                        'email', 'meeting', 'discussion', 'conversation', 'message'
                    ]
                ]
                
                for word in business_words[:5]:  # Top 5 words per email
                    keyword_frequency[word] += 1
        
        # Find keywords appearing in multiple emails (emerging patterns)
        for keyword, frequency in keyword_frequency.items():
            if frequency >= max(2, email_count * 0.3):  # At least 30% of emails or 2+ mentions
                emergence_score = min(1.0, frequency / email_count)
                
                patterns.append({
                    'keywords': [keyword],
                    'emergence_score': emergence_score,
                    'frequency': frequency,
                    'email_coverage': frequency / email_count
                })
        
        return sorted(patterns, key=lambda x: x['emergence_score'], reverse=True)[:5]
    
    def _analyze_meeting_success_probability(self, meeting: CalendarEvent, session) -> Optional[PredictionResult]:
        """Analyze probability of meeting success based on historical data"""
        try:
            # Factors influencing meeting success
            success_factors = []
            
            # Preparation factor
            if hasattr(meeting, 'preparation_priority') and meeting.preparation_priority:
                prep_factor = meeting.preparation_priority
                success_factors.append(('preparation', prep_factor))
            
            # Attendee relationship strength
            if meeting.attendee_intelligence:
                # Parse attendee intelligence for relationship strength
                rel_strength = 0.7  # Simplified
                success_factors.append(('relationships', rel_strength))
            
            # Meeting duration appropriateness
            if meeting.start_time and meeting.end_time:
                duration_minutes = (meeting.end_time - meeting.start_time).total_seconds() / 60
                # Optimal meeting length factor
                if 30 <= duration_minutes <= 60:
                    duration_factor = 0.8
                elif 15 <= duration_minutes < 30 or 60 < duration_minutes <= 90:
                    duration_factor = 0.6
                else:
                    duration_factor = 0.4
                success_factors.append(('duration', duration_factor))
            
            if success_factors:
                # Calculate overall success probability
                avg_factor = sum(factor[1] for factor in success_factors) / len(success_factors)
                
                return PredictionResult(
                    prediction_type='meeting_success_probability',
                    confidence=avg_factor,
                    predicted_value=f"{int(avg_factor * 100)}% success probability",
                    reasoning=f"Based on {len(success_factors)} success factors: {', '.join([f[0] for f in success_factors])}",
                    time_horizon='short_term',
                    data_points_used=len(success_factors),
                    created_at=datetime.utcnow()
                )
            
        except Exception as e:
            logger.error(f"Failed to analyze meeting success probability: {str(e)}")
        
        return None
    
    def _detect_project_formation_signals(self, recent_emails: List[Email]) -> List[Dict]:
        """Detect signals indicating potential project formation"""
        signals = []
        
        # Project formation keywords
        project_keywords = [
            'project', 'initiative', 'proposal', 'collaboration', 'partnership',
            'development', 'implementation', 'launch', 'rollout', 'strategy'
        ]
        
        decision_keywords = [
            'decision', 'approve', 'budget', 'funding', 'resources', 'timeline',
            'deadline', 'commitment', 'agreement', 'contract'
        ]
        
        # Analyze emails for project signals
        project_signals = defaultdict(int)
        decision_signals = defaultdict(int)
        
        for email in recent_emails:
            content = (email.ai_summary or '').lower()
            
            # Count project-related terms
            for keyword in project_keywords:
                if keyword in content:
                    project_signals[keyword] += 1
            
            # Count decision-related terms
            for keyword in decision_keywords:
                if keyword in content:
                    decision_signals[keyword] += 1
        
        # Generate signal analysis
        if project_signals and decision_signals:
            project_score = min(1.0, sum(project_signals.values()) / len(recent_emails))
            decision_score = min(1.0, sum(decision_signals.values()) / len(recent_emails))
            
            combined_probability = (project_score + decision_score) / 2
            
            if combined_probability > 0.5:
                top_project_terms = sorted(project_signals.items(), key=lambda x: x[1], reverse=True)[:3]
                top_decision_terms = sorted(decision_signals.items(), key=lambda x: x[1], reverse=True)[:3]
                
                signals.append({
                    'probability': combined_probability,
                    'description': f"Potential project formation involving {', '.join([term[0] for term in top_project_terms])}",
                    'reasoning': f"Project terms: {', '.join([term[0] for term in top_project_terms])}. Decision terms: {', '.join([term[0] for term in top_decision_terms])}",
                    'signal_strength': combined_probability,
                    'project_terms': dict(top_project_terms),
                    'decision_terms': dict(top_decision_terms)
                })
        
        return signals
    
    def _predict_decision_timing(self, user_id: int) -> List[PredictionResult]:
        """Predict when important decisions might be made"""
        predictions = []
        
        try:
            with get_db_manager().get_session() as session:
                # Look for decision-pending patterns in recent communications
                recent_emails = session.query(Email).filter(
                    Email.user_id == user_id,
                    Email.email_date > datetime.utcnow() - timedelta(days=21),
                    Email.strategic_importance > 0.5
                ).all()
                
                decision_patterns = self._analyze_decision_patterns(recent_emails)
                
                for pattern in decision_patterns:
                    if pattern['urgency_score'] > 0.6:
                        predictions.append(PredictionResult(
                            prediction_type='decision_timing',
                            confidence=pattern['confidence'],
                            predicted_value=pattern['predicted_timeframe'],
                            reasoning=pattern['reasoning'],
                            time_horizon=pattern['time_horizon'],
                            data_points_used=len(recent_emails),
                            created_at=datetime.utcnow()
                        ))
            
        except Exception as e:
            logger.error(f"Failed to predict decision timing: {str(e)}")
        
        return predictions
    
    def _analyze_decision_patterns(self, emails: List[Email]) -> List[Dict]:
        """Analyze patterns that indicate pending decisions"""
        patterns = []
        
        # Decision urgency indicators
        urgency_keywords = [
            'urgent', 'asap', 'immediately', 'critical', 'deadline', 'time-sensitive',
            'need decision', 'waiting for', 'pending approval', 'final decision'
        ]
        
        timeline_keywords = [
            'this week', 'next week', 'end of month', 'quarter end', 'by friday',
            'before weekend', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday'
        ]
        
        # Analyze emails for decision patterns
        decision_emails = []
        for email in emails:
            content = (email.ai_summary or '').lower()
            urgency_score = 0
            timeline_hints = []
            
            # Check for urgency indicators
            for keyword in urgency_keywords:
                if keyword in content:
                    urgency_score += 0.2
            
            # Check for timeline hints
            for keyword in timeline_keywords:
                if keyword in content:
                    timeline_hints.append(keyword)
                    urgency_score += 0.1
            
            if urgency_score > 0.3:
                decision_emails.append({
                    'email': email,
                    'urgency_score': min(1.0, urgency_score),
                    'timeline_hints': timeline_hints,
                    'content_snippet': content[:200]
                })
        
        # Generate decision patterns
        if decision_emails:
            avg_urgency = sum(e['urgency_score'] for e in decision_emails) / len(decision_emails)
            
            # Predict timeframe based on timeline hints
            if any('this week' in e['timeline_hints'] for e in decision_emails):
                timeframe = 'Within 1 week'
                horizon = 'short_term'
            elif any('next week' in e['timeline_hints'] for e in decision_emails):
                timeframe = 'Within 2 weeks'
                horizon = 'short_term'
            else:
                timeframe = 'Within 1 month'
                horizon = 'medium_term'
            
            patterns.append({
                'urgency_score': avg_urgency,
                'confidence': avg_urgency,
                'predicted_timeframe': timeframe,
                'time_horizon': horizon,
                'reasoning': f"Decision urgency detected in {len(decision_emails)} recent communications",
                'decision_emails_count': len(decision_emails)
            })
        
        return patterns
    
    # =====================================================================
    # PATTERN DETECTION HELPERS
    # =====================================================================
    
    def _detect_communication_patterns(self, user_id: int, session) -> List[TrendPattern]:
        """Detect patterns in communication frequency and timing"""
        patterns = []
        
        try:
            # Get email activity over time
            emails = session.query(Email).filter(
                Email.user_id == user_id,
                Email.email_date > datetime.utcnow() - timedelta(days=90)
            ).order_by(Email.email_date).all()
            
            # Group by day and count
            daily_counts = defaultdict(int)
            for email in emails:
                if email.email_date:
                    day = email.email_date.date()
                    daily_counts[day] += 1
            
            # Convert to time series and analyze trend
            if len(daily_counts) > 7:
                data_points = [(datetime.combine(day, datetime.min.time()), count) 
                              for day, count in sorted(daily_counts.items())]
                
                trend_analysis = self._analyze_time_series_trend(data_points)
                
                if trend_analysis['confidence'] > 0.5:
                    pattern = TrendPattern(
                        entity_type='communication',
                        entity_id=user_id,
                        pattern_type='growth' if trend_analysis['direction'] > 0 else 'decline',
                        strength=abs(trend_analysis['slope']),
                        confidence=trend_analysis['confidence'],
                        data_points=data_points,
                        prediction=trend_analysis['predicted_next_value']
                    )
                    patterns.append(pattern)
            
        except Exception as e:
            logger.error(f"Failed to detect communication patterns: {str(e)}")
        
        return patterns
    
    def _detect_topic_evolution_patterns(self, user_id: int, session) -> List[TrendPattern]:
        """Detect how topics evolve over time"""
        patterns = []
        
        try:
            topics = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.total_mentions > 2
            ).all()
            
            for topic in topics:
                mention_history = self._get_topic_mention_history(topic, session)
                
                if len(mention_history) > 3:
                    trend_analysis = self._analyze_time_series_trend(mention_history)
                    
                    if trend_analysis['confidence'] > 0.4:
                        pattern = TrendPattern(
                            entity_type='topic',
                            entity_id=topic.id,
                            pattern_type='growth' if trend_analysis['direction'] > 0 else 'decline',
                            strength=abs(trend_analysis['slope']),
                            confidence=trend_analysis['confidence'],
                            data_points=mention_history,
                            prediction=trend_analysis['predicted_next_value']
                        )
                        patterns.append(pattern)
            
        except Exception as e:
            logger.error(f"Failed to detect topic evolution patterns: {str(e)}")
        
        return patterns
    
    def _detect_relationship_evolution_patterns(self, user_id: int, session) -> List[TrendPattern]:
        """Detect how relationships evolve over time"""
        patterns = []
        
        try:
            people = session.query(Person).filter(
                Person.user_id == user_id,
                Person.total_interactions > 3
            ).all()
            
            for person in people:
                # Analyze interaction frequency over time
                interaction_pattern = self._analyze_relationship_interaction_pattern(person, session)
                
                if interaction_pattern and interaction_pattern['confidence'] > 0.4:
                    pattern = TrendPattern(
                        entity_type='relationship',
                        entity_id=person.id,
                        pattern_type=interaction_pattern['pattern_type'],
                        strength=interaction_pattern['strength'],
                        confidence=interaction_pattern['confidence'],
                        data_points=interaction_pattern['data_points']
                    )
                    patterns.append(pattern)
            
        except Exception as e:
            logger.error(f"Failed to detect relationship evolution patterns: {str(e)}")
        
        return patterns
    
    def _detect_temporal_patterns(self, user_id: int, session) -> List[TrendPattern]:
        """Detect time-based behavioral patterns"""
        patterns = []
        
        try:
            # Analyze email timing patterns
            emails = session.query(Email).filter(
                Email.user_id == user_id,
                Email.email_date > datetime.utcnow() - timedelta(days=60)
            ).all()
            
            # Group by hour of day
            hourly_activity = defaultdict(int)
            for email in emails:
                if email.email_date:
                    hour = email.email_date.hour
                    hourly_activity[hour] += 1
            
            # Find peak activity hours
            if hourly_activity:
                peak_hours = sorted(hourly_activity.items(), key=lambda x: x[1], reverse=True)[:3]
                total_emails = sum(hourly_activity.values())
                
                for hour, count in peak_hours:
                    if count / total_emails > 0.15:  # More than 15% of activity
                        pattern = TrendPattern(
                            entity_type='temporal',
                            entity_id=user_id,
                            pattern_type='peak_activity',
                            strength=count / total_emails,
                            confidence=0.8,
                            data_points=[(datetime.now().replace(hour=hour), count)]
                        )
                        patterns.append(pattern)
            
        except Exception as e:
            logger.error(f"Failed to detect temporal patterns: {str(e)}")
        
        return patterns
    
    def _analyze_relationship_interaction_pattern(self, person: Person, session) -> Optional[Dict]:
        """Analyze interaction pattern for a specific relationship"""
        try:
            # Get emails from this person over time
            emails = session.query(Email).filter(
                Email.user_id == person.user_id,
                Email.sender == person.email_address,
                Email.email_date > datetime.utcnow() - timedelta(days=90)
            ).order_by(Email.email_date).all()
            
            if len(emails) < 3:
                return None
            
            # Group by week
            weekly_counts = defaultdict(int)
            for email in emails:
                if email.email_date:
                    week_start = email.email_date.replace(hour=0, minute=0, second=0, microsecond=0)
                    week_start = week_start - timedelta(days=week_start.weekday())
                    weekly_counts[week_start] += 1
            
            # Analyze trend
            data_points = [(week, count) for week, count in sorted(weekly_counts.items())]
            
            if len(data_points) > 2:
                trend_analysis = self._analyze_time_series_trend(data_points)
                
                return {
                    'pattern_type': 'growth' if trend_analysis['direction'] > 0 else 'decline',
                    'strength': abs(trend_analysis['slope']),
                    'confidence': trend_analysis['confidence'],
                    'data_points': data_points
                }
            
        except Exception as e:
            logger.error(f"Failed to analyze relationship interaction pattern: {str(e)}")
        
        return None
    
    def _generate_pattern_based_predictions(self, user_id: int, patterns: Dict) -> List[PredictionResult]:
        """Generate predictions based on detected patterns"""
        predictions = []
        
        try:
            # Communication pattern predictions
            for pattern in patterns.get('communication_patterns', []):
                if pattern.confidence > 0.6:
                    pred = PredictionResult(
                        prediction_type=f'communication_{pattern.pattern_type}',
                        confidence=pattern.confidence,
                        predicted_value=pattern.prediction,
                        reasoning=f"Communication activity showing {pattern.pattern_type} pattern",
                        time_horizon='short_term',
                        data_points_used=len(pattern.data_points),
                        created_at=datetime.utcnow()
                    )
                    predictions.append(pred)
            
            # Topic pattern predictions
            for pattern in patterns.get('topic_patterns', []):
                if pattern.confidence > 0.5:
                    pred = PredictionResult(
                        prediction_type=f'topic_{pattern.pattern_type}',
                        confidence=pattern.confidence,
                        predicted_value=pattern.prediction,
                        reasoning=f"Topic showing {pattern.pattern_type} trend in mentions",
                        time_horizon='medium_term',
                        data_points_used=len(pattern.data_points),
                        created_at=datetime.utcnow()
                    )
                    predictions.append(pred)
            
        except Exception as e:
            logger.error(f"Failed to generate pattern-based predictions: {str(e)}")
        
        return predictions
    
    def _store_predictions(self, user_id: int, predictions: List[PredictionResult]):
        """Store high-confidence predictions as intelligence insights"""
        try:
            with get_db_manager().get_session() as session:
                for prediction in predictions:
                    if prediction.confidence > 0.7:  # Only store high-confidence predictions
                        insight = IntelligenceInsight(
                            user_id=user_id,
                            insight_type=f'prediction_{prediction.prediction_type}',
                            title=f"Predictive Insight: {prediction.prediction_type.replace('_', ' ').title()}",
                            description=f"{prediction.predicted_value}. {prediction.reasoning}",
                            priority='high' if prediction.confidence > 0.8 else 'medium',
                            confidence=prediction.confidence,
                            status='new'
                        )
                        session.add(insight)
                
                session.commit()
                
        except Exception as e:
            logger.error(f"Failed to store predictions: {str(e)}")
    
    def _get_active_users_for_analysis(self) -> List[int]:
        """Get users with recent activity for pattern analysis"""
        try:
            with get_db_manager().get_session() as session:
                # Users with emails in last 7 days
                recent_cutoff = datetime.utcnow() - timedelta(days=7)
                
                active_user_ids = session.query(Email.user_id).filter(
                    Email.email_date > recent_cutoff
                ).distinct().all()
                
                return [user_id[0] for user_id in active_user_ids]
                
        except Exception as e:
            logger.error(f"Failed to get active users: {str(e)}")
            return []

# Global instance
predictive_analytics = PredictiveAnalytics()

============================================================
FILE: archive/refactor_docs/3_enhanced_AI_processing_pipeline.txt
============================================================
# Enhanced AI Processing Pipeline - Context-Aware Intelligence
# This replaces the scattered AI processing with unified, context-aware analysis

import json
import logging
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timezone
import anthropic
from dataclasses import dataclass

from config.settings import settings
from processors.unified_entity_engine import entity_engine, EntityContext
from models.enhanced_models import Email, CalendarEvent, Topic, Person, Task

logger = logging.getLogger(__name__)

@dataclass
class ProcessingResult:
    """Result container for AI processing"""
    success: bool
    entities_created: Dict[str, int]  # Type -> count
    entities_updated: Dict[str, int]
    insights_generated: List[str]
    processing_time: float
    error: Optional[str] = None

class EnhancedAIProcessor:
    """
    Context-aware AI processing that builds on existing knowledge.
    This is the brain that turns raw data into intelligent insights.
    """
    
    def __init__(self):
        self.client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        self.model = "claude-3-5-sonnet-20241022"
        self.entity_engine = entity_engine
        
    # =====================================================================
    # UNIFIED EMAIL PROCESSING - SINGLE PASS WITH CONTEXT
    # =====================================================================
    
    def process_email_with_context(self, email_data: Dict, user_id: int, existing_context: Dict = None) -> ProcessingResult:
        """
        Process email with full context awareness in a single AI call.
        This replaces multiple separate prompts with one intelligent analysis.
        """
        start_time = datetime.utcnow()
        result = ProcessingResult(
            success=False,
            entities_created={'people': 0, 'topics': 0, 'tasks': 0, 'projects': 0},
            entities_updated={'people': 0, 'topics': 0, 'tasks': 0, 'projects': 0},
            insights_generated=[],
            processing_time=0.0
        )
        
        try:
            # Step 1: Gather existing context for this user
            context = self._gather_user_context(user_id, existing_context)
            
            # Step 2: Prepare comprehensive prompt with existing knowledge
            analysis_prompt = self._prepare_unified_email_prompt(email_data, context)
            
            # Step 3: Single AI analysis call
            claude_response = self._call_claude_unified_analysis(analysis_prompt)
            
            if not claude_response:
                result.error = "Failed to get AI analysis"
                return result
            
            # Step 4: Parse comprehensive response
            analysis = self._parse_unified_analysis(claude_response)
            
            # Step 5: Create/update entities with context
            processing_context = EntityContext(
                source_type='email',
                source_id=email_data.get('id'),
                user_id=user_id,
                confidence=analysis.get('overall_confidence', 0.8)
            )
            
            # Process people (including signature analysis)
            people_result = self._process_people_from_analysis(analysis.get('people', []), processing_context)
            result.entities_created['people'] = people_result['created']
            result.entities_updated['people'] = people_result['updated']
            
            # Process topics (check existing first)
            topics_result = self._process_topics_from_analysis(analysis.get('topics', []), processing_context)
            result.entities_created['topics'] = topics_result['created']
            result.entities_updated['topics'] = topics_result['updated']
            
            # Process tasks (with full context story)
            tasks_result = self._process_tasks_from_analysis(analysis.get('tasks', []), processing_context)
            result.entities_created['tasks'] = tasks_result['created']
            
            # Process projects (check for augmentation)
            projects_result = self._process_projects_from_analysis(analysis.get('projects', []), processing_context)
            result.entities_created['projects'] = projects_result['created']
            result.entities_updated['projects'] = projects_result['updated']
            
            # Create entity relationships
            self._create_entity_relationships(analysis, processing_context)
            
            # Store email intelligence
            self._store_email_intelligence(email_data, analysis, user_id)
            
            # Generate insights
            result.insights_generated = analysis.get('strategic_insights', [])
            
            result.success = True
            result.processing_time = (datetime.utcnow() - start_time).total_seconds()
            
            logger.info(f"Successfully processed email with context in {result.processing_time:.2f}s")
            
        except Exception as e:
            logger.error(f"Failed to process email with context: {str(e)}")
            result.error = str(e)
            result.processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        return result
    
    # =====================================================================
    # CALENDAR EVENT ENHANCEMENT WITH EMAIL INTELLIGENCE
    # =====================================================================
    
    def enhance_calendar_event_with_intelligence(self, event_data: Dict, user_id: int) -> ProcessingResult:
        """
        Enhance calendar events with email intelligence and create prep tasks.
        This addresses your concern about connecting email insights to calendar events.
        """
        start_time = datetime.utcnow()
        result = ProcessingResult(
            success=False,
            entities_created={'tasks': 0, 'people': 0},
            entities_updated={'events': 0, 'people': 0},
            insights_generated=[],
            processing_time=0.0
        )
        
        try:
            # Step 1: Analyze attendees and find existing relationships
            attendee_intelligence = self._analyze_event_attendees(event_data, user_id)
            
            # Step 2: Find related email intelligence for these people
            email_context = self._find_related_email_intelligence(attendee_intelligence, user_id)
            
            # Step 3: Generate enhanced meeting context
            enhancement_prompt = self._prepare_meeting_enhancement_prompt(event_data, attendee_intelligence, email_context)
            
            # Step 4: AI analysis for meeting preparation
            claude_response = self._call_claude_meeting_enhancement(enhancement_prompt)
            
            if claude_response:
                enhancement = self._parse_meeting_enhancement(claude_response)
                
                processing_context = EntityContext(
                    source_type='calendar',
                    source_id=event_data.get('id'),
                    user_id=user_id,
                    confidence=0.8
                )
                
                # Create preparation tasks
                if enhancement.get('prep_tasks'):
                    for task_data in enhancement['prep_tasks']:
                        task = self.entity_engine.create_task_with_full_context(
                            description=task_data['description'],
                            assignee_email=None,  # User's own prep tasks
                            topic_names=task_data.get('topics', []),
                            context=processing_context,
                            priority=task_data.get('priority', 'medium')
                        )
                        if task:
                            result.entities_created['tasks'] += 1
                
                # Update event with business context
                self._update_event_intelligence(event_data, enhancement, user_id)
                result.entities_updated['events'] = 1
                
                result.insights_generated = enhancement.get('insights', [])
                result.success = True
            
            result.processing_time = (datetime.utcnow() - start_time).total_seconds()
            
        except Exception as e:
            logger.error(f"Failed to enhance calendar event: {str(e)}")
            result.error = str(e)
            result.processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        return result
    
    # =====================================================================
    # CONTEXT GATHERING AND PROMPT PREPARATION
    # =====================================================================
    
    def _gather_user_context(self, user_id: int, existing_context: Dict = None) -> Dict:
        """Gather comprehensive user context for AI processing"""
        try:
            from models.database import get_db_manager
            
            context = {
                'existing_people': [],
                'existing_topics': [],
                'active_projects': [],
                'recent_insights': [],
                'communication_patterns': {}
            }
            
            if existing_context:
                context.update(existing_context)
                return context
            
            with get_db_manager().get_session() as session:
                # Get recent people (last 30 days)
                recent_people = session.query(Person).filter(
                    Person.user_id == user_id,
                    Person.last_contact > datetime.utcnow() - timedelta(days=30)
                ).limit(20).all()
                
                context['existing_people'] = [
                    {
                        'name': p.name,
                        'email': p.email_address,
                        'company': p.company,
                        'relationship': p.relationship_type,
                        'importance': p.importance_level
                    }
                    for p in recent_people
                ]
                
                # Get active topics
                active_topics = session.query(Topic).filter(
                    Topic.user_id == user_id,
                    Topic.total_mentions > 1
                ).order_by(Topic.last_mentioned.desc()).limit(15).all()
                
                context['existing_topics'] = [
                    {
                        'name': t.name,
                        'description': t.description,
                        'keywords': t.keywords,
                        'mentions': t.total_mentions,
                        'is_official': t.is_official
                    }
                    for t in active_topics
                ]
                
                # Get active projects
                active_projects = session.query(Project).filter(
                    Project.user_id == user_id,
                    Project.status == 'active'
                ).limit(10).all()
                
                context['active_projects'] = [
                    {
                        'name': p.name,
                        'description': p.description,
                        'status': p.status,
                        'priority': p.priority
                    }
                    for p in active_projects
                ]
            
            return context
            
        except Exception as e:
            logger.error(f"Failed to gather user context: {str(e)}")
            return {}
    
    def _prepare_unified_email_prompt(self, email_data: Dict, context: Dict) -> str:
        """Prepare comprehensive email analysis prompt with existing context"""
        
        # Format existing context for Claude
        context_summary = self._format_context_for_claude(context)
        
        prompt = f"""You are an AI Chief of Staff analyzing business email communication with access to the user's existing business intelligence context.

EXISTING BUSINESS CONTEXT:
{context_summary}

EMAIL TO ANALYZE:
From: {email_data.get('sender_name', '')} <{email_data.get('sender', '')}>
Subject: {email_data.get('subject', '')}
Date: {email_data.get('email_date', '')}

Content:
{email_data.get('body_clean', '')}

ANALYSIS INSTRUCTIONS:
Provide comprehensive analysis in the following JSON format. Use the existing context to:
1. Match people to existing contacts (avoid duplicates)
2. Connect topics to existing business themes
3. Identify project connections and updates
4. Generate contextual tasks with business rationale
5. Extract strategic insights based on patterns

{{
    "overall_confidence": 0.0-1.0,
    "business_summary": "Concise business-focused summary for display",
    "category": "meeting|project|decision|information|relationship",
    "sentiment": "positive|neutral|negative|urgent",
    "strategic_importance": 0.0-1.0,
    
    "people": [
        {{
            "email": "required",
            "name": "extracted or inferred name",
            "is_existing": true/false,
            "existing_person_match": "name if matched to existing context",
            "role_in_email": "sender|recipient|mentioned",
            "professional_context": "title, company, relationship insights",
            "signature_data": "extracted title, company, phone, etc if available",
            "importance_level": 0.0-1.0
        }}
    ],
    
    "topics": [
        {{
            "name": "topic name",
            "is_existing": true/false,
            "existing_topic_match": "name if matched to existing",
            "description": "what this topic covers",
            "keywords": ["keyword1", "keyword2"],
            "strategic_importance": 0.0-1.0,
            "new_information": "what's new about this topic from this email"
        }}
    ],
    
    "tasks": [
        {{
            "description": "clear actionable task",
            "assignee_email": "who should do this or null for user",
            "context_rationale": "WHY this task exists - business context",
            "related_topics": ["topic names"],
            "related_people": ["email addresses"],
            "priority": "high|medium|low",
            "due_date_hint": "extracted date or timing hint",
            "confidence": 0.0-1.0
        }}
    ],
    
    "projects": [
        {{
            "name": "project name",
            "is_existing": true/false,
            "existing_project_match": "name if matched",
            "description": "project description",
            "new_information": "what's new about this project",
            "stakeholders": ["email addresses of involved people"],
            "status_update": "current status or progress",
            "priority": "high|medium|low"
        }}
    ],
    
    "strategic_insights": [
        "Key business insights that connect to existing context or reveal new patterns"
    ],
    
    "entity_relationships": [
        {{
            "entity_a": {{"type": "person|topic|project", "identifier": "email or name"}},
            "entity_b": {{"type": "person|topic|project", "identifier": "email or name"}},
            "relationship_type": "collaborates_on|discusses|leads|reports_to",
            "strength": 0.0-1.0
        }}
    ]
}}

Focus on business intelligence that builds on existing context rather than isolated data extraction."""
        
        return prompt
    
    def _format_context_for_claude(self, context: Dict) -> str:
        """Format user context in a readable way for Claude"""
        sections = []
        
        if context.get('existing_people'):
            people_summary = []
            for person in context['existing_people'][:10]:  # Limit for token efficiency
                people_summary.append(f"- {person['name']} ({person['email']}) - {person.get('company', 'Unknown')} - {person.get('relationship', 'contact')}")
            sections.append(f"EXISTING PEOPLE:\n" + "\n".join(people_summary))
        
        if context.get('existing_topics'):
            topics_summary = []
            for topic in context['existing_topics'][:10]:
                status = "OFFICIAL" if topic.get('is_official') else f"{topic.get('mentions', 0)} mentions"
                topics_summary.append(f"- {topic['name']} ({status}) - {topic.get('description', 'No description')}")
            sections.append(f"EXISTING TOPICS:\n" + "\n".join(topics_summary))
        
        if context.get('active_projects'):
            projects_summary = []
            for project in context['active_projects'][:5]:
                projects_summary.append(f"- {project['name']} ({project['status']}) - {project.get('description', '')}")
            sections.append(f"ACTIVE PROJECTS:\n" + "\n".join(projects_summary))
        
        return "\n\n".join(sections) if sections else "No existing context available."
    
    # =====================================================================
    # AI RESPONSE PROCESSING
    # =====================================================================
    
    def _call_claude_unified_analysis(self, prompt: str) -> Optional[str]:
        """Call Claude for unified email analysis"""
        try:
            message = self.client.messages.create(
                model=self.model,
                max_tokens=4000,  # Increased for comprehensive analysis
                temperature=0.1,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return message.content[0].text.strip()
            
        except Exception as e:
            logger.error(f"Failed to call Claude for unified analysis: {str(e)}")
            return None
    
    def _parse_unified_analysis(self, response: str) -> Dict:
        """Parse Claude's comprehensive analysis response"""
        try:
            # Find JSON in response
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                logger.warning("No JSON found in Claude response")
                return {}
            
            json_text = response[json_start:json_end]
            analysis = json.loads(json_text)
            
            logger.info(f"Parsed unified analysis with {len(analysis.get('people', []))} people, "
                       f"{len(analysis.get('topics', []))} topics, {len(analysis.get('tasks', []))} tasks")
            
            return analysis
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Claude analysis JSON: {str(e)}")
            return {}
        except Exception as e:
            logger.error(f"Failed to parse Claude analysis: {str(e)}")
            return {}
    
    def _process_people_from_analysis(self, people_data: List[Dict], context: EntityContext) -> Dict:
        """Process people from unified analysis"""
        result = {'created': 0, 'updated': 0}
        
        for person_data in people_data:
            email = person_data.get('email')
            name = person_data.get('name')
            
            if not email:
                continue
            
            # Add signature data to processing metadata
            if person_data.get('signature_data'):
                context.processing_metadata = {'signature': person_data['signature_data']}
            
            person = self.entity_engine.create_or_update_person(email, name, context)
            
            if person:
                if person_data.get('is_existing'):
                    result['updated'] += 1
                else:
                    result['created'] += 1
        
        return result
    
    def _process_topics_from_analysis(self, topics_data: List[Dict], context: EntityContext) -> Dict:
        """Process topics from unified analysis with existing topic checking"""
        result = {'created': 0, 'updated': 0}
        
        for topic_data in topics_data:
            topic_name = topic_data.get('name')
            description = topic_data.get('description')
            keywords = topic_data.get('keywords', [])
            
            if not topic_name:
                continue
            
            topic = self.entity_engine.create_or_update_topic(
                topic_name=topic_name,
                description=description,
                keywords=keywords,
                context=context
            )
            
            if topic:
                if topic_data.get('is_existing'):
                    result['updated'] += 1
                else:
                    result['created'] += 1
        
        return result
    
    def _process_tasks_from_analysis(self, tasks_data: List[Dict], context: EntityContext) -> Dict:
        """Process tasks from unified analysis with full context stories"""
        result = {'created': 0}
        
        for task_data in tasks_data:
            description = task_data.get('description')
            assignee_email = task_data.get('assignee_email')
            related_topics = task_data.get('related_topics', [])
            priority = task_data.get('priority', 'medium')
            
            if not description:
                continue
            
            # Parse due date hint
            due_date = None
            due_date_hint = task_data.get('due_date_hint')
            if due_date_hint:
                due_date = self._parse_due_date_hint(due_date_hint)
            
            # Set confidence from analysis
            context.confidence = task_data.get('confidence', 0.8)
            
            # Add context rationale to processing metadata
            if task_data.get('context_rationale'):
                context.processing_metadata = {
                    'context_rationale': task_data['context_rationale'],
                    'related_people': task_data.get('related_people', [])
                }
            
            task = self.entity_engine.create_task_with_full_context(
                description=description,
                assignee_email=assignee_email,
                topic_names=related_topics,
                context=context,
                due_date=due_date,
                priority=priority
            )
            
            if task:
                result['created'] += 1
        
        return result
    
    def _process_projects_from_analysis(self, projects_data: List[Dict], context: EntityContext) -> Dict:
        """Process projects from unified analysis with augmentation logic"""
        result = {'created': 0, 'updated': 0}
        
        try:
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                for project_data in projects_data:
                    project_name = project_data.get('name')
                    
                    if not project_name:
                        continue
                    
                    # Check for existing project
                    existing_project = session.query(Project).filter(
                        Project.user_id == context.user_id,
                        Project.name.ilike(f"%{project_name}%")
                    ).first()
                    
                    if existing_project and project_data.get('is_existing'):
                        # Augment existing project
                        updated = self._augment_existing_project(existing_project, project_data, session)
                        if updated:
                            result['updated'] += 1
                    
                    elif not existing_project:
                        # Create new project
                        new_project = self._create_new_project(project_data, context, session)
                        if new_project:
                            result['created'] += 1
                
                session.commit()
        
        except Exception as e:
            logger.error(f"Failed to process projects: {str(e)}")
        
        return result
    
    def _create_entity_relationships(self, analysis: Dict, context: EntityContext):
        """Create relationships between entities based on analysis"""
        relationships = analysis.get('entity_relationships', [])
        
        for rel_data in relationships:
            entity_a = rel_data.get('entity_a', {})
            entity_b = rel_data.get('entity_b', {})
            relationship_type = rel_data.get('relationship_type', 'related')
            
            # Find actual entity IDs
            entity_a_id = self._find_entity_id(entity_a, context.user_id)
            entity_b_id = self._find_entity_id(entity_b, context.user_id)
            
            if entity_a_id and entity_b_id:
                self.entity_engine.create_entity_relationship(
                    entity_a['type'], entity_a_id,
                    entity_b['type'], entity_b_id,
                    relationship_type,
                    context
                )
    
    def _store_email_intelligence(self, email_data: Dict, analysis: Dict, user_id: int):
        """Store processed email intelligence in optimized format"""
        try:
            from models.database import get_db_manager
            
            # Create content hash for deduplication
            content = email_data.get('body_clean', '')
            content_hash = hashlib.sha256(content.encode()).hexdigest()
            
            # Store in blob storage (simplified for now - would use S3/GCS in production)
            blob_key = f"emails/{user_id}/{content_hash}.txt"
            
            with get_db_manager().get_session() as session:
                # Check if email already exists
                existing_email = session.query(Email).filter(
                    Email.gmail_id == email_data.get('gmail_id')
                ).first()
                
                if existing_email:
                    # Update existing email with new intelligence
                    existing_email.ai_summary = analysis.get('business_summary')
                    existing_email.business_category = analysis.get('category')
                    existing_email.sentiment = analysis.get('sentiment')
                    existing_email.strategic_importance = analysis.get('strategic_importance', 0.5)
                    existing_email.processed_at = datetime.utcnow()
                else:
                    # Create new email record
                    email_record = Email(
                        user_id=user_id,
                        gmail_id=email_data.get('gmail_id'),
                        subject=email_data.get('subject'),
                        sender=email_data.get('sender'),
                        sender_name=email_data.get('sender_name'),
                        email_date=email_data.get('email_date'),
                        ai_summary=analysis.get('business_summary'),
                        business_category=analysis.get('category'),
                        sentiment=analysis.get('sentiment'),
                        strategic_importance=analysis.get('strategic_importance', 0.5),
                        content_hash=content_hash,
                        blob_storage_key=blob_key,
                        processed_at=datetime.utcnow(),
                        processing_version="unified_v1.0"
                    )
                    session.add(email_record)
                
                session.commit()
                
        except Exception as e:
            logger.error(f"Failed to store email intelligence: {str(e)}")
    
    # =====================================================================
    # MEETING ENHANCEMENT METHODS
    # =====================================================================
    
    def _analyze_event_attendees(self, event_data: Dict, user_id: int) -> Dict:
        """Analyze meeting attendees and find existing relationships"""
        try:
            from models.database import get_db_manager
            
            attendee_intelligence = {
                'known_attendees': [],
                'unknown_attendees': [],
                'relationship_strength': 0.0,
                'total_attendees': 0
            }
            
            attendees = event_data.get('attendees', [])
            attendee_intelligence['total_attendees'] = len(attendees)
            
            with get_db_manager().get_session() as session:
                for attendee in attendees:
                    email = attendee.get('email', '').lower()
                    if not email:
                        continue
                    
                    # Find existing person
                    existing_person = session.query(Person).filter(
                        Person.user_id == user_id,
                        Person.email_address == email
                    ).first()
                    
                    if existing_person:
                        attendee_info = {
                            'email': email,
                            'name': existing_person.name,
                            'company': existing_person.company,
                            'title': existing_person.title,
                            'relationship': existing_person.relationship_type,
                            'importance': existing_person.importance_level,
                            'last_contact': existing_person.last_contact,
                            'total_interactions': existing_person.total_interactions
                        }
                        attendee_intelligence['known_attendees'].append(attendee_info)
                        attendee_intelligence['relationship_strength'] += existing_person.importance_level or 0.5
                    else:
                        attendee_intelligence['unknown_attendees'].append({
                            'email': email,
                            'name': attendee.get('displayName', email.split('@')[0])
                        })
            
            # Calculate average relationship strength
            if attendee_intelligence['known_attendees']:
                attendee_intelligence['relationship_strength'] /= len(attendee_intelligence['known_attendees'])
            
            return attendee_intelligence
            
        except Exception as e:
            logger.error(f"Failed to analyze event attendees: {str(e)}")
            return {}
    
    def _find_related_email_intelligence(self, attendee_intelligence: Dict, user_id: int) -> Dict:
        """Find email intelligence related to meeting attendees"""
        try:
            from models.database import get_db_manager
            
            email_context = {
                'recent_communications': [],
                'shared_topics': [],
                'collaboration_patterns': {},
                'strategic_context': []
            }
            
            known_attendees = attendee_intelligence.get('known_attendees', [])
            if not known_attendees:
                return email_context
            
            attendee_emails = [a['email'] for a in known_attendees]
            
            with get_db_manager().get_session() as session:
                # Get recent emails with these attendees (last 30 days)
                from datetime import timedelta
                recent_cutoff = datetime.utcnow() - timedelta(days=30)
                
                recent_emails = session.query(Email).filter(
                    Email.user_id == user_id,
                    Email.email_date > recent_cutoff,
                    Email.sender.in_(attendee_emails)
                ).order_by(Email.email_date.desc()).limit(10).all()
                
                for email in recent_emails:
                    if email.ai_summary:
                        email_context['recent_communications'].append({
                            'sender': email.sender_name or email.sender,
                            'subject': email.subject,
                            'summary': email.ai_summary,
                            'date': email.email_date,
                            'category': email.business_category,
                            'importance': email.strategic_importance
                        })
                
                # Find shared topics from recent communications
                # This would be more sophisticated in production
                for email in recent_emails:
                    if hasattr(email, 'primary_topic') and email.primary_topic:
                        topic_info = {
                            'name': email.primary_topic.name,
                            'description': email.primary_topic.description
                        }
                        if topic_info not in email_context['shared_topics']:
                            email_context['shared_topics'].append(topic_info)
            
            return email_context
            
        except Exception as e:
            logger.error(f"Failed to find related email intelligence: {str(e)}")
            return {}
    
    def _prepare_meeting_enhancement_prompt(self, event_data: Dict, attendee_intelligence: Dict, email_context: Dict) -> str:
        """Prepare prompt for meeting enhancement with intelligence"""
        
        # Format attendee intelligence
        known_attendees_summary = ""
        if attendee_intelligence.get('known_attendees'):
            attendees_list = []
            for attendee in attendee_intelligence['known_attendees']:
                attendees_list.append(f"- {attendee['name']} ({attendee['email']}) - {attendee.get('company', 'Unknown')} - {attendee.get('title', 'Unknown role')} - {attendee['total_interactions']} previous interactions")
            known_attendees_summary = "\n".join(attendees_list)
        
        # Format email context
        recent_comms_summary = ""
        if email_context.get('recent_communications'):
            comms_list = []
            for comm in email_context['recent_communications'][:5]:  # Limit for token efficiency
                comms_list.append(f"- {comm['sender']}: {comm['subject']} ({comm['date'].strftime('%Y-%m-%d')}) - {comm['summary']}")
            recent_comms_summary = "\n".join(comms_list)
        
        # Format shared topics
        topics_summary = ""
        if email_context.get('shared_topics'):
            topics_list = [f"- {topic['name']}: {topic.get('description', 'No description')}" for topic in email_context['shared_topics']]
            topics_summary = "\n".join(topics_list)
        
        prompt = f"""You are an AI Chief of Staff preparing intelligent meeting enhancement and preparation tasks.

MEETING DETAILS:
Title: {event_data.get('title', 'Untitled Meeting')}
Date/Time: {event_data.get('start_time', 'Unknown time')}
Duration: {event_data.get('duration', 'Unknown')}
Location: {event_data.get('location', 'Not specified')}
Description: {event_data.get('description', 'No description')}

ATTENDEE INTELLIGENCE:
{known_attendees_summary}

Unknown Attendees: {len(attendee_intelligence.get('unknown_attendees', []))}
Total Attendees: {attendee_intelligence.get('total_attendees', 0)}
Relationship Strength: {attendee_intelligence.get('relationship_strength', 0.0):.2f}/1.0

RECENT EMAIL CONTEXT WITH ATTENDEES:
{recent_comms_summary}

SHARED TOPICS FROM RECENT COMMUNICATIONS:
{topics_summary}

ANALYSIS INSTRUCTIONS:
Generate intelligent meeting preparation and context enhancement in JSON format:

{{
    "business_context": "Comprehensive context about this meeting's business purpose and attendee relationships",
    "attendee_intelligence": "Summary of key attendees and their roles in this meeting context",
    "preparation_priority": 0.0-1.0,
    "strategic_importance": 0.0-1.0,
    
    "prep_tasks": [
        {{
            "description": "Specific preparation task",
            "rationale": "Why this preparation is important",
            "topics": ["related topic names"],
            "priority": "high|medium|low",
            "estimated_time": "time estimate in minutes",
            "due_before_meeting": "how far in advance to complete"
        }}
    ],
    
    "discussion_topics": [
        {{
            "topic": "Discussion topic based on email intelligence",
            "context": "Background from recent communications",
            "priority": "high|medium|low"
        }}
    ],
    
    "relationship_insights": [
        {{
            "attendee": "attendee name",
            "insight": "Key insight about this person relevant to the meeting",
            "recent_context": "Recent communication context"
        }}
    ],
    
    "insights": [
        "Strategic insights about this meeting based on attendee intelligence and email context"
    ],
    
    "follow_up_recommendations": [
        "Recommended follow-up actions based on meeting purpose and attendee relationships"
    ]
}}

Focus on actionable preparation that leverages the email intelligence and attendee relationships to make this meeting more effective."""
        
        return prompt
    
    def _call_claude_meeting_enhancement(self, prompt: str) -> Optional[str]:
        """Call Claude for meeting enhancement analysis"""
        try:
            message = self.client.messages.create(
                model=self.model,
                max_tokens=3000,
                temperature=0.1,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return message.content[0].text.strip()
            
        except Exception as e:
            logger.error(f"Failed to call Claude for meeting enhancement: {str(e)}")
            return None
    
    def _parse_meeting_enhancement(self, response: str) -> Dict:
        """Parse Claude's meeting enhancement response"""
        try:
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                logger.warning("No JSON found in meeting enhancement response")
                return {}
            
            json_text = response[json_start:json_end]
            enhancement = json.loads(json_text)
            
            return enhancement
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse meeting enhancement JSON: {str(e)}")
            return {}
        except Exception as e:
            logger.error(f"Failed to parse meeting enhancement: {str(e)}")
            return {}
    
    def _update_event_intelligence(self, event_data: Dict, enhancement: Dict, user_id: int):
        """Update calendar event with intelligence"""
        try:
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                event = session.query(CalendarEvent).filter(
                    CalendarEvent.google_event_id == event_data.get('google_event_id')
                ).first()
                
                if event:
                    event.business_context = enhancement.get('business_context')
                    event.attendee_intelligence = enhancement.get('attendee_intelligence')
                    event.preparation_priority = enhancement.get('preparation_priority', 0.5)
                    event.updated_at = datetime.utcnow()
                    session.commit()
                    
        except Exception as e:
            logger.error(f"Failed to update event intelligence: {str(e)}")
    
    # =====================================================================
    # UTILITY METHODS
    # =====================================================================
    
    def _parse_due_date_hint(self, hint: str) -> Optional[datetime]:
        """Parse due date hint into actual datetime"""
        # This would be more sophisticated in production
        from dateutil import parser as date_parser
        try:
            return date_parser.parse(hint, fuzzy=True)
        except:
            return None
    
    def _find_entity_id(self, entity_info: Dict, user_id: int) -> Optional[int]:
        """Find actual entity ID from analysis data"""
        try:
            from models.database import get_db_manager
            
            entity_type = entity_info.get('type')
            identifier = entity_info.get('identifier')
            
            if not entity_type or not identifier:
                return None
            
            with get_db_manager().get_session() as session:
                if entity_type == 'person':
                    entity = session.query(Person).filter(
                        Person.user_id == user_id,
                        Person.email_address == identifier
                    ).first()
                elif entity_type == 'topic':
                    entity = session.query(Topic).filter(
                        Topic.user_id == user_id,
                        Topic.name == identifier
                    ).first()
                elif entity_type == 'project':
                    entity = session.query(Project).filter(
                        Project.user_id == user_id,
                        Project.name == identifier
                    ).first()
                else:
                    return None
                
                return entity.id if entity else None
                
        except Exception as e:
            logger.error(f"Failed to find entity ID: {str(e)}")
            return None
    
    def _augment_existing_project(self, project: Project, project_data: Dict, session) -> bool:
        """Augment existing project with new information"""
        updated = False
        
        new_info = project_data.get('new_information')
        if new_info:
            if project.description:
                project.description = f"{project.description}. {new_info}"
            else:
                project.description = new_info
            updated = True
        
        status_update = project_data.get('status_update')
        if status_update:
            # This would be more sophisticated - parse status from text
            project.updated_at = datetime.utcnow()
            updated = True
        
        return updated
    
    def _create_new_project(self, project_data: Dict, context: EntityContext, session) -> Optional[Project]:
        """Create new project from analysis"""
        try:
            project = Project(
                user_id=context.user_id,
                name=project_data['name'],
                description=project_data.get('description'),
                status='active',
                priority=project_data.get('priority', 'medium'),
                created_at=datetime.utcnow()
            )
            
            session.add(project)
            return project
            
        except Exception as e:
            logger.error(f"Failed to create new project: {str(e)}")
            return None

# Global instance
enhanced_ai_processor = EnhancedAIProcessor()

============================================================
FILE: archive/refactor_docs/5_enhanced_API_integration_layer.txt
============================================================
# Enhanced API Integration Layer - Real-Time Intelligence
# This replaces your current main.py with entity-centric, real-time processing

from flask import Flask, request, jsonify, session, redirect, url_for
from flask_cors import CORS
import logging
from datetime import datetime, timedelta
import json
from typing import Dict, List, Optional

from config.settings import settings
from processors.realtime_processing import realtime_processor, EventType
from processors.unified_entity_engine import entity_engine, EntityContext
from processors.enhanced_ai_pipeline import enhanced_ai_processor
from integrations.gmail_fetcher import gmail_fetcher
from integrations.calendar_fetcher import calendar_fetcher
from models.database import get_db_manager
from models.enhanced_models import User, Topic, Person, Task, CalendarEvent, IntelligenceInsight

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
app.secret_key = settings.SECRET_KEY
CORS(app)

# Start real-time processor
realtime_processor.start()

# =====================================================================
# ENHANCED UNIFIED PROCESSING ENDPOINTS
# =====================================================================

@app.route('/api/unified-intelligence-sync', methods=['POST'])
def unified_intelligence_sync():
    """
    Enhanced unified processing that integrates email, calendar, and generates
    real-time intelligence with entity-centric architecture.
    """
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        # Get processing parameters
        data = request.get_json() or {}
        max_emails = data.get('max_emails', 20)
        days_back = data.get('days_back', 7)
        days_forward = data.get('days_forward', 14)
        force_refresh = data.get('force_refresh', False)
        
        processing_summary = {
            'success': True,
            'processing_stages': {},
            'entity_intelligence': {},
            'insights_generated': [],
            'real_time_processing': True,
            'next_steps': []
        }
        
        # Stage 1: Fetch and process emails in real-time
        logger.info(f"Starting unified intelligence sync for {user_email}")
        
        # Fetch emails
        email_result = gmail_fetcher.fetch_recent_emails(
            user_email, max_emails=max_emails, days_back=days_back, force_refresh=force_refresh
        )
        
        processing_summary['processing_stages']['emails_fetched'] = email_result.get('emails_fetched', 0)
        
        if email_result.get('success') and email_result.get('emails'):
            # Process each email through real-time pipeline
            for email_data in email_result['emails']:
                realtime_processor.process_new_email(email_data, user.id, priority=3)
        
        # Stage 2: Fetch and enhance calendar events
        calendar_result = calendar_fetcher.fetch_calendar_events(
            user_email, days_back=3, days_forward=days_forward, create_prep_tasks=True
        )
        
        processing_summary['processing_stages']['calendar_events_fetched'] = calendar_result.get('events_fetched', 0)
        
        if calendar_result.get('success') and calendar_result.get('events'):
            # Process each calendar event through real-time pipeline
            for event_data in calendar_result['events']:
                realtime_processor.process_new_calendar_event(event_data, user.id, priority=4)
        
        # Stage 3: Generate comprehensive business intelligence
        intelligence_summary = generate_360_business_intelligence(user.id)
        processing_summary['entity_intelligence'] = intelligence_summary
        
        # Stage 4: Generate proactive insights
        proactive_insights = entity_engine.generate_proactive_insights(user.id)
        processing_summary['insights_generated'] = [
            {
                'type': insight.insight_type,
                'title': insight.title,
                'description': insight.description,
                'priority': insight.priority
            }
            for insight in proactive_insights
        ]
        
        # Generate next steps based on intelligence
        processing_summary['next_steps'] = generate_intelligent_next_steps(intelligence_summary, proactive_insights)
        
        logger.info(f"Completed unified intelligence sync for {user_email}: "
                   f"{processing_summary['processing_stages']['emails_fetched']} emails, "
                   f"{processing_summary['processing_stages']['calendar_events_fetched']} events, "
                   f"{len(proactive_insights)} insights")
        
        return jsonify(processing_summary)
        
    except Exception as e:
        logger.error(f"Failed unified intelligence sync: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'processing_stages': {},
            'real_time_processing': False
        }), 500

# =====================================================================
# ENTITY-CENTRIC API ENDPOINTS
# =====================================================================

@app.route('/api/topics', methods=['GET', 'POST'])
def topics_api():
    """Enhanced topics API with intelligence accumulation"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    user = get_db_manager().get_user_by_email(user_email)
    if not user:
        return jsonify({'success': False, 'error': 'User not found'}), 404
    
    try:
        with get_db_manager().get_session() as session:
            people = session.query(Person).filter(Person.user_id == user.id).all()
            
            people_data = []
            for person in people:
                # Calculate relationship metrics
                relationship_strength = calculate_relationship_strength(person)
                communication_frequency = calculate_communication_frequency(person)
                topic_connections = len(person.topics)
                
                person_data = {
                    'id': person.id,
                    'name': person.name,
                    'email_address': person.email_address,
                    'phone': person.phone,
                    'company': person.company,
                    'title': person.title,
                    'relationship_type': person.relationship_type,
                    'importance_level': person.importance_level,
                    'total_interactions': person.total_interactions,
                    'last_contact': person.last_contact.isoformat() if person.last_contact else None,
                    'linkedin_url': person.linkedin_url,
                    'professional_story': person.professional_story,
                    'created_at': person.created_at.isoformat(),
                    'updated_at': person.updated_at.isoformat(),
                    
                    # Relationship intelligence
                    'relationship_intelligence': {
                        'strength': relationship_strength,
                        'communication_frequency': communication_frequency,
                        'topic_connections': topic_connections,
                        'engagement_score': calculate_engagement_score(person)
                    },
                    
                    # Connected topics
                    'connected_topics': [
                        {
                            'name': topic.name,
                            'affinity_score': get_person_topic_affinity(person.id, topic.id)
                        }
                        for topic in person.topics[:5]  # Top 5 topics
                    ]
                }
                people_data.append(person_data)
            
            # Sort by importance and recent activity
            people_data.sort(key=lambda x: (x['importance_level'] or 0, x['total_interactions']), reverse=True)
            
            return jsonify({
                'success': True,
                'people': people_data,
                'summary': {
                    'total_people': len(people_data),
                    'high_importance': len([p for p in people_data if (p['importance_level'] or 0) > 0.7]),
                    'recent_contacts': len([p for p in people_data if p['last_contact'] and 
                                          datetime.fromisoformat(p['last_contact']) > datetime.utcnow() - timedelta(days=30)]),
                    'topic_connected': len([p for p in people_data if p['relationship_intelligence']['topic_connections'] > 0])
                }
            })
            
    except Exception as e:
        logger.error(f"Failed to get people with relationship intelligence: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/tasks', methods=['GET'])
def tasks_with_context_intelligence():
    """Get tasks with full context stories and entity relationships"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    user = get_db_manager().get_user_by_email(user_email)
    if not user:
        return jsonify({'success': False, 'error': 'User not found'}), 404
    
    try:
        status_filter = request.args.get('status', None)
        limit = int(request.args.get('limit', 50))
        
        with get_db_manager().get_session() as session:
            query = session.query(Task).filter(Task.user_id == user.id)
            
            if status_filter:
                query = query.filter(Task.status == status_filter)
            
            tasks = query.order_by(Task.created_at.desc()).limit(limit).all()
            
            tasks_data = []
            for task in tasks:
                # Get related entities
                related_topics = [topic.name for topic in task.topics]
                assignee_info = None
                if task.assignee:
                    assignee_info = {
                        'name': task.assignee.name,
                        'email': task.assignee.email_address
                    }
                
                task_data = {
                    'id': task.id,
                    'description': task.description,
                    'context_story': task.context_story,
                    'priority': task.priority,
                    'status': task.status,
                    'category': task.category,
                    'confidence': task.confidence,
                    'due_date': task.due_date.isoformat() if task.due_date else None,
                    'created_at': task.created_at.isoformat(),
                    'updated_at': task.updated_at.isoformat(),
                    'completed_at': task.completed_at.isoformat() if task.completed_at else None,
                    
                    # Entity relationships
                    'assignee': assignee_info,
                    'related_topics': related_topics,
                    'source_type': 'email' if task.source_email_id else 'calendar' if task.source_event_id else 'manual',
                    
                    # Business context
                    'business_intelligence': {
                        'has_context': bool(task.context_story),
                        'entity_connections': len(related_topics) + (1 if assignee_info else 0),
                        'strategic_importance': calculate_task_strategic_importance(task)
                    }
                }
                tasks_data.append(task_data)
            
            return jsonify({
                'success': True,
                'tasks': tasks_data,
                'summary': {
                    'total_tasks': len(tasks_data),
                    'by_status': {
                        'pending': len([t for t in tasks_data if t['status'] == 'pending']),
                        'in_progress': len([t for t in tasks_data if t['status'] == 'in_progress']),
                        'completed': len([t for t in tasks_data if t['status'] == 'completed'])
                    },
                    'by_priority': {
                        'high': len([t for t in tasks_data if t['priority'] == 'high']),
                        'medium': len([t for t in tasks_data if t['priority'] == 'medium']),
                        'low': len([t for t in tasks_data if t['priority'] == 'low'])
                    },
                    'with_context': len([t for t in tasks_data if t['business_intelligence']['has_context']])
                }
            })
            
    except Exception as e:
        logger.error(f"Failed to get tasks with context intelligence: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/intelligence-insights', methods=['GET'])
def get_intelligence_insights():
    """Get proactive intelligence insights"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    user = get_db_manager().get_user_by_email(user_email)
    if not user:
        return jsonify({'success': False, 'error': 'User not found'}), 404
    
    try:
        status_filter = request.args.get('status', 'new')
        insight_type = request.args.get('type', None)
        limit = int(request.args.get('limit', 20))
        
        with get_db_manager().get_session() as session:
            query = session.query(IntelligenceInsight).filter(
                IntelligenceInsight.user_id == user.id
            )
            
            if status_filter:
                query = query.filter(IntelligenceInsight.status == status_filter)
            
            if insight_type:
                query = query.filter(IntelligenceInsight.insight_type == insight_type)
            
            # Filter out expired insights
            query = query.filter(
                (IntelligenceInsight.expires_at.is_(None)) | 
                (IntelligenceInsight.expires_at > datetime.utcnow())
            )
            
            insights = query.order_by(
                IntelligenceInsight.priority.desc(),
                IntelligenceInsight.created_at.desc()
            ).limit(limit).all()
            
            insights_data = []
            for insight in insights:
                insight_data = {
                    'id': insight.id,
                    'insight_type': insight.insight_type,
                    'title': insight.title,
                    'description': insight.description,
                    'priority': insight.priority,
                    'confidence': insight.confidence,
                    'status': insight.status,
                    'user_feedback': insight.user_feedback,
                    'created_at': insight.created_at.isoformat(),
                    'expires_at': insight.expires_at.isoformat() if insight.expires_at else None,
                    
                    # Related entity info
                    'related_entity': {
                        'type': insight.related_entity_type,
                        'id': insight.related_entity_id
                    } if insight.related_entity_type else None
                }
                insights_data.append(insight_data)
            
            return jsonify({
                'success': True,
                'insights': insights_data,
                'summary': {
                    'total_insights': len(insights_data),
                    'by_type': count_by_field(insights_data, 'insight_type'),
                    'by_priority': count_by_field(insights_data, 'priority'),
                    'actionable': len([i for i in insights_data if i['status'] == 'new'])
                }
            })
            
    except Exception as e:
        logger.error(f"Failed to get intelligence insights: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/insights/<int:insight_id>/feedback', methods=['POST'])
def submit_insight_feedback(insight_id: int):
    """Submit feedback on intelligence insight"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    user = get_db_manager().get_user_by_email(user_email)
    if not user:
        return jsonify({'success': False, 'error': 'User not found'}), 404
    
    try:
        feedback_data = request.get_json()
        feedback_type = feedback_data.get('feedback')  # helpful, not_helpful, acted_on
        
        if feedback_type not in ['helpful', 'not_helpful', 'acted_on']:
            return jsonify({'success': False, 'error': 'Invalid feedback type'}), 400
        
        # Process feedback through real-time system
        realtime_processor.process_user_action(
            'insight_feedback',
            {
                'insight_id': insight_id,
                'feedback': feedback_type
            },
            user.id
        )
        
        return jsonify({
            'success': True,
            'message': 'Feedback submitted successfully'
        })
        
    except Exception as e:
        logger.error(f"Failed to submit insight feedback: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

# =====================================================================
# BUSINESS INTELLIGENCE GENERATION
# =====================================================================

def generate_360_business_intelligence(user_id: int) -> Dict:
    """Generate comprehensive 360-degree business intelligence"""
    try:
        intelligence = {
            'entity_summary': {},
            'relationship_intelligence': {},
            'strategic_insights': {},
            'activity_patterns': {},
            'intelligence_quality': {}
        }
        
        with get_db_manager().get_session() as session:
            # Entity summary
            topics_count = session.query(Topic).filter(Topic.user_id == user_id).count()
            people_count = session.query(Person).filter(Person.user_id == user_id).count()
            tasks_count = session.query(Task).filter(Task.user_id == user_id).count()
            events_count = session.query(CalendarEvent).filter(CalendarEvent.user_id == user_id).count()
            
            intelligence['entity_summary'] = {
                'topics': topics_count,
                'people': people_count,
                'tasks': tasks_count,
                'calendar_events': events_count,
                'total_entities': topics_count + people_count + tasks_count + events_count
            }
            
            # Relationship intelligence
            from models.enhanced_models import EntityRelationship
            relationships_count = session.query(EntityRelationship).filter(
                EntityRelationship.user_id == user_id
            ).count()
            
            # Active topics (mentioned in last 30 days)
            active_topics = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.last_mentioned > datetime.utcnow() - timedelta(days=30)
            ).count()
            
            # Recent contacts
            recent_contacts = session.query(Person).filter(
                Person.user_id == user_id,
                Person.last_contact > datetime.utcnow() - timedelta(days=30)
            ).count()
            
            intelligence['relationship_intelligence'] = {
                'total_relationships': relationships_count,
                'active_topics': active_topics,
                'recent_contacts': recent_contacts,
                'relationship_density': relationships_count / max(1, people_count + topics_count)
            }
            
            # Activity patterns
            recent_tasks = session.query(Task).filter(
                Task.user_id == user_id,
                Task.created_at > datetime.utcnow() - timedelta(days=7)
            ).count()
            
            intelligence['activity_patterns'] = {
                'tasks_this_week': recent_tasks,
                'average_daily_tasks': recent_tasks / 7,
                'topic_momentum': active_topics / max(1, topics_count)
            }
            
            # Intelligence quality metrics
            high_confidence_tasks = session.query(Task).filter(
                Task.user_id == user_id,
                Task.confidence > 0.8
            ).count()
            
            tasks_with_context = session.query(Task).filter(
                Task.user_id == user_id,
                Task.context_story.isnot(None)
            ).count()
            
            intelligence['intelligence_quality'] = {
                'high_confidence_extractions': high_confidence_tasks / max(1, tasks_count),
                'contextualized_tasks': tasks_with_context / max(1, tasks_count),
                'entity_interconnection': relationships_count / max(1, intelligence['entity_summary']['total_entities'])
            }
        
        return intelligence
        
    except Exception as e:
        logger.error(f"Failed to generate 360 business intelligence: {str(e)}")
        return {}

def generate_intelligent_next_steps(intelligence_summary: Dict, insights: List) -> List[str]:
    """Generate intelligent next steps based on business intelligence"""
    next_steps = []
    
    try:
        entity_summary = intelligence_summary.get('entity_summary', {})
        relationship_intel = intelligence_summary.get('relationship_intelligence', {})
        activity_patterns = intelligence_summary.get('activity_patterns', {})
        
        # Intelligence-driven recommendations
        if entity_summary.get('topics', 0) > 5 and relationship_intel.get('relationship_density', 0) < 0.3:
            next_steps.append(" Consider connecting related topics and people to improve relationship intelligence")
        
        if activity_patterns.get('topic_momentum', 0) > 0.7:
            next_steps.append(" High topic activity detected - schedule strategic planning time")
        
        if relationship_intel.get('recent_contacts', 0) < relationship_intel.get('total_relationships', 0) * 0.3:
            next_steps.append(" Reach out to important contacts you haven't spoken with recently")
        
        if activity_patterns.get('tasks_this_week', 0) > 10:
            next_steps.append(" High task volume - consider prioritizing and delegating")
        
        # Insight-driven recommendations
        high_priority_insights = [i for i in insights if i.priority == 'high']
        if high_priority_insights:
            next_steps.append(f" Address {len(high_priority_insights)} high-priority insights")
        
        # Default recommendations if no specific patterns
        if not next_steps:
            next_steps.append(" Continue processing communications to build business intelligence")
            next_steps.append(" Review insights regularly to stay ahead of opportunities")
        
    except Exception as e:
        logger.error(f"Failed to generate intelligent next steps: {str(e)}")
        next_steps = ["Continue building your business intelligence through regular processing"]
    
    return next_steps

# =====================================================================
# UTILITY FUNCTIONS
# =====================================================================

def calculate_topic_activity_level(topic: Topic) -> float:
    """Calculate topic activity level based on mentions and recency"""
    if not topic.total_mentions or not topic.last_mentioned:
        return 0.0
    
    # Factor in recency and frequency
    days_since_mention = (datetime.utcnow() - topic.last_mentioned).days
    recency_factor = max(0, 1 - (days_since_mention / 30))  # Decay over 30 days
    frequency_factor = min(1.0, topic.total_mentions / 10)  # Normalize to 10 mentions
    
    return (recency_factor + frequency_factor) / 2

def calculate_topic_momentum(topic: Topic) -> float:
    """Calculate topic momentum (trending up or down)"""
    # This would analyze mention patterns over time
    # For now, return activity level as proxy
    return calculate_topic_activity_level(topic)

def calculate_relationship_strength(person: Person) -> float:
    """Calculate relationship strength score"""
    factors = []
    
    # Interaction frequency
    if person.total_interactions:
        factors.append(min(1.0, person.total_interactions / 20))
    
    # Recency
    if person.last_contact:
        days_since = (datetime.utcnow() - person.last_contact).days
        recency_score = max(0, 1 - (days_since / 90))  # Decay over 90 days
        factors.append(recency_score)
    
    # Importance level
    if person.importance_level:
        factors.append(person.importance_level)
    
    return sum(factors) / len(factors) if factors else 0.0

def calculate_communication_frequency(person: Person) -> str:
    """Calculate communication frequency category"""
    if not person.last_contact or not person.total_interactions:
        return 'never'
    
    days_since = (datetime.utcnow() - person.last_contact).days
    
    if days_since <= 7:
        return 'weekly'
    elif days_since <= 30:
        return 'monthly'
    elif days_since <= 90:
        return 'quarterly'
    else:
        return 'rarely'

def calculate_engagement_score(person: Person) -> float:
    """Calculate overall engagement score"""
    relationship_strength = calculate_relationship_strength(person)
    topic_connections = len(person.topics) if hasattr(person, 'topics') else 0
    topic_factor = min(1.0, topic_connections / 5)  # Normalize to 5 topics
    
    return (relationship_strength + topic_factor) / 2

def get_person_topic_affinity(person_id: int, topic_id: int) -> float:
    """Get person-topic affinity score"""
    try:
        from models.enhanced_models import person_topic_association
        from models.database import get_db_manager
        
        with get_db_manager().get_session() as session:
            result = session.execute(
                person_topic_association.select().where(
                    (person_topic_association.c.person_id == person_id) &
                    (person_topic_association.c.topic_id == topic_id)
                )
            ).first()
            
            return result.affinity_score if result else 0.5
            
    except Exception as e:
        logger.error(f"Failed to get person-topic affinity: {str(e)}")
        return 0.5

def calculate_task_strategic_importance(task: Task) -> float:
    """Calculate strategic importance of task"""
    factors = []
    
    # Priority factor
    priority_scores = {'high': 1.0, 'medium': 0.6, 'low': 0.3}
    factors.append(priority_scores.get(task.priority, 0.5))
    
    # Confidence factor
    factors.append(task.confidence)
    
    # Context factor
    if task.context_story:
        factors.append(0.8)
    
    # Entity connections factor
    connections = len(task.topics) if hasattr(task, 'topics') else 0
    if task.assignee:
        connections += 1
    factors.append(min(1.0, connections / 3))
    
    return sum(factors) / len(factors)

def count_by_field(data: List[Dict], field: str) -> Dict:
    """Count occurrences of field values"""
    counts = {}
    for item in data:
        value = item.get(field, 'unknown')
        counts[value] = counts.get(value, 0) + 1
    return counts

if __name__ == '__main__':
    app.run(
        host=settings.HOST,
        port=settings.PORT,
        debug=settings.DEBUG
    )': False, 'error': 'User not found'}), 404
    
    if request.method == 'GET':
        return get_topics_with_intelligence(user.id)
    elif request.method == 'POST':
        return create_topic_with_intelligence(user.id, request.get_json())

def get_topics_with_intelligence(user_id: int):
    """Get topics with accumulated intelligence and relationships"""
    try:
        with get_db_manager().get_session() as session:
            topics = session.query(Topic).filter(Topic.user_id == user_id).all()
            
            topics_data = []
            for topic in topics:
                # Get related entities count
                related_people = len(topic.people)
                related_tasks = len(topic.tasks)
                related_events = len(topic.events)
                
                topic_data = {
                    'id': topic.id,
                    'name': topic.name,
                    'description': topic.description,
                    'keywords': topic.keywords.split(',') if topic.keywords else [],
                    'is_official': topic.is_official,
                    'confidence_score': topic.confidence_score,
                    'total_mentions': topic.total_mentions,
                    'last_mentioned': topic.last_mentioned.isoformat() if topic.last_mentioned else None,
                    'strategic_importance': topic.strategic_importance,
                    'intelligence_summary': topic.intelligence_summary,
                    'created_at': topic.created_at.isoformat(),
                    'updated_at': topic.updated_at.isoformat(),
                    'version': topic.version,
                    
                    # Relationship intelligence
                    'related_entities': {
                        'people': related_people,
                        'tasks': related_tasks,
                        'events': related_events,
                        'total': related_people + related_tasks + related_events
                    },
                    
                    # Activity metrics
                    'activity_level': calculate_topic_activity_level(topic),
                    'momentum': calculate_topic_momentum(topic)
                }
                topics_data.append(topic_data)
            
            # Sort by strategic importance and recent activity
            topics_data.sort(key=lambda x: (x['strategic_importance'], x['total_mentions']), reverse=True)
            
            return jsonify({
                'success': True,
                'topics': topics_data,
                'summary': {
                    'total_topics': len(topics_data),
                    'official_topics': len([t for t in topics_data if t['is_official']]),
                    'active_topics': len([t for t in topics_data if t['activity_level'] > 0.5]),
                    'high_momentum': len([t for t in topics_data if t['momentum'] > 0.7])
                }
            })
            
    except Exception as e:
        logger.error(f"Failed to get topics with intelligence: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

def create_topic_with_intelligence(user_id: int, topic_data: Dict):
    """Create topic with intelligent categorization and relationships"""
    try:
        name = topic_data.get('name', '').strip()
        description = topic_data.get('description', '')
        keywords = topic_data.get('keywords', [])
        
        if not name:
            return jsonify({'success': False, 'error': 'Topic name is required'}), 400
        
        # Create entity context
        context = EntityContext(
            source_type='manual',
            user_id=user_id,
            confidence=1.0  # High confidence for manual creation
        )
        
        # Create topic through unified engine
        topic = entity_engine.create_or_update_topic(
            topic_name=name,
            description=description,
            keywords=keywords,
            context=context
        )
        
        if topic:
            # Mark as official since it was manually created
            with get_db_manager().get_session() as session:
                topic = session.merge(topic)
                topic.is_official = True
                session.commit()
            
            # Trigger real-time analysis to find related entities
            realtime_processor.process_entity_update(
                'topic', topic.id, {'manual_creation': True}, user_id
            )
            
            return jsonify({
                'success': True,
                'topic': {
                    'id': topic.id,
                    'name': topic.name,
                    'description': topic.description,
                    'is_official': topic.is_official,
                    'created_at': topic.created_at.isoformat()
                }
            })
        else:
            return jsonify({'success': False, 'error': 'Failed to create topic'}), 500
            
    except Exception as e:
        logger.error(f"Failed to create topic with intelligence: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/people', methods=['GET'])
def people_with_relationship_intelligence():
    """Get people with comprehensive relationship intelligence"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    user = get_db_manager().get_user_by_email(user_email)
    if not user:
        return jsonify({'success

============================================================
FILE: archive/refactor_docs/6_enhanced_frontend.txt
============================================================
<a href="/tasks" class="nav-link flex items-center gap-3 px-3 py-2 cursor-pointer hover:bg-[#223649] rounded-full transition-colors">
                                    <div class="text-white"></div>
                                    <p class="text-white text-sm font-medium leading-normal">Context Tasks</p>
                                </a>
                                
                                <a href="/calendar" class="nav-link flex items-center gap-3 px-3 py-2 cursor-pointer hover:bg-[#223649] rounded-full transition-colors">
                                    <div class="text-white"></div>
                                    <p class="text-white text-sm font-medium leading-normal">Smart Calendar</p>
                                </a>
                            </div>
                        </div>
                        
                        <!-- Real-Time Intelligence Actions -->
                        <div class="space-y-3">
                            <button id="unifiedSyncBtn" class="w-full flex items-center justify-center gap-2 px-4 py-3 bg-gradient-to-r from-blue-600 to-purple-600 text-white rounded-full text-sm font-bold hover:from-blue-700 hover:to-purple-700 transition-all">
                                <span class="text-lg"></span>
                                <span>Unified Intelligence Sync</span>
                            </button>
                            
                            <button id="generateInsightsBtn" class="w-full flex items-center justify-center gap-2 px-4 py-2 bg-[#223649] text-white rounded-full text-sm font-medium hover:bg-[#314d68] transition-colors">
                                <span></span>
                                <span>Generate Insights</span>
                            </button>
                        </div>
                    </div>
                </div>
                
                <!-- Main Intelligence Dashboard -->
                <div class="layout-content-container flex flex-col max-w-[960px] flex-1">
                    <!-- Dashboard Header -->
                    <div class="px-4 py-6">
                        <div class="flex items-center justify-between mb-4">
                            <div>
                                <h1 class="text-3xl font-bold text-white mb-2"> Intelligence Dashboard</h1>
                                <p class="text-[#90aecb] text-lg">Your AI-powered business intelligence center with real-time entity connections</p>
                            </div>
                            <div class="text-right">
                                <div class="text-[#90aecb] text-sm">Last Intelligence Update</div>
                                <div id="lastUpdateTime" class="text-white font-medium">--</div>
                            </div>
                        </div>
                        
                        <!-- Real-Time Intelligence Metrics -->
                        <div class="grid grid-cols-4 gap-4 mb-8">
                            <div class="bg-[#223649] p-4 rounded-lg border border-[#314d68] intelligence-card strategic-importance-high">
                                <div class="flex items-center justify-between mb-2">
                                    <h3 class="text-sm font-medium text-[#90aecb]">Active Insights</h3>
                                    <span class="text-2xl"></span>
                                </div>
                                <p id="activeInsights" class="text-2xl font-bold text-white">0</p>
                                <p class="text-xs text-[#90aecb] mt-1">High-priority intelligence</p>
                            </div>

                            <div class="bg-[#223649] p-4 rounded-lg border border-[#314d68] intelligence-card strategic-importance-medium">
                                <div class="flex items-center justify-between mb-2">
                                    <h3 class="text-sm font-medium text-[#90aecb]">Entity Network</h3>
                                    <span class="text-2xl"></span>
                                </div>
                                <p id="entityNetworkSize" class="text-2xl font-bold text-white">0</p>
                                <p class="text-xs text-[#90aecb] mt-1">Connected entities</p>
                            </div>

                            <div class="bg-[#223649] p-4 rounded-lg border border-[#314d68] intelligence-card strategic-importance-medium">
                                <div class="flex items-center justify-between mb-2">
                                    <h3 class="text-sm font-medium text-[#90aecb]">Topic Momentum</h3>
                                    <span class="text-2xl"></span>
                                </div>
                                <p id="topicMomentum" class="text-2xl font-bold text-white">0%</p>
                                <p class="text-xs text-[#90aecb] mt-1">Business velocity</p>
                            </div>

                            <div class="bg-[#223649] p-4 rounded-lg border border-[#314d68] intelligence-card strategic-importance-low">
                                <div class="flex items-center justify-between mb-2">
                                    <h3 class="text-sm font-medium text-[#90aecb]">Relationship Score</h3>
                                    <span class="text-2xl"></span>
                                </div>
                                <p id="relationshipScore" class="text-2xl font-bold text-white">0</p>
                                <p class="text-xs text-[#90aecb] mt-1">Network strength</p>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Proactive Intelligence Insights -->
                    <div class="px-4 mb-8">
                        <div class="flex items-center justify-between mb-4">
                            <h2 class="text-xl font-semibold text-white"> Proactive Intelligence</h2>
                            <div class="flex gap-2">
                                <button class="insight-filter active px-3 py-1 text-sm rounded bg-[#0b80ee] text-white" data-filter="all">All</button>
                                <button class="insight-filter px-3 py-1 text-sm rounded bg-[#223649] text-[#90aecb] hover:bg-[#314d68]" data-filter="high">High Priority</button>
                                <button class="insight-filter px-3 py-1 text-sm rounded bg-[#223649] text-[#90aecb] hover:bg-[#314d68]" data-filter="relationship">Relationships</button>
                                <button class="insight-filter px-3 py-1 text-sm rounded bg-[#223649] text-[#90aecb] hover:bg-[#314d68]" data-filter="topics">Topics</button>
                            </div>
                        </div>
                        
                        <div id="proactiveInsightsContainer" class="space-y-3">
                            <!-- Insights will be loaded here -->
                            <div class="text-center py-8 text-[#90aecb]">
                                <div class="text-4xl mb-2"></div>
                                <p>AI is analyzing your business intelligence...</p>
                                <p class="text-sm mt-1">Proactive insights will appear here</p>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Entity Intelligence Network -->
                    <div class="px-4 mb-8">
                        <h2 class="text-xl font-semibold text-white mb-4"> Entity Intelligence Network</h2>
                        
                        <div class="grid grid-cols-1 lg:grid-cols-2 gap-6">
                            <!-- Topics Brain -->
                            <div class="bg-[#223649] rounded-lg border border-[#314d68] p-6">
                                <div class="flex items-center justify-between mb-4">
                                    <h3 class="text-white font-semibold"> Topics Brain</h3>
                                    <span class="text-xs bg-purple-600 text-white px-2 py-1 rounded-full">Central Intelligence</span>
                                </div>
                                
                                <div id="topicsBrainContainer" class="space-y-3 max-h-64 overflow-y-auto">
                                    <!-- Topics will be loaded here -->
                                </div>
                                
                                <div class="mt-4 pt-3 border-t border-[#314d68]">
                                    <button class="w-full text-center text-[#0b80ee] text-sm hover:underline" onclick="location.href='/topics'">
                                        View Full Topics Intelligence 
                                    </button>
                                </div>
                            </div>
                            
                            <!-- Relationship Intelligence -->
                            <div class="bg-[#223649] rounded-lg border border-[#314d68] p-6">
                                <div class="flex items-center justify-between mb-4">
                                    <h3 class="text-white font-semibold"> Relationship Intelligence</h3>
                                    <span class="text-xs bg-green-600 text-white px-2 py-1 rounded-full">Dynamic</span>
                                </div>
                                
                                <div id="relationshipIntelContainer" class="space-y-3 max-h-64 overflow-y-auto">
                                    <!-- Relationships will be loaded here -->
                                </div>
                                
                                <div class="mt-4 pt-3 border-t border-[#314d68]">
                                    <button class="w-full text-center text-[#0b80ee] text-sm hover:underline" onclick="location.href='/people'">
                                        View Full Relationship Map 
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Context-Aware Tasks -->
                    <div class="px-4 mb-8">
                        <div class="flex items-center justify-between mb-4">
                            <h2 class="text-xl font-semibold text-white"> Context-Aware Tasks</h2>
                            <div class="text-sm text-[#90aecb]">
                                <span id="tasksWithContext">0</span> tasks with business context
                            </div>
                        </div>
                        
                        <div id="contextTasksContainer" class="space-y-3">
                            <!-- Context tasks will be loaded here -->
                        </div>
                        
                        <div class="mt-4 text-center">
                            <button class="text-[#0b80ee] text-sm hover:underline" onclick="location.href='/tasks'">
                                View All Context Tasks 
                            </button>
                        </div>
                    </div>
                    
                    <!-- AI Chat Interface -->
                    <div class="px-4 py-6 border-t border-[#314d68]">
                        <h2 class="text-xl font-semibold text-white mb-4"> Intelligence Assistant</h2>
                        
                        <div class="bg-[#223649] rounded-lg border border-[#314d68] p-4">
                            <div id="chatHistory" class="space-y-3 mb-4 max-h-64 overflow-y-auto">
                                <div class="text-[#90aecb] text-sm text-center py-4">
                                    Ask me anything about your business intelligence, relationships, or strategic insights...
                                </div>
                            </div>
                            
                            <div class="flex gap-3">
                                <input id="chatInput" type="text" 
                                       placeholder="Ask about your business intelligence..." 
                                       class="flex-1 bg-[#101a23] text-white border border-[#314d68] rounded-lg px-4 py-2 focus:outline-none focus:border-[#0b80ee] transition-colors">
                                <button id="sendChatBtn" 
                                        class="px-6 py-2 bg-[#0b80ee] text-white rounded-lg hover:bg-blue-600 transition-colors">
                                    Send
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        // Global state for real-time intelligence
        let intelligenceState = {
            entities: { topics: 0, people: 0, tasks: 0, events: 0 },
            insights: [],
            relationships: [],
            processing: false,
            lastUpdate: null
        };

        // Initialize dashboard
        document.addEventListener('DOMContentLoaded', function() {
            initializeIntelligenceDashboard();
            setupEventListeners();
            startRealTimeUpdates();
        });

        function initializeIntelligenceDashboard() {
            loadIntelligenceMetrics();
            loadProactiveInsights();
            loadEntityNetwork();
            loadContextTasks();
            updateLastUpdateTime();
        }

        function setupEventListeners() {
            // Unified sync button
            document.getElementById('unifiedSyncBtn').addEventListener('click', runUnifiedIntelligenceSync);
            
            // Generate insights button
            document.getElementById('generateInsightsBtn').addEventListener('click', generateProactiveInsights);
            
            // Chat functionality
            document.getElementById('sendChatBtn').addEventListener('click', sendIntelligenceChat);
            document.getElementById('chatInput').addEventListener('keypress', function(e) {
                if (e.key === 'Enter') sendIntelligenceChat();
            });
            
            // Insight filters
            document.querySelectorAll('.insight-filter').forEach(btn => {
                btn.addEventListener('click', function() {
                    filterInsights(this.dataset.filter);
                    updateFilterButtons(this);
                });
            });
        }

        function startRealTimeUpdates() {
            // Update metrics every 30 seconds
            setInterval(() => {
                if (!intelligenceState.processing) {
                    loadIntelligenceMetrics();
                    updateLastUpdateTime();
                }
            }, 30000);
            
            // Check for new insights every 60 seconds
            setInterval(() => {
                loadProactiveInsights();
            }, 60000);
        }

        async function runUnifiedIntelligenceSync() {
            const btn = document.getElementById('unifiedSyncBtn');
            const originalText = btn.innerHTML;
            
            try {
                intelligenceState.processing = true;
                showProcessingStatus(true);
                
                btn.innerHTML = '<div class="animate-spin rounded-full h-4 w-4 border-b-2 border-white"></div> Processing Intelligence...';
                btn.disabled = true;
                
                // Show AI thinking indicator
                document.getElementById('aiThinkingIndicator').classList.remove('hidden');
                
                const response = await fetch('/api/unified-intelligence-sync', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        max_emails: 25,
                        days_back: 7,
                        days_forward: 14,
                        force_refresh: false
                    })
                });
                
                const data = await response.json();
                
                if (data.success) {
                    // Update intelligence state
                    intelligenceState.entities = data.entity_intelligence?.entity_summary || intelligenceState.entities;
                    intelligenceState.insights = data.insights_generated || [];
                    
                    // Show detailed success message
                    showIntelligenceSuccess(data);
                    
                    // Refresh all dashboard components
                    await refreshIntelligenceDashboard();
                    
                } else {
                    showError('Intelligence sync failed: ' + (data.error || 'Unknown error'));
                }
                
            } catch (error) {
                console.error('Intelligence sync error:', error);
                showError('Failed to sync intelligence. Please try again.');
            } finally {
                intelligenceState.processing = false;
                showProcessingStatus(false);
                btn.innerHTML = originalText;
                btn.disabled = false;
                document.getElementById('aiThinkingIndicator').classList.add('hidden');
            }
        }

        async function loadIntelligenceMetrics() {
            try {
                // This would call a new endpoint for real-time metrics
                const response = await fetch('/api/intelligence-metrics');
                const data = await response.json();
                
                if (data.success) {
                    updateIntelligenceMetrics(data.metrics);
                }
            } catch (error) {
                console.error('Failed to load intelligence metrics:', error);
            }
        }

        function updateIntelligenceMetrics(metrics) {
            // Update entity counts
            document.getElementById('entityCount').textContent = metrics.total_entities || 0;
            document.getElementById('topicsCount').textContent = `${metrics.topics || 0} Topics`;
            document.getElementById('peopleCount').textContent = `${metrics.people || 0} People`;
            
            // Update intelligence score (0-100)
            const score = Math.round((metrics.intelligence_quality || 0) * 100);
            document.getElementById('intelligenceScore').textContent = score;
            document.getElementById('intelligenceProgress').style.width = score + '%';
            
            // Update other metrics
            document.getElementById('activeInsights').textContent = metrics.active_insights || 0;
            document.getElementById('entityNetworkSize').textContent = metrics.entity_relationships || 0;
            document.getElementById('topicMomentum').textContent = Math.round((metrics.topic_momentum || 0) * 100) + '%';
            document.getElementById('relationshipScore').textContent = Math.round((metrics.relationship_density || 0) * 100);
        }

        async function loadProactiveInsights() {
            try {
                const response = await fetch('/api/intelligence-insights?status=new&limit=10');
                const data = await response.json();
                
                if (data.success) {
                    displayProactiveInsights(data.insights);
                }
            } catch (error) {
                console.error('Failed to load proactive insights:', error);
            }
        }

        function displayProactiveInsights(insights) {
            const container = document.getElementById('proactiveInsightsContainer');
            
            if (!insights || insights.length === 0) {
                container.innerHTML = `
                    <div class="text-center py-8 text-[#90aecb]">
                        <div class="text-4xl mb-2"></div>
                        <p>No new insights available</p>
                        <p class="text-sm mt-1">Process more data to generate intelligence</p>
                    </div>
                `;
                return;
            }
            
            container.innerHTML = insights.map(insight => createInsightCard(insight)).join('');
        }

        function createInsightCard(insight) {
            const priorityColors = {
                'high': 'border-red-500 bg-red-500 bg-opacity-10',
                'medium': 'border-yellow-500 bg-yellow-500 bg-opacity-10',
                'low': 'border-green-500 bg-green-500 bg-opacity-10'
            };
            
            const typeEmojis = {
                'relationship_alert': '',
                'topic_momentum': '',
                'meeting_prep': '',
                'project_attention': '',
                'important_contact': '',
                'urgent_task': ''
            };
            
            const colorClass = priorityColors[insight.priority] || priorityColors['medium'];
            const emoji = typeEmojis[insight.insight_type] || '';
            
            return `
                <div class="intelligence-card bg-[#182734] rounded-lg border ${colorClass} p-4" data-insight-type="${insight.insight_type}" data-priority="${insight.priority}">
                    <div class="flex items-start gap-4">
                        <div class="text-2xl">${emoji}</div>
                        <div class="flex-1 min-w-0">
                            <div class="flex items-start justify-between gap-3">
                                <h3 class="text-white font-medium leading-tight">${escapeHtml(insight.title)}</h3>
                                <div class="flex gap-2 shrink-0">
                                    <span class="text-xs px-2 py-1 rounded-full ${getPriorityColor(insight.priority)}">${insight.priority.toUpperCase()}</span>
                                    <span class="text-xs px-2 py-1 rounded-full bg-[#314d68] text-[#90aecb]">${Math.round(insight.confidence * 100)}%</span>
                                </div>
                            </div>
                            <p class="text-[#90aecb] text-sm mt-2 leading-relaxed">${escapeHtml(insight.description)}</p>
                            <div class="flex items-center justify-between mt-3">
                                <span class="text-xs text-[#78879a]">${formatTimeAgo(insight.created_at)}</span>
                                <div class="flex gap-2">
                                    <button onclick="markInsightFeedback(${insight.id}, 'helpful')" class="text-xs px-3 py-1 bg-green-600 hover:bg-green-700 text-white rounded"> Helpful</button>
                                    <button onclick="markInsightFeedback(${insight.id}, 'not_helpful')" class="text-xs px-3 py-1 bg-gray-600 hover:bg-gray-700 text-white rounded"> Not helpful</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            `;
        }

        async function loadEntityNetwork() {
            try {
                // Load topics brain
                const topicsResponse = await fetch('/api/topics?limit=5');
                const topicsData = await topicsResponse.json();
                
                if (topicsData.success) {
                    displayTopicsBrain(topicsData.topics);
                }
                
                // Load relationship intelligence
                const peopleResponse = await fetch('/api/people?limit=5');
                const peopleData = await peopleResponse.json();
                
                if (peopleData.success) {
                    displayRelationshipIntelligence(peopleData.people);
                }
                
            } catch (error) {
                console.error('Failed to load entity network:', error);
            }
        }

        function displayTopicsBrain(topics) {
            const container = document.getElementById('topicsBrainContainer');
            
            if (!topics || topics.length === 0) {
                container.innerHTML = '<div class="text-[#90aecb] text-sm text-center py-4">No topics discovered yet</div>';
                return;
            }
            
            container.innerHTML = topics.map(topic => `
                <div class="entity-connection rounded-lg p-3 hover:bg-[#314d68] transition-colors cursor-pointer">
                    <div class="flex items-center justify-between">
                        <div class="flex-1 min-w-0">
                            <h4 class="text-white font-medium text-sm truncate">${escapeHtml(topic.name)}</h4>
                            <div class="flex items-center gap-2 mt-1">
                                <span class="text-xs text-[#90aecb]">${topic.total_mentions} mentions</span>
                                ${topic.is_official ? '<span class="text-xs bg-blue-600 text-white px-2 py-1 rounded">Official</span>' : ''}
                            </div>
                        </div>
                        <div class="text-lg">${topic.activity_level > 0.7 ? '' : topic.activity_level > 0.4 ? '' : ''}</div>
                    </div>
                    ${topic.related_entities && topic.related_entities.total > 0 ? `
                        <div class="mt-2 text-xs text-[#78879a]">
                            Connected: ${topic.related_entities.people} people, ${topic.related_entities.tasks} tasks
                        </div>
                    ` : ''}
                </div>
            `).join('');
        }

        function displayRelationshipIntelligence(people) {
            const container = document.getElementById('relationshipIntelContainer');
            
            if (!people || people.length === 0) {
                container.innerHTML = '<div class="text-[#90aecb] text-sm text-center py-4">No relationship data yet</div>';
                return;
            }
            
            container.innerHTML = people.map(person => `
                <div class="entity-connection rounded-lg p-3 hover:bg-[#314d68] transition-colors cursor-pointer">
                    <div class="flex items-center gap-3">
                        <div class="w-8 h-8 rounded-full bg-gradient-to-br from-blue-500 to-purple-600 flex items-center justify-center text-white text-xs font-bold">
                            ${getInitials(person.name)}
                        </div>
                        <div class="flex-1 min-w-0">
                            <h4 class="text-white font-medium text-sm truncate">${escapeHtml(person.name)}</h4>
                            <div class="text-xs text-[#90aecb] truncate">${escapeHtml(person.company || 'Unknown company')}</div>
                        </div>
                        <div class="text-right">
                            <div class="text-xs text-[#90aecb]">${person.total_interactions} interactions</div>
                            <div class="text-xs text-yellow-400">${Math.round((person.importance_level || 0) * 100)}% importance</div>
                        </div>
                    </div>
                    ${person.relationship_intelligence ? `
                        <div class="mt-2 text-xs text-[#78879a]">
                            Strength: ${Math.round(person.relationship_intelligence.strength * 100)}%  
                            Topics: ${person.relationship_intelligence.topic_connections}
                        </div>
                    ` : ''}
                </div>
            `).join('');
        }

        async function loadContextTasks() {
            try {
                const response = await fetch('/api/tasks?limit=5');
                const data = await response.json();
                
                if (data.success) {
                    displayContextTasks(data.tasks);
                    document.getElementById('tasksWithContext').textContent = 
                        data.summary?.with_context || 0;
                }
            } catch (error) {
                console.error('Failed to load context tasks:', error);
            }
        }

        function displayContextTasks(tasks) {
            const container = document.getElementById('contextTasksContainer');
            
            if (!tasks || tasks.length === 0) {
                container.innerHTML = '<div class="text-[#90aecb] text-sm text-center py-4">No context tasks yet</div>';
                return;
            }
            
            container.innerHTML = tasks.slice(0, 3).map(task => `
                <div class="intelligence-card bg-[#223649] rounded-lg border border-[#314d68] p-4">
                    <div class="flex items-start justify-between gap-3">
                        <div class="flex-1 min-w-0">
                            <h4 class="text-white font-medium text-sm leading-tight">${escapeHtml(task.description)}</h4>
                            ${task.context_story ? `
                                <p class="text-[#90aecb] text-xs mt-2 leading-relaxed">${escapeHtml(task.context_story)}</p>
                            ` : ''}
                            <div class="flex items-center gap-2 mt-3">
                                <span class="text-xs px-2 py-1 rounded ${getPriorityColor(task.priority)}">${task.priority.toUpperCase()}</span>
                                ${task.related_topics.length > 0 ? `
                                    <span class="text-xs text-[#90aecb]">Topics: ${task.related_topics.slice(0, 2).join(', ')}</span>
                                ` : ''}
                            </div>
                        </div>
                        <div class="text-lg"></div>
                    </div>
                </div>
            `).join('');
        }

        async function sendIntelligenceChat() {
            const input = document.getElementById('chatInput');
            const message = input.value.trim();
            
            if (!message) return;
            
            const chatHistory = document.getElementById('chatHistory');
            
            // Add user message
            addChatMessage('user', message, chatHistory);
            input.value = '';
            
            // Add loading message
            const loadingId = addChatMessage('ai', 'Analyzing your business intelligence...', chatHistory);
            
            try {
                const response = await fetch('/api/chat-with-knowledge', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        message: message,
                        include_context: true
                    })
                });
                
                const data = await response.json();
                
                // Remove loading message
                document.getElementById(loadingId).remove();
                
                if (data.success) {
                    addChatMessage('ai', data.response, chatHistory);
                } else {
                    addChatMessage('ai', 'Sorry, I encountered an error processing your request.', chatHistory);
                }
                
            } catch (error) {
                console.error('Chat error:', error);
                document.getElementById(loadingId).remove();
                addChatMessage('ai', 'Sorry, I encountered a network error.', chatHistory);
            }
        }

        function addChatMessage(sender, message, container) {
            const messageId = 'msg-' + Date.now() + '-' + Math.random().toString(36).substr(2, 9);
            const isUser = sender === 'user';
            
            const messageDiv = document.createElement('div');
            messageDiv.id = messageId;
            messageDiv.className = `flex ${isUser ? 'justify-end' : 'justify-start'}`;
            messageDiv.innerHTML = `
                <div class="max-w-[80%] ${isUser ? 'bg-[#0b80ee] text-white' : 'bg-[#314d68] text-white'} rounded-lg px-4 py-2">
                    <div class="text-xs opacity-70 mb-1">${isUser ? 'You' : 'AI Assistant'}</div>
                    <div class="text-sm whitespace-pre-wrap">${escapeHtml(message)}</div>
                </div>
            `;
            
            container.appendChild(messageDiv);
            container.scrollTop = container.scrollHeight;
            
            return messageId;
        }

        async function refreshIntelligenceDashboard() {
            loadIntelligenceMetrics();
            loadProactiveInsights();
            loadEntityNetwork();
            loadContextTasks();
            updateLastUpdateTime();
        }

        async function generateProactiveInsights() {
            const btn = document.getElementById('generateInsightsBtn');
            const originalText = btn.innerHTML;
            
            try {
                btn.innerHTML = '<div class="animate-spin rounded-full h-4 w-4 border-b-2 border-white"></div> Generating...';
                btn.disabled = true;
                
                // This would trigger proactive insight generation
                const response = await fetch('/api/generate-insights', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' }
                });
                
                const data = await response.json();
                
                if (data.success) {
                    showSuccess(`Generated ${data.insights_count || 0} new insights`);
                    loadProactiveInsights();
                } else {
                    showError('Failed to generate insights');
                }
                
            } catch (error) {
                console.error('Failed to generate insights:', error);
                showError('Failed to generate insights');
            } finally {
                btn.innerHTML = originalText;
                btn.disabled = false;
            }
        }

        function filterInsights(filter) {
            const insights = document.querySelectorAll('[data-insight-type]');
            
            insights.forEach(insight => {
                const shouldShow = filter === 'all' || 
                                 (filter === 'high' && insight.dataset.priority === 'high') ||
                                 (filter === 'relationship' && insight.dataset.insightType.includes('relationship')) ||
                                 (filter === 'topics' && insight.dataset.insightType.includes('topic'));
                
                insight.style.display = shouldShow ? 'block' : 'none';
            });
        }

        function updateFilterButtons(activeBtn) {
            document.querySelectorAll('.insight-filter').forEach(btn => {
                btn.classList.remove('active', 'bg-[#0b80ee]', 'text-white');
                btn.classList.add('bg-[#223649]', 'text-[#90aecb]');
            });
            
            activeBtn.classList.add('active', 'bg-[#0b80ee]', 'text-white');
            activeBtn.classList.remove('bg-[#223649]', 'text-[#90aecb]');
        }

        async function markInsightFeedback(insightId, feedback) {
            try {
                const response = await fetch(`/api/insights/${insightId}/feedback`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ feedback: feedback })
                });
                
                if (response.ok) {
                    showSuccess(`Feedback submitted: ${feedback}`);
                    // Remove or update the insight card
                    const insightCard = document.querySelector(`[data-insight-id="${insightId}"]`);
                    if (insightCard) {
                        insightCard.style.opacity = '0.5';
                        insightCard.querySelector('.flex.gap-2').innerHTML = 
                            `<span class="text-xs text-green-400"> ${feedback.replace('_', ' ')}</span>`;
                    }
                } else {
                    showError('Failed to submit feedback');
                }
                
            } catch (error) {
                console.error('Failed to submit feedback:', error);
                showError('Failed to submit feedback');
            }
        }

        function showProcessingStatus(show) {
            const statusBar = document.getElementById('processingStatus');
            if (show) {
                statusBar.classList.remove('hidden');
                // Simulate progress
                let progress = 0;
                const interval = setInterval(() => {
                    progress += Math.random() * 15;
                    if (progress >= 100) {
                        progress = 100;
                        clearInterval(interval);
                    }
                    document.getElementById('processingProgress').textContent = Math.round(progress) + '%';
                }, 500);
            } else {
                statusBar.classList.add('hidden');
            }
        }

        function showIntelligenceSuccess(data) {
            const stages = data.processing_stages || {};
            const intelligence = data.entity_intelligence || {};
            const insights = data.insights_generated || [];
            
            let message = ` Intelligence Sync Complete!\n\n`;
            message += ` Processed ${stages.emails_fetched || 0} emails\n`;
            message += ` Enhanced ${stages.calendar_events_fetched || 0} calendar events\n`;
            message += ` Generated ${insights.length} strategic insights\n`;
            message += ` Updated ${intelligence.entity_summary?.total_entities || 0} entity connections\n\n`;
            
            if (data.next_steps && data.next_steps.length > 0) {
                message += `Next Steps:\n${data.next_steps.join('\n')}`;
            }
            
            showSuccess(message);
        }

        function updateLastUpdateTime() {
            const now = new Date();
            document.getElementById('lastUpdateTime').textContent = 
                now.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' });
        }

        // Utility functions
        function escapeHtml(text) {
            if (!text) return '';
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }

        function getPriorityColor(priority) {
            const colors = {
                'high': 'bg-red-600 text-white',
                'medium': 'bg-yellow-600 text-white',
                'low': 'bg-green-600 text-white'
            };
            return colors[priority] || colors['medium'];
        }

        function getInitials(name) {
            if (!name) return '?';
            const words = name.split(' ').filter(word => word.length > 0);
            if (words.length === 0) return '?';
            if (words.length === 1) return words[0].charAt(0).toUpperCase();
            return (words[0].charAt(0) + words[words.length - 1].charAt(0)).toUpperCase();
        }

        function formatTimeAgo(dateString) {
            if (!dateString) return 'Unknown time';
            
            const date = new Date(dateString);
            const now = new Date();
            const diffMs = now - date;
            const diffMins = Math.round(diffMs / 60000);
            const diffHours = Math.round(diffMs / 3600000);
            const diffDays = Math.round(diffMs / 86400000);
            
            if (diffMins < 1) return 'Just now';
            if (diffMins < 60) return `${diffMins}m ago`;
            if (diffHours < 24) return `${diffHours}h ago`;
            if (diffDays < 7) return `${diffDays}d ago`;
            return date.toLocaleDateString();
        }

        function showSuccess(message) {
            const toast = document.createElement('div');
            toast.className = 'fixed top-4 right-4 bg-green-600 text-white px-6 py-4 rounded-lg z-50 max-w-md shadow-lg';
            toast.style.whiteSpace = 'pre-line';
            toast.style.fontSize = '14px';
            toast.style.lineHeight = '1.4';
            toast.textContent = message;
            document.body.appendChild(toast);
            
            setTimeout(() => {
                if (document.body.contains(toast)) {
                    document.body.removeChild(toast);
                }
            }, message.includes('') ? 8000 : 4000);
        }

        function showError(message) {
            const toast = document.createElement('div');
            toast.className = 'fixed top-4 right-4 bg-red-600 text-white px-4 py-2 rounded-lg z-50';
            toast.textContent = message;
            document.body.appendChild(toast);
            
            setTimeout(() => {
                if (document.body.contains(toast)) {
                    document.body.removeChild(toast);
                }
            }, 5000);
        }
    </script>
</body>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Chief of Staff - Intelligence Dashboard</title>
    
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="" />
    <link rel="stylesheet" as="style" onload="this.rel='stylesheet'" 
          href="https://fonts.googleapis.com/css2?display=swap&family=Inter%3Awght%40400%3B500%3B700%3B900&family=Noto+Sans%3Awght%40400%3B500%3B700%3B900" />
    
    <script src="https://cdn.tailwindcss.com?plugins=forms,container-queries"></script>
    
    <style>
        .intelligence-card { 
            transition: all 0.3s ease; 
            border-left: 4px solid transparent;
        }
        .intelligence-card:hover { 
            transform: translateY(-2px); 
            box-shadow: 0 8px 25px rgba(11, 128, 238, 0.15);
            border-left-color: #0b80ee;
        }
        .insight-pulse { animation: pulse 2s cubic-bezier(0.4, 0, 0.6, 1) infinite; }
        .entity-connection { 
            background: linear-gradient(135deg, rgba(11, 128, 238, 0.1), rgba(144, 174, 203, 0.1));
            border: 1px solid rgba(11, 128, 238, 0.2);
        }
        .real-time-indicator {
            position: relative;
            overflow: hidden;
        }
        .real-time-indicator::before {
            content: '';
            position: absolute;
            top: 0;
            left: -100%;
            width: 100%;
            height: 100%;
            background: linear-gradient(90deg, transparent, rgba(11, 128, 238, 0.4), transparent);
            animation: shimmer 2s infinite;
        }
        @keyframes shimmer {
            0% { left: -100%; }
            100% { left: 100%; }
        }
        .strategic-importance-high { border-left-color: #ef4444; }
        .strategic-importance-medium { border-left-color: #f59e0b; }
        .strategic-importance-low { border-left-color: #10b981; }
    </style>
</head>
<body class="bg-[#101a23] text-white min-h-screen">
    <div class="relative flex size-full min-h-screen flex-col bg-[#101a23] dark group/design-root overflow-x-hidden" 
         style='font-family: Inter, "Noto Sans", sans-serif;'>
        
        <!-- Real-Time Processing Status Bar -->
        <div id="processingStatus" class="bg-gradient-to-r from-blue-600 to-purple-600 text-white text-center py-2 text-sm hidden">
            <span class="real-time-indicator px-4 py-1 rounded-full bg-black bg-opacity-20">
                 AI Intelligence Processing... <span id="processingProgress">0%</span>
            </span>
        </div>
        
        <div class="layout-container flex h-full grow flex-col">
            <div class="gap-1 px-6 flex flex-1 justify-center py-5">
                
                <!-- Left Sidebar -->
                <div class="layout-content-container flex flex-col w-80">
                    <div class="flex h-full min-h-[700px] flex-col justify-between bg-[#101a23] p-4">
                        <div class="flex flex-col gap-4">
                            <!-- Enhanced Logo/Header -->
                            <div class="flex gap-3 items-center">
                                <div class="bg-gradient-to-br from-blue-500 to-purple-600 rounded-full size-12 flex items-center justify-center text-white font-bold text-lg relative">
                                    
                                    <div id="aiThinkingIndicator" class="absolute -top-1 -right-1 w-3 h-3 bg-green-400 rounded-full hidden insight-pulse"></div>
                                </div>
                                <div>
                                    <h1 class="text-white text-base font-medium leading-normal">AI Chief of Staff</h1>
                                    <p class="text-[#90aecb] text-xs">Intelligence Platform</p>
                                </div>
                            </div>
                            
                            <!-- Intelligence Summary Cards -->
                            <div class="space-y-2">
                                <div class="bg-[#223649] rounded-lg p-3 intelligence-card">
                                    <div class="flex items-center justify-between">
                                        <span class="text-[#90aecb] text-xs">Active Intelligence</span>
                                        <span id="intelligenceScore" class="text-white font-bold text-lg">--</span>
                                    </div>
                                    <div class="mt-1">
                                        <div class="bg-[#101a23] rounded-full h-2">
                                            <div id="intelligenceProgress" class="bg-gradient-to-r from-blue-500 to-purple-500 h-2 rounded-full transition-all duration-500" style="width: 0%"></div>
                                        </div>
                                    </div>
                                </div>
                                
                                <div class="bg-[#223649] rounded-lg p-3 intelligence-card">
                                    <div class="flex items-center justify-between">
                                        <span class="text-[#90aecb] text-xs">Entity Network</span>
                                        <span id="entityCount" class="text-white font-bold text-lg">--</span>
                                    </div>
                                    <div class="flex gap-2 mt-2 text-xs">
                                        <span class="bg-blue-600 px-2 py-1 rounded text-white" id="topicsCount">0 Topics</span>
                                        <span class="bg-green-600 px-2 py-1 rounded text-white" id="peopleCount">0 People</span>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Navigation Menu -->
                            <div class="flex flex-col gap-2">
                                <div class="flex items-center gap-3 px-3 py-2 rounded-full bg-[#223649]">
                                    <div class="text-white">
                                        <svg xmlns="http://www.w3.org/2000/svg" width="20px" height="20px" fill="currentColor" viewBox="0 0 256 256">
                                            <path d="M224,115.55V208a16,16,0,0,1-16,16H168a16,16,0,0,1-16-16V168a8,8,0,0,0-8-8H112a8,8,0,0,0-8,8v40a16,16,0,0,1-16,16H48a16,16,0,0,1-16-16V115.55a16,16,0,0,1,5.17-11.78l80-75.48.11-.11a16,16,0,0,1,21.53,0,1.14,1.14,0,0,0,.11.11l80,75.48A16,16,0,0,1,224,115.55Z"></path>
                                        </svg>
                                    </div>
                                    <p class="text-white text-sm font-medium leading-normal">Intelligence Hub</p>
                                </div>
                                
                                <a href="/topics" class="nav-link flex items-center gap-3 px-3 py-2 cursor-pointer hover:bg-[#223649] rounded-full transition-colors">
                                    <div class="text-white"></div>
                                    <p class="text-white text-sm font-medium leading-normal">Topics Brain</p>
                                </a>
                                
                                <a href="/people" class="nav-link flex items-center gap-3 px-3 py-2 cursor-pointer hover:bg-[#223649] rounded-full transition-colors">
                                    <div class="text-white"></div>
                                    <p class="text-white text-sm font-medium leading-normal">Relationship Intel</p>
                                </a>
                                
                                <a href="/tasks" class="nav-link flex items-center gap

============================================================
FILE: archive/refactor_docs/instructions.txt
============================================================
#  AI Chief of Staff Transformation Instructions

## Complete Step-by-Step Migration Guide for AI Implementation

** CURRENT PROJECT STATUS: ~90% COMPLETE** 
-  **Entity-Centric Database**: 100% Complete
-  **AI Processing Pipeline**: 95% Complete  
-  **API Layer**: 90% Complete
-  **React Frontend**: 90% Complete (JUST ENHANCED!)
-  **WebSocket Integration**: 70% Complete
-  **Testing & Production**: 25% Complete

---

## **PHASE 1: PREPARATION & BACKUP (Steps 1-5)**  **COMPLETED**

###  1. Create Backup and Archive Structure
```bash
# Create backup directory
mkdir -p backup/v1_original
mkdir -p backup/v1_original/models
mkdir -p backup/v1_original/processors  
mkdir -p backup/v1_original/templates
mkdir -p backup/v1_original/integrations
```

###  2. Backup Existing Core Files
```bash
# Backup current models
cp models/database.py backup/v1_original/models/
cp models/*.py backup/v1_original/models/

# Backup current processors
cp processors/*.py backup/v1_original/processors/

# Backup current templates
cp templates/*.html backup/v1_original/templates/

# Backup main application file
cp main.py backup/v1_original/

# Backup integrations
cp integrations/*.py backup/v1_original/integrations/
```

###  3. Create New Directory Structure
```bash
# Create new directories for enhanced architecture
mkdir -p models/enhanced_models
mkdir -p processors/enhanced_processors
mkdir -p processors/analytics
mkdir -p static/js
mkdir -p static/css
mkdir -p templates/enhanced
mkdir -p tests/unit
mkdir -p tests/integration
mkdir -p logs
mkdir -p data/migrations
```

###  4. Tag Current Version in Git
```bash
git add .
git commit -m "Backup: Original v1 implementation before entity-centric transformation"
git tag v1-original
git push origin v1-original
```

###  5. Create Migration Plan Document
```bash
# Create migration tracking file
touch MIGRATION_LOG.md
echo "# AI Chief of Staff Migration Log" > MIGRATION_LOG.md
echo "## Phase 1: Entity-Centric Foundation" >> MIGRATION_LOG.md
echo "Started: $(date)" >> MIGRATION_LOG.md
```

---

## **PHASE 2: DATABASE MODEL TRANSFORMATION (Steps 6-15)**  **COMPLETED**

###  6. Create Enhanced Database Models
**Create new file:** `models/enhanced_models.py`
- Copy the complete enhanced database models from the Entity-Centric Database artifact
- This replaces the existing basic models with relationship-aware entities

###  7. Update Database Manager 
**Modify existing file:** `models/database.py`
```python
# At the top, add imports for new models
from models.enhanced_models import (
    Topic, Person, Task, Email, CalendarEvent, Project, 
    EntityRelationship, IntelligenceInsight,
    person_topic_association, task_topic_association, event_topic_association
)

# Add new methods to DatabaseManager class:
def get_user_topics_with_intelligence(self, user_id: int, limit: int = None):
    """Get topics with relationship intelligence"""
    # Implementation here

def get_entity_relationships(self, user_id: int, entity_type: str = None):
    """Get entity relationships for network analysis"""
    # Implementation here

def get_intelligence_insights(self, user_id: int, status: str = None):
    """Get proactive intelligence insights"""
    # Implementation here
```

###  8. Create Database Migration Script
**Create new file:** `data/migrations/001_entity_centric_migration.py`
```python
"""
Migration script to transform v1 database to entity-centric structure
"""
from sqlalchemy import create_engine, text
from models.database import get_db_manager
from models.enhanced_models import Base

def migrate_to_entity_centric():
    """Migrate existing data to new entity-centric structure"""
    # Create new tables alongside old ones
    engine = create_engine(DATABASE_URL)
    Base.metadata.create_all(engine)
    
    # Migration logic here
    pass

if __name__ == "__main__":
    migrate_to_entity_centric()
```

###  9. Update Settings Configuration
**Modify existing file:** `config/settings.py`
```python
# Add new settings for enhanced features
class Settings:
    # ... existing settings ...
    
    # Enhanced AI Processing
    ENABLE_REAL_TIME_PROCESSING = os.getenv('ENABLE_REAL_TIME_PROCESSING', 'True').lower() == 'true'
    ENABLE_PREDICTIVE_ANALYTICS = os.getenv('ENABLE_PREDICTIVE_ANALYTICS', 'True').lower() == 'true'
    
    # Entity Processing
    MAX_ENTITY_RELATIONSHIPS = int(os.getenv('MAX_ENTITY_RELATIONSHIPS', '1000'))
    ENTITY_CONFIDENCE_THRESHOLD = float(os.getenv('ENTITY_CONFIDENCE_THRESHOLD', '0.7'))
    
    # Real-time Processing
    PROCESSING_QUEUE_SIZE = int(os.getenv('PROCESSING_QUEUE_SIZE', '1000'))
    WORKER_THREAD_COUNT = int(os.getenv('WORKER_THREAD_COUNT', '3'))
```

###  10. Archive Old Model Files
```bash
# Move old model files to backup
mv models/old_models_*.py backup/v1_original/models/ 2>/dev/null || true
```

---

## **PHASE 3: PROCESSOR TRANSFORMATION (Steps 11-20)**  **95% COMPLETED**

###  11. Create Unified Entity Engine
**Create new file:** `processors/unified_entity_engine.py`
- Copy the complete Unified Entity Engine code from the artifact
- This becomes the central hub for all entity creation and management

###  12. Create Enhanced AI Pipeline  
**Create new file:** `processors/enhanced_ai_pipeline.py`
- Copy the Enhanced AI Processing Pipeline code from the artifact
- This replaces scattered AI processing with unified, context-aware analysis

###  13. Create Real-Time Processor
**Create new file:** `processors/realtime_processing.py`
- Copy the Real-Time Processing Pipeline code from the artifact
- This enables continuous intelligence vs. batch processing

###  14. Create Predictive Analytics Engine
**Create new file:** `processors/analytics/predictive_analytics.py`
- Copy the Predictive Analytics Engine code from the artifact
- This enables future intelligence and pattern prediction

###  15. Archive Old Processors
```bash
# Archive old processor files
mv processors/email_intelligence.py backup/v1_original/processors/email_intelligence_v1.py
mv processors/task_extractor.py backup/v1_original/processors/task_extractor_v1.py

# Keep email_normalizer.py and gmail_fetcher.py as they're still useful
# But mark them for enhancement
mv processors/email_normalizer.py processors/email_normalizer_legacy.py
mv integrations/gmail_fetcher.py integrations/gmail_fetcher_legacy.py
```

###  16. Update Email Normalizer
**Modify file:** `processors/email_normalizer_legacy.py`  `processors/email_normalizer.py`
```python
# Enhance to preserve signatures instead of removing them
def normalize_email_content(self, email_data: Dict) -> Dict:
    # ... existing normalization ...
    
    # NEW: Extract and preserve signature intelligence
    signature_data = self._extract_signature_intelligence(email_data.get('body_html', ''))
    normalized_data['signature_intelligence'] = signature_data
    
    return normalized_data

def _extract_signature_intelligence(self, html_content: str) -> Dict:
    """Extract professional intelligence from email signatures"""
    # Implementation to extract titles, companies, phone numbers, etc.
    pass
```

###  17. Update Gmail Fetcher
**Modify file:** `integrations/gmail_fetcher_legacy.py`  `integrations/gmail_fetcher.py`
```python
# Add integration with real-time processor
from processors.realtime_processing import realtime_processor

class GmailFetcher:
    def fetch_recent_emails(self, user_email: str, **kwargs):
        # ... existing fetch logic ...
        
        # NEW: Send to real-time processor instead of just storing
        for email_data in emails:
            user = get_db_manager().get_user_by_email(user_email)
            realtime_processor.process_new_email(email_data, user.id, priority=3)
        
        return result
```

###  18. Update Calendar Fetcher  
**Modify file:** `integrations/calendar_fetcher.py`
```python
# Add real-time processing integration
from processors.realtime_processing import realtime_processor
from processors.enhanced_ai_pipeline import enhanced_ai_processor

class CalendarFetcher:
    def fetch_calendar_events(self, user_email: str, **kwargs):
        # ... existing fetch logic ...
        
        # NEW: Enhanced calendar processing with intelligence
        for event_data in events:
            user = get_db_manager().get_user_by_email(user_email)
            
            # Process through enhanced AI pipeline
            enhanced_ai_processor.enhance_calendar_event_with_intelligence(
                event_data, user.id
            )
            
            # Send to real-time processor
            realtime_processor.process_new_calendar_event(event_data, user.id, priority=4)
        
        return result
```

###  19. Create Processor Integration Manager
**Create new file:** `processors/__init__.py`
```python
"""
Enhanced Processor Integration Manager
Coordinates all processing components
"""
from .unified_entity_engine import entity_engine
from .enhanced_ai_pipeline import enhanced_ai_processor  
from .realtime_processing import realtime_processor
from .analytics.predictive_analytics import predictive_analytics

class ProcessorManager:
    """Coordinates all processing components"""
    
    def __init__(self):
        self.entity_engine = entity_engine
        self.ai_processor = enhanced_ai_processor
        self.realtime_processor = realtime_processor
        self.predictive_analytics = predictive_analytics
    
    def start_all_processors(self):
        """Start all processing components"""
        self.realtime_processor.start()
        self.predictive_analytics.start()
    
    def stop_all_processors(self):
        """Stop all processing components"""
        self.realtime_processor.stop()
        self.predictive_analytics.stop()

# Global instance
processor_manager = ProcessorManager()
```

###  20. Update Processor Imports
**Modify file:** `processors/email_intelligence.py`
```python
# Archive this file - functionality moved to enhanced_ai_pipeline.py
# Add deprecation notice at top:
"""
DEPRECATED: This file has been replaced by enhanced_ai_pipeline.py
Keeping for reference during migration only.
Remove after migration is complete.
"""
```

---

## **PHASE 4: API LAYER TRANSFORMATION (Steps 21-30)**  **90% COMPLETED**

###  21. Archive Current Main Application
```bash
# Archive current main.py
mv main.py backup/v1_original/main_v1.py
```

###  22. Create Enhanced API Layer
**Create new file:** `main.py`
- Copy the Enhanced API Integration Layer code from the artifact
- This replaces the old API with entity-centric, real-time endpoints

###  23. Create API Utilities
**Create new file:** `api/utils.py`
```python
"""
API utilities for enhanced intelligence endpoints
"""
from typing import Dict, List, Any
from datetime import datetime

def format_entity_response(entity_data: Any, entity_type: str) -> Dict:
    """Format entity data for API responses"""
    pass

def calculate_intelligence_metrics(user_id: int) -> Dict:
    """Calculate intelligence quality metrics"""
    pass

def format_insight_response(insights: List) -> Dict:
    """Format intelligence insights for API"""
    pass
```

###  24. Create WebSocket Handler (for real-time updates)
**Create new file:** `api/websocket_handler.py`
```python
"""
WebSocket handler for real-time intelligence updates
"""
from flask_socketio import SocketIO, emit
from processors.realtime_processing import realtime_processor

class IntelligenceWebSocket:
    def __init__(self, app):
        self.socketio = SocketIO(app, cors_allowed_origins="*")
        self.setup_handlers()
    
    def setup_handlers(self):
        @self.socketio.on('connect')
        def handle_connect():
            # Register user for real-time insights
            pass
    
    def send_insight_to_user(self, user_id: int, insight: Dict):
        """Send real-time insight to user"""
        pass
```

###  25. Update Route Structure
**Create new file:** `api/__init__.py`
```python
"""
Enhanced API package initialization
"""
from flask import Blueprint

# Create API blueprints
intelligence_bp = Blueprint('intelligence', __name__, url_prefix='/api')
entities_bp = Blueprint('entities', __name__, url_prefix='/api')
analytics_bp = Blueprint('analytics', __name__, url_prefix='/api')

# Import route handlers
from . import intelligence_routes
from . import entity_routes  
from . import analytics_routes
```

###  26. Create Intelligence Routes
**Create new file:** `api/intelligence_routes.py`
```python
"""
Intelligence-specific API routes
"""
from flask import request, jsonify
from api import intelligence_bp
from processors import processor_manager

@intelligence_bp.route('/intelligence-insights', methods=['GET'])
def get_intelligence_insights():
    """Get proactive intelligence insights"""
    pass

@intelligence_bp.route('/unified-intelligence-sync', methods=['POST'])  
def unified_intelligence_sync():
    """Enhanced unified processing endpoint"""
    pass
```

###  27. Create Entity Routes
**Create new file:** `api/entity_routes.py`
```python
"""
Entity-specific API routes
"""
from flask import request, jsonify
from api import entities_bp
from processors.unified_entity_engine import entity_engine

@entities_bp.route('/topics', methods=['GET', 'POST'])
def topics_api():
    """Enhanced topics API with intelligence accumulation"""
    pass

@entities_bp.route('/people', methods=['GET'])
def people_with_relationship_intelligence():
    """Get people with comprehensive relationship intelligence"""
    pass
```

###  28. Create Analytics Routes
**Create new file:** `api/analytics_routes.py`
```python
"""
Analytics and prediction API routes
"""
from flask import request, jsonify
from api import analytics_bp
from processors.analytics.predictive_analytics import predictive_analytics

@analytics_bp.route('/predictions', methods=['GET'])
def get_predictions():
    """Get predictive analytics insights"""
    pass

@analytics_bp.route('/patterns', methods=['GET'])
def get_patterns():
    """Get detected patterns in user data"""
    pass
```

###  29. Update Flask App Configuration
**Modify file:** `config/app_config.py` (create if doesn't exist)
```python
"""
Enhanced Flask application configuration
"""
from flask import Flask
from flask_cors import CORS
from config.settings import settings
from api import intelligence_bp, entities_bp, analytics_bp
from api.websocket_handler import IntelligenceWebSocket
from processors import processor_manager

def create_app():
    app = Flask(__name__)
    app.secret_key = settings.SECRET_KEY
    
    # Enable CORS
    CORS(app)
    
    # Register blueprints
    app.register_blueprint(intelligence_bp)
    app.register_blueprint(entities_bp)
    app.register_blueprint(analytics_bp)
    
    # Initialize WebSocket
    websocket = IntelligenceWebSocket(app)
    
    # Start processors
    processor_manager.start_all_processors()
    
    return app, websocket
```

###  30. Update Main Application Entry Point
**Modify file:** `main.py` (update the bottom section)
```python
# At the bottom of main.py, replace the old app.run() with:

from config.app_config import create_app

if __name__ == '__main__':
    app, websocket = create_app()
    
    # Run with WebSocket support
    websocket.socketio.run(
        app,
        host=settings.HOST,
        port=settings.PORT,
        debug=settings.DEBUG
    )
```

---

## **PHASE 5: FRONTEND TRANSFORMATION (Steps 31-40)**  **90% COMPLETED (JUST ENHANCED!)**

###  31. Archive Current Templates
```bash
# Archive current templates
mkdir -p backup/v1_original/templates
cp templates/*.html backup/v1_original/templates/
```

###  32. Create Enhanced Frontend Structure
```bash
mkdir -p templates/components
mkdir -p static/js/components
mkdir -p static/css/enhanced
```

###  33. Create Enhanced Intelligence Dashboard
**Replace file:** `templates/home.html`
- Copy the Enhanced Frontend - Real-Time Intelligence Dashboard from the artifact
- This replaces the basic dashboard with real-time intelligence features

###  34. Update Navigation Templates
**Modify files:** `templates/tasks.html`, `templates/people.html`, `templates/calendar.html`, `templates/knowledge.html`
```html
<!-- Add to each template's header -->
<script src="/static/js/intelligence-client.js"></script>
<link rel="stylesheet" href="/static/css/enhanced/intelligence-dashboard.css">

<!-- Add real-time connection status -->
<div id="realtime-status" class="hidden">
    <span class="text-green-400"> Intelligence Connected</span>
</div>
```

###  35. Create Intelligence Client Library
**Create new file:** `static/js/intelligence-client.js`
```javascript
/**
 * Real-time intelligence client for WebSocket communication
 */
class IntelligenceClient {
    constructor() {
        this.socket = null;
        this.reconnectAttempts = 0;
        this.maxReconnectAttempts = 5;
    }
    
    connect() {
        this.socket = io();
        this.setupEventHandlers();
    }
    
    setupEventHandlers() {
        this.socket.on('connect', () => {
            console.log('Connected to intelligence server');
            this.updateConnectionStatus(true);
        });
        
        this.socket.on('intelligence_insight', (data) => {
            this.handleNewInsight(data);
        });
    }
    
    handleNewInsight(insight) {
        // Display real-time insight to user
        this.showInsightNotification(insight);
    }
}

// Global intelligence client
const intelligenceClient = new IntelligenceClient();
```

###  36. Create Enhanced CSS Styles
**Create new file:** `static/css/enhanced/intelligence-dashboard.css`
```css
/* Enhanced Intelligence Dashboard Styles */
.intelligence-card { 
    transition: all 0.3s ease; 
    border-left: 4px solid transparent;
}

.intelligence-card:hover { 
    transform: translateY(-2px); 
    box-shadow: 0 8px 25px rgba(11, 128, 238, 0.15);
    border-left-color: #0b80ee;
}

.entity-connection { 
    background: linear-gradient(135deg, rgba(11, 128, 238, 0.1), rgba(144, 174, 203, 0.1));
    border: 1px solid rgba(11, 128, 238, 0.2);
}

.real-time-indicator {
    position: relative;
    overflow: hidden;
}

.real-time-indicator::before {
    content: '';
    position: absolute;
    top: 0;
    left: -100%;
    width: 100%;
    height: 100%;
    background: linear-gradient(90deg, transparent, rgba(11, 128, 238, 0.4), transparent);
    animation: shimmer 2s infinite;
}

@keyframes shimmer {
    0% { left: -100%; }
    100% { left: 100%; }
}
```

###  37. Update Template Base
**Create new file:** `templates/base_enhanced.html`
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{% block title %}AI Chief of Staff{% endblock %}</title>
    
    <!-- Enhanced styles and scripts -->
    <link rel="stylesheet" href="/static/css/enhanced/intelligence-dashboard.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.0.0/socket.io.js"></script>
    <script src="/static/js/intelligence-client.js"></script>
    
    {% block head %}{% endblock %}
</head>
<body>
    <!-- Real-time intelligence status -->
    <div id="intelligence-status-bar" class="hidden"></div>
    
    {% block content %}{% endblock %}
    
    <script>
        // Initialize intelligence client on page load
        document.addEventListener('DOMContentLoaded', function() {
            intelligenceClient.connect();
        });
    </script>
</body>
</html>
```

###  38. Update Existing Templates to Use Enhanced Base
**Modify all template files:**
```html
<!-- Change the first line in each template from: -->
<!-- {% extends "base.html" %} -->
<!-- To: -->
{% extends "base_enhanced.html" %}
```

###  39. Create Component Templates
**Create new file:** `templates/components/insight_card.html`
```html
<!-- Reusable insight card component -->
<div class="intelligence-card bg-[#182734] rounded-lg border p-4" 
     data-insight-type="{{ insight.insight_type }}" 
     data-priority="{{ insight.priority }}">
    <!-- Insight content -->
</div>
```

###  40. Create Intelligence Dashboard Components
**Create new file:** `templates/components/entity_network.html`
```html
<!-- Entity network visualization component -->
<div class="entity-network-container">
    <!-- Network visualization -->
</div>
```

###  **NEW: React Intelligence Dashboard (Steps 40A-40E)**  **COMPLETED TODAY!**

###  40A. Create React Application
**Created:** `frontend/` - Modern React TypeScript application with:
- Intelligence Dashboard with real-time metrics
- Proactive Insights panel with filtering  
- Entity Network visualization (Topics Brain, Relationship Intelligence)
- Intelligence Actions panel for AI operations
- AI Chat interface with business context
- Advanced Settings panel with integrations

###  40B. Enhanced Settings Panel (JUST COMPLETED!)
**Enhanced:** `frontend/src/App.tsx` with comprehensive settings:
-  **Integration Status Display**: Gmail, Google Calendar, Claude AI with connection status
-  **Email Sync with Progress Bar**: Real-time progress tracking for email synchronization
-  **Database Flush Functionality**: Complete database reset with confirmation modal
-  **Intelligence Processing**: Meeting intelligence and business insights generation
-  **System Status Dashboard**: Real-time metrics and processing health

###  40C. React Component Architecture
**Implemented:**
- TypeScript interfaces for all data types
- Modern React hooks and state management
- Responsive design with Tailwind CSS
- Error handling and loading states
- Modal system for detailed views

###  40D. API Integration
**Connected:**
- All major API endpoints for data fetching
- Real-time intelligence metrics
- Email sync with progress feedback
- Database management operations
- Error handling and notifications

###  40E. WebSocket Integration (NEXT PRIORITY)
**Needed:**
- Flask WebSocket endpoints for real-time updates
- React WebSocket client for live intelligence
- Real-time insight delivery to dashboard

---

## **PHASE 6: TESTING & VALIDATION (Steps 41-50)**  **25% COMPLETED**

###  41. Create Test Structure
```bash
mkdir -p tests/unit/models
mkdir -p tests/unit/processors  
mkdir -p tests/integration
mkdir -p tests/data
```

###  42. Create Database Migration Tests
**Create new file:** `tests/unit/test_database_migration.py`
```python
"""
Tests for database migration to entity-centric structure
"""
import unittest
from models.enhanced_models import Topic, Person, Task
from data.migrations.001_entity_centric_migration import migrate_to_entity_centric

class TestDatabaseMigration(unittest.TestCase):
    def test_entity_creation(self):
        """Test that entities can be created with relationships"""
        pass
    
    def test_relationship_mapping(self):
        """Test entity relationship creation"""
        pass
```

###  43. Create Entity Engine Tests  
**Create new file:** `tests/unit/processors/test_unified_entity_engine.py`
```python
"""
Tests for unified entity engine
"""
import unittest
from processors.unified_entity_engine import entity_engine, EntityContext

class TestUnifiedEntityEngine(unittest.TestCase):
    def test_topic_creation_with_context(self):
        """Test topic creation with existing context checking"""
        pass
    
    def test_person_creation_asymmetry_fix(self):
        """Test that people are created from both email and calendar sources"""
        pass
```

###  44. Create AI Pipeline Tests
**Create new file:** `tests/unit/processors/test_enhanced_ai_pipeline.py`
```python
"""
Tests for enhanced AI processing pipeline
"""
import unittest
from processors.enhanced_ai_pipeline import enhanced_ai_processor

class TestEnhancedAIPipeline(unittest.TestCase):
    def test_unified_email_processing(self):
        """Test single-pass email processing with context"""
        pass
    
    def test_signature_preservation(self):
        """Test that email signatures are preserved and analyzed"""
        pass
```

###  45. Create Real-Time Processing Tests
**Create new file:** `tests/unit/processors/test_realtime_processing.py`
```python
"""
Tests for real-time processing pipeline
"""
import unittest
from processors.realtime_processing import realtime_processor

class TestRealTimeProcessing(unittest.TestCase):
    def test_event_queuing(self):
        """Test that events are properly queued and processed"""
        pass
    
    def test_proactive_insights(self):
        """Test proactive insight generation"""
        pass
```

###  46. Create Integration Tests
**Create new file:** `tests/integration/test_end_to_end_processing.py`
```python
"""
End-to-end integration tests
"""
import unittest

class TestEndToEndProcessing(unittest.TestCase):
    def test_email_to_intelligence_pipeline(self):
        """Test complete pipeline from email to intelligence insight"""
        pass
    
    def test_entity_cross_reference(self):
        """Test that entities are properly cross-referenced"""
        pass
```

###  47. Create API Tests
**Create new file:** `tests/integration/test_enhanced_api.py`
```python
"""
Tests for enhanced API endpoints
"""
import unittest
from main import app

class TestEnhancedAPI(unittest.TestCase):
    def setUp(self):
        self.app = app.test_client()
    
    def test_unified_intelligence_sync(self):
        """Test unified intelligence sync endpoint"""
        pass
    
    def test_entity_endpoints(self):
        """Test entity-centric API endpoints"""
        pass
```

###  48. Create Test Data
**Create new file:** `tests/data/sample_entities.json`
```json
{
    "topics": [
        {
            "name": "Machine Learning",
            "description": "AI and ML discussions",
            "keywords": ["AI", "ML", "machine learning", "neural networks"]
        }
    ],
    "people": [
        {
            "name": "John Doe",
            "email": "john@company.com",
            "company": "Tech Corp",
            "title": "CTO"
        }
    ]
}
```

###  49. Run Test Suite
```bash
# Create test runner script
cat > run_tests.sh << 'EOF'
#!/bin/bash
echo "Running AI Chief of Staff Migration Tests..."

# Unit tests
python -m pytest tests/unit/ -v

# Integration tests  
python -m pytest tests/integration/ -v

echo "All tests completed."
EOF

chmod +x run_tests.sh
```

###  50. Validate Migration
**Create new file:** `validate_migration.py`
```python
"""
Migration validation script
"""
def validate_database_migration():
    """Validate that migration completed successfully"""
    # Check new tables exist
    # Validate data integrity
    # Test entity relationships
    pass

def validate_processor_functionality():
    """Validate that processors work correctly"""
    # Test entity engine
    # Test AI pipeline
    # Test real-time processing
    pass

def validate_api_functionality():
    """Validate that API endpoints work"""
    # Test intelligence endpoints
    # Test entity endpoints
    # Test WebSocket connections
    pass

if __name__ == "__main__":
    print(" Validating AI Chief of Staff Migration...")
    validate_database_migration()
    validate_processor_functionality() 
    validate_api_functionality()
    print(" Migration validation complete!")
```

---

## **PHASE 7: DEPLOYMENT & CLEANUP (Steps 51-55)**  **80% COMPLETED**

###  51. Update Requirements
**Modify file:** `requirements.txt`
```txt
# Add new dependencies
flask-socketio>=5.0.0
numpy>=1.21.0  # For predictive analytics
scikit-learn>=1.0.0  # For pattern detection

# Update existing
anthropic>=0.8.0
sqlalchemy>=1.4.0
```

###  52. Update Environment Configuration
**Modify file:** `.env.example`
```bash
# Add new environment variables
ENABLE_REAL_TIME_PROCESSING=true
ENABLE_PREDICTIVE_ANALYTICS=true
MAX_ENTITY_RELATIONSHIPS=1000
ENTITY_CONFIDENCE_THRESHOLD=0.7
PROCESSING_QUEUE_SIZE=1000
WORKER_THREAD_COUNT=3
```

###  53. Create Deployment Scripts
**Create new file:** `deploy/startup.py`
```python
"""
Enhanced startup script for AI Chief of Staff
"""
from config.app_config import create_app
from processors import processor_manager
from data.migrations.001_entity_centric_migration import migrate_to_entity_centric

def main():
    print(" Starting AI Chief of Staff - Enhanced Edition")
    
    # Run migrations
    print(" Running database migrations...")
    migrate_to_entity_centric()
    
    # Create app
    print(" Creating application...")
    app, websocket = create_app()
    
    # Start processors
    print(" Starting intelligence processors...")
    processor_manager.start_all_processors()
    
    print(" AI Chief of Staff ready!")
    
    # Run application
    websocket.socketio.run(
        app,
        host=settings.HOST,
        port=settings.PORT,
        debug=settings.DEBUG
    )

if __name__ == "__main__":
    main()
```

###  54. Clean Up Deprecated Files
```bash
# Remove deprecated files after successful migration
rm -f processors/email_intelligence.py
rm -f processors/task_extractor_v1.py  
rm -f models/old_*.py

# Update imports in any remaining files
find . -name "*.py" -exec sed -i 's/from processors.email_intelligence/from processors.enhanced_ai_pipeline/g' {} \;
```

###  55. Create Final Migration Documentation
**Create new file:** `MIGRATION_COMPLETE.md`
```markdown
# AI Chief of Staff Migration Complete

## What Changed
-  Database: Entity-centric with relationship intelligence  
-  Processors: Unified engine with real-time processing
-  API: Enhanced endpoints with 360-context intelligence
-  Frontend: Real-time dashboard with predictive insights

## New Capabilities
-  Topics as persistent memory containers
-  Unified entity creation across all sources  
-  Context-aware task generation
-  Predictive analytics and pattern detection
-  Real-time intelligence processing

## Architecture Benefits
- No more siloed data processing
- Entity relationships reveal hidden patterns
- Proactive vs reactive intelligence
- Scalable cloud-ready design

## Next Steps
1. Monitor real-time processing performance
2. Train predictive models with user data
3. Expand entity types and relationships
4. Add advanced analytics features

Migration completed: $(date)
```

---

## ** IMMEDIATE NEXT STEPS (HIGH PRIORITY)**

### ** 1. Complete WebSocket Integration (4-6 hours)**
- **Backend**: Add Flask-SocketIO endpoints to `main.py`
- **Frontend**: Implement React WebSocket hooks for real-time updates
- **Priority**: HIGH - This will make the dashboard truly live

### ** 2. Production Deployment Setup (2-3 hours)**
- **Flask Production**: Configure for production serving of React build
- **Environment**: Set up production environment variables
- **Testing**: Verify all functionality works in production mode

### ** 3. React Testing & Polish (4-6 hours)**
- **Testing**: Add React Testing Library tests
- **Error Handling**: Enhanced error boundaries and recovery
- **Performance**: Code splitting and optimization

---

## ** FINAL VERIFICATION CHECKLIST**

### ** Database Layer**  **COMPLETE**
-  Enhanced models with entity relationships created
-  Migration scripts tested and working  
-  Old data successfully migrated
-  New tables and relationships functional

### ** Processing Layer**  **95% COMPLETE**
-  Unified entity engine operational
-  Enhanced AI pipeline processing emails
-  Real-time processor handling events
-  Predictive analytics generating insights

### ** API Layer**  **90% COMPLETE**
-  Enhanced endpoints returning entity intelligence
-  WebSocket connections for real-time updates (NEXT PRIORITY)
-  Old endpoints deprecated gracefully
-  New intelligence metrics available

### ** Frontend Layer**  **90% COMPLETE (JUST ENHANCED!)**
-  Intelligence dashboard displaying real-time data
-  Entity network visualizations working
-  Proactive insights appearing automatically  
-  Advanced settings with integrations and database management
-  WebSocket client connecting successfully (NEXT PRIORITY)

### ** Integration Layer**  **95% COMPLETE**
-  Gmail fetcher using new processing pipeline
-  Calendar fetcher creating intelligent prep tasks
-  Email normalizer preserving signature intelligence
-  All processors working together seamlessly

---

## ** PROJECT COMPLETION SUMMARY**

**Overall Progress: ~90% Complete**

### ** Major Achievements Today:**
1. ** React Frontend Enhanced**: Added comprehensive settings panel with progress bars, database management, and integration status
2. ** Error Handling**: Fixed all React linter errors and TypeScript issues  
3. ** UI Polish**: Professional settings interface with modals, confirmations, and real-time feedback
4. ** API Integration**: Connected all major backend endpoints to React frontend

### ** Next Steps to 100% Completion:**
1. **WebSocket Integration** (4-6 hours) - Make dashboard truly real-time
2. **Production Setup** (2-3 hours) - Deploy and test in production
3. **Testing & Documentation** (4-6 hours) - Complete test suite and documentation

** Your AI Chief of Staff is now a sophisticated, entity-centric intelligence platform with a modern React frontend, comprehensive settings management, and real-time processing capabilities!**

============================================================
FILE: archive/refactor_docs/4_realtime_proccessing_pipeline.txt
============================================================
# Real-Time Processing Pipeline - Proactive Intelligence
# This transforms the system from batch processing to continuous intelligence

import asyncio
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import json
from dataclasses import dataclass, asdict
from enum import Enum
import threading
import queue
import time

from processors.enhanced_ai_pipeline import enhanced_ai_processor
from processors.unified_entity_engine import entity_engine, EntityContext
from models.enhanced_models import IntelligenceInsight, Person, Topic, Task, CalendarEvent

logger = logging.getLogger(__name__)

class EventType(Enum):
    NEW_EMAIL = "new_email"
    NEW_CALENDAR_EVENT = "new_calendar_event"
    ENTITY_UPDATE = "entity_update"
    USER_ACTION = "user_action"
    SCHEDULED_ANALYSIS = "scheduled_analysis"

@dataclass
class ProcessingEvent:
    event_type: EventType
    user_id: int
    data: Dict
    timestamp: datetime
    priority: int = 5  # 1-10, 1 = highest priority
    correlation_id: Optional[str] = None

class RealTimeProcessor:
    """
    Real-time processing engine that provides continuous intelligence.
    This is what transforms your system from reactive to proactive.
    """
    
    def __init__(self):
        self.processing_queue = queue.PriorityQueue()
        self.running = False
        self.worker_threads = []
        self.user_contexts = {}  # Cache user contexts for efficiency
        self.insight_callbacks = {}  # User-specific insight delivery callbacks
        
    def start(self, num_workers: int = 3):
        """Start the real-time processing engine"""
        self.running = True
        
        # Start worker threads
        for i in range(num_workers):
            worker = threading.Thread(target=self._process_events_worker, name=f"RTProcessor-{i}")
            worker.daemon = True
            worker.start()
            self.worker_threads.append(worker)
        
        # Start periodic analysis thread
        scheduler = threading.Thread(target=self._scheduled_analysis_worker, name="RTScheduler")
        scheduler.daemon = True
        scheduler.start()
        self.worker_threads.append(scheduler)
        
        logger.info(f"Started real-time processor with {num_workers} workers")
    
    def stop(self):
        """Stop the real-time processing engine"""
        self.running = False
        for worker in self.worker_threads:
            worker.join(timeout=5)
        logger.info("Stopped real-time processor")
    
    # =====================================================================
    # EVENT INGESTION METHODS
    # =====================================================================
    
    def process_new_email(self, email_data: Dict, user_id: int, priority: int = 5):
        """Process new email in real-time"""
        event = ProcessingEvent(
            event_type=EventType.NEW_EMAIL,
            user_id=user_id,
            data=email_data,
            timestamp=datetime.utcnow(),
            priority=priority
        )
        self._queue_event(event)
    
    def process_new_calendar_event(self, event_data: Dict, user_id: int, priority: int = 5):
        """Process new calendar event in real-time"""
        event = ProcessingEvent(
            event_type=EventType.NEW_CALENDAR_EVENT,
            user_id=user_id,
            data=event_data,
            timestamp=datetime.utcnow(),
            priority=priority
        )
        self._queue_event(event)
    
    def process_entity_update(self, entity_type: str, entity_id: int, update_data: Dict, user_id: int):
        """Process entity update and trigger related intelligence updates"""
        event = ProcessingEvent(
            event_type=EventType.ENTITY_UPDATE,
            user_id=user_id,
            data={
                'entity_type': entity_type,
                'entity_id': entity_id,
                'update_data': update_data
            },
            timestamp=datetime.utcnow(),
            priority=3  # Higher priority for entity updates
        )
        self._queue_event(event)
    
    def process_user_action(self, action_type: str, action_data: Dict, user_id: int):
        """Process user action and learn from feedback"""
        event = ProcessingEvent(
            event_type=EventType.USER_ACTION,
            user_id=user_id,
            data={
                'action_type': action_type,
                'action_data': action_data
            },
            timestamp=datetime.utcnow(),
            priority=4
        )
        self._queue_event(event)
    
    # =====================================================================
    # CORE PROCESSING WORKERS
    # =====================================================================
    
    def _process_events_worker(self):
        """Main event processing worker"""
        while self.running:
            try:
                # Get event from queue (blocks until available or timeout)
                try:
                    priority, event = self.processing_queue.get(timeout=1.0)
                except queue.Empty:
                    continue
                
                logger.debug(f"Processing {event.event_type.value} for user {event.user_id}")
                
                # Process based on event type
                if event.event_type == EventType.NEW_EMAIL:
                    self._process_new_email_event(event)
                elif event.event_type == EventType.NEW_CALENDAR_EVENT:
                    self._process_new_calendar_event(event)
                elif event.event_type == EventType.ENTITY_UPDATE:
                    self._process_entity_update_event(event)
                elif event.event_type == EventType.USER_ACTION:
                    self._process_user_action_event(event)
                elif event.event_type == EventType.SCHEDULED_ANALYSIS:
                    self._process_scheduled_analysis_event(event)
                
                # Mark task as done
                self.processing_queue.task_done()
                
            except Exception as e:
                logger.error(f"Error in event processing worker: {str(e)}")
                time.sleep(0.1)  # Brief pause on error
    
    def _scheduled_analysis_worker(self):
        """Worker for periodic intelligence analysis"""
        while self.running:
            try:
                # Run scheduled analysis every 15 minutes
                time.sleep(900)  # 15 minutes
                
                # Get active users (those with recent activity)
                active_users = self._get_active_users()
                
                for user_id in active_users:
                    event = ProcessingEvent(
                        event_type=EventType.SCHEDULED_ANALYSIS,
                        user_id=user_id,
                        data={'analysis_type': 'proactive_insights'},
                        timestamp=datetime.utcnow(),
                        priority=7  # Lower priority for scheduled analysis
                    )
                    self._queue_event(event)
                
            except Exception as e:
                logger.error(f"Error in scheduled analysis worker: {str(e)}")
    
    # =====================================================================
    # EVENT PROCESSING METHODS
    # =====================================================================
    
    def _process_new_email_event(self, event: ProcessingEvent):
        """Process new email with real-time intelligence generation"""
        try:
            email_data = event.data
            user_id = event.user_id
            
            # Get cached user context for efficiency
            context = self._get_cached_user_context(user_id)
            
            # Process email with enhanced AI pipeline
            result = enhanced_ai_processor.process_email_with_context(email_data, user_id, context)
            
            if result.success:
                # Update cached context with new information
                self._update_cached_context(user_id, result)
                
                # Generate immediate insights
                immediate_insights = self._generate_immediate_insights(email_data, result, user_id)
                
                # Deliver insights to user
                self._deliver_insights_to_user(user_id, immediate_insights)
                
                # Check for entity cross-references and augmentations
                self._check_cross_entity_augmentations(result, user_id)
                
                logger.info(f"Processed new email in real-time for user {user_id}: "
                           f"{result.entities_created} entities created, {len(immediate_insights)} insights")
            
        except Exception as e:
            logger.error(f"Failed to process new email event: {str(e)}")
    
    def _process_new_calendar_event(self, event: ProcessingEvent):
        """Process new calendar event with intelligence enhancement"""
        try:
            event_data = event.data
            user_id = event.user_id
            
            # Enhance calendar event with email intelligence
            result = enhanced_ai_processor.enhance_calendar_event_with_intelligence(event_data, user_id)
            
            if result.success:
                # Generate meeting preparation insights
                prep_insights = self._generate_meeting_prep_insights(event_data, result, user_id)
                
                # Deliver insights to user
                self._deliver_insights_to_user(user_id, prep_insights)
                
                # Update cached context
                self._update_cached_context(user_id, result)
                
                logger.info(f"Enhanced calendar event in real-time for user {user_id}: "
                           f"{result.entities_created['tasks']} prep tasks created")
            
        except Exception as e:
            logger.error(f"Failed to process new calendar event: {str(e)}")
    
    def _process_entity_update_event(self, event: ProcessingEvent):
        """Process entity updates and propagate intelligence"""
        try:
            entity_type = event.data['entity_type']
            entity_id = event.data['entity_id']
            update_data = event.data['update_data']
            user_id = event.user_id
            
            # Create entity context
            context = EntityContext(
                source_type='update',
                user_id=user_id,
                confidence=0.9
            )
            
            # Augment entity with new data
            entity_engine.augment_entity_from_source(entity_type, entity_id, update_data, context)
            
            # Find related entities that might need updates
            related_entities = self._find_related_entities(entity_type, entity_id, user_id)
            
            # Propagate intelligence to related entities
            for related_entity in related_entities:
                self._propagate_intelligence_update(
                    related_entity['type'], 
                    related_entity['id'], 
                    entity_type, 
                    entity_id, 
                    update_data, 
                    user_id
                )
            
            # Generate insights from entity updates
            update_insights = self._generate_entity_update_insights(entity_type, entity_id, update_data, user_id)
            self._deliver_insights_to_user(user_id, update_insights)
            
            logger.info(f"Processed entity update for {entity_type}:{entity_id}, "
                       f"propagated to {len(related_entities)} related entities")
            
        except Exception as e:
            logger.error(f"Failed to process entity update event: {str(e)}")
    
    def _process_user_action_event(self, event: ProcessingEvent):
        """Process user actions and learn from feedback"""
        try:
            action_type = event.data['action_type']
            action_data = event.data['action_data']
            user_id = event.user_id
            
            # Learning from user feedback
            if action_type == 'insight_feedback':
                self._learn_from_insight_feedback(action_data, user_id)
            elif action_type == 'task_completion':
                self._learn_from_task_completion(action_data, user_id)
            elif action_type == 'topic_management':
                self._learn_from_topic_management(action_data, user_id)
            elif action_type == 'relationship_update':
                self._learn_from_relationship_update(action_data, user_id)
            
            logger.debug(f"Processed user action: {action_type} for user {user_id}")
            
        except Exception as e:
            logger.error(f"Failed to process user action event: {str(e)}")
    
    def _process_scheduled_analysis_event(self, event: ProcessingEvent):
        """Process scheduled proactive analysis"""
        try:
            user_id = event.user_id
            analysis_type = event.data.get('analysis_type', 'proactive_insights')
            
            if analysis_type == 'proactive_insights':
                # Generate proactive insights
                insights = entity_engine.generate_proactive_insights(user_id)
                
                if insights:
                    self._deliver_insights_to_user(user_id, insights)
                    logger.info(f"Generated {len(insights)} proactive insights for user {user_id}")
            
        except Exception as e:
            logger.error(f"Failed to process scheduled analysis: {str(e)}")
    
    # =====================================================================
    # INTELLIGENCE GENERATION METHODS
    # =====================================================================
    
    def _generate_immediate_insights(self, email_data: Dict, processing_result: Any, user_id: int) -> List[IntelligenceInsight]:
        """Generate immediate insights from new email processing"""
        insights = []
        
        try:
            # Insight 1: Important person contact
            sender = email_data.get('sender', '')
            if sender and self._is_important_person(sender, user_id):
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='important_contact',
                    title=f"New email from important contact",
                    description=f"Received email from {email_data.get('sender_name', sender)}. "
                               f"Subject: {email_data.get('subject', 'No subject')}",
                    priority='high',
                    confidence=0.9,
                    related_entity_type='person',
                    status='new'
                )
                insights.append(insight)
            
            # Insight 2: Urgent task detection
            if processing_result.entities_created.get('tasks', 0) > 0:
                # Check if any high-priority tasks were created
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='urgent_task',
                    title=f"New tasks extracted from email",
                    description=f"Created {processing_result.entities_created['tasks']} tasks from recent email. "
                               f"Review and prioritize action items.",
                    priority='medium',
                    confidence=0.8,
                    related_entity_type='task',
                    status='new'
                )
                insights.append(insight)
            
            # Insight 3: Topic momentum detection
            if processing_result.entities_created.get('topics', 0) > 0 or processing_result.entities_updated.get('topics', 0) > 0:
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='topic_momentum',
                    title=f"Business topic activity detected",
                    description=f"Recent email activity relates to your business topics. "
                               f"Consider scheduling focused time for strategic planning.",
                    priority='medium',
                    confidence=0.7,
                    related_entity_type='topic',
                    status='new'
                )
                insights.append(insight)
            
        except Exception as e:
            logger.error(f"Failed to generate immediate insights: {str(e)}")
        
        return insights
    
    def _generate_meeting_prep_insights(self, event_data: Dict, processing_result: Any, user_id: int) -> List[IntelligenceInsight]:
        """Generate meeting preparation insights"""
        insights = []
        
        try:
            meeting_title = event_data.get('title', 'Unknown Meeting')
            meeting_time = event_data.get('start_time')
            
            # Calculate time until meeting
            if meeting_time:
                time_until = meeting_time - datetime.utcnow()
                
                if time_until.total_seconds() > 0 and time_until.days <= 2:  # Within 48 hours
                    # High-priority preparation insight
                    insight = IntelligenceInsight(
                        user_id=user_id,
                        insight_type='meeting_prep',
                        title=f"Prepare for '{meeting_title}'",
                        description=f"Meeting in {time_until.days} days, {time_until.seconds // 3600} hours. "
                                   f"AI has generated preparation tasks based on attendee intelligence.",
                        priority='high' if time_until.days == 0 else 'medium',
                        confidence=0.9,
                        related_entity_type='event',
                        status='new',
                        expires_at=meeting_time
                    )
                    insights.append(insight)
            
            # Insight about preparation tasks created
            if processing_result.entities_created.get('tasks', 0) > 0:
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='prep_tasks_generated',
                    title=f"Meeting preparation tasks created",
                    description=f"Generated {processing_result.entities_created['tasks']} preparation tasks "
                               f"for '{meeting_title}' based on your email history with attendees.",
                    priority='medium',
                    confidence=0.8,
                    related_entity_type='task',
                    status='new'
                )
                insights.append(insight)
            
        except Exception as e:
            logger.error(f"Failed to generate meeting prep insights: {str(e)}")
        
        return insights
    
    def _generate_entity_update_insights(self, entity_type: str, entity_id: int, update_data: Dict, user_id: int) -> List[IntelligenceInsight]:
        """Generate insights from entity updates"""
        insights = []
        
        try:
            if entity_type == 'topic' and update_data.get('mentions', 0) > 0:
                # Topic becoming hot
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='topic_momentum',
                    title=f"Topic gaining momentum",
                    description=f"Business topic receiving increased attention. "
                               f"Consider preparing materials or scheduling focused discussion.",
                    priority='medium',
                    confidence=0.7,
                    related_entity_type='topic',
                    related_entity_id=entity_id,
                    status='new'
                )
                insights.append(insight)
            
            elif entity_type == 'person' and update_data.get('interaction'):
                # Relationship activity
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='relationship_activity',
                    title=f"Recent contact activity",
                    description=f"Ongoing communication with important contact. "
                               f"Relationship engagement is active.",
                    priority='low',
                    confidence=0.6,
                    related_entity_type='person',
                    related_entity_id=entity_id,
                    status='new'
                )
                insights.append(insight)
            
        except Exception as e:
            logger.error(f"Failed to generate entity update insights: {str(e)}")
        
        return insights
    
    # =====================================================================
    # CONTEXT MANAGEMENT AND CACHING
    # =====================================================================
    
    def _get_cached_user_context(self, user_id: int) -> Dict:
        """Get cached user context for efficient processing"""
        if user_id not in self.user_contexts:
            # Load context from enhanced AI processor
            context = enhanced_ai_processor._gather_user_context(user_id)
            self.user_contexts[user_id] = {
                'context': context,
                'last_updated': datetime.utcnow(),
                'version': 1
            }
        else:
            # Check if context needs refresh (every 30 minutes)
            cached = self.user_contexts[user_id]
            if datetime.utcnow() - cached['last_updated'] > timedelta(minutes=30):
                context = enhanced_ai_processor._gather_user_context(user_id)
                cached['context'] = context
                cached['last_updated'] = datetime.utcnow()
                cached['version'] += 1
        
        return self.user_contexts[user_id]['context']
    
    def _update_cached_context(self, user_id: int, processing_result: Any):
        """Update cached context with new processing results"""
        if user_id not in self.user_contexts:
            return
        
        cached = self.user_contexts[user_id]
        
        # Update context with new entities
        if hasattr(processing_result, 'entities_created'):
            # This would update the cached context with newly created entities
            # Implementation would depend on the specific structure
            cached['last_updated'] = datetime.utcnow()
            cached['version'] += 1
    
    def _find_related_entities(self, entity_type: str, entity_id: int, user_id: int) -> List[Dict]:
        """Find entities related to the updated entity"""
        related_entities = []
        
        try:
            from models.database import get_db_manager
            from models.enhanced_models import EntityRelationship
            
            with get_db_manager().get_session() as session:
                # Find direct relationships
                relationships = session.query(EntityRelationship).filter(
                    EntityRelationship.user_id == user_id,
                    ((EntityRelationship.entity_type_a == entity_type) & (EntityRelationship.entity_id_a == entity_id)) |
                    ((EntityRelationship.entity_type_b == entity_type) & (EntityRelationship.entity_id_b == entity_id))
                ).all()
                
                for rel in relationships:
                    if rel.entity_type_a == entity_type and rel.entity_id_a == entity_id:
                        related_entities.append({
                            'type': rel.entity_type_b,
                            'id': rel.entity_id_b,
                            'relationship': rel.relationship_type
                        })
                    else:
                        related_entities.append({
                            'type': rel.entity_type_a,
                            'id': rel.entity_id_a,
                            'relationship': rel.relationship_type
                        })
            
        except Exception as e:
            logger.error(f"Failed to find related entities: {str(e)}")
        
        return related_entities
    
    def _propagate_intelligence_update(self, target_entity_type: str, target_entity_id: int, 
                                     source_entity_type: str, source_entity_id: int, 
                                     update_data: Dict, user_id: int):
        """Propagate intelligence updates to related entities"""
        try:
            # Create propagation context
            context = EntityContext(
                source_type='propagation',
                user_id=user_id,
                confidence=0.7,
                processing_metadata={
                    'source_entity': f"{source_entity_type}:{source_entity_id}",
                    'propagation_data': update_data
                }
            )
            
            # Determine what intelligence to propagate based on entity types
            propagation_data = {}
            
            if source_entity_type == 'topic' and target_entity_type == 'person':
                # Topic update affecting person
                propagation_data = {
                    'topic_activity': True,
                    'related_topic_update': update_data
                }
            elif source_entity_type == 'person' and target_entity_type == 'topic':
                # Person update affecting topic
                propagation_data = {
                    'person_interaction': True,
                    'related_person_update': update_data
                }
            
            if propagation_data:
                entity_engine.augment_entity_from_source(
                    target_entity_type, target_entity_id, propagation_data, context
                )
            
        except Exception as e:
            logger.error(f"Failed to propagate intelligence update: {str(e)}")
    
    # =====================================================================
    # USER FEEDBACK AND LEARNING
    # =====================================================================
    
    def _learn_from_insight_feedback(self, feedback_data: Dict, user_id: int):
        """Learn from user feedback on insights"""
        try:
            insight_id = feedback_data.get('insight_id')
            feedback_type = feedback_data.get('feedback')  # helpful, not_helpful, etc.
            
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                insight = session.query(IntelligenceInsight).filter(
                    IntelligenceInsight.id == insight_id,
                    IntelligenceInsight.user_id == user_id
                ).first()
                
                if insight:
                    insight.user_feedback = feedback_type
                    insight.updated_at = datetime.utcnow()
                    session.commit()
                    
                    # Adjust future insight generation based on feedback
                    self._adjust_insight_generation(insight.insight_type, feedback_type, user_id)
            
        except Exception as e:
            logger.error(f"Failed to learn from insight feedback: {str(e)}")
    
    def _learn_from_task_completion(self, completion_data: Dict, user_id: int):
        """Learn from task completion patterns"""
        try:
            task_id = completion_data.get('task_id')
            completion_time = completion_data.get('completion_time')
            
            # This would analyze task completion patterns to improve future task extraction
            # For example: tasks that take longer than estimated, tasks that are never completed, etc.
            
        except Exception as e:
            logger.error(f"Failed to learn from task completion: {str(e)}")
    
    def _learn_from_topic_management(self, topic_data: Dict, user_id: int):
        """Learn from user topic management actions"""
        try:
            action = topic_data.get('action')  # create, merge, delete, etc.
            
            # This would learn user preferences for topic organization
            # and improve future topic extraction and categorization
            
        except Exception as e:
            logger.error(f"Failed to learn from topic management: {str(e)}")
    
    def _learn_from_relationship_update(self, relationship_data: Dict, user_id: int):
        """Learn from relationship updates"""
        try:
            # Learn how users categorize and prioritize relationships
            # to improve future relationship intelligence
            pass
            
        except Exception as e:
            logger.error(f"Failed to learn from relationship update: {str(e)}")
    
    def _adjust_insight_generation(self, insight_type: str, feedback: str, user_id: int):
        """Adjust future insight generation based on user feedback"""
        # This would implement adaptive insight generation
        # For example: if user consistently marks "relationship_alert" as not helpful,
        # reduce frequency or adjust criteria for that insight type
        pass
    
    # =====================================================================
    # UTILITY METHODS
    # =====================================================================
    
    def _queue_event(self, event: ProcessingEvent):
        """Queue event for processing"""
        # Priority queue uses tuple (priority, item)
        self.processing_queue.put((event.priority, event))
    
    def _get_active_users(self) -> List[int]:
        """Get users with recent activity for scheduled analysis"""
        try:
            from models.database import get_db_manager
            
            # Users with activity in last 24 hours
            cutoff = datetime.utcnow() - timedelta(hours=24)
            
            with get_db_manager().get_session() as session:
                # This would query for users with recent email processing or other activity
                # For now, return empty list
                return []
            
        except Exception as e:
            logger.error(f"Failed to get active users: {str(e)}")
            return []
    
    def _is_important_person(self, email: str, user_id: int) -> bool:
        """Check if person is marked as important"""
        try:
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                person = session.query(Person).filter(
                    Person.user_id == user_id,
                    Person.email_address == email.lower(),
                    Person.importance_level > 0.7
                ).first()
                
                return person is not None
                
        except Exception as e:
            logger.error(f"Failed to check person importance: {str(e)}")
            return False
    
    def _deliver_insights_to_user(self, user_id: int, insights: List[IntelligenceInsight]):
        """Deliver insights to user through registered callbacks"""
        if not insights:
            return
        
        try:
            # Store insights in database
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                for insight in insights:
                    session.add(insight)
                session.commit()
            
            # Deliver through callbacks (WebSocket, push notifications, etc.)
            if user_id in self.insight_callbacks:
                callback = self.insight_callbacks[user_id]
                callback(insights)
            
            logger.info(f"Delivered {len(insights)} insights to user {user_id}")
            
        except Exception as e:
            logger.error(f"Failed to deliver insights to user: {str(e)}")
    
    def register_insight_callback(self, user_id: int, callback):
        """Register callback for delivering insights to specific user"""
        self.insight_callbacks[user_id] = callback
    
    def unregister_insight_callback(self, user_id: int):
        """Unregister insight callback for user"""
        if user_id in self.insight_callbacks:
            del self.insight_callbacks[user_id]

# Global instance
realtime_processor = RealTimeProcessor()

============================================================
FILE: archive/refactor_docs/2_unified_entity_engine_centralintelligencehub.txt
============================================================
# Unified Entity Engine - Central Intelligence Hub
# This replaces the scattered entity creation across multiple files

import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timezone
import json
import hashlib
from sqlalchemy.orm import Session
from dataclasses import dataclass

from models.database import get_db_manager
from models.enhanced_models import (
    Topic, Person, Task, Email, CalendarEvent, Project, 
    EntityRelationship, IntelligenceInsight
)

logger = logging.getLogger(__name__)

@dataclass
class EntityContext:
    """Container for entity creation context"""
    source_type: str  # email, calendar, manual
    source_id: Optional[int] = None
    confidence: float = 0.8
    user_id: int = None
    processing_metadata: Dict = None

class UnifiedEntityEngine:
    """
    Central hub for all entity creation, updating, and relationship management.
    This is the brain that ensures consistency across all data sources.
    """
    
    def __init__(self):
        self.db_manager = get_db_manager()
        
    # =====================================================================
    # CORE ENTITY CREATION METHODS
    # =====================================================================
    
    def create_or_update_person(self, 
                               email: str, 
                               name: str = None, 
                               context: EntityContext = None) -> Person:
        """
        Unified person creation from ANY source (email, calendar, manual).
        This solves the asymmetry problem you identified.
        """
        try:
            with self.db_manager.get_session() as session:
                # Always check for existing person first
                existing_person = session.query(Person).filter(
                    Person.user_id == context.user_id,
                    Person.email_address == email.lower()
                ).first()
                
                if existing_person:
                    # Update existing person with new information
                    updated = self._update_person_intelligence(existing_person, name, context, session)
                    if updated:
                        session.commit()
                        logger.info(f"Updated existing person: {existing_person.name} ({email})")
                    return existing_person
                
                # Create new person
                person = Person(
                    user_id=context.user_id,
                    email_address=email.lower(),
                    name=name or self._extract_name_from_email(email),
                    created_at=datetime.utcnow()
                )
                
                # Add source-specific intelligence
                self._enrich_person_from_context(person, context)
                
                session.add(person)
                session.commit()
                
                logger.info(f"Created new person: {person.name} ({email}) from {context.source_type}")
                return person
                
        except Exception as e:
            logger.error(f"Failed to create/update person {email}: {str(e)}")
            return None
    
    def create_or_update_topic(self, 
                              topic_name: str, 
                              description: str = None,
                              keywords: List[str] = None,
                              context: EntityContext = None) -> Topic:
        """
        Topics as the central brain - always check existing first, then augment.
        This solves your topic duplication concern.
        """
        try:
            with self.db_manager.get_session() as session:
                # Intelligent topic matching - exact name or similar
                existing_topic = self._find_matching_topic(topic_name, context.user_id, session)
                
                if existing_topic:
                    # Augment existing topic with new intelligence
                    updated = self._augment_topic_intelligence(existing_topic, description, keywords, context, session)
                    if updated:
                        existing_topic.updated_at = datetime.utcnow()
                        existing_topic.version += 1
                        session.commit()
                        logger.info(f"Augmented existing topic: {existing_topic.name}")
                    return existing_topic
                
                # Create new topic
                topic = Topic(
                    user_id=context.user_id,
                    name=topic_name.strip().title(),
                    description=description,
                    keywords=','.join(keywords) if keywords else None,
                    confidence_score=context.confidence,
                    total_mentions=1,
                    last_mentioned=datetime.utcnow(),
                    created_at=datetime.utcnow()
                )
                
                # Generate AI intelligence summary for new topic
                topic.intelligence_summary = self._generate_topic_intelligence_summary(topic_name, description, keywords)
                
                session.add(topic)
                session.commit()
                
                logger.info(f"Created new topic: {topic_name}")
                return topic
                
        except Exception as e:
            logger.error(f"Failed to create/update topic {topic_name}: {str(e)}")
            return None
    
    def create_task_with_full_context(self,
                                    description: str,
                                    assignee_email: str = None,
                                    topic_names: List[str] = None,
                                    context: EntityContext = None,
                                    due_date: datetime = None,
                                    priority: str = 'medium') -> Task:
        """
        Create tasks with full context story and entity relationships.
        This addresses your concern about tasks existing "in the air".
        """
        try:
            with self.db_manager.get_session() as session:
                # Create the task
                task = Task(
                    user_id=context.user_id,
                    description=description,
                    priority=priority,
                    due_date=due_date,
                    confidence=context.confidence,
                    created_at=datetime.utcnow()
                )
                
                # Generate context story - WHY this task exists
                task.context_story = self._generate_task_context_story(
                    description, assignee_email, topic_names, context
                )
                
                # Link to assignee if provided
                if assignee_email:
                    assignee = self.create_or_update_person(assignee_email, context=context)
                    if assignee:
                        task.assignee_id = assignee.id
                
                # Link to topics
                if topic_names:
                    for topic_name in topic_names:
                        topic = self.create_or_update_topic(topic_name, context=context)
                        if topic:
                            task.topics.append(topic)
                
                # Link to source
                if context.source_type == 'email' and context.source_id:
                    task.source_email_id = context.source_id
                elif context.source_type == 'calendar' and context.source_id:
                    task.source_event_id = context.source_id
                
                session.add(task)
                session.commit()
                
                logger.info(f"Created task with full context: {description[:50]}...")
                return task
                
        except Exception as e:
            logger.error(f"Failed to create task: {str(e)}")
            return None
    
    # =====================================================================
    # RELATIONSHIP INTELLIGENCE METHODS
    # =====================================================================
    
    def create_entity_relationship(self, 
                                 entity_a_type: str, entity_a_id: int,
                                 entity_b_type: str, entity_b_id: int,
                                 relationship_type: str,
                                 context: EntityContext) -> EntityRelationship:
        """Create intelligent relationships between any entities"""
        try:
            with self.db_manager.get_session() as session:
                # Check if relationship already exists
                existing = session.query(EntityRelationship).filter(
                    EntityRelationship.user_id == context.user_id,
                    EntityRelationship.entity_type_a == entity_a_type,
                    EntityRelationship.entity_id_a == entity_a_id,
                    EntityRelationship.entity_type_b == entity_b_type,
                    EntityRelationship.entity_id_b == entity_b_id
                ).first()
                
                if existing:
                    # Strengthen existing relationship
                    existing.total_interactions += 1
                    existing.last_interaction = datetime.utcnow()
                    existing.strength = min(1.0, existing.strength + 0.1)
                    session.commit()
                    return existing
                
                # Create new relationship
                relationship = EntityRelationship(
                    user_id=context.user_id,
                    entity_type_a=entity_a_type,
                    entity_id_a=entity_a_id,
                    entity_type_b=entity_b_type,
                    entity_id_b=entity_b_id,
                    relationship_type=relationship_type,
                    strength=0.5,
                    last_interaction=datetime.utcnow(),
                    total_interactions=1
                )
                
                # Generate context summary
                relationship.context_summary = self._generate_relationship_context(
                    entity_a_type, entity_a_id, entity_b_type, entity_b_id, session
                )
                
                session.add(relationship)
                session.commit()
                
                logger.info(f"Created relationship: {entity_a_type}:{entity_a_id} -> {entity_b_type}:{entity_b_id}")
                return relationship
                
        except Exception as e:
            logger.error(f"Failed to create entity relationship: {str(e)}")
            return None
    
    def augment_entity_from_source(self, 
                                  entity_type: str, 
                                  entity_id: int, 
                                  new_data: Dict,
                                  context: EntityContext):
        """
        Real-time entity augmentation when new data arrives.
        This solves your concern about not connecting new emails to existing calendar events.
        """
        try:
            with self.db_manager.get_session() as session:
                if entity_type == 'topic':
                    topic = session.query(Topic).get(entity_id)
                    if topic:
                        self._augment_topic_from_new_data(topic, new_data, context, session)
                        
                elif entity_type == 'person':
                    person = session.query(Person).get(entity_id)
                    if person:
                        self._augment_person_from_new_data(person, new_data, context, session)
                        
                elif entity_type == 'event':
                    event = session.query(CalendarEvent).get(entity_id)
                    if event:
                        self._augment_event_from_new_data(event, new_data, context, session)
                
                session.commit()
                logger.info(f"Augmented {entity_type}:{entity_id} with new {context.source_type} data")
                
        except Exception as e:
            logger.error(f"Failed to augment entity {entity_type}:{entity_id}: {str(e)}")
    
    # =====================================================================
    # INTELLIGENCE GENERATION METHODS
    # =====================================================================
    
    def generate_proactive_insights(self, user_id: int) -> List[IntelligenceInsight]:
        """
        Generate proactive insights based on entity patterns and relationships.
        This is where the predictive intelligence happens.
        """
        insights = []
        
        try:
            with self.db_manager.get_session() as session:
                # Insight 1: Relationship gaps (haven't contacted important people)
                relationship_insights = self._detect_relationship_gaps(user_id, session)
                insights.extend(relationship_insights)
                
                # Insight 2: Topic momentum (topics getting hot)
                topic_insights = self._detect_topic_momentum(user_id, session)
                insights.extend(topic_insights)
                
                # Insight 3: Meeting preparation needs
                meeting_prep_insights = self._detect_meeting_prep_needs(user_id, session)
                insights.extend(meeting_prep_insights)
                
                # Insight 4: Project attention needed
                project_insights = self._detect_project_attention_needs(user_id, session)
                insights.extend(project_insights)
                
                # Save insights to database
                for insight in insights:
                    session.add(insight)
                
                session.commit()
                logger.info(f"Generated {len(insights)} proactive insights for user {user_id}")
                
        except Exception as e:
            logger.error(f"Failed to generate proactive insights: {str(e)}")
            
        return insights
    
    # =====================================================================
    # HELPER METHODS
    # =====================================================================
    
    def _find_matching_topic(self, topic_name: str, user_id: int, session: Session) -> Optional[Topic]:
        """Intelligent topic matching using exact name, keywords, or similarity"""
        # Exact match first
        exact_match = session.query(Topic).filter(
            Topic.user_id == user_id,
            Topic.name.ilike(f"%{topic_name}%")
        ).first()
        
        if exact_match:
            return exact_match
        
        # Keyword matching
        topics = session.query(Topic).filter(Topic.user_id == user_id).all()
        for topic in topics:
            if topic.keywords:
                keywords = [k.strip().lower() for k in topic.keywords.split(',')]
                if topic_name.lower() in keywords:
                    return topic
        
        return None
    
    def _generate_task_context_story(self, description: str, assignee_email: str, 
                                   topic_names: List[str], context: EntityContext) -> str:
        """Generate WHY this task exists - the narrative context"""
        story_parts = []
        
        # Source context
        if context.source_type == 'email':
            story_parts.append(f"Task extracted from email communication")
        elif context.source_type == 'calendar':
            story_parts.append(f"Task generated for meeting preparation")
        
        # Topic context
        if topic_names:
            story_parts.append(f"Related to: {', '.join(topic_names)}")
        
        # Assignee context
        if assignee_email:
            story_parts.append(f"Assigned to: {assignee_email}")
        
        # Confidence context
        confidence_level = "high" if context.confidence > 0.8 else "medium" if context.confidence > 0.5 else "low"
        story_parts.append(f"Confidence: {confidence_level}")
        
        return ". ".join(story_parts)
    
    def _generate_topic_intelligence_summary(self, name: str, description: str, keywords: List[str]) -> str:
        """Generate AI summary of what we know about this topic"""
        # This would call Claude for intelligence generation
        # For now, return a structured summary
        parts = [f"Topic: {name}"]
        if description:
            parts.append(f"Description: {description}")
        if keywords:
            parts.append(f"Keywords: {', '.join(keywords)}")
        return ". ".join(parts)
    
    def _detect_relationship_gaps(self, user_id: int, session: Session) -> List[IntelligenceInsight]:
        """Detect important people user hasn't contacted recently"""
        insights = []
        
        # Find high-importance people with no recent contact
        important_people = session.query(Person).filter(
            Person.user_id == user_id,
            Person.importance_level > 0.7,
            Person.last_contact < datetime.utcnow() - timedelta(days=14)
        ).limit(5).all()
        
        for person in important_people:
            insight = IntelligenceInsight(
                user_id=user_id,
                insight_type='relationship_alert',
                title=f"Haven't connected with {person.name} recently",
                description=f"Last contact was {person.last_contact.strftime('%Y-%m-%d')}. "
                           f"Consider reaching out about relevant topics.",
                priority='medium',
                confidence=0.8,
                related_entity_type='person',
                related_entity_id=person.id
            )
            insights.append(insight)
        
        return insights
    
    def _detect_topic_momentum(self, user_id: int, session: Session) -> List[IntelligenceInsight]:
        """Detect topics that are gaining momentum"""
        insights = []
        
        # Topics mentioned frequently in recent emails (last 7 days)
        from datetime import timedelta
        week_ago = datetime.utcnow() - timedelta(days=7)
        
        hot_topics = session.query(Topic).filter(
            Topic.user_id == user_id,
            Topic.last_mentioned > week_ago,
            Topic.total_mentions > 3
        ).order_by(Topic.total_mentions.desc()).limit(3).all()
        
        for topic in hot_topics:
            insight = IntelligenceInsight(
                user_id=user_id,
                insight_type='topic_momentum',
                title=f"'{topic.name}' is trending in your communications",
                description=f"Mentioned {topic.total_mentions} times recently. "
                           f"Consider preparing materials or scheduling focused time.",
                priority='medium',
                confidence=0.7,
                related_entity_type='topic',
                related_entity_id=topic.id
            )
            insights.append(insight)
        
        return insights
    
    def _detect_meeting_prep_needs(self, user_id: int, session: Session) -> List[IntelligenceInsight]:
        """Detect upcoming meetings that need preparation"""
        insights = []
        
        # Meetings in next 48 hours with high prep priority
        tomorrow = datetime.utcnow() + timedelta(hours=48)
        
        upcoming_meetings = session.query(CalendarEvent).filter(
            CalendarEvent.user_id == user_id,
            CalendarEvent.start_time.between(datetime.utcnow(), tomorrow),
            CalendarEvent.preparation_priority > 0.7
        ).all()
        
        for meeting in upcoming_meetings:
            insight = IntelligenceInsight(
                user_id=user_id,
                insight_type='meeting_prep',
                title=f"Prepare for '{meeting.title}'",
                description=f"High-priority meeting on {meeting.start_time.strftime('%Y-%m-%d %H:%M')}. "
                           f"{meeting.business_context}",
                priority='high',
                confidence=0.9,
                related_entity_type='event',
                related_entity_id=meeting.id,
                expires_at=meeting.start_time
            )
            insights.append(insight)
        
        return insights
    
    def _detect_project_attention_needs(self, user_id: int, session: Session) -> List[IntelligenceInsight]:
        """Detect projects that need attention"""
        insights = []
        
        # Projects with no recent activity
        stale_projects = session.query(Project).filter(
            Project.user_id == user_id,
            Project.status == 'active',
            Project.updated_at < datetime.utcnow() - timedelta(days=7)
        ).limit(3).all()
        
        for project in stale_projects:
            insight = IntelligenceInsight(
                user_id=user_id,
                insight_type='project_attention',
                title=f"Project '{project.name}' needs attention",
                description=f"No recent activity since {project.updated_at.strftime('%Y-%m-%d')}. "
                           f"Consider checking in with stakeholders.",
                priority='medium',
                confidence=0.6,
                related_entity_type='project',
                related_entity_id=project.id
            )
            insights.append(insight)
        
        return insights
    
    def _update_person_intelligence(self, person: Person, name: str, context: EntityContext, session: Session) -> bool:
        """Update existing person with new intelligence"""
        updated = False
        
        # Update name if we have a better one
        if name and name != person.name and len(name) > len(person.name or ""):
            person.name = name
            updated = True
        
        # Update interaction count
        person.total_interactions += 1
        person.last_contact = datetime.utcnow()
        updated = True
        
        # Extract additional info from context
        if context.processing_metadata:
            # Extract from email signature if available
            if 'signature' in context.processing_metadata:
                signature_info = self._extract_from_signature(context.processing_metadata['signature'])
                if signature_info.get('title') and not person.title:
                    person.title = signature_info['title']
                    updated = True
                if signature_info.get('company') and not person.company:
                    person.company = signature_info['company']
                    updated = True
                if signature_info.get('phone') and not person.phone:
                    person.phone = signature_info['phone']
                    updated = True
        
        return updated
    
    def _enrich_person_from_context(self, person: Person, context: EntityContext):
        """Enrich new person with context-specific intelligence"""
        if not context.processing_metadata:
            return
        
        # Extract from email signature
        if 'signature' in context.processing_metadata:
            signature_info = self._extract_from_signature(context.processing_metadata['signature'])
            person.title = signature_info.get('title')
            person.company = signature_info.get('company')
            person.phone = signature_info.get('phone')
            person.linkedin_url = signature_info.get('linkedin')
        
        # Set relationship type based on source
        if context.source_type == 'email':
            person.relationship_type = 'email_contact'
        elif context.source_type == 'calendar':
            person.relationship_type = 'meeting_attendee'
    
    def _extract_from_signature(self, signature_text: str) -> Dict[str, str]:
        """Extract structured data from email signature"""
        import re
        
        info = {}
        
        # Extract title (common patterns)
        title_patterns = [
            r'(?:^|\n)([A-Z][a-z]+ [A-Z][a-z]+)\s*(?:\n|$)',  # Job Title
            r'(?:^|\n)([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s*(?:\n|$)'
        ]
        
        # Extract company (usually follows title)
        company_patterns = [
            r'(?:^|\n)([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*(?:\s+Inc\.?|\s+LLC|\s+Corp\.?))\s*(?:\n|$)',
            r'(?:^|\n)(@\s*)?([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s*(?:\n|$)'
        ]
        
        # Extract phone
        phone_pattern = r'(\+?1?[-.\s]?\(?[0-9]{3}\)?[-.\s]?[0-9]{3}[-.\s]?[0-9]{4})'
        phone_match = re.search(phone_pattern, signature_text)
        if phone_match:
            info['phone'] = phone_match.group(1)
        
        # Extract LinkedIn
        linkedin_pattern = r'linkedin\.com/in/([a-zA-Z0-9-]+)'
        linkedin_match = re.search(linkedin_pattern, signature_text)
        if linkedin_match:
            info['linkedin'] = f"https://linkedin.com/in/{linkedin_match.group(1)}"
        
        return info
    
    def _extract_name_from_email(self, email: str) -> str:
        """Extract probable name from email address"""
        local_part = email.split('@')[0]
        
        # Handle common patterns
        if '.' in local_part:
            parts = local_part.split('.')
            return ' '.join([part.capitalize() for part in parts])
        elif '_' in local_part:
            parts = local_part.split('_')
            return ' '.join([part.capitalize() for part in parts])
        else:
            return local_part.capitalize()
    
    def _augment_topic_intelligence(self, topic: Topic, description: str, keywords: List[str], context: EntityContext, session: Session) -> bool:
        """Augment existing topic with new intelligence"""
        updated = False
        
        # Update description if we have new information
        if description and description not in (topic.description or ""):
            if topic.description:
                topic.description = f"{topic.description}. {description}"
            else:
                topic.description = description
            updated = True
        
        # Add new keywords
        if keywords:
            existing_keywords = set(topic.keywords.split(',')) if topic.keywords else set()
            new_keywords = set(keywords) - existing_keywords
            if new_keywords:
                all_keywords = existing_keywords | new_keywords
                topic.keywords = ','.join(sorted(all_keywords))
                updated = True
        
        # Update usage statistics
        topic.total_mentions += 1
        topic.last_mentioned = datetime.utcnow()
        updated = True
        
        # Increase confidence if mentioned frequently
        if topic.total_mentions > 5:
            topic.confidence_score = min(1.0, topic.confidence_score + 0.1)
            updated = True
        
        return updated
    
    def _generate_relationship_context(self, entity_a_type: str, entity_a_id: int, 
                                     entity_b_type: str, entity_b_id: int, session: Session) -> str:
        """Generate context summary for entity relationship"""
        # This would use Claude to generate intelligent relationship context
        # For now, return a basic summary
        return f"Relationship between {entity_a_type}:{entity_a_id} and {entity_b_type}:{entity_b_id}"
    
    def _augment_topic_from_new_data(self, topic: Topic, new_data: Dict, context: EntityContext, session: Session):
        """Augment topic when new data arrives"""
        if 'mentions' in new_data:
            topic.total_mentions += new_data['mentions']
        
        if 'new_keywords' in new_data:
            existing = set(topic.keywords.split(',')) if topic.keywords else set()
            updated_keywords = existing | set(new_data['new_keywords'])
            topic.keywords = ','.join(sorted(updated_keywords))
        
        topic.last_mentioned = datetime.utcnow()
    
    def _augment_person_from_new_data(self, person: Person, new_data: Dict, context: EntityContext, session: Session):
        """Augment person when new data arrives"""
        if 'interaction' in new_data:
            person.total_interactions += 1
            person.last_contact = datetime.utcnow()
        
        if 'title' in new_data and not person.title:
            person.title = new_data['title']
        
        if 'company' in new_data and not person.company:
            person.company = new_data['company']
    
    def _augment_event_from_new_data(self, event: CalendarEvent, new_data: Dict, context: EntityContext, session: Session):
        """Augment calendar event when new data arrives (e.g., from related emails)"""
        if 'business_context' in new_data:
            if event.business_context:
                event.business_context = f"{event.business_context}. {new_data['business_context']}"
            else:
                event.business_context = new_data['business_context']
        
        if 'preparation_priority' in new_data:
            # Increase preparation priority based on email intelligence
            event.preparation_priority = min(1.0, event.preparation_priority + 0.2)

# Global instance
entity_engine = UnifiedEntityEngine()

============================================================
FILE: archive/old_docs/Priority_Fixes_v1.txt
============================================================
 AI Chief of Staff - PRIORITY FIXES v1.0
Critical Issues That Must Be Resolved Immediately

Based on actual performance analysis of the knowledge base, these are the **specific, actionable fixes** needed to bring the system from 70% to 85%+ quality.



 CRITICAL ISSUE #1: PEOPLE EXTRACTION FAILURE

**Current State:** Only 3 contacts extracted from 44 business emails (25% success rate)
**Target State:** 8+ contacts extracted (75%+ success rate)

**Specific Missing Contacts from Real Data:**
- Amit Shine (amit@session-42.com) - Board meeting organizer
- Barry Braxton (barry@tmium.com) - HitCraft business relationship
- Yoav Yogev (yoav@made.co.il) - CFO sending financial updates
- Sofia Rivo (srivo@nvidia.com) - NVIDIA partnership contact
- Alon Matas (alon@alonmatas.com) - Tel Aviv business meeting
- Dania Fried (daniafried@tauex.tau.ac.il) - Conference organizer

### ROOT CAUSE ANALYSIS:

**Problem 1: Overly Restrictive Filtering**
```python
# CURRENT (blocks legitimate contacts):
'admin@', 'info@', 'contact@', 'help@', 'service@', 'team@'

# SHOULD BE (only block obvious automation):
'noreply', 'no-reply', 'donotreply', 'mailer-daemon', 'robot@', 'bot@'
```

**Problem 2: Not Using Debug Functions**
- Production uses restrictive `_process_human_contacts_only()`
- Should use permissive `_process_human_contacts_only_debug()`

**Problem 3: Weak Claude Prompts**
- Not emphasizing sender extraction strongly enough
- Missing explicit instructions to extract mentioned people

### SPECIFIC FIXES NEEDED:

**Fix 1: Update Contact Filtering (IMMEDIATE)**
File: `chief_of_staff_ai/processors/email_intelligence.py`
```python
def _is_non_human_contact(self, email_address: str) -> bool:
    # ONLY filter obvious automation
    definite_non_human_patterns = [
        'noreply', 'no-reply', 'donotreply', 'mailer-daemon',
        'postmaster@', 'daemon@', 'bounce@', 'robot@', 'bot@',
        'automated@', 'system@notification'
    ]
    # REMOVE: 'admin@', 'info@', 'team@', 'hello@' - these are real people!
```

**Fix 2: Use Debug Functions in Production**
File: `chief_of_staff_ai/processors/email_intelligence.py`
```python
# Change line ~102:
people_count = self._process_human_contacts_only_debug(user.id, analysis, email)
# Instead of:
people_count = self._process_human_contacts_only(user.id, analysis, email)
```

**Fix 3: Enhance Claude Prompts**
```
PEOPLE EXTRACTION: Extract ALL human contacts with professional relevance (be generous!)
- ALWAYS extract the sender if they're a real person  
- Extract anyone mentioned by name with business context
- Include names even with limited contact information
- Default to is_human_contact: true for most senders
```



 CRITICAL ISSUE #2: TOPIC EXTRACTION COMPLETE FAILURE

**Current State:** 0 topics identified from 44 business emails
**Target State:** 6+ clear business topics identified

**Missing Topics from Real Email Content:**
- "HitCraft" (mentioned in 10+ emails - main project)
- "Board Meeting" (recurring business activity)
- "Fundraising" (Cross Atlantic VC, investor discussions)
- "AI in Music" (technology focus area)
- "Session-42" (company name and operations)
- "Certification" (FairlyTrained program)
- "NVIDIA Partnership" (technology collaboration)

### ROOT CAUSE ANALYSIS:

**Problem 1: Topics Not Saving to Database**
- Claude generates topics in JSON response
- Topics not being persisted to database properly
- No topic sync from email analysis to topic table

**Problem 2: Weak Topic Extraction**
- Claude prompt examples too generic ("business topic 1")
- Missing specific guidance on what constitutes a business topic
- No emphasis on extracting project names, company names, technologies

### SPECIFIC FIXES NEEDED:

**Fix 1: Implement Topic Persistence**
File: `chief_of_staff_ai/processors/email_intelligence.py`
```python
def _update_email_with_insights(self, email: Email, analysis: Dict):
    # ADD: Save topics to database
    topics = analysis.get('topics', [])
    if topics:
        for topic_name in topics:
            self._create_or_update_topic(email.user_id, topic_name)
```

**Fix 2: Enhance Topic Examples in Claude Prompt**
```
"topics": ["HitCraft", "board meeting", "fundraising", "AI in music", "certification", "business development"]
// Extract: project names, company names, technologies, business areas, meeting types
```

**Fix 3: Add Topic Sync API Endpoint**
Create `/api/sync-topics` to extract topics from existing email analysis and save to database.



 CRITICAL ISSUE #3: GENERIC INSIGHTS INSTEAD OF SPECIFIC

**Current State:** 60% generic insights ("timeline coordination")
**Target State:** 80% specific insights ("HitCraft launch timeline with Cross Atlantic VC")

**Examples of Generic  Specific Improvements:**
- "Timeline coordination"  "HitCraft launch timeline with Random Forest VC partners"
- "Cross-timezone collaboration"  "Israel-US investor meeting coordination with Cross Atlantic VC"
- "Project management"  "Session-42 board meeting preparation with Amit Shine"
- "Business development"  "NVIDIA Inception program partnership opportunities"

### ROOT CAUSE ANALYSIS:

**Problem 1: Missing Business Entity Context**
- Claude doesn't know about specific projects, people, companies
- Insights generated without connection to extracted entities
- No relationship mapping between insights and business context

**Problem 2: Generic Prompting**
- Business insights prompt too vague
- Missing specific instruction to reference people/projects/companies
- No validation for insight specificity

### SPECIFIC FIXES NEEDED:

**Fix 1: Add Business Context to Claude Prompts**
```python
business_context_str = f"""
Current Projects: {', '.join(user_context['existing_projects'])}
Key Contacts: {', '.join(user_context['key_contacts'][:5])}
Business Focus: {', '.join(user_context['official_topics'])}
"""
```

**Fix 2: Enhance Insight Instructions**
```
BUSINESS INSIGHTS: Extract any strategic value, opportunities, or challenges
- Reference specific people, projects, and companies mentioned
- Connect insights to extracted business entities
- Be specific rather than generic (use names, projects, companies)
```

**Fix 3: Add Insight Specificity Validation**
```python
def _validate_insight_specificity(self, insight: str, analysis: Dict) -> bool:
    # Check if insight mentions specific people/projects from analysis
    entities = analysis.get('people', []) + analysis.get('project', {}).get('name', [])
    return any(entity.get('name', '') in insight for entity in entities)
```



 IMPLEMENTATION PRIORITY ORDER

**Week 1: People Extraction Fix (Highest Impact)**
1. Update `_is_non_human_contact()` filtering logic  DONE
2. Switch to debug extraction functions in production
3. Enhance Claude prompts for people extraction  DONE  
4. Test with current email set

**Week 2: Topic Organization (High Impact)**
1. Implement topic persistence from email analysis
2. Create topic sync API endpoint
3. Update Claude prompts with better topic examples  DONE
4. Sync existing email topics to database

**Week 3: Insight Quality (Medium Impact)**
1. Add business context to Claude prompts
2. Enhance insight specificity instructions
3. Implement insight validation
4. Test improved insight quality

**Week 4: Integration & Testing**
1. Full pipeline testing with improvements
2. Quality metrics validation
3. Performance monitoring
4. Documentation updates



 EXPECTED RESULTS AFTER FIXES

**People Extraction:**
- Before: 3 contacts from 44 emails (7% capture rate)
- After: 8-12 contacts from 44 emails (18-27% capture rate)
- Improvement: 160-280% increase

**Topic Organization:**
- Before: 0 topics identified
- After: 6-10 clear business topics
- Improvement: New capability

**Insight Quality:**
- Before: 60% actionable, 40% generic
- After: 80% actionable, 20% generic  
- Improvement: 33% quality increase

**Overall System Quality:**
- Before: 70% (good foundation, poor extraction)
- After: 85% (good foundation, strong extraction)
- Improvement: 21% overall quality increase



 SUCCESS CRITERIA

**Minimum Acceptable Results:**
- Extract 6+ people from current 44 email set
- Identify 4+ clear business topics
- 75% of insights mention specific people/projects/companies

**Target Results:**
- Extract 8+ people with rich context
- Identify 6+ topics with proper categorization
- 80% of insights are specific and actionable

**Stretch Results:**
- Extract 10+ people with relationship mapping
- Identify 8+ topics with clustering
- 85% of insights drive actual business decisions



 IMPLEMENTATION NOTES

**Testing Strategy:**
1. Use current knowledge base as baseline
2. Re-process emails after each fix
3. Compare extraction results quantitatively
4. Validate with actual business contacts/topics

**Risk Mitigation:**
1. Make changes incrementally (one fix at a time)
2. Keep debug logging for troubleshooting
3. Maintain rollback capability for each change
4. Test with small batch before full processing

**Quality Validation:**
1. Manual review of extracted people vs. actual email senders
2. Topic validation against email content analysis
3. Insight specificity scoring (count entity mentions)
4. User feedback on improvement relevance



This document focuses on **specific, actionable fixes** that address the exact gaps identified in the current system. Each fix targets a measured problem with clear success criteria and expected impact. 

============================================================
FILE: archive/old_docs/HOW_IT_WORKS_FOR_KIDS.txt
============================================================
 HOW YOUR AI CHIEF OF STAFF WORKS
=====================================
(Explained for a 10-year-old!)

Imagine you have a super smart robot friend that helps you organize your life!
This robot is like having a personal assistant that never gets tired and remembers EVERYTHING.

 WHAT IS IT?
==============
Your AI Chief of Staff is like a magical filing cabinet that:
- Reads your emails (like a super-fast reader!)
- Looks at your calendar (like checking your schedule)
- Remembers everyone you know (like a phone book with superpowers)
- Helps you remember what you need to do (like the world's best to-do list)
- Tells you important things you might forget (like a very smart reminder)

 THE MAGIC FLOW - What Happens Step by Step:
===============================================

 STEP 1: THE COLLECTOR (Email & Calendar Sync)
Your robot friend goes to your email inbox and calendar, just like how you might 
check your mailbox and look at a wall calendar. But this robot can read 
SUPER FAST - like reading 100 emails in the time it takes you to read one!

What it collects:
- All your emails (who sent them, what they're about)
- Your calendar events (meetings, appointments)
- Who you talk to the most
- What projects you're working on

 STEP 2: THE SMART BRAIN (AI Processing)
Now the robot puts on its thinking cap! It reads everything and asks:
- "Is this email important or just junk mail?"
- "Who are the most important people in this person's life?"
- "What projects is this person working on?"
- "Is there anything urgent they need to do?"

It's like having a friend who can read your diary and then help you organize 
your whole life based on what they learned!

 STEP 3: THE ORGANIZER (Entity Creation)
The robot creates different "folders" in its brain:

 PEOPLE FOLDER:
- Remembers everyone you email
- Knows who's your boss, friend, family
- Tracks how often you talk to them
- Remembers what they do for work

 TOPIC FOLDER:
- Groups similar things together
- Like putting all "school stuff" in one pile
- All "family stuff" in another pile
- All "fun stuff" in another pile

 TASK FOLDER:
- Makes a list of things you need to do
- Knows which ones are super important
- Remembers why you need to do them

 PROJECT FOLDER:
- Keeps track of big things you're working on
- Like a science project that takes weeks

 STEP 4: THE CONNECTION MAKER (Relationship Builder)
This is the coolest part! The robot draws invisible lines between everything:

- It knows that "Mom" is connected to "family dinner" 
- It knows that "Math homework" is connected to "Mr. Smith" (your teacher)
- It knows that "Birthday party planning" connects to your "Friends" and "Weekend"

It's like making a giant spider web where everything that belongs together 
gets connected with invisible string!

 STEP 5: THE FORTUNE TELLER (Predictive Insights)
Now your robot friend becomes a fortune teller! It looks at all the patterns and says:

- "Hey! You usually call Grandma on Sundays, but you haven't this week!"
- "Your science project is due in 3 days and you haven't started!"
- "Your friend Sarah always emails you before her birthday - it's coming up!"
- "You have 3 meetings this week about the same topic - maybe it's important!"

 STEP 6: THE HELPER DASHBOARD (Web Interface)
Finally, you get a magic window (website at http://localhost:8080) where you can:

- See all your important people
- Check your tasks organized by importance
- Look at your upcoming events
- Read the robot's helpful suggestions
- Tell the robot to fetch new emails

 THE MAGIC CYCLE:
===================
This happens over and over, like a merry-go-round:

Collect  Think  Organize  Connect  Predict  Show You  Repeat!

Every time you get new emails or add calendar events, your robot friend 
gets smarter and better at helping you!

 WHY IS THIS AWESOME?
=======================
- Never forget important people or tasks
- Know what's most important right now
- Get reminded of things before you forget
- See connections you might miss
- Have a super-organized digital life

It's like having a best friend who:
- Has a perfect memory
- Never gets tired
- Loves organizing things
- Always wants to help you succeed
- Works 24/7 just for you!

 IN SIMPLE WORDS:
===================
Your AI Chief of Staff is a robot that reads your emails and calendar,
remembers everything important, organizes it all perfectly, and then 
tells you the most helpful things so you can be super organized and 
never forget anything important!

It's like having a magical assistant that makes your life easier! 

===================================================================
Made with  by your AI Chief of Staff System
Running on port 8080  Always learning  Always helping
=================================================================== 

============================================================
FILE: archive/old_docs/Spec.txt
============================================================
 AI Chief of Staff  Technical Product Specification (v4.0 - Smart Contact Strategy)

 Goal

A revolutionary AI-powered Chief of Staff web application that transforms Gmail into a comprehensive business intelligence platform using engagement-driven processing. Built with Flask, powered by Claude 4 Sonnet, featuring the groundbreaking Smart Contact Strategy that focuses AI processing on content that actually matters to the user's business.



 Architecture Overview

1. Production Backend Stack
		Language: Python 3.10+
		Framework: Flask with Flask-Session
		Database: SQLAlchemy with SQLite (development) / PostgreSQL (production)
		AI Engine: Claude 4 Sonnet (Anthropic) for intelligent email analysis
		Authentication: Google OAuth 2.0 with Gmail API integration
		Deployment: Heroku-ready with Procfile and requirements.txt
		Session Management: Secure multi-tenant isolation with UUID-based sessions
		NEW: Smart Contact Strategy for engagement-driven processing

2. Frontend Architecture
		Modern responsive HTML/CSS/JavaScript
		4-section dashboard design:
		- Business Knowledge: Metrics, topics, opportunities, challenges
		- Email Insights: AI-analyzed emails with summaries and categorization
		- Tasks: Enhanced actionable items with context and confidence scores
		- People: Professional network with relationship mapping
		Real-time API integration with jQuery/fetch
		Mobile-responsive design with professional UI/UX
		NEW: Expandable rich context cards with progressive disclosure

3. Database Schema Enhancement
		Users: Multi-tenant user management with Gmail integration
		Emails: Normalized email storage with AI analysis metadata
		Tasks: Context-rich actionable items with confidence scoring
		People: Professional network with relationship analysis
		Projects: Business initiative organization
		Topics: Intelligent content categorization with merging capabilities
		NEW: TrustedContacts: Engagement-based contact database
		NEW: ContactContexts, TaskContexts, TopicKnowledgeBase: Rich context storage



 Data Ingestion & Processing (REVOLUTIONARY SMART CONTACT STRATEGY)

Gmail Integration with Engagement Focus:
		OAuth 2.0 flow with proper token management
		NEW: Sent email analysis to build Trusted Contact Database
		Smart email classification using engagement patterns
		Force refresh capability for re-analysis
		Proper thread handling and metadata extraction

Smart Processing Pipeline:
	1.	Foundation: Analyze sent emails to build trusted contact database
	2.	Classification: Smart decision tree for incoming emails
		- Trusted contact?  Full AI analysis
		- Unknown + obvious spam?  Skip entirely
		- Unknown + business-like?  Quick AI relevance check
	3.	Processing: Cost-optimized AI analysis based on classification
	4.	Integration: Unified pipeline that generates insights immediately
	5.	Learning: Adaptive patterns based on user engagement

Engagement Metrics:
		Contact relationship strength (frequency + recency + topic overlap)
		Topic relevance scoring (sent vs received mentions)
		Bidirectional communication patterns
		Business context from professional signatures
		Gmail links for source email access



 Core Intelligence (Claude 4 Sonnet) - ENGAGEMENT-OPTIMIZED

1. Smart Email Classification
		Trusted contact identification from sent email analysis
		Newsletter and automated message detection
		Quick relevance assessment for unknown senders
		Cost optimization through intelligent filtering
		Processing decision confidence scoring

2. Enhanced Task Extraction with Rich Context
		Context-aware actionable item identification
		Sender relationship analysis and trust scoring
		Due date extraction with natural language processing
		Confidence scoring for task validity
		Expandable context: background, stakeholders, business impact
		Direct Gmail linking for source verification

3. Business Knowledge Graph with Engagement Weighting
		People network analysis with professional context
		Project interconnections and dependencies
		Topic clustering based on engagement patterns
		Decision tracking and outcome analysis
		Communication pattern insights with relationship strength

4. Unified Processing Flow
		Single-pass processing eliminates token waste
		Immediate strategic insight generation
		Smart caching to avoid re-processing
		Cost-optimized AI usage through engagement filtering



 Core Features (ENHANCED WITH RICH CONTEXT)

Business Intelligence Dashboard:
		Real-time metrics and KPI tracking
		Strategic opportunity identification based on engagement patterns
		Challenge and risk monitoring from trusted communications
		Topic trend analysis weighted by user engagement
		Decision outcome tracking from bidirectional conversations

Email Management with Smart Filtering:
		Engagement-driven email processing
		AI-powered email summaries for relevant content only
		Intelligent filtering based on trusted contact database
		Priority inbox with engagement scoring
		Direct Gmail integration links

Task Management with Rich Context:
		Context-rich actionable items with expandable details
		Background context from email thread analysis
		Stakeholder identification and relationship mapping
		Business impact assessment and timeline context
		Project and initiative tagging with cross-references

Professional Network with Engagement Intelligence:
		Trusted contact database from sent email analysis
		Relationship strength scoring and communication patterns
		Professional context from signature analysis
		Topic expertise identification from email content
		Expandable contact cards with comprehensive context



 API Architecture (ENHANCED FOR SMART PROCESSING)

RESTful Endpoints:
		/api/fetch-emails: Gmail integration and fetching
		/api/process-emails: Unified smart processing pipeline
		/api/build-trusted-contacts: Sent email analysis for contact database
		/api/chat: Claude conversation interface
		/api/chat-with-knowledge: Context-aware AI assistance
		/api/status: System health and user metrics
		/api/emails, /api/tasks, /api/people: Data access with rich context
		/api/topics: Topic management with knowledge repositories
		NEW: /api/contact-contexts, /api/task-contexts: Rich context access

Smart Processing Endpoints:
		/api/classify-email: Smart email classification service
		/api/engagement-metrics: User engagement pattern analysis
		/api/processing-efficiency: Cost optimization reporting
		/api/update-engagement-patterns: Adaptive learning updates

Authentication Flow:
		Google OAuth 2.0 with proper state validation
		Secure session management with UUID isolation
		Multi-tenant data separation
		Token refresh and error handling



 Security & Privacy (ENHANCED FOR ENGAGEMENT DATA)

Multi-Tenant Architecture:
		Complete user data isolation including engagement patterns
		Session-based security with CSRF protection
		Per-user database queries with user ID validation
		Secure credential storage and token management

Data Protection:
		Encrypted sensitive data storage including engagement metrics
		Secure API key management via environment variables
		Session timeout and automatic cleanup
		Gmail API rate limiting and error handling
		Privacy protection for contact engagement analysis



 Production Deployment (UPDATED FOR v4.0)

Heroku Configuration:
		Procfile with gunicorn WSGI server
		PostgreSQL database addon integration
		Environment variable configuration
		Automatic dependency management
		NEW: Enhanced database schema for rich context

Local Development:
		SQLite database for rapid development
		Hot reload with Flask debug mode
		Comprehensive logging and error handling
		Test data isolation and cleanup
		NEW: Development tools for engagement pattern testing

Required Environment Variables:
		GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET
		ANTHROPIC_API_KEY (Claude 4 Sonnet)
		SECRET_KEY for session security
		DATABASE_URL for production PostgreSQL



 Advanced Features (NEW IN v4.0)

Smart Contact Strategy:
		Sent email analysis for trusted contact identification
		Engagement-based email classification and processing
		Cost optimization through intelligent filtering
		Adaptive learning from user behavior patterns

Rich Context System:
		Expandable contact cards with comprehensive professional context
		Task context including background, stakeholders, and business impact
		Topic knowledge repositories with methodology and decision history
		Progressive disclosure UI for detailed information

Unified Processing Pipeline:
		Single-pass email processing with immediate insight generation
		Smart caching to avoid redundant AI processing
		Cost tracking and optimization reporting
		Efficiency metrics and processing decision transparency

Topic Intelligence Enhancement:
		Engagement-weighted topic relevance scoring
		Comprehensive knowledge bases for each topic
		Business context and decision history tracking
		Agent-like topic expertise development

Knowledge Integration:
		Context-aware Claude conversations using engagement data
		Business knowledge synthesis weighted by user interaction
		Multi-source information correlation with trust scoring
		Intelligent response generation based on proven interests



 Success Metrics (v4.0 ENHANCED)

Processing Quality and Efficiency:
		70-90% reduction in AI processing costs through smart filtering
		95%+ accuracy in contact importance classification
		60%+ reduction in irrelevant content processing
		Professional context captured for all trusted contacts

User Experience with Rich Context:
		Sub-2-second dashboard load times
		100% email-to-insight traceability
		90% of displayed information provides actionable business insight
		High engagement with expandable content features

Business Intelligence Relevance:
		Knowledge base reflects actual business interests (engagement-driven)
		Strategic insights actionable and relevant to user's work
		Topic knowledge becomes comprehensive business reference
		AI chat provides highly relevant responses based on engagement patterns

Cost Optimization:
		Dramatic reduction in AI processing costs vs. brute-force approach
		Higher value extracted per token spent on relevant content
		Sustainable scaling with email volume growth
		ROI improvement through engagement-focused intelligence



 Roadmap

Current Production Features (v4.0 - Smart Contact Strategy):
		Revolutionary engagement-driven email processing
		Trusted contact database from sent email analysis
		Smart email classification with cost optimization
		Rich context system with expandable UI components
		Unified processing pipeline eliminating token waste
		Enhanced topic intelligence with knowledge repositories

Future Enhancements (v5.0):
		Calendar integration with meeting analysis using engagement patterns
		Voice interface and meeting transcription with contact prioritization
		Advanced automation and workflow triggers based on trusted contacts
		External API integrations (Slack, Notion) with engagement weighting
		Mobile application with smart notification filtering
		Enterprise SSO and team collaboration with engagement insights



This specification reflects the revolutionary Smart Contact Strategy implementation that transforms the AI Chief of Staff into an engagement-driven business intelligence platform, dramatically improving relevance while optimizing costs through intelligent processing decisions based on demonstrated user engagement patterns.

============================================================
FILE: archive/old_docs/Smart_Contact_Strategy_v4.txt
============================================================
 Smart Contact Strategy v4.0 - Technical Architecture

Revolutionary Engagement-Driven Email Intelligence System



 Core Strategy Overview

The Smart Contact Strategy fundamentally changes how AI processes business communications by using ENGAGEMENT PATTERNS rather than content volume to determine importance and relevance.

**Core Principle**: "If I send emails about/to it, it matters to my business intelligence."



 Technical Architecture

 1. Trusted Contact Database

Primary Data Source: Gmail Sent Folder Analysis
```python
# Sent Email Analysis Pipeline
sent_emails = gmail_api.fetch_sent_emails(days_back=365)  # Full year history
trusted_contacts = extract_all_recipients(sent_emails)

# Build engagement metrics for each contact
for contact in trusted_contacts:
    engagement_score = calculate_engagement(
        frequency=count_emails_to_contact(contact),
        recency=last_email_date(contact),
        topic_overlap=shared_topics(contact),
        thread_depth=conversation_threads(contact)
    )
```

Data Schema Enhancement:
```sql
-- New table for trusted contacts
CREATE TABLE trusted_contacts (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id),
    email_address VARCHAR(255) NOT NULL,
    engagement_score FLOAT DEFAULT 0.0,
    first_sent_date TIMESTAMP,
    last_sent_date TIMESTAMP,
    total_sent_emails INTEGER DEFAULT 0,
    topics_discussed TEXT[],
    relationship_strength ENUM('high', 'medium', 'low'),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Enhancement to existing people table
ALTER TABLE people ADD COLUMN is_trusted_contact BOOLEAN DEFAULT FALSE;
ALTER TABLE people ADD COLUMN engagement_score FLOAT DEFAULT 0.0;
ALTER TABLE people ADD COLUMN bidirectional_topics TEXT[];
```



 2. Smart Email Processing Decision Engine

Incoming Email Classification Algorithm:
```python
def classify_incoming_email(email):
    sender = email.sender
    
    # Step 1: Check trusted contact database
    trusted_contact = db.query(TrustedContact).filter_by(
        user_id=current_user.id, 
        email_address=sender
    ).first()
    
    if trusted_contact:
        return ProcessingDecision(
            action="ANALYZE_WITH_AI",
            confidence="HIGH",
            reason="From trusted contact",
            priority=trusted_contact.engagement_score
        )
    
    # Step 2: Check for obvious newsletters/spam
    if is_obvious_newsletter(email):
        return ProcessingDecision(
            action="SKIP",
            confidence="HIGH", 
            reason="Newsletter/automated content"
        )
    
    # Step 3: Quick AI relevance check for unknowns
    if appears_business_relevant(email):
        ai_assessment = claude_quick_relevance_check(email, user_context)
        return ProcessingDecision(
            action="CONDITIONAL_ANALYZE" if ai_assessment.relevant else "SKIP",
            confidence="MEDIUM",
            reason=ai_assessment.reasoning
        )
    
    return ProcessingDecision(action="SKIP", confidence="HIGH", reason="No engagement pattern")
```



 3. Topic Interest Scoring

Engagement-Based Topic Weighting:
```python
def calculate_topic_relevance(topic, user_id):
    # Count mentions in sent vs received emails
    sent_mentions = count_topic_in_sent_emails(topic, user_id)
    received_mentions = count_topic_in_received_emails(topic, user_id)
    bidirectional_threads = count_bidirectional_topic_threads(topic, user_id)
    
    # Weight formula (heavily favors engagement)
    relevance_score = (
        sent_mentions * 3.0 +           # High weight for topics I write about
        bidirectional_threads * 5.0 +   # Highest weight for discussions
        received_mentions * 0.5         # Low weight for receive-only
    ) / (sent_mentions + received_mentions + bidirectional_threads)
    
    return min(relevance_score, 1.0)

# Topic filtering in AI processing
def should_process_for_topic(email, topic):
    topic_relevance = get_topic_relevance(topic, email.user_id)
