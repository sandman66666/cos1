
            topics = session.query(Topic).filter(Topic.user_id == user.id).all()
            
            topics_data = []
            for topic in topics:
                topic_data = {
                    'id': topic.id,
                    'name': topic.name,
                    'description': topic.description,
                    'keywords': topic.keywords.split(',') if topic.keywords else [],
                    'is_official': topic.is_official,
                    'confidence_score': topic.confidence_score,
                    'total_mentions': topic.total_mentions,
                    'last_mentioned': topic.last_mentioned.isoformat() if topic.last_mentioned else None,
                    'intelligence_summary': topic.intelligence_summary,
                    'strategic_importance': topic.strategic_importance,
                    'created_at': topic.created_at.isoformat(),
                    'updated_at': topic.updated_at.isoformat(),
                    'version': topic.version,
                    
                    # Relationship data
                    'connected_people': len(topic.people),
                    'related_tasks': len(topic.tasks),
                    'connected_events': len(topic.events),
                    'total_connections': len(topic.people) + len(topic.tasks) + len(topic.events)
                }
                topics_data.append(topic_data)
            
            # Sort by strategic importance and recent activity
            topics_data.sort(key=lambda x: (x['strategic_importance'], x['total_mentions']), reverse=True)
            
            return jsonify({
                'success': True,
                'topics': topics_data,
                'summary': {
                    'total_topics': len(topics_data),
                    'official_topics': len([t for t in topics_data if t['is_official']]),
                    'high_importance': len([t for t in topics_data if t['strategic_importance'] > 0.7]),
                    'recently_active': len([t for t in topics_data if t['last_mentioned'] and 
                                          datetime.fromisoformat(t['last_mentioned']) > datetime.utcnow() - timedelta(days=7)]),
                    'highly_connected': len([t for t in topics_data if t['total_connections'] > 3])
                }
            })
            
    except Exception as e:
        logger.error(f"Failed to get topics with intelligence: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/entities/people', methods=['GET'])
def get_people_with_relationship_intelligence():
    """Get people with comprehensive relationship intelligence"""
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        with get_db_manager().get_session() as session:
            people = session.query(Person).filter(Person.user_id == user.id).all()
            
            people_data = []
            for person in people:
                person_data = {
                    'id': person.id,
                    'name': person.name,
                    'email_address': person.email_address,
                    'phone': person.phone,
                    'company': person.company,
                    'title': person.title,
                    'relationship_type': person.relationship_type,
                    'importance_level': person.importance_level,
                    'communication_frequency': person.communication_frequency,
                    'last_contact': person.last_contact.isoformat() if person.last_contact else None,
                    'total_interactions': person.total_interactions,
                    'linkedin_url': person.linkedin_url,
                    'professional_story': person.professional_story,
                    'created_at': person.created_at.isoformat(),
                    'updated_at': person.updated_at.isoformat(),
                    
                    # Relationship intelligence
                    'connected_topics': [{'name': topic.name, 'strategic_importance': topic.strategic_importance} for topic in person.topics],
                    'assigned_tasks': len(person.tasks_assigned),
                    'mentioned_in_tasks': len(person.tasks_mentioned),
                    'topic_connections': len(person.topics),
                    'engagement_score': calculate_person_engagement_score(person)
                }
                people_data.append(person_data)
            
            # Sort by importance and recent activity
            people_data.sort(key=lambda x: (x['importance_level'] or 0, x['total_interactions']), reverse=True)
            
            return jsonify({
                'success': True,
                'people': people_data,
                'summary': {
                    'total_people': len(people_data),
                    'high_importance': len([p for p in people_data if (p['importance_level'] or 0) > 0.7]),
                    'recent_contacts': len([p for p in people_data if p['last_contact'] and 
                                          datetime.fromisoformat(p['last_contact']) > datetime.utcnow() - timedelta(days=30)]),
                    'highly_connected': len([p for p in people_data if p['topic_connections'] > 2]),
                    'with_professional_story': len([p for p in people_data if p['professional_story']])
                }
            })
            
    except Exception as e:
        logger.error(f"Failed to get people with relationship intelligence: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/intelligence/insights', methods=['GET'])
def get_proactive_intelligence_insights():
    """Get proactive intelligence insights with filtering"""
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        # Get query parameters
        status_filter = request.args.get('status', 'new')
        insight_type = request.args.get('type', None)
        limit = int(request.args.get('limit', 20))
        
        with get_db_manager().get_session() as session:
            query = session.query(IntelligenceInsight).filter(
                IntelligenceInsight.user_id == user.id
            )
            
            if status_filter:
                query = query.filter(IntelligenceInsight.status == status_filter)
            
            if insight_type:
                query = query.filter(IntelligenceInsight.insight_type == insight_type)
            
            # Filter out expired insights
            query = query.filter(
                (IntelligenceInsight.expires_at.is_(None)) | 
                (IntelligenceInsight.expires_at > datetime.utcnow())
            )
            
            insights = query.order_by(
                IntelligenceInsight.priority.desc(),
                IntelligenceInsight.created_at.desc()
            ).limit(limit).all()
            
            insights_data = []
            for insight in insights:
                insight_data = {
                    'id': insight.id,
                    'insight_type': insight.insight_type,
                    'title': insight.title,
                    'description': insight.description,
                    'priority': insight.priority,
                    'confidence': insight.confidence,
                    'status': insight.status,
                    'user_feedback': insight.user_feedback,
                    'created_at': insight.created_at.isoformat(),
                    'expires_at': insight.expires_at.isoformat() if insight.expires_at else None,
                    'related_entity': {
                        'type': insight.related_entity_type,
                        'id': insight.related_entity_id
                    } if insight.related_entity_type else None
                }
                insights_data.append(insight_data)
            
            return jsonify({
                'success': True,
                'insights': insights_data,
                'summary': {
                    'total_insights': len(insights_data),
                    'by_type': count_insights_by_type(insights_data),
                    'by_priority': count_insights_by_priority(insights_data),
                    'actionable': len([i for i in insights_data if i['status'] == 'new']),
                    'high_confidence': len([i for i in insights_data if i['confidence'] > 0.8])
                }
            })
            
    except Exception as e:
        logger.error(f"Failed to get intelligence insights: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/intelligence/generate-insights', methods=['POST'])
def generate_proactive_insights():
    """Generate proactive insights manually (for testing)"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    user = get_db_manager().get_user_by_email(user_email)
    if not user:
        return jsonify({'success': False, 'error': 'User not found'}), 404
    
    try:
        # Generate insights using entity engine
        insights = entity_engine.generate_proactive_insights(user.id)
        
        return jsonify({
            'success': True,
            'insights_generated': len(insights),
            'insights': [
                {
                    'type': insight.insight_type,
                    'title': insight.title,
                    'description': insight.description,
                    'priority': insight.priority,
                    'confidence': insight.confidence,
                    'created_at': insight.created_at.isoformat()
                }
                for insight in insights
            ]
        })
        
    except Exception as e:
        logger.error(f"Failed to generate proactive insights: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/real-time/status', methods=['GET'])
def get_realtime_processing_status():
    """Get real-time processing status and statistics"""
    try:
        stats = realtime_processor.get_stats()
        queue_status = realtime_processor.get_queue_status()
        
        return jsonify({
            'success': True,
            'real_time_processing': {
                'is_running': stats['is_running'],
                'queue_size': stats['queue_size'],
                'workers_active': stats['workers_active'],
                'events_processed': stats['events_processed'],
                'events_failed': stats['events_failed'],
                'avg_processing_time': stats['avg_processing_time'],
                'last_processed': stats['last_processed'].isoformat() if stats['last_processed'] else None
            },
            'performance_metrics': {
                'processing_rate': stats['events_processed'] / max(1, (datetime.utcnow() - realtime_processor.stats.get('start_time', datetime.utcnow())).total_seconds() / 60),  # events per minute
                'error_rate': stats['events_failed'] / max(1, stats['events_processed'] + stats['events_failed']),
                'queue_utilization': stats['queue_size'] / 1000  # Assume max queue size of 1000
            }
        })
        
    except Exception as e:
        logger.error(f"Failed to get real-time status: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

# =====================================================================
# ENHANCED METRICS AND FEEDBACK ENDPOINTS (MISSING FROM DASHBOARD)
# =====================================================================

@app.route('/api/entities/metrics', methods=['GET'])
def get_entity_metrics():
    """Get comprehensive entity metrics for dashboard"""
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        with get_db_manager().get_session() as session:
            # Entity counts
            topics_count = session.query(Topic).filter(Topic.user_id == user.id).count()
            people_count = session.query(Person).filter(Person.user_id == user.id).count()
            tasks_count = session.query(Task).filter(Task.user_id == user.id).count()
            insights_count = session.query(IntelligenceInsight).filter(
                IntelligenceInsight.user_id == user.id,
                IntelligenceInsight.status == 'new'
            ).count()
            
            # Active relationships
            relationships_count = session.query(EntityRelationship).filter(
                EntityRelationship.user_id == user.id
            ).count()
            
            # Calculate intelligence quality score
            high_conf_topics = session.query(Topic).filter(
                Topic.user_id == user.id,
                Topic.confidence_score > 0.8
            ).count()
            
            topics_with_summary = session.query(Topic).filter(
                Topic.user_id == user.id,
                Topic.intelligence_summary.isnot(None)
            ).count()
            
            intelligence_quality = 0.0
            if topics_count > 0:
                intelligence_quality = (high_conf_topics + topics_with_summary) / (topics_count * 2)
            
            # Topic momentum (topics active in last 7 days)
            week_ago = datetime.utcnow() - timedelta(days=7)
            active_topics = session.query(Topic).filter(
                Topic.user_id == user.id,
                Topic.last_mentioned > week_ago
            ).count()
            
            topic_momentum = 0.0
            if topics_count > 0:
                topic_momentum = active_topics / topics_count
            
            # Relationship density
            relationship_density = 0.0
            total_entities = topics_count + people_count
            if total_entities > 0:
                relationship_density = relationships_count / total_entities
            
            metrics = {
                'total_entities': topics_count + people_count + tasks_count,
                'topics': topics_count,
                'people': people_count,
                'tasks': tasks_count,
                'active_insights': insights_count,
                'entity_relationships': relationships_count,
                'intelligence_quality': intelligence_quality,
                'topic_momentum': topic_momentum,
                'relationship_density': relationship_density
            }
            
            return jsonify({'success': True, 'metrics': metrics})
            
    except Exception as e:
        logger.error(f"Failed to get entity metrics: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/intelligence/feedback', methods=['POST'])
def record_insight_feedback():
    """Record user feedback on intelligence insights"""
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        data = request.get_json()
        insight_id = data.get('insight_id')
        feedback = data.get('feedback')
        
        if not insight_id or not feedback:
            return jsonify({'success': False, 'error': 'Missing insight_id or feedback'}), 400
        
        with get_db_manager().get_session() as session:
            insight = session.query(IntelligenceInsight).filter(
                IntelligenceInsight.id == insight_id,
                IntelligenceInsight.user_id == user.id
            ).first()
            
            if not insight:
                return jsonify({'success': False, 'error': 'Insight not found'}), 404
            
            insight.user_feedback = feedback
            insight.updated_at = datetime.utcnow()
            
            # Mark as reviewed if feedback provided
            if insight.status == 'new':
                insight.status = 'reviewed'
            
            session.commit()
            
            return jsonify({'success': True, 'message': 'Feedback recorded'})
            
    except Exception as e:
        logger.error(f"Failed to record insight feedback: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/tasks', methods=['GET'])
def get_enhanced_tasks():
    """Enhanced task endpoint with context information"""
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        limit = request.args.get('limit', 20, type=int)
        with_context = request.args.get('with_context', 'false').lower() == 'true'
        status = request.args.get('status')
        
        with get_db_manager().get_session() as session:
            query = session.query(Task).filter(Task.user_id == user.id)
            
            if with_context:
                query = query.filter(Task.context_story.isnot(None))
            
            if status:
                query = query.filter(Task.status == status)
            
            tasks = query.order_by(Task.created_at.desc()).limit(limit).all()
            
            tasks_data = []
            for task in tasks:
                task_data = {
                    'id': task.id,
                    'description': task.description,
                    'status': task.status,
                    'priority': task.priority,
                    'confidence': task.confidence,
                    'context_story': task.context_story,
                    'due_date': task.due_date.isoformat() if task.due_date else None,
                    'created_at': task.created_at.isoformat(),
                    'assignee': {
                        'name': task.assignee.name if task.assignee else None,
                        'email': task.assignee.email_address if task.assignee else None
                    } if task.assignee else None
                }
                tasks_data.append(task_data)
            
            return jsonify({'success': True, 'tasks': tasks_data})
            
    except Exception as e:
        logger.error(f"Failed to get enhanced tasks: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

# =====================================================================
# UNIFIED INTELLIGENCE SYNC ENDPOINT (MISSING FROM REFACTOR)
# =====================================================================

@app.route('/api/unified-intelligence-sync', methods=['POST'])
def unified_intelligence_sync():
    """
    Enhanced unified processing that integrates email, calendar, and generates
    real-time intelligence with entity-centric architecture.
    """
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        # Get processing parameters
        data = request.get_json() or {}
        max_emails = data.get('max_emails', 20)
        days_back = data.get('days_back', 7)
        days_forward = data.get('days_forward', 14)
        force_refresh = data.get('force_refresh', False)
        
        processing_summary = {
            'success': True,
            'processing_stages': {},
            'entity_intelligence': {},
            'insights_generated': [],
            'real_time_processing': True,
            'next_steps': []
        }
        
        # Stage 1: Fetch and process emails in real-time
        logger.info(f"Starting unified intelligence sync for {user_email}")
        
        # Fetch emails
        email_result = gmail_fetcher.fetch_recent_emails(
            user_email, max_emails=max_emails, days_back=days_back, force_refresh=force_refresh
        )
        
        processing_summary['processing_stages']['emails_fetched'] = email_result.get('emails_fetched', 0)
        
        if email_result.get('success') and email_result.get('emails'):
            # Process each email through real-time pipeline
            for email_data in email_result['emails']:
                realtime_processor.process_new_email(email_data, user.id, priority=3)
        
        # Stage 2: Fetch and enhance calendar events (if calendar fetcher available)
        try:
            from ingest.calendar_fetcher import calendar_fetcher
            calendar_result = calendar_fetcher.fetch_calendar_events(
                user_email, days_back=3, days_forward=days_forward, create_prep_tasks=True
            )
            
            processing_summary['processing_stages']['calendar_events_fetched'] = calendar_result.get('events_fetched', 0)
            
            if calendar_result.get('success') and calendar_result.get('events'):
                # Process each calendar event through real-time pipeline
                for event_data in calendar_result['events']:
                    realtime_processor.process_new_calendar_event(event_data, user.id, priority=4)
        except ImportError:
            logger.info("Calendar fetcher not available, skipping calendar processing")
            processing_summary['processing_stages']['calendar_events_fetched'] = 0
        
        # Stage 3: Generate comprehensive business intelligence
        intelligence_summary = generate_360_business_intelligence(user.id)
        processing_summary['entity_intelligence'] = intelligence_summary
        
        # Stage 4: Generate proactive insights
        proactive_insights = entity_engine.generate_proactive_insights(user.id)
        processing_summary['insights_generated'] = [
            {
                'type': insight.insight_type if hasattr(insight, 'insight_type') else 'general',
                'title': insight.title if hasattr(insight, 'title') else 'Insight',
                'description': insight.description if hasattr(insight, 'description') else 'No description',
                'priority': insight.priority if hasattr(insight, 'priority') else 'medium',
                'confidence': insight.confidence if hasattr(insight, 'confidence') else 0.5
            }
            for insight in proactive_insights
        ]
        
        # Generate next steps based on intelligence
        processing_summary['next_steps'] = generate_intelligent_next_steps(intelligence_summary, proactive_insights)
        
        logger.info(f"Completed unified intelligence sync for {user_email}: "
                   f"{processing_summary['processing_stages']['emails_fetched']} emails, "
                   f"{processing_summary['processing_stages']['calendar_events_fetched']} events, "
                   f"{len(proactive_insights)} insights")
        
        return jsonify(processing_summary)
        
    except Exception as e:
        logger.error(f"Failed unified intelligence sync: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'processing_stages': {},
            'real_time_processing': False
        }), 500

# =====================================================================
# BUSINESS INTELLIGENCE GENERATION (MISSING FROM REFACTOR)
# =====================================================================

def generate_360_business_intelligence(user_id: int) -> Dict:
    """Generate comprehensive 360-degree business intelligence"""
    try:
        intelligence = {
            'entity_summary': {},
            'relationship_intelligence': {},
            'strategic_insights': {},
            'activity_patterns': {},
            'intelligence_quality': {}
        }
        
        with get_db_manager().get_session() as session:
            # Entity summary
            topics_count = session.query(Topic).filter(Topic.user_id == user_id).count()
            people_count = session.query(Person).filter(Person.user_id == user_id).count()
            tasks_count = session.query(Task).filter(Task.user_id == user_id).count()
            
            from models.database import CalendarEvent
            events_count = session.query(CalendarEvent).filter(CalendarEvent.user_id == user_id).count()
            
            intelligence['entity_summary'] = {
                'topics': topics_count,
                'people': people_count,
                'tasks': tasks_count,
                'calendar_events': events_count,
                'total_entities': topics_count + people_count + tasks_count + events_count
            }
            
            # Relationship intelligence
            from models.database import EntityRelationship
            relationships_count = session.query(EntityRelationship).filter(
                EntityRelationship.user_id == user_id
            ).count()
            
            # Active topics (mentioned in last 30 days)
            active_topics = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.last_mentioned > datetime.utcnow() - timedelta(days=30)
            ).count()
            
            # Recent contacts
            recent_contacts = session.query(Person).filter(
                Person.user_id == user_id,
                Person.last_contact > datetime.utcnow() - timedelta(days=30)
            ).count()
            
            intelligence['relationship_intelligence'] = {
                'total_relationships': relationships_count,
                'active_topics': active_topics,
                'recent_contacts': recent_contacts,
                'relationship_density': relationships_count / max(1, people_count + topics_count)
            }
            
            # Activity patterns
            recent_tasks = session.query(Task).filter(
                Task.user_id == user_id,
                Task.created_at > datetime.utcnow() - timedelta(days=7)
            ).count()
            
            intelligence['activity_patterns'] = {
                'tasks_this_week': recent_tasks,
                'average_daily_tasks': recent_tasks / 7,
                'topic_momentum': active_topics / max(1, topics_count)
            }
            
            # Intelligence quality metrics
            high_confidence_tasks = session.query(Task).filter(
                Task.user_id == user_id,
                Task.confidence > 0.8
            ).count()
            
            tasks_with_context = session.query(Task).filter(
                Task.user_id == user_id,
                Task.context_story.isnot(None)
            ).count()
            
            intelligence['intelligence_quality'] = {
                'high_confidence_extractions': high_confidence_tasks / max(1, tasks_count),
                'contextualized_tasks': tasks_with_context / max(1, tasks_count),
                'entity_interconnection': relationships_count / max(1, intelligence['entity_summary']['total_entities'])
            }
        
        return intelligence
        
    except Exception as e:
        logger.error(f"Failed to generate 360 business intelligence: {str(e)}")
        return {}

def generate_intelligent_next_steps(intelligence_summary: Dict, insights: List) -> List[str]:
    """Generate intelligent next steps based on business intelligence"""
    next_steps = []
    
    try:
        entity_summary = intelligence_summary.get('entity_summary', {})
        relationship_intel = intelligence_summary.get('relationship_intelligence', {})
        activity_patterns = intelligence_summary.get('activity_patterns', {})
        
        # Suggest next steps based on data
        if entity_summary.get('total_entities', 0) < 10:
            next_steps.append("Process more email data to build comprehensive business intelligence")
        
        if relationship_intel.get('relationship_density', 0) < 0.3:
            next_steps.append("Focus on building relationship connections between contacts and topics")
        
        if activity_patterns.get('tasks_this_week', 0) > 10:
            next_steps.append("Consider prioritizing and organizing your task backlog")
        
        if len(insights) > 5:
            next_steps.append("Review and act on high-priority insights")
        elif len(insights) < 2:
            next_steps.append("Continue processing communications to generate more insights")
        
        # Always suggest at least one action
        if not next_steps:
            next_steps.append("Continue using the system to build your business intelligence")
        
    except Exception as e:
        logger.error(f"Failed to generate intelligent next steps: {str(e)}")
        next_steps = ["Continue building your business intelligence"]
    
    return next_steps

def calculate_relationship_strength(person: Person) -> float:
    """Calculate relationship strength for a person"""
    score = 0.0
    
    # Interaction frequency
    if person.total_interactions > 10:
        score += 0.3
    elif person.total_interactions > 5:
        score += 0.2
    elif person.total_interactions > 0:
        score += 0.1
    
    # Recent contact
    if person.last_contact and person.last_contact > datetime.utcnow() - timedelta(days=7):
        score += 0.3
    elif person.last_contact and person.last_contact > datetime.utcnow() - timedelta(days=30):
        score += 0.2
    
    # Importance level
    if person.importance_level:
        score += person.importance_level * 0.4
    
    return min(1.0, score)

def calculate_communication_frequency(person: Person) -> str:
    """Calculate communication frequency description"""
    if not person.last_contact:
        return "No recent contact"
    
    days_since = (datetime.utcnow() - person.last_contact).days
    
    if days_since <= 7:
        return "Weekly"
    elif days_since <= 30:
        return "Monthly"
    elif days_since <= 90:
        return "Quarterly"
    else:
        return "Infrequent"

def calculate_engagement_score(person: Person) -> float:
    """Calculate engagement score for a person"""
    score = 0.0
    
    # Interaction frequency
    if person.total_interactions > 10:
        score += 0.3
    elif person.total_interactions > 5:
        score += 0.2
    elif person.total_interactions > 0:
        score += 0.1
    
    # Recent contact
    if person.last_contact and person.last_contact > datetime.utcnow() - timedelta(days=7):
        score += 0.3
    elif person.last_contact and person.last_contact > datetime.utcnow() - timedelta(days=30):
        score += 0.2
    
    # Professional context
    if person.professional_story:
        score += 0.2
    
    # Topic connections
    topic_count = len(person.topics) if person.topics else 0
    if topic_count > 3:
        score += 0.2
    elif topic_count > 0:
        score += 0.1
    
    return min(1.0, score)

def get_person_topic_affinity(person_id: int, topic_id: int) -> float:
    """Get affinity score between person and topic"""
    try:
        from models.database import get_db_manager
        from models.database import person_topic_association
        
        with get_db_manager().get_session() as session:
            # Query the association table for affinity score
            result = session.execute(
                person_topic_association.select().where(
                    (person_topic_association.c.person_id == person_id) &
                    (person_topic_association.c.topic_id == topic_id)
                )
            ).first()
            
            return result.affinity_score if result else 0.5
            
    except Exception as e:
        logger.error(f"Failed to get person-topic affinity: {str(e)}")
        return 0.5

def calculate_task_strategic_importance(task: Task) -> float:
    """Calculate strategic importance of a task"""
    importance = 0.0
    
    # High priority tasks are more strategic
    if task.priority == 'high':
        importance += 0.4
    elif task.priority == 'medium':
        importance += 0.2
    
    # Tasks with context are more strategic
    if task.context_story:
        importance += 0.3
    
    # Tasks with high confidence are more strategic
    if task.confidence > 0.8:
        importance += 0.2
    
    # Tasks connected to multiple topics are more strategic
    topic_count = len(task.topics) if task.topics else 0
    if topic_count > 2:
        importance += 0.1
    
    return min(1.0, importance)

def count_by_field(data_list: List[Dict], field: str) -> Dict:
    """Count items by a specific field"""
    counts = {}
    for item in data_list:
        value = item.get(field, 'unknown')
        counts[value] = counts.get(value, 0) + 1
    return counts

# =====================================================================
# HELPER FUNCTIONS FOR ENHANCED PROCESSING
# =====================================================================

def add_version_headers(response, version: str):
    """Add version headers to API responses"""
    response.headers['X-API-Version'] = version
    response.headers['X-Enhanced-Features'] = 'true'
    return response

def generate_entity_intelligence_summary(user_id: int) -> Dict:
    """Generate comprehensive entity intelligence summary"""
    try:
        with get_db_manager().get_session() as session:
            # Count entities
            topics_count = session.query(Topic).filter(Topic.user_id == user_id).count()
            people_count = session.query(Person).filter(Person.user_id == user_id).count()
            tasks_count = session.query(Task).filter(Task.user_id == user_id).count()
            insights_count = session.query(IntelligenceInsight).filter(IntelligenceInsight.user_id == user_id).count()
            
            # Active entities (recently updated)
            week_ago = datetime.utcnow() - timedelta(days=7)
            active_topics = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.last_mentioned > week_ago
            ).count()
            
            recent_contacts = session.query(Person).filter(
                Person.user_id == user_id,
                Person.last_contact > week_ago
            ).count()
            
            return {
                'entity_counts': {
                    'topics': topics_count,
                    'people': people_count,
                    'tasks': tasks_count,
                    'insights': insights_count,
                    'total_entities': topics_count + people_count + tasks_count
                },
                'activity_metrics': {
                    'active_topics': active_topics,
                    'recent_contacts': recent_contacts,
                    'activity_rate': (active_topics + recent_contacts) / max(1, topics_count + people_count)
                },
                'intelligence_density': {
                    'topics_per_person': topics_count / max(1, people_count),
                    'tasks_per_topic': tasks_count / max(1, topics_count),
                    'insights_per_entity': insights_count / max(1, topics_count + people_count)
                }
            }
            
    except Exception as e:
        logger.error(f"Failed to generate entity intelligence summary: {str(e)}")
        return {}

def analyze_entity_relationships(user_id: int) -> Dict:
    """Analyze entity relationships and connection patterns"""
    try:
        with get_db_manager().get_session() as session:
            relationships = session.query(EntityRelationship).filter(
                EntityRelationship.user_id == user_id
            ).all()
            
            relationship_types = {}
            strong_relationships = 0
            
            for rel in relationships:
                rel_type = rel.relationship_type
                relationship_types[rel_type] = relationship_types.get(rel_type, 0) + 1
                
                if rel.strength > 0.7:
                    strong_relationships += 1
            
            return {
                'total_relationships': len(relationships),
                'relationship_types': relationship_types,
                'strong_relationships': strong_relationships,
                'relationship_density': len(relationships) / max(1, session.query(Topic).filter(Topic.user_id == user_id).count() + session.query(Person).filter(Person.user_id == user_id).count()),
                'avg_relationship_strength': sum(rel.strength for rel in relationships) / max(1, len(relationships))
            }
            
    except Exception as e:
        logger.error(f"Failed to analyze entity relationships: {str(e)}")
        return {}

def calculate_intelligence_quality_metrics(user_id: int) -> Dict:
    """Calculate intelligence quality metrics"""
    try:
        with get_db_manager().get_session() as session:
            # High confidence entities
            high_conf_topics = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.confidence_score > 0.8
            ).count()
            
            high_conf_tasks = session.query(Task).filter(
                Task.user_id == user_id,
                Task.confidence > 0.8
            ).count()
            
            # Entities with context
            topics_with_summary = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.intelligence_summary.isnot(None)
            ).count()
            
            tasks_with_context = session.query(Task).filter(
                Task.user_id == user_id,
                Task.context_story.isnot(None)
            ).count()
            
            total_topics = session.query(Topic).filter(Topic.user_id == user_id).count()
            total_tasks = session.query(Task).filter(Task.user_id == user_id).count()
            
            return {
                'confidence_metrics': {
                    'high_confidence_topics': high_conf_topics / max(1, total_topics),
                    'high_confidence_tasks': high_conf_tasks / max(1, total_tasks)
                },
                'context_richness': {
                    'topics_with_intelligence': topics_with_summary / max(1, total_topics),
                    'tasks_with_context': tasks_with_context / max(1, total_tasks)
                },
                'overall_quality_score': (
                    (high_conf_topics / max(1, total_topics)) +
                    (high_conf_tasks / max(1, total_tasks)) +
                    (topics_with_summary / max(1, total_topics)) +
                    (tasks_with_context / max(1, total_tasks))
                ) / 4
            }
            
    except Exception as e:
        logger.error(f"Failed to calculate intelligence quality metrics: {str(e)}")
        return {}

def calculate_person_engagement_score(person: Person) -> float:
    """Calculate engagement score for a person"""
    score = 0.0
    
    # Interaction frequency
    if person.total_interactions > 10:
        score += 0.3
    elif person.total_interactions > 5:
        score += 0.2
    elif person.total_interactions > 0:
        score += 0.1
    
    # Recent contact
    if person.last_contact and person.last_contact > datetime.utcnow() - timedelta(days=7):
        score += 0.3
    elif person.last_contact and person.last_contact > datetime.utcnow() - timedelta(days=30):
        score += 0.2
    
    # Professional context
    if person.professional_story:
        score += 0.2
    
    # Topic connections
    topic_count = len(person.topics) if person.topics else 0
    if topic_count > 3:
        score += 0.2
    elif topic_count > 0:
        score += 0.1
    
    return min(1.0, score)

def count_insights_by_type(insights_data: List[Dict]) -> Dict:
    """Count insights by type"""
    counts = {}
    for insight in insights_data:
        insight_type = insight['insight_type']
        counts[insight_type] = counts.get(insight_type, 0) + 1
    return counts

def count_insights_by_priority(insights_data: List[Dict]) -> Dict:
    """Count insights by priority"""
    counts = {}
    for insight in insights_data:
        priority = insight['priority']
        counts[priority] = counts.get(priority, 0) + 1
    return counts

# =====================================================================
# ERROR HANDLERS
# =====================================================================

@app.errorhandler(404)
def not_found_error(error):
    return render_template('404.html', api_version=CURRENT_VERSION), 404

@app.errorhandler(500)
def internal_error(error):
    logger.error(f"Internal server error: {str(error)}")
    return render_template('500.html', api_version=CURRENT_VERSION), 500

# =====================================================================
# APPLICATION STARTUP
# =====================================================================

if __name__ == '__main__':
    # Validate configuration
    config_errors = settings.validate_config()
    if config_errors:
        logger.error("Configuration errors:")
        for error in config_errors:
            logger.error(f"  - {error}")
        exit(1)
    
    logger.info("Starting AI Chief of Staff Enhanced Application v2.0...")
    logger.info(f"Database URL: {settings.DATABASE_URL}")
    logger.info(f"Environment: {'Production' if settings.is_production() else 'Development'}")
    logger.info(f"API Version: {CURRENT_VERSION}")
    
    # Initialize database with enhanced models
    try:
        get_db_manager().initialize_database()
        logger.info("Enhanced database initialized successfully")
    except Exception as e:
        logger.error(f"Database initialization failed: {str(e)}")
        exit(1)
    
    # Initialize processor manager
    try:
        # Test processor manager connection
        stats_result = processor_manager.get_processing_statistics()
        if stats_result['success']:
            logger.info("Processor manager initialized successfully")
        else:
            logger.warning(f"Processor manager warning: {stats_result['error']}")
    except Exception as e:
        logger.error(f"Processor manager initialization failed: {str(e)}")
        # Don't exit - application can still run with reduced functionality
    
    # Start real-time processing engine
    try:
        from processors.realtime_processing import realtime_processor
        realtime_processor.start(num_workers=3)
        logger.info("Real-time processing engine started successfully")
    except Exception as e:
        logger.error(f"Real-time processor startup failed: {str(e)}")
        # Don't exit - application can still run with batch processing
    
    logger.info("Enhanced API endpoints registered:")
    logger.info("  - Legacy compatibility maintained")
    logger.info("  - New v2 APIs available")
    logger.info("  - Real-time processing enabled")
    logger.info("  - Entity management active")
    logger.info("  - Analytics engine ready")
    
    # Start the application
    app.run(
        host='0.0.0.0',
        port=settings.PORT,
        debug=settings.DEBUG
    ) 


================================================================================
FILE: archive/backup_files/v1_original/main.py
PURPOSE: Core Flask app with Claude 4 Opus integration, Google OAuth, and tier system endpoints
================================================================================
# Main Flask application for AI Chief of Staff

import os
import logging
from datetime import datetime
from flask import Flask, render_template, request, jsonify, session, redirect, url_for, flash
import anthropic

from config.settings import settings
from auth.gmail_auth import gmail_auth
from ingest.gmail_fetcher import gmail_fetcher
from processors.email_normalizer import email_normalizer
from processors.task_extractor import task_extractor
from models.database import get_db_manager, Email, Task

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Create Flask application
app = Flask(__name__)
app.config['SECRET_KEY'] = settings.SECRET_KEY
app.config['SESSION_TYPE'] = 'filesystem'

# Initialize Claude client for chat
claude_client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)

@app.route('/')
def index():
    """Main dashboard route"""
    user_email = session.get('user_email')
    
    if not user_email:
        return render_template('login.html')
    
    try:
        # Get user information
        user_info = gmail_auth.get_user_by_email(user_email)
        if not user_info:
            session.clear()
            return render_template('login.html')
        
        # Get user statistics
        user = get_db_manager().get_user_by_email(user_email)
        user_stats = {
            'total_emails': 0,
            'total_tasks': 0,
            'pending_tasks': 0,
            'completed_tasks': 0
        }
        
        if user:
            with get_db_manager().get_session() as db_session:
                user_stats['total_emails'] = db_session.query(Email).filter(
                    Email.user_id == user.id
                ).count()
                
                user_stats['total_tasks'] = db_session.query(Task).filter(
                    Task.user_id == user.id
                ).count()
                
                user_stats['pending_tasks'] = db_session.query(Task).filter(
                    Task.user_id == user.id,
                    Task.status == 'pending'
                ).count()
                
                user_stats['completed_tasks'] = db_session.query(Task).filter(
                    Task.user_id == user.id,
                    Task.status == 'completed'
                ).count()
        
        return render_template('dashboard.html', 
                             user_info=user_info, 
                             user_stats=user_stats)
    
    except Exception as e:
        logger.error(f"Dashboard error for {user_email}: {str(e)}")
        flash('An error occurred loading your dashboard. Please try again.', 'error')
        return render_template('dashboard.html', 
                             user_info={'email': user_email}, 
                             user_stats={'total_emails': 0, 'total_tasks': 0})

@app.route('/login')
def login():
    """Login page"""
    return render_template('login.html')

@app.route('/auth/google')
def auth_google():
    """Initiate Google OAuth flow"""
    try:
        auth_url, state = gmail_auth.get_authorization_url('user_session')
        session['oauth_state'] = state
        return redirect(auth_url)
    except Exception as e:
        logger.error(f"Google auth initiation error: {str(e)}")
        flash('Failed to initiate Google authentication. Please try again.', 'error')
        return redirect(url_for('login'))

@app.route('/auth/callback')
def auth_callback():
    """Handle OAuth callback"""
    try:
        authorization_code = request.args.get('code')
        state = request.args.get('state')
        
        if not authorization_code:
            flash('Authorization failed. Please try again.', 'error')
            return redirect(url_for('login'))
        
        # Handle OAuth callback
        result = gmail_auth.handle_oauth_callback(authorization_code, state)
        
        if result['success']:
            session['user_email'] = result['user_email']
            session['authenticated'] = True
            flash(f'Successfully authenticated as {result["user_email"]}!', 'success')
            return redirect(url_for('index'))
        else:
            flash(f'Authentication failed: {result["error"]}', 'error')
            return redirect(url_for('login'))
    
    except Exception as e:
        logger.error(f"OAuth callback error: {str(e)}")
        flash('Authentication error occurred. Please try again.', 'error')
        return redirect(url_for('login'))

@app.route('/logout')
def logout():
    """Logout user"""
    user_email = session.get('user_email')
    session.clear()
    
    if user_email:
        flash(f'Successfully logged out from {user_email}', 'success')
    
    return redirect(url_for('login'))

@app.route('/api/process-emails', methods=['POST'])
def api_process_emails():
    """API endpoint to fetch and process emails"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        data = request.get_json() or {}
        days_back = data.get('days_back', 7)
        limit = data.get('limit', 50)
        force_refresh = data.get('force_refresh', False)
        
        # Step 1: Fetch emails
        logger.info(f"Fetching emails for {user_email}")
        fetch_result = gmail_fetcher.fetch_recent_emails(
            user_email, 
            days_back=days_back, 
            limit=limit,
            force_refresh=force_refresh
        )
        
        if not fetch_result['success']:
            return jsonify({
                'success': False, 
                'error': f"Failed to fetch emails: {fetch_result.get('error')}"
            }), 400
        
        # Step 2: Normalize emails
        logger.info(f"Normalizing emails for {user_email}")
        normalize_result = email_normalizer.normalize_user_emails(user_email, limit)
        
        # Step 3: Extract tasks
        logger.info(f"Extracting tasks for {user_email}")
        task_result = task_extractor.extract_tasks_for_user(user_email, limit)
        
        return jsonify({
            'success': True,
            'fetch_result': fetch_result,
            'normalize_result': normalize_result,
            'task_result': task_result,
            'processed_at': datetime.utcnow().isoformat()
        })
    
    except Exception as e:
        logger.error(f"Email processing error for {user_email}: {str(e)}")
        return jsonify({
            'success': False, 
            'error': f"Processing failed: {str(e)}"
        }), 500

@app.route('/api/emails')
def api_get_emails():
    """API endpoint to get user emails"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        limit = request.args.get('limit', 50, type=int)
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        emails = get_db_manager().get_user_emails(user.id, limit)
        
        return jsonify({
            'success': True,
            'emails': [email.to_dict() for email in emails],
            'count': len(emails)
        })
    
    except Exception as e:
        logger.error(f"Get emails error for {user_email}: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/tasks')
def api_get_tasks():
    """API endpoint to get user tasks"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        status = request.args.get('status')
        limit = request.args.get('limit', 100, type=int)
        
        result = task_extractor.get_user_tasks(user_email, status, limit)
        return jsonify(result)
    
    except Exception as e:
        logger.error(f"Get tasks error for {user_email}: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/tasks/<int:task_id>/status', methods=['PUT'])
def api_update_task_status(task_id):
    """API endpoint to update task status"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        data = request.get_json()
        new_status = data.get('status')
        
        if new_status not in ['pending', 'in_progress', 'completed', 'cancelled']:
            return jsonify({'success': False, 'error': 'Invalid status'}), 400
        
        result = task_extractor.update_task_status(user_email, task_id, new_status)
        return jsonify(result)
    
    except Exception as e:
        logger.error(f"Update task status error for {user_email}: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/chat', methods=['POST'])
def api_chat():
    """API endpoint for Claude chat"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        data = request.get_json()
        message = data.get('message', '').strip()
        
        if not message:
            return jsonify({'success': False, 'error': 'Message is required'}), 400
        
        # Get user context for better responses
        user = get_db_manager().get_user_by_email(user_email)
        context_info = ""
        
        if user:
            with get_db_manager().get_session() as db_session:
                recent_tasks = db_session.query(Task).filter(
                    Task.user_id == user.id,
                    Task.status == 'pending'
                ).order_by(Task.created_at.desc()).limit(5).all()
                
                if recent_tasks:
                    task_list = "\n".join([f"- {task.description}" for task in recent_tasks])
                    context_info = f"\n\nYour recent pending tasks:\n{task_list}"
        
        # Build system prompt with context
        system_prompt = f"""You are an AI Chief of Staff assistant helping {user_email}. 
You have access to their email-derived tasks and can help with work organization, prioritization, and productivity.

Be helpful, professional, and concise. Focus on actionable advice related to their work and tasks.{context_info}"""
        
        # Call Claude
        response = claude_client.messages.create(
            model=settings.CLAUDE_MODEL,
            max_tokens=1000,
            temperature=0.3,
            system=system_prompt,
            messages=[{
                "role": "user",
                "content": message
            }]
        )
        
        reply = response.content[0].text
        
        return jsonify({
            'success': True,
            'message': message,
            'reply': reply,
            'timestamp': datetime.utcnow().isoformat()
        })
    
    except Exception as e:
        logger.error(f"Chat error for {user_email}: {str(e)}")
        return jsonify({
            'success': False, 
            'error': 'Failed to process chat message'
        }), 500

@app.route('/api/status')
def api_status():
    """API endpoint to get system status"""
    user_email = session.get('user_email')
    
    status = {
        'authenticated': bool(user_email),
        'user_email': user_email,
        'timestamp': datetime.utcnow().isoformat(),
        'database_connected': True,
        'gmail_auth_available': bool(settings.GOOGLE_CLIENT_ID and settings.GOOGLE_CLIENT_SECRET),
        'claude_available': bool(settings.ANTHROPIC_API_KEY)
    }
    
    # Test database connection
    try:
        get_db_manager().get_session().close()
    except Exception as e:
        status['database_connected'] = False
        status['database_error'] = str(e)
    
    # Test Gmail auth if user is authenticated
    if user_email:
        try:
            auth_status = gmail_auth.get_authentication_status(user_email)
            status['gmail_auth_status'] = auth_status
        except Exception as e:
            status['gmail_auth_error'] = str(e)
    
    return jsonify(status)

@app.errorhandler(404)
def not_found_error(error):
    return render_template('404.html'), 404

@app.errorhandler(500)
def internal_error(error):
    logger.error(f"Internal server error: {str(error)}")
    return render_template('500.html'), 500

if __name__ == '__main__':
    # Validate configuration
    config_errors = settings.validate_config()
    if config_errors:
        logger.error("Configuration errors:")
        for error in config_errors:
            logger.error(f"  - {error}")
        exit(1)
    
    logger.info("Starting AI Chief of Staff web application...")
    logger.info(f"Database URL: {settings.DATABASE_URL}")
    logger.info(f"Environment: {'Production' if settings.is_production() else 'Development'}")
    
    # Initialize database
    try:
        get_db_manager().initialize_database()
        logger.info("Database initialized successfully")
    except Exception as e:
        logger.error(f"Database initialization failed: {str(e)}")
        exit(1)
    
    app.run(
        host='0.0.0.0',
        port=settings.PORT,
        debug=settings.DEBUG
    ) 


================================================================================
FILE: chief_of_staff_ai/config/settings.py
PURPOSE: Claude 4 Opus configuration with agent capabilities and MCP connectors
================================================================================
import os
from typing import Dict, List
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

class Settings:
    """Application settings and configuration"""
    
    # Flask Configuration
    SECRET_KEY = os.getenv('SECRET_KEY', 'dev-secret-key-change-in-production')
    FLASK_ENV = os.getenv('FLASK_ENV', 'development')
    DEBUG = os.getenv('FLASK_DEBUG', 'True').lower() == 'true'
    PORT = int(os.getenv('PORT', 8080))
    
    # Database Configuration
    DATABASE_URL = os.getenv('DATABASE_URL')
    if not DATABASE_URL:
        # Default to SQLite for local development
        DATABASE_URL = 'sqlite:///chief_of_staff.db'
    else:
        # Handle Heroku PostgreSQL URL format
        if DATABASE_URL.startswith('postgres://'):
            DATABASE_URL = DATABASE_URL.replace('postgres://', 'postgresql://', 1)
    
    # Google OAuth Configuration
    GOOGLE_CLIENT_ID = os.getenv('GOOGLE_CLIENT_ID')
    GOOGLE_CLIENT_SECRET = os.getenv('GOOGLE_CLIENT_SECRET')
    GOOGLE_REDIRECT_URI = os.getenv('GOOGLE_REDIRECT_URI', 'http://127.0.0.1:8080/auth/callback')
    
    # Gmail API Configuration
    GMAIL_SCOPES = [
        'openid',
        'https://www.googleapis.com/auth/gmail.readonly',
        'https://www.googleapis.com/auth/gmail.send',  # Added for draft sending
        'https://www.googleapis.com/auth/calendar.readonly',
        'https://www.googleapis.com/auth/userinfo.email',
        'https://www.googleapis.com/auth/userinfo.profile'
    ]
    
    # Claude 4 Opus with Agent Capabilities Configuration
    ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')
    CLAUDE_MODEL = os.getenv('CLAUDE_MODEL', "claude-opus-4-20250514")  # Claude 4 Opus
    
    # Agent Capability Settings
    ENABLE_CODE_EXECUTION = os.getenv('ENABLE_CODE_EXECUTION', 'true').lower() == 'true'
    ENABLE_FILES_API = os.getenv('ENABLE_FILES_API', 'true').lower() == 'true'
    ENABLE_MCP_CONNECTOR = os.getenv('ENABLE_MCP_CONNECTOR', 'true').lower() == 'true'
    EXTENDED_CACHE_TTL = int(os.getenv('EXTENDED_CACHE_TTL', '3600'))  # 1 hour caching
    
    # Agent Behavior Configuration
    AUTONOMOUS_CONFIDENCE_THRESHOLD = float(os.getenv('AUTONOMOUS_CONFIDENCE_THRESHOLD', '0.85'))
    SUPERVISED_CONFIDENCE_THRESHOLD = float(os.getenv('SUPERVISED_CONFIDENCE_THRESHOLD', '0.70'))
    CODE_EXECUTION_TIMEOUT = int(os.getenv('CODE_EXECUTION_TIMEOUT', '300'))  # 5 minutes max per execution
    
    # MCP Server Configuration
    MCP_SERVERS = {
        'zapier': {
            'url': os.getenv('ZAPIER_MCP_URL', 'https://api.zapier.com/v1/mcp'),
            'token': os.getenv('ZAPIER_MCP_TOKEN')
        },
        'gmail': {
            'url': os.getenv('GMAIL_MCP_URL', 'https://gmail-mcp.zapier.com/v1'),
            'token': os.getenv('GMAIL_MCP_TOKEN')
        },
        'linkedin': {
            'url': os.getenv('LINKEDIN_MCP_URL', 'https://linkedin-mcp.example.com/v1'),
            'token': os.getenv('LINKEDIN_MCP_TOKEN')
        },
        'business_intel': {
            'url': os.getenv('BUSINESS_INTEL_MCP_URL', 'https://business-intel-mcp.example.com/v1'),
            'token': os.getenv('BUSINESS_INTEL_TOKEN')
        },
        'crm': {
            'url': os.getenv('CRM_MCP_URL', 'https://crm-mcp.zapier.com/v1'),
            'token': os.getenv('CRM_MCP_TOKEN')
        },
        'news_monitoring': {
            'url': os.getenv('NEWS_MCP_URL', 'https://news-mcp.example.com/v1'),
            'token': os.getenv('NEWS_MCP_TOKEN')
        },
        'market_research': {
            'url': os.getenv('MARKET_RESEARCH_MCP_URL', 'https://market-research-mcp.example.com/v1'),
            'token': os.getenv('MARKET_RESEARCH_TOKEN')
        }
    }
    
    # Autonomous Agent Settings
    ENABLE_AUTONOMOUS_EMAIL_RESPONSES = os.getenv('ENABLE_AUTONOMOUS_EMAIL_RESPONSES', 'false').lower() == 'true'  # Disabled by default
    ENABLE_AUTONOMOUS_PARTNERSHIP_WORKFLOWS = os.getenv('ENABLE_AUTONOMOUS_PARTNERSHIP_WORKFLOWS', 'true').lower() == 'true'
    ENABLE_AUTONOMOUS_INVESTOR_NURTURING = os.getenv('ENABLE_AUTONOMOUS_INVESTOR_NURTURING', 'true').lower() == 'true'
    
    # Email Draft Mode - NEW SETTING
    ENABLE_EMAIL_DRAFT_MODE = os.getenv('ENABLE_EMAIL_DRAFT_MODE', 'true').lower() == 'true'  # Always draft first
    AUTO_SEND_THRESHOLD = float(os.getenv('AUTO_SEND_THRESHOLD', '0.99'))  # Impossibly high threshold
    
    # Agent Workflow Rate Limits
    MAX_AUTONOMOUS_ACTIONS_PER_HOUR = int(os.getenv('MAX_AUTONOMOUS_ACTIONS_PER_HOUR', '10'))
    MAX_AUTONOMOUS_EMAILS_PER_DAY = int(os.getenv('MAX_AUTONOMOUS_EMAILS_PER_DAY', '20'))
    
    # Email Processing Configuration
    EMAIL_FETCH_LIMIT = int(os.getenv('EMAIL_FETCH_LIMIT', 50))
    EMAIL_DAYS_BACK = int(os.getenv('EMAIL_DAYS_BACK', 30))
    EMAIL_BATCH_SIZE = int(os.getenv('EMAIL_BATCH_SIZE', 10))
    
    # Multi-tenant Configuration
    MAX_USERS_PER_INSTANCE = int(os.getenv('MAX_USERS_PER_INSTANCE', 1000))
    USER_DATA_RETENTION_DAYS = int(os.getenv('USER_DATA_RETENTION_DAYS', 365))
    
    # Application Settings
    HOST: str = os.getenv('HOST', '0.0.0.0')
    
    # Google OAuth & APIs
    OPENAI_API_KEY: str = os.getenv('OPENAI_API_KEY', '')
    OPENAI_REDIRECT_URI: str = os.getenv('OPENAI_REDIRECT_URI', 'http://localhost:8080/auth/openai/callback')
    
    # Calendar API Settings
    CALENDAR_SCOPES = [
        'https://www.googleapis.com/auth/calendar.readonly'
    ]
    
    # AI & Language Models
    OPENAI_MODEL: str = os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo')
    
    # Redis Settings (for Celery)
    REDIS_URL: str = os.getenv('REDIS_URL', 'redis://localhost:6379/0')
    
    # Vector Database Settings
    VECTOR_DB_TYPE: str = os.getenv('VECTOR_DB_TYPE', 'faiss')  # faiss, weaviate, qdrant
    VECTOR_DB_PATH: str = os.getenv('VECTOR_DB_PATH', 'data/vector_store')
    EMBEDDING_MODEL: str = os.getenv('EMBEDDING_MODEL', 'all-MiniLM-L6-v2')
    
    # Task Extraction Settings
    TASK_EXTRACTION_PROMPT_VERSION: str = os.getenv('TASK_EXTRACTION_PROMPT_VERSION', 'v1')
    ENABLE_AUTO_TASK_EXTRACTION: bool = os.getenv('ENABLE_AUTO_TASK_EXTRACTION', 'True').lower() == 'true'
    
    # Memory & Context Settings
    MAX_CONVERSATION_HISTORY: int = int(os.getenv('MAX_CONVERSATION_HISTORY', '20'))
    CONTEXT_WINDOW_SIZE: int = int(os.getenv('CONTEXT_WINDOW_SIZE', '8000'))
    
    # Security Settings
    SESSION_TIMEOUT_HOURS: int = int(os.getenv('SESSION_TIMEOUT_HOURS', '24'))
    ENABLE_OFFLINE_MODE: bool = os.getenv('ENABLE_OFFLINE_MODE', 'False').lower() == 'true'
    
    # Logging Settings
    LOG_LEVEL: str = os.getenv('LOG_LEVEL', 'INFO')
    LOG_FILE: str = os.getenv('LOG_FILE', 'logs/chief_of_staff.log')
    
    # File Storage Settings
    UPLOAD_FOLDER: str = os.getenv('UPLOAD_FOLDER', 'data/uploads')
    MAX_UPLOAD_SIZE: int = int(os.getenv('MAX_UPLOAD_SIZE', '16777216'))  # 16MB
    ALLOWED_EXTENSIONS = {'txt', 'pdf', 'docx', 'doc', 'md'}
    
    # WebSocket Configuration for Real-time Agent Updates
    ENABLE_WEBSOCKET = os.getenv('ENABLE_WEBSOCKET', 'true').lower() == 'true'
    WEBSOCKET_PORT = int(os.getenv('WEBSOCKET_PORT', '5001'))
    
    @classmethod
    def validate_config(cls) -> List[str]:
        """
        Validate required configuration settings
        
        Returns:
            List of missing or invalid configuration items
        """
        errors = []
        
        # Required settings
        required_settings = [
            ('GOOGLE_CLIENT_ID', cls.GOOGLE_CLIENT_ID),
            ('GOOGLE_CLIENT_SECRET', cls.GOOGLE_CLIENT_SECRET),
            ('ANTHROPIC_API_KEY', cls.ANTHROPIC_API_KEY)
        ]
        
        for setting_name, setting_value in required_settings:
            if not setting_value:
                errors.append(f"Missing required setting: {setting_name}")
        
        # Validate database URL
        if not cls.DATABASE_URL:
            errors.append("DATABASE_URL is required")
        
        # Validate agent configuration
        if cls.ENABLE_CODE_EXECUTION and not cls.ANTHROPIC_API_KEY:
            errors.append("CODE_EXECUTION requires ANTHROPIC_API_KEY")
            
        if cls.AUTONOMOUS_CONFIDENCE_THRESHOLD < 0.5 or cls.AUTONOMOUS_CONFIDENCE_THRESHOLD > 1.0:
            errors.append("AUTONOMOUS_CONFIDENCE_THRESHOLD must be between 0.5 and 1.0")
        
        return errors
    
    @classmethod
    def get_gmail_auth_config(cls) -> Dict:
        """
        Get Gmail OAuth configuration for Google Auth library
        
        Returns:
            Dictionary with OAuth configuration
        """
        return {
            "web": {
                "client_id": cls.GOOGLE_CLIENT_ID,
                "client_secret": cls.GOOGLE_CLIENT_SECRET,
                "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                "token_uri": "https://oauth2.googleapis.com/token",
                "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
                "redirect_uris": [cls.GOOGLE_REDIRECT_URI]
            }
        }
    
    @classmethod
    def get_mcp_servers_config(cls) -> Dict:
        """Get MCP servers configuration for agent capabilities"""
        return {
            server_name: config for server_name, config in cls.MCP_SERVERS.items()
            if config.get('token')  # Only include servers with valid tokens
        }
    
    @classmethod
    def is_production(cls) -> bool:
        """Check if running in production environment"""
        return cls.FLASK_ENV == 'production' or 'heroku' in cls.DATABASE_URL.lower()
    
    @classmethod
    def is_heroku(cls) -> bool:
        """Check if running on Heroku"""
        return bool(os.getenv('DYNO'))
    
    @classmethod
    def create_directories(cls):
        """Create necessary directories"""
        directories = [
            'data',
            'data/uploads',
            'data/vector_store',
            'data/agent_files',  # For Files API
            'logs',
            'tests/data'
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)

# Initialize settings instance
settings = Settings()

# Validate required settings on import
try:
    settings.validate_config()
except ValueError as e:
    print(f"Configuration Error: {e}")
    print("Please check your .env file and ensure all required variables are set.")


================================================================================
FILE: chief_of_staff_ai/auth/gmail_auth.py
PURPOSE: Google OAuth integration for Gmail API access
================================================================================
# Handles Gmail OAuth setup

import os
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Optional, Tuple
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import Flow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from config.settings import settings
from models.database import get_db_manager

logger = logging.getLogger(__name__)

class GmailAuthHandler:
    """Handles Gmail OAuth authentication and token management with database persistence"""
    
    def __init__(self):
        self.client_id = settings.GOOGLE_CLIENT_ID
        self.client_secret = settings.GOOGLE_CLIENT_SECRET
        self.redirect_uri = settings.GOOGLE_REDIRECT_URI
        self.scopes = settings.GMAIL_SCOPES
        
    def get_authorization_url(self, user_id: str, state: str = None) -> Tuple[str, str]:
        """
        Generate OAuth authorization URL for Gmail access
        
        Args:
            user_id: Unique identifier for the user
            state: Optional state parameter for security
            
        Returns:
            Tuple of (authorization_url, state)
        """
        try:
            flow = Flow.from_client_config(
                settings.get_gmail_auth_config(),
                scopes=self.scopes
            )
            flow.redirect_uri = self.redirect_uri
            
            auth_url, state = flow.authorization_url(
                access_type='offline',
                include_granted_scopes='true',
                state=state or user_id,
                prompt='consent'  # Force consent to get refresh token
            )
            
            logger.info(f"Generated authorization URL for user {user_id}")
            return auth_url, state
            
        except Exception as e:
            logger.error(f"Failed to generate authorization URL: {str(e)}")
            raise
    
    def handle_oauth_callback(self, authorization_code: str, state: str = None) -> Dict:
        """
        Handle OAuth callback and exchange authorization code for tokens
        
        Args:
            authorization_code: Authorization code from OAuth callback
            state: State parameter from OAuth callback
            
        Returns:
            Dictionary containing success status and user info or error
        """
        try:
            flow = Flow.from_client_config(
                settings.get_gmail_auth_config(),
                scopes=self.scopes
            )
            flow.redirect_uri = self.redirect_uri
            
            # Exchange authorization code for tokens
            # Note: Google automatically adds 'openid' scope when requesting profile/email
            # We need to handle this gracefully
            try:
                flow.fetch_token(code=authorization_code)
            except Exception as token_error:
                # If there's a scope mismatch due to automatic 'openid' scope, try a more permissive approach
                if "scope" in str(token_error).lower():
                    logger.warning(f"Scope validation issue, retrying with relaxed validation: {str(token_error)}")
                    # Create a new flow with additional scopes including openid
                    extended_scopes = self.scopes + ['openid']
                    flow = Flow.from_client_config(
                        settings.get_gmail_auth_config(),
                        scopes=extended_scopes
                    )
                    flow.redirect_uri = self.redirect_uri
                    flow.fetch_token(code=authorization_code)
                else:
                    raise token_error
            
            credentials = flow.credentials
            
            # Get user information
            user_info = self._get_user_info(credentials)
            
            if not user_info.get('email'):
                raise Exception("Failed to get user email from Google")
            
            # Prepare credentials for database storage
            credentials_data = {
                'access_token': credentials.token,
                'refresh_token': credentials.refresh_token,
                'expires_at': credentials.expiry,
                'scopes': credentials.scopes
            }
            
            # Create or update user in database
            user = get_db_manager().create_or_update_user(user_info, credentials_data)
            
            logger.info(f"Successfully authenticated user: {user.email}")
            
            return {
                'success': True,
                'user_info': user_info,
                'user_email': user.email,
                'access_token': credentials.token,
                'has_refresh_token': bool(credentials.refresh_token),
                'user_id': user.id
            }
            
        except Exception as e:
            logger.error(f"OAuth callback error: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_valid_credentials(self, user_email: str) -> Optional[Credentials]:
        """
        Get valid credentials for a user, refreshing if necessary
        
        Args:
            user_email: Email of the user
            
        Returns:
            Valid Credentials object or None
        """
        try:
            # Get user from database
            user = get_db_manager().get_user_by_email(user_email)
            if not user or not user.access_token:
                logger.warning(f"No stored credentials for user: {user_email}")
                return None
            
            # Create credentials object
            credentials = Credentials(
                token=user.access_token,
                refresh_token=user.refresh_token,
                token_uri="https://oauth2.googleapis.com/token",
                client_id=self.client_id,
                client_secret=self.client_secret,
                scopes=user.scopes or self.scopes
            )
            
            # Set expiry if available
            if user.token_expires_at:
                credentials.expiry = user.token_expires_at
            
            # Check if credentials are expired and refresh if possible
            if credentials.expired and credentials.refresh_token:
                logger.info(f"Refreshing expired credentials for user: {user_email}")
                credentials.refresh(Request())
                
                # Update stored credentials in database
                credentials_data = {
                    'access_token': credentials.token,
                    'refresh_token': credentials.refresh_token,
                    'expires_at': credentials.expiry,
                    'scopes': credentials.scopes
                }
                get_db_manager().create_or_update_user(user.to_dict(), credentials_data)
                
            elif credentials.expired:
                logger.warning(f"Credentials expired and no refresh token for user: {user_email}")
                return None
            
            return credentials
            
        except Exception as e:
            logger.error(f"Failed to get valid credentials for {user_email}: {str(e)}")
            return None
    
    def revoke_credentials(self, user_email: str) -> bool:
        """
        Revoke stored credentials for a user
        
        Args:
            user_email: Email of the user
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Get user from database
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return False
            
            # Clear credentials in database
            credentials_data = {
                'access_token': None,
                'refresh_token': None,
                'expires_at': None,
                'scopes': []
            }
            get_db_manager().create_or_update_user(user.to_dict(), credentials_data)
            
            logger.info(f"Revoked credentials for user: {user_email}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to revoke credentials for {user_email}: {str(e)}")
            return False
    
    def is_authenticated(self, user_email: str) -> bool:
        """
        Check if user has valid authentication
        
        Args:
            user_email: Email of the user
            
        Returns:
            True if user has valid credentials, False otherwise
        """
        credentials = self.get_valid_credentials(user_email)
        return credentials is not None
    
    def test_gmail_access(self, user_email: str) -> bool:
        """
        Test if Gmail access is working for a user
        
        Args:
            user_email: Email of the user
            
        Returns:
            True if Gmail access is working, False otherwise
        """
        try:
            credentials = self.get_valid_credentials(user_email)
            if not credentials:
                return False
            
            # Build Gmail service and test with a simple call
            service = build('gmail', 'v1', credentials=credentials)
            profile = service.users().getProfile(userId='me').execute()
            
            logger.info(f"Gmail access test successful for {user_email}")
            return True
            
        except Exception as e:
            logger.error(f"Gmail access test failed for {user_email}: {str(e)}")
            return False
    
    def get_user_by_email(self, user_email: str) -> Optional[Dict]:
        """
        Get user information by email
        
        Args:
            user_email: Email of the user
            
        Returns:
            User dictionary or None
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            return user.to_dict() if user else None
        except Exception as e:
            logger.error(f"Failed to get user {user_email}: {str(e)}")
            return None
    
    def _get_user_info(self, credentials: Credentials) -> Dict:
        """
        Get user information from Google OAuth2 API
        
        Args:
            credentials: Valid Google credentials
            
        Returns:
            Dictionary containing user information
        """
        try:
            oauth2_service = build('oauth2', 'v2', credentials=credentials)
            user_info = oauth2_service.userinfo().get().execute()
            return user_info
            
        except Exception as e:
            logger.error(f"Failed to get user info: {str(e)}")
            return {}
    
    def get_authentication_status(self, user_email: str) -> Dict:
        """
        Get detailed authentication status for a user
        
        Args:
            user_email: Email of the user
            
        Returns:
            Dictionary with authentication status details
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {
                    'authenticated': False,
                    'gmail_access': False,
                    'error': 'User not found'
                }
            
            credentials = self.get_valid_credentials(user_email)
            if not credentials:
                return {
                    'authenticated': False,
                    'gmail_access': False,
                    'error': 'No valid credentials'
                }
            
            gmail_access = self.test_gmail_access(user_email)
            
            return {
                'authenticated': True,
                'gmail_access': gmail_access,
                'has_refresh_token': bool(user.refresh_token),
                'token_expired': credentials.expired if credentials else True,
                'scopes': user.scopes or [],
                'user_info': user.to_dict()
            }
            
        except Exception as e:
            logger.error(f"Failed to get authentication status for {user_email}: {str(e)}")
            return {
                'authenticated': False,
                'gmail_access': False,
                'error': str(e)
            }

# Create global instance
gmail_auth = GmailAuthHandler()


================================================================================
FILE: chief_of_staff_ai/agents/intelligence_agent.py
PURPOSE: AI agent: Intelligence Agent
================================================================================
import asyncio
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from anthropic import AsyncAnthropic
from config.settings import settings
import logging
import io
import base64

logger = logging.getLogger(__name__)

class IntelligenceAgent:
    """Enhanced Intelligence Agent with Code Execution and Files API"""
    
    def __init__(self, api_key: str = None):
        self.claude = AsyncAnthropic(api_key=api_key or settings.ANTHROPIC_API_KEY)
        self.model = settings.CLAUDE_MODEL
        self.enable_code_execution = settings.ENABLE_CODE_EXECUTION
        self.enable_files_api = settings.ENABLE_FILES_API
        self.cache_ttl = settings.EXTENDED_CACHE_TTL
    
    async def analyze_relationship_intelligence_with_data(self, person_data: Dict, email_history: List[Dict]) -> Dict:
        """Advanced relationship analysis with data visualization using code execution"""
        
        logger.info(f"🧠 Analyzing relationship intelligence for {person_data.get('name', 'Unknown')} with code execution")
        
        try:
            # Upload email data using Files API if enabled
            emails_file_id = None
            if self.enable_files_api and email_history:
                emails_file_id = await self._upload_email_data_to_files_api(email_history)
            
            analysis_prompt = f"""You are an advanced relationship intelligence analyst. Analyze this contact's communication patterns using data science.

**Person:** {json.dumps(person_data, indent=2)}

**Email History Count:** {len(email_history)} emails

**Task:** Perform comprehensive relationship analysis with advanced data visualizations.

**Analysis Required:**
1. Communication frequency trends over time (line chart)
2. Response time patterns analysis (histogram)
3. Email sentiment evolution over time
4. Topic frequency analysis (bar chart)
5. Engagement level scoring with statistical confidence
6. Predictive relationship health metrics

**Use code execution to:**
- Create comprehensive data visualizations
- Calculate statistical significance of patterns
- Generate predictive insights using data science
- Build relationship scoring algorithms
- Identify optimal communication timing

**Generate detailed analysis with data-driven insights and actionable recommendations.**"""

            messages = [{"role": "user", "content": analysis_prompt}]
            
            # Prepare tools for Claude 4 Opus
            tools = []
            if self.enable_code_execution:
                tools.append({
                    "type": "code_execution",
                    "name": "code_execution"
                })
            
            if self.enable_files_api:
                tools.append({
                    "type": "files_api", 
                    "name": "files_api"
                })
            
            # Headers for agent capabilities
            headers = {}
            capabilities = []
            if self.enable_code_execution:
                capabilities.append("code-execution-2025-01-01")
            if self.enable_files_api:
                capabilities.append("files-api-2025-01-01")
                
            if capabilities:
                headers["anthropic-beta"] = ",".join(capabilities)
            
            # Make the request with agent capabilities
            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=4000,
                messages=messages,
                tools=tools if tools else None,
                files=[emails_file_id] if emails_file_id else None,
                headers=headers if headers else None
            )
            
            return self._parse_analysis_response(response, person_data)
            
        except Exception as e:
            logger.error(f"Error in relationship intelligence analysis: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'insights': f"Error analyzing relationship with {person_data.get('name', 'Unknown')}",
                'visualizations': [],
                'metrics': {},
                'recommendations': []
            }

    async def generate_strategic_market_intelligence(self, business_context: Dict, goals: List[Dict]) -> Dict:
        """Generate strategic intelligence with market data analysis"""
        
        logger.info(f"📊 Generating strategic market intelligence for {len(goals)} goals")
        
        try:
            intelligence_prompt = f"""You are a strategic business intelligence analyst. Generate comprehensive market intelligence with advanced analytics.

**Business Context:**
{json.dumps(business_context, indent=2)}

**Strategic Goals:**
{json.dumps(goals, indent=2)}

**Advanced Analysis Tasks:**
1. Market opportunity sizing with statistical modeling
2. Competitive landscape analysis with data visualization
3. Industry trend correlation with goal alignment
4. Resource optimization using mathematical models
5. Risk assessment with probability distributions
6. Strategic pathway optimization using decision trees

**Use code execution to:**
- Build predictive models for market opportunities
- Create comprehensive strategic dashboards
- Model multiple scenarios with Monte Carlo simulation
- Calculate ROI projections with confidence intervals
- Generate quantified strategic recommendations
- Visualize market trends and competitive positioning

**Provide actionable intelligence with statistical confidence levels.**"""

            messages = [{"role": "user", "content": intelligence_prompt}]
            
            tools = []
            headers = {}
            capabilities = []
            
            if self.enable_code_execution:
                tools.append({
                    "type": "code_execution",
                    "name": "code_execution"
                })
                capabilities.append("code-execution-2025-01-01")
            
            if capabilities:
                headers["anthropic-beta"] = ",".join(capabilities)

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=4000,
                messages=messages,
                tools=tools if tools else None,
                headers=headers if headers else None
            )
            
            return self._parse_intelligence_response(response, business_context, goals)
            
        except Exception as e:
            logger.error(f"Error in strategic market intelligence: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'strategic_insights': [],
                'market_analysis': {},
                'recommendations': []
            }

    async def analyze_goal_achievement_patterns(self, user_goals: List[Dict], historical_data: Dict) -> Dict:
        """Analyze goal achievement patterns using advanced analytics"""
        
        logger.info(f"🎯 Analyzing goal achievement patterns for {len(user_goals)} goals")
        
        try:
            pattern_analysis_prompt = f"""Analyze goal achievement patterns using advanced data science.

**Goals to Analyze:**
{json.dumps(user_goals, indent=2)}

**Historical Performance Data:**
{json.dumps(historical_data, indent=2)}

**Advanced Pattern Analysis:**
1. Goal completion rate trends over time
2. Resource allocation efficiency analysis
3. Success factor correlation analysis
4. Bottleneck identification using statistical methods
5. Predictive success probability modeling
6. Optimal timing and resource allocation

**Use code execution to:**
- Build machine learning models for goal prediction
- Create goal achievement probability scores
- Generate resource optimization recommendations
- Identify success patterns and failure modes
- Visualize goal momentum and trajectory
- Calculate expected completion dates with confidence intervals

**Deliver insights that can accelerate goal achievement.**"""

            messages = [{"role": "user", "content": pattern_analysis_prompt}]
            
            tools = []
            headers = {}
            
            if self.enable_code_execution:
                tools.append({
                    "type": "code_execution",
                    "name": "code_execution"
                })
                headers["anthropic-beta"] = "code-execution-2025-01-01"

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=4000,
                messages=messages,
                tools=tools if tools else None,
                headers=headers if headers else None
            )
            
            return self._parse_goal_analysis_response(response, user_goals)
            
        except Exception as e:
            logger.error(f"Error in goal achievement analysis: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'patterns': [],
                'predictions': {},
                'recommendations': []
            }

    async def _upload_email_data_to_files_api(self, email_history: List[Dict]) -> str:
        """Upload email data using Files API for persistent analysis"""
        
        try:
            # Convert to DataFrame and prepare for analysis
            df = pd.DataFrame(email_history)
            
            # Enhance data for analysis
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'], errors='coerce')
            
            # Save as CSV
            csv_content = df.to_csv(index=False)
            
            # Upload to Files API
            file_response = await self.claude.files.create(
                file=csv_content.encode(),
                purpose="agent_analysis",
                filename=f"email_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            )
            
            logger.info(f"📁 Uploaded email data to Files API: {file_response.id}")
            return file_response.id
            
        except Exception as e:
            logger.error(f"Error uploading to Files API: {str(e)}")
            return None

    def _parse_analysis_response(self, response, person_data: Dict) -> Dict:
        """Parse Claude's response and extract insights + generated files"""
        
        try:
            analysis = {
                'success': True,
                'person': person_data.get('name', 'Unknown'),
                'insights': '',
                'visualizations': [],
                'metrics': {},
                'recommendations': [],
                'confidence_score': 0.0,
                'data_driven': True
            }
            
            # Extract text content
            if response.content:
                for content_block in response.content:
                    if hasattr(content_block, 'text'):
                        analysis['insights'] += content_block.text
                    elif hasattr(content_block, 'type') and content_block.type == 'tool_result':
                        # Handle code execution results
                        if 'matplotlib' in str(content_block) or 'chart' in str(content_block):
                            analysis['visualizations'].append({
                                'type': 'chart',
                                'description': 'Data visualization generated',
                                'data': str(content_block)
                            })
                        elif 'pandas' in str(content_block) or 'statistical' in str(content_block):
                            analysis['metrics']['statistical_analysis'] = str(content_block)
            
            # Extract key metrics from the response
            if 'confidence' in analysis['insights'].lower():
                try:
                    # Simple confidence extraction - could be enhanced
                    analysis['confidence_score'] = 0.8
                except:
                    analysis['confidence_score'] = 0.7
            
            # Generate recommendations based on analysis
            if analysis['insights']:
                analysis['recommendations'] = [
                    "Review relationship intelligence insights",
                    "Act on high-confidence recommendations",
                    "Monitor relationship health metrics"
                ]
            
            return analysis
            
        except Exception as e:
            logger.error(f"Error parsing analysis response: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'insights': 'Error parsing analysis results',
                'visualizations': [],
                'metrics': {},
                'recommendations': []
            }

    def _parse_intelligence_response(self, response, business_context: Dict, goals: List[Dict]) -> Dict:
        """Parse strategic intelligence response"""
        
        try:
            intelligence = {
                'success': True,
                'strategic_insights': [],
                'market_analysis': {},
                'recommendations': [],
                'confidence_level': 'high',
                'analysis_timestamp': datetime.now().isoformat()
            }
            
            # Extract insights from response
            if response.content:
                content_text = ''
                for content_block in response.content:
                    if hasattr(content_block, 'text'):
                        content_text += content_block.text
                    elif hasattr(content_block, 'type') and content_block.type == 'tool_result':
                        intelligence['market_analysis']['data_analysis'] = str(content_block)
                
                # Generate structured insights
                intelligence['strategic_insights'] = [
                    {
                        'insight_type': 'market_opportunity',
                        'title': 'Strategic Market Analysis',
                        'description': content_text[:200] + '...' if len(content_text) > 200 else content_text,
                        'confidence': 0.85,
                        'priority': 'high'
                    }
                ]
                
                intelligence['recommendations'] = [
                    "Execute highest-probability strategic initiatives",
                    "Monitor market indicators continuously",
                    "Optimize resource allocation based on analysis"
                ]
            
            return intelligence
            
        except Exception as e:
            logger.error(f"Error parsing intelligence response: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'strategic_insights': [],
                'market_analysis': {},
                'recommendations': []
            }

    def _parse_goal_analysis_response(self, response, user_goals: List[Dict]) -> Dict:
        """Parse goal achievement analysis response"""
        
        try:
            goal_analysis = {
                'success': True,
                'patterns': [],
                'predictions': {},
                'recommendations': [],
                'analyzed_goals': len(user_goals),
                'analysis_timestamp': datetime.now().isoformat()
            }
            
            if response.content:
                content_text = ''
                for content_block in response.content:
                    if hasattr(content_block, 'text'):
                        content_text += content_block.text
                    elif hasattr(content_block, 'type') and content_block.type == 'tool_result':
                        goal_analysis['predictions']['statistical_model'] = str(content_block)
                
                # Extract patterns and recommendations
                goal_analysis['patterns'] = [
                    {
                        'pattern_type': 'achievement_rate',
                        'description': 'Goal completion pattern analysis',
                        'confidence': 0.8
                    }
                ]
                
                goal_analysis['recommendations'] = [
                    "Focus on high-probability goals first",
                    "Allocate resources based on success patterns",
                    "Implement predictive monitoring"
                ]
            
            return goal_analysis
            
        except Exception as e:
            logger.error(f"Error parsing goal analysis response: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'patterns': [],
                'predictions': {},
                'recommendations': []
            }

    async def enhance_knowledge_topic_with_external_data(self, topic_name: str, topic_description: str, user_context: Dict) -> Dict:
        """Enhance a knowledge topic with external intelligence using agent capabilities"""
        
        enhancement_prompt = f"""You are an AI intelligence agent enhancing the knowledge topic "{topic_name}" with external research and insights.

**Topic to Enhance:**
Name: {topic_name}
Description: {topic_description}

**User Context:**
{json.dumps(user_context, indent=2)}

**Enhancement Tasks:**
1. **Market Intelligence**: Research current market trends related to this topic
2. **Competitive Analysis**: Identify key players and competitive landscape
3. **Industry Insights**: Find relevant industry developments and news
4. **Best Practices**: Research best practices and methodologies 
5. **Opportunity Analysis**: Identify potential opportunities and partnerships
6. **Risk Assessment**: Analyze potential risks and challenges
7. **Strategic Recommendations**: Provide actionable recommendations

**Use Code Execution for:**
- Data analysis and trend identification
- Market sizing and competitive mapping
- ROI calculations and impact analysis
- Visualization of insights and trends

**Enhancement Focus:**
- Provide insights that build on the existing email-based knowledge
- Focus on external intelligence that complements internal communications
- Identify strategic opportunities and timing
- Suggest actions based on external trends and internal context

Return comprehensive enhancement data in JSON format:
{{
    "market_intelligence": {{
        "current_trends": ["trend1", "trend2"],
        "market_size": "Data about market size and growth",
        "key_drivers": ["driver1", "driver2"],
        "future_outlook": "Predictions and forecasts"
    }},
    "competitive_landscape": {{
        "key_players": ["company1", "company2"],
        "competitive_advantages": ["advantage1", "advantage2"],
        "market_positioning": "How this topic relates to competitive positioning",
        "partnership_opportunities": ["potential partner1", "potential partner2"]
    }},
    "strategic_insights": {{
        "opportunities": ["opportunity1", "opportunity2"],
        "risks": ["risk1", "risk2"], 
        "timing_factors": ["timing consideration1", "timing consideration2"],
        "success_metrics": ["metric1", "metric2"]
    }},
    "actionable_recommendations": [
        {{
            "recommendation": "Specific recommendation",
            "rationale": "Why this is recommended",
            "priority": "high/medium/low",
            "timeline": "When to implement",
            "resources_needed": "What resources are required",
            "expected_impact": "Expected business impact"
        }}
    ],
    "external_resources": {{
        "research_sources": ["source1", "source2"],
        "industry_reports": ["report1", "report2"],
        "expert_contacts": ["expert1", "expert2"],
        "tools_and_platforms": ["tool1", "tool2"]
    }},
    "enhancement_summary": "Summary of how this external intelligence enhances the internal knowledge"
}}"""

        response = await self.claude.messages.create(
            model=self.model,
            max_tokens=4000,
            messages=[{"role": "user", "content": enhancement_prompt}],
            tools=[
                {
                    "type": "code_execution",
                    "name": "code_execution"
                }
            ],
            headers={
                "anthropic-beta": "code-execution-2025-01-01"
            }
        )
        
        # Parse enhancement response
        enhancement_text = response.content[0].text.strip()
        
        # Extract JSON from response
        import re
        json_start = enhancement_text.find('{')
        json_end = enhancement_text.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_text = enhancement_text[json_start:json_end]
            try:
                enhancement_data = json.loads(json_text)
                enhancement_data['enhancement_timestamp'] = datetime.now().isoformat()
                enhancement_data['enhancement_agent'] = 'intelligence_agent'
                return enhancement_data
            except json.JSONDecodeError:
                logger.error(f"Failed to parse enhancement JSON for topic: {topic_name}")
                return None
        
        return None 


================================================================================
FILE: chief_of_staff_ai/agents/email_agent.py
PURPOSE: AI agent: Email Agent
================================================================================
import asyncio
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from anthropic import AsyncAnthropic
from config.settings import settings
import logging

logger = logging.getLogger(__name__)

class AutonomousEmailAgent:
    """Autonomous Email Agent with Extended Thinking and Response Capabilities"""
    
    def __init__(self, api_key: str = None):
        self.claude = AsyncAnthropic(api_key=api_key or settings.ANTHROPIC_API_KEY)
        self.model = settings.CLAUDE_MODEL
        self.autonomous_threshold = settings.AUTONOMOUS_CONFIDENCE_THRESHOLD
        self.supervised_threshold = settings.SUPERVISED_CONFIDENCE_THRESHOLD
        self.max_autonomous_per_day = settings.MAX_AUTONOMOUS_EMAILS_PER_DAY
        self.cache_ttl = settings.EXTENDED_CACHE_TTL
    
    async def process_incoming_email_autonomously(self, email_data: Dict, user_context: Dict) -> Dict:
        """Process incoming email with extended thinking and autonomous response"""
        
        logger.info(f"📧 Processing email autonomously: {email_data.get('subject', 'No subject')}")
        
        try:
            # Use extended prompt caching for user context (1 hour TTL)
            cached_context_prompt = f"""You are the AI Chief of Staff for {user_context['user_name']}.

**Complete Business Context:**
{json.dumps(user_context.get('business_context', {}), indent=2)}

**Communication Style:**
{json.dumps(user_context.get('communication_style', {}), indent=2)}

**Strategic Goals:**
{json.dumps(user_context.get('goals', []), indent=2)}

**Relationship Intelligence:**
{json.dumps(user_context.get('relationship_data', {}), indent=2)}

This context is cached for efficient processing of multiple emails."""

            email_analysis_prompt = f"""Analyze this incoming email and determine autonomous action using EXTENDED THINKING.

**Incoming Email:**
Subject: {email_data.get('subject', 'No subject')}
From: {email_data.get('sender', 'Unknown')}
Date: {email_data.get('date', 'Unknown')}
Body: {email_data.get('body', 'No content')[:1000]}...

**COMPREHENSIVE ANALYSIS FRAMEWORK:**

1. **Strategic Relevance Assessment**:
   - How does this email relate to user's strategic goals?
   - What business opportunities or risks does it present?
   - What is the potential impact on key relationships?

2. **Relationship Context Analysis**:
   - What's the relationship history with this sender?
   - What's their tier in the user's network (Tier 1, 2, or lower)?
   - What communication patterns exist with this person?

3. **Urgency and Timing Assessment**:
   - What's the true urgency level (not just stated)?
   - Are there time-sensitive elements requiring immediate action?
   - What are the consequences of delayed response?

4. **Response Necessity Evaluation**:
   - Does this email require a response at all?
   - What type of response would be most appropriate?
   - What are the risks of autonomous vs manual response?

5. **Autonomous Action Decision**:
   - Can this be handled autonomously with high confidence?
   - What level of risk exists with autonomous action?
   - Should this be queued for approval or manual review?

**DECISION MATRIX:**
- Confidence > 85% AND Risk = Low: Execute autonomous response
- Confidence 70-85% OR Risk = Medium: Queue for approval  
- Confidence < 70% OR Risk = High: Flag for manual review

**Use EXTENDED THINKING to:**
- Deeply analyze the email's strategic implications
- Consider multiple response strategies and their outcomes
- Evaluate short-term and long-term relationship impact
- Assess business risks and opportunities
- Determine the optimal course of action

Think through this comprehensively and provide detailed decision rationale."""

            messages = [
                {"role": "system", "content": cached_context_prompt},
                {"role": "user", "content": email_analysis_prompt}
            ]
            
            # Headers for extended thinking and caching
            headers = {
                "anthropic-beta": "extended-thinking-2025-01-01"
            }
            
            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=4000,
                messages=messages,
                headers=headers,
                thinking_mode="extended"  # Enable extended thinking
            )
            
            return await self._process_email_decision(response, email_data, user_context)
            
        except Exception as e:
            logger.error(f"Error in autonomous email processing: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'action_taken': 'error',
                'requires_manual_review': True
            }

    async def craft_autonomous_response(self, email_data: Dict, decision_analysis: Dict, user_context: Dict) -> Dict:
        """Craft autonomous email response that perfectly matches user's style"""
        
        logger.info(f"✍️ Crafting autonomous response with style matching")
        
        try:
            response_prompt = f"""Craft an autonomous email response that is indistinguishable from the user's own writing.

**Original Email to Respond To:**
{json.dumps(email_data, indent=2)}

**Decision Analysis:**
{json.dumps(decision_analysis, indent=2)}

**User's Communication Style Profile:**
{json.dumps(user_context.get('communication_style', {}), indent=2)}

**RESPONSE CRAFTING REQUIREMENTS:**

1. **Perfect Style Matching**:
   - Match the user's tone, formality level, and writing patterns
   - Use their typical greeting and closing phrases
   - Reflect their communication personality and preferences

2. **Strategic Alignment**:
   - Align response with user's strategic goals and priorities
   - Consider the relationship tier and appropriate level of engagement
   - Include value-driven content that strengthens the relationship

3. **Appropriate Relationship Management**:
   - Acknowledge the sender's communication appropriately
   - Maintain or enhance the professional relationship
   - Set appropriate expectations for next steps

4. **Clear Value Delivery**:
   - Provide helpful information or next steps
   - Demonstrate understanding of the sender's needs
   - Position the user as responsive and professional

5. **Professional Excellence**:
   - Maintain high professional standards
   - Be concise but comprehensive
   - Include appropriate call-to-action or follow-up

**Use EXTENDED THINKING to:**
- Analyze the user's communication patterns and preferences
- Consider the relationship dynamics and appropriate tone
- Craft a response that adds genuine value
- Ensure the response advances strategic objectives
- Validate that the response sounds authentically like the user

Generate a complete email response including subject line and signature."""

            messages = [{"role": "user", "content": response_prompt}]
            
            headers = {
                "anthropic-beta": "extended-thinking-2025-01-01"
            }

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=2000,
                messages=messages,
                thinking_mode="extended",
                headers=headers
            )
            
            return self._parse_response_content(response, email_data)
            
        except Exception as e:
            logger.error(f"Error crafting autonomous response: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'subject': 'Error generating response',
                'body': 'Error occurred while generating response'
            }

    async def _process_email_decision(self, analysis_response, email_data: Dict, user_context: Dict) -> Dict:
        """Process the email analysis and execute autonomous actions"""
        
        try:
            # Parse Claude's extended thinking analysis
            decision = self._parse_decision_analysis(analysis_response)
            
            # Check if draft mode is enabled (safer approach)
            draft_mode = settings.ENABLE_EMAIL_DRAFT_MODE if hasattr(settings, 'ENABLE_EMAIL_DRAFT_MODE') else True
            
            # Check daily autonomous email limits
            daily_count = await self._get_daily_autonomous_count(user_context.get('user_id'))
            if daily_count >= self.max_autonomous_per_day:
                logger.warning(f"Daily autonomous email limit reached: {daily_count}/{self.max_autonomous_per_day}")
                return {
                    'action_taken': 'queued_for_approval',
                    'reason': 'daily_limit_reached',
                    'decision': decision
                }
            
            # ALWAYS CREATE DRAFT MODE - Modified logic
            if draft_mode or not settings.ENABLE_AUTONOMOUS_EMAIL_RESPONSES:
                # Create draft for user review regardless of confidence
                logger.info(f"📝 Creating email draft for review (confidence: {decision['confidence']:.2f})")
                
                draft_content = await self.craft_autonomous_response(
                    email_data, decision, user_context
                )
                
                # Store draft for review (instead of sending)
                draft_id = await self._create_email_draft(email_data, decision, draft_content, user_context)
                
                return {
                    'success': True,
                    'action_taken': 'draft_created_for_review',
                    'confidence': decision['confidence'],
                    'draft_id': draft_id,
                    'draft_preview': draft_content['body'][:200] + '...',
                    'strategic_impact': decision.get('strategic_impact', 'medium'),
                    'draft_quality': 'high' if decision['confidence'] > 0.8 else 'good',
                    'ready_to_send': decision['confidence'] > 0.85,
                    'review_required': True
                }
            
            # Original autonomous logic (only if autonomous mode explicitly enabled)
            elif decision['autonomous_action'] and decision['confidence'] > self.autonomous_threshold:
                # Execute autonomous response
                logger.info(f"🤖 Executing autonomous email response (confidence: {decision['confidence']:.2f})")
                
                response_content = await self.craft_autonomous_response(
                    email_data, decision, user_context
                )
                
                # Send email via MCP connector (Gmail integration)
                send_result = await self._send_email_via_mcp(
                    to=email_data['sender'],
                    subject=response_content['subject'],
                    body=response_content['body'],
                    user_context=user_context
                )
                
                # Log autonomous action
                await self._log_autonomous_action(email_data, decision, response_content, send_result)
                
                return {
                    'success': True,
                    'action_taken': 'autonomous_response_sent',
                    'confidence': decision['confidence'],
                    'response_preview': response_content['body'][:200] + '...',
                    'strategic_impact': decision.get('strategic_impact', 'medium'),
                    'send_result': send_result
                }
            
            elif decision['confidence'] > self.supervised_threshold:
                # Queue for approval
                logger.info(f"📋 Queuing email for approval (confidence: {decision['confidence']:.2f})")
                await self._queue_for_approval(email_data, decision, user_context)
                return {
                    'success': True,
                    'action_taken': 'queued_for_approval',
                    'confidence': decision['confidence'],
                    'decision': decision,
                    'estimated_response': await self._generate_draft_response(email_data, decision, user_context)
                }
            
            else:
                # Flag for manual review
                logger.info(f"🚨 Flagging email for manual review (confidence: {decision['confidence']:.2f})")
                await self._flag_for_manual_review(email_data, decision)
                return {
                    'success': True,
                    'action_taken': 'flagged_for_review',
                    'confidence': decision['confidence'],
                    'reason': decision.get('review_reason', 'Low confidence or high risk'),
                    'requires_manual_attention': True
                }
                
        except Exception as e:
            logger.error(f"Error processing email decision: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'action_taken': 'error',
                'requires_manual_review': True
            }

    async def _send_email_via_mcp(self, to: str, subject: str, body: str, user_context: Dict) -> Dict:
        """Send email using MCP connector via Gmail"""
        
        logger.info(f"📤 Sending autonomous email via MCP to {to}")
        
        try:
            # Check if MCP is enabled and configured
            if not settings.ENABLE_MCP_CONNECTOR:
                logger.warning("MCP connector not enabled, simulating email send")
                return {
                    'success': True,
                    'simulated': True,
                    'message': 'Email send simulated (MCP not configured)'
                }
            
            send_prompt = f"""Send an email using the Gmail MCP connector.

**Email Details:**
- To: {to}
- Subject: {subject}
- Body: {body}

**User Context:**
- Email Signature: {user_context.get('email_signature', '')}
- From: {user_context.get('user_email', '')}

Execute this email send operation and confirm delivery."""

            # Configure MCP servers for Gmail
            mcp_servers = []
            gmail_config = settings.MCP_SERVERS.get('gmail')
            if gmail_config and gmail_config.get('token'):
                mcp_servers.append({
                    "name": "gmail",
                    "url": gmail_config['url'],
                    "authorization_token": gmail_config['token']
                })

            if not mcp_servers:
                logger.warning("Gmail MCP server not configured, simulating send")
                return {
                    'success': True,
                    'simulated': True,
                    'message': 'Email send simulated (Gmail MCP not configured)'
                }

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=500,
                messages=[{"role": "user", "content": send_prompt}],
                mcp_servers=mcp_servers,
                headers={
                    "anthropic-beta": "mcp-client-2025-04-04"
                }
            )
            
            return {
                'success': True,
                'mcp_response': str(response),
                'sent_to': to,
                'subject': subject
            }
            
        except Exception as e:
            logger.error(f"Error sending email via MCP: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'sent_to': to,
                'subject': subject
            }

    def _parse_decision_analysis(self, response) -> Dict:
        """Parse Claude's extended thinking analysis"""
        
        try:
            decision = {
                'confidence': 0.5,
                'autonomous_action': False,
                'strategic_impact': 'unknown',
                'risk_level': 'unknown',
                'reasoning': '',
                'recommended_action': 'manual_review'
            }
            
            if response.content:
                content_text = ''
                for content_block in response.content:
                    if hasattr(content_block, 'text'):
                        content_text += content_block.text
                
                decision['reasoning'] = content_text
                
                # Extract confidence score (simplified parsing)
                if 'confidence' in content_text.lower():
                    try:
                        # Look for confidence percentages
                        import re
                        confidence_match = re.search(r'confidence[:\s]*(\d+)%?', content_text.lower())
                        if confidence_match:
                            decision['confidence'] = int(confidence_match.group(1)) / 100.0
                    except:
                        pass
                
                # Determine autonomous action eligibility
                if 'autonomous' in content_text.lower() and 'execute' in content_text.lower():
                    decision['autonomous_action'] = True
                    decision['recommended_action'] = 'autonomous_response'
                elif 'approval' in content_text.lower() or 'queue' in content_text.lower():
                    decision['recommended_action'] = 'queue_for_approval'
                else:
                    decision['recommended_action'] = 'manual_review'
                
                # Extract strategic impact
                if 'high impact' in content_text.lower() or 'strategic' in content_text.lower():
                    decision['strategic_impact'] = 'high'
                elif 'medium impact' in content_text.lower():
                    decision['strategic_impact'] = 'medium'
                else:
                    decision['strategic_impact'] = 'low'
                    
                # Extract risk level
                if 'high risk' in content_text.lower():
                    decision['risk_level'] = 'high'
                elif 'medium risk' in content_text.lower():
                    decision['risk_level'] = 'medium'
                else:
                    decision['risk_level'] = 'low'
            
            return decision
            
        except Exception as e:
            logger.error(f"Error parsing decision analysis: {str(e)}")
            return {
                'confidence': 0.3,
                'autonomous_action': False,
                'strategic_impact': 'unknown',
                'risk_level': 'high',
                'reasoning': f'Error parsing analysis: {str(e)}',
                'recommended_action': 'manual_review'
            }

    def _parse_response_content(self, response, email_data: Dict) -> Dict:
        """Parse the autonomous response content"""
        
        try:
            response_content = {
                'success': True,
                'subject': f"Re: {email_data.get('subject', 'Your message')}",
                'body': '',
                'signature': '',
                'style_matched': True
            }
            
            if response.content:
                content_text = ''
                for content_block in response.content:
                    if hasattr(content_block, 'text'):
                        content_text += content_block.text
                
                # Extract subject line
                import re
                subject_match = re.search(r'subject[:\s]+(.*?)(?:\n|$)', content_text, re.IGNORECASE)
                if subject_match:
                    response_content['subject'] = subject_match.group(1).strip()
                
                # Extract body content (simplified - would need more sophisticated parsing)
                response_content['body'] = content_text
            
            return response_content
            
        except Exception as e:
            logger.error(f"Error parsing response content: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'subject': f"Re: {email_data.get('subject', 'Your message')}",
                'body': 'Error generating response content'
            }

    async def _get_daily_autonomous_count(self, user_id: int) -> int:
        """Get count of autonomous emails sent today"""
        # This would query the database for autonomous actions today
        # For now, return 0 (would need database integration)
        return 0

    async def _log_autonomous_action(self, email_data: Dict, decision: Dict, response_content: Dict, send_result: Dict):
        """Log autonomous action for monitoring and learning"""
        
        try:
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'action_type': 'autonomous_email_response',
                'email_subject': email_data.get('subject', ''),
                'sender': email_data.get('sender', ''),
                'confidence': decision['confidence'],
                'strategic_impact': decision['strategic_impact'],
                'response_subject': response_content['subject'],
                'send_success': send_result.get('success', False),
                'simulated': send_result.get('simulated', False)
            }
            
            logger.info(f"📝 Logged autonomous action: {json.dumps(log_entry)}")
            
            # This would save to database for monitoring and improvement
            
        except Exception as e:
            logger.error(f"Error logging autonomous action: {str(e)}")

    async def _create_email_draft(self, email_data: Dict, decision: Dict, draft_content: Dict, user_context: Dict) -> str:
        """Create and store email draft for user review"""
        
        try:
            import uuid
            draft_id = str(uuid.uuid4())
            
            draft_data = {
                'draft_id': draft_id,
                'created_at': datetime.now().isoformat(),
                'original_email': {
                    'subject': email_data.get('subject', ''),
                    'sender': email_data.get('sender', ''),
                    'date': email_data.get('date', ''),
                    'body': email_data.get('body', '')[:500] + '...'  # Truncated for storage
                },
                'draft_response': {
                    'subject': draft_content['subject'],
                    'body': draft_content['body'],
                    'recipient': email_data.get('sender', '')
                },
                'ai_analysis': {
                    'confidence': decision['confidence'],
                    'strategic_impact': decision.get('strategic_impact', 'medium'),
                    'reasoning': decision.get('reasoning', '')[:300] + '...',
                    'risk_level': decision.get('risk_level', 'low')
                },
                'user_id': user_context.get('user_id'),
                'status': 'pending_review',
                'ready_to_send': decision['confidence'] > 0.85
            }
            
            # Store draft (this would integrate with your database)
            # For now, log it for demonstration
            logger.info(f"📧 Created email draft {draft_id} for review")
            logger.info(f"   To: {draft_data['draft_response']['recipient']}")
            logger.info(f"   Subject: {draft_data['draft_response']['subject']}")
            logger.info(f"   Confidence: {decision['confidence']:.1%}")
            logger.info(f"   Quality: {'High' if decision['confidence'] > 0.8 else 'Good'}")
            
            # This would save to database:
            # await save_email_draft_to_database(draft_data)
            
            return draft_id
            
        except Exception as e:
            logger.error(f"Error creating email draft: {str(e)}")
            return f"draft_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    async def _queue_for_approval(self, email_data: Dict, decision: Dict, user_context: Dict):
        """Queue email action for user approval"""
        logger.info(f"📋 Queued email for approval: {email_data.get('subject', 'No subject')}")
        # This would add to approval queue in database

    async def _flag_for_manual_review(self, email_data: Dict, decision: Dict):
        """Flag email for manual review"""
        logger.info(f"🚨 Flagged email for manual review: {email_data.get('subject', 'No subject')}")
        # This would add to manual review queue in database

    async def _generate_draft_response(self, email_data: Dict, decision: Dict, user_context: Dict) -> Dict:
        """Generate draft response for approval queue"""
        # Simplified version of craft_autonomous_response for preview
        return {
            'subject': f"Re: {email_data.get('subject', 'Your message')}",
            'body': f"Draft response for approval (confidence: {decision['confidence']:.0%})",
            'status': 'draft'
        } 


================================================================================
FILE: chief_of_staff_ai/agents/mcp_agent.py
PURPOSE: AI agent: Mcp Agent
================================================================================
import asyncio
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from anthropic import AsyncAnthropic
from config.settings import settings
import logging

logger = logging.getLogger(__name__)

class MCPConnectorAgent:
    """MCP Connector Agent for External Data and Workflow Automation"""
    
    def __init__(self, api_key: str = None):
        self.claude = AsyncAnthropic(api_key=api_key or settings.ANTHROPIC_API_KEY)
        self.model = settings.CLAUDE_MODEL
        self.enable_mcp = settings.ENABLE_MCP_CONNECTOR
        self.mcp_servers = settings.get_mcp_servers_config()
    
    async def enrich_contact_with_external_data(self, person_data: Dict) -> Dict:
        """Use MCP connector to enrich contact data from external sources"""
        
        logger.info(f"🔍 Enriching contact data for {person_data.get('name', 'Unknown')} using MCP")
        
        try:
            if not self.enable_mcp:
                logger.warning("MCP connector not enabled, returning mock enrichment")
                return self._generate_mock_enrichment(person_data)
            
            enrichment_prompt = f"""Enrich this contact's profile using all available MCP servers and external data sources.

**Contact to Enrich:**
{json.dumps(person_data, indent=2)}

**COMPREHENSIVE ENRICHMENT TASKS:**

1. **Professional Intelligence**:
   - Search LinkedIn for recent activity and career updates
   - Find current company information and role details
   - Identify professional achievements and milestones
   - Discover mutual connections and network overlap

2. **Company Intelligence**:
   - Research company news, funding status, and market position
   - Find recent press releases and strategic announcements
   - Analyze company growth trajectory and market opportunities
   - Identify key decision makers and organizational structure

3. **Relationship Mapping**:
   - Find mutual connections and warm introduction paths
   - Identify shared professional interests and experiences
   - Map relationship strength and interaction history
   - Discover collaboration opportunities and timing

4. **Strategic Context**:
   - Gather industry context and market positioning
   - Identify business development opportunities
   - Find relevant news, trends, and market dynamics
   - Assess strategic value and partnership potential

5. **Timing Intelligence**:
   - Identify optimal engagement timing and context
   - Find recent triggers for outreach (job changes, funding, etc.)
   - Discover upcoming events or opportunities
   - Assess relationship momentum and receptivity

**Use all available MCP tools to gather comprehensive intelligence and provide actionable insights.**"""

            # Configure available MCP servers
            available_mcp_servers = []
            
            # LinkedIn Research Server
            if 'linkedin' in self.mcp_servers:
                available_mcp_servers.append({
                    "name": "linkedin_research",
                    "url": self.mcp_servers['linkedin']['url'],
                    "authorization_token": self.mcp_servers['linkedin']['token']
                })
            
            # Business Intelligence Server
            if 'business_intel' in self.mcp_servers:
                available_mcp_servers.append({
                    "name": "business_intelligence",
                    "url": self.mcp_servers['business_intel']['url'],
                    "authorization_token": self.mcp_servers['business_intel']['token']
                })
            
            # News Monitoring Server
            if 'news_monitoring' in self.mcp_servers:
                available_mcp_servers.append({
                    "name": "news_monitoring",
                    "url": self.mcp_servers['news_monitoring']['url'],
                    "authorization_token": self.mcp_servers['news_monitoring']['token']
                })

            if not available_mcp_servers:
                logger.warning("No MCP servers configured, using fallback enrichment")
                return self._generate_fallback_enrichment(person_data)

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=3000,
                messages=[{"role": "user", "content": enrichment_prompt}],
                mcp_servers=available_mcp_servers,
                headers={
                    "anthropic-beta": "mcp-client-2025-04-04"
                }
            )
            
            return self._parse_enrichment_response(response, person_data)
            
        except Exception as e:
            logger.error(f"Error enriching contact data: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'enrichment_data': {},
                'person_name': person_data.get('name', 'Unknown')
            }

    async def automate_business_workflow(self, workflow_request: Dict) -> Dict:
        """Use MCP connector to automate business workflows via Zapier and other services"""
        
        logger.info(f"⚡ Automating business workflow: {workflow_request.get('workflow_type', 'Unknown')}")
        
        try:
            if not self.enable_mcp:
                logger.warning("MCP connector not enabled, simulating workflow execution")
                return self._simulate_workflow_execution(workflow_request)
            
            automation_prompt = f"""Execute this business workflow automation request using available MCP tools.

**Workflow Request:**
{json.dumps(workflow_request, indent=2)}

**AVAILABLE AUTOMATION CAPABILITIES:**

1. **Email Operations**:
   - Send emails via Gmail MCP connector
   - Create email templates and sequences
   - Schedule follow-up emails
   - Track email engagement

2. **CRM Operations**:
   - Update contact records and relationship data
   - Create tasks and follow-up reminders
   - Log interactions and communication history
   - Generate reports and analytics

3. **Calendar Management**:
   - Schedule meetings and appointments
   - Send calendar invites and reminders
   - Block time for important activities
   - Coordinate across multiple calendars

4. **Communication**:
   - Post updates to Slack channels
   - Send SMS notifications for urgent items
   - Create and share documents
   - Coordinate team communications

5. **Project Management**:
   - Create tasks in project management tools
   - Update project status and milestones
   - Assign responsibilities and deadlines
   - Generate progress reports

6. **Data Management**:
   - Update spreadsheets and databases
   - Generate and distribute reports
   - Backup important information
   - Synchronize data across platforms

**Execute the requested workflow using appropriate MCP tools and provide detailed execution results.**"""

            # Configure automation MCP servers
            automation_servers = []
            
            # Zapier for general automation
            if 'zapier' in self.mcp_servers:
                automation_servers.append({
                    "name": "zapier",
                    "url": self.mcp_servers['zapier']['url'],
                    "authorization_token": self.mcp_servers['zapier']['token']
                })
            
            # Gmail for email automation
            if 'gmail' in self.mcp_servers:
                automation_servers.append({
                    "name": "gmail",
                    "url": self.mcp_servers['gmail']['url'],
                    "authorization_token": self.mcp_servers['gmail']['token']
                })
            
            # CRM for customer relationship automation
            if 'crm' in self.mcp_servers:
                automation_servers.append({
                    "name": "crm",
                    "url": self.mcp_servers['crm']['url'],
                    "authorization_token": self.mcp_servers['crm']['token']
                })

            if not automation_servers:
                logger.warning("No automation MCP servers configured")
                return self._simulate_workflow_execution(workflow_request)

            response = await self.claude.messages.create(
                model=self.model,
                max_tokens=2000,
