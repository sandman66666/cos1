tion of legacy email normalizer"""
    
    def normalize_gmail_email(self, email_data: Dict) -> Dict[str, Any]:
        """Normalize Gmail email - legacy adapter"""
        try:
            # Basic normalization - just pass through the data
            normalized = email_data.copy()
            normalized['normalized'] = True
            normalized['processing_notes'] = ['Legacy adapter stub - basic normalization']
            return normalized
        except Exception as e:
            logger.error(f"Error in legacy email normalization: {str(e)}")
            return {
                'error': True,
                'error_message': str(e)
            }
    
    def normalize_user_emails(self, user_email: str, limit: int = 50) -> Dict[str, Any]:
        """Normalize user emails in batch - legacy adapter"""
        try:
            return {
                'success': True,
                'emails_normalized': 0,
                'processing_notes': ['Legacy adapter stub - no emails normalized']
            }
        except Exception as e:
            logger.error(f"Error in legacy batch normalization: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }

# Global instances for legacy compatibility
task_extractor = TaskExtractor()
email_intelligence = EmailIntelligence()
email_normalizer = EmailNormalizer() 

============================================================
FILE: chief_of_staff_ai/processors/email_normalizer.py
============================================================
# Normalizes raw Gmail data into clean format

import re
import logging
from datetime import datetime
from typing import Dict, List, Optional
from html import unescape
from bs4 import BeautifulSoup

from models.database import get_db_manager, Email

logger = logging.getLogger(__name__)

class EmailNormalizer:
    """Normalizes emails into clean, standardized format with entity extraction"""
    
    def __init__(self):
        self.version = "1.0"
        
    def normalize_user_emails(self, user_email: str, limit: int = None) -> Dict:
        """
        Normalize all emails for a user that haven't been normalized yet
        
        Args:
            user_email: Email of the user
            limit: Maximum number of emails to process
            
        Returns:
            Dictionary with normalization results
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            # Get emails that need normalization
            with get_db_manager().get_session() as session:
                emails = session.query(Email).filter(
                    Email.user_id == user.id,
                    Email.body_clean.is_(None)  # Not normalized yet
                ).limit(limit or 100).all()
            
            if not emails:
                logger.info(f"No emails to normalize for {user_email}")
                return {
                    'success': True,
                    'user_email': user_email,
                    'processed': 0,
                    'message': 'No emails need normalization'
                }
            
            processed_count = 0
            error_count = 0
            
            for email in emails:
                try:
                    # Convert database email to dict for processing
                    email_dict = {
                        'id': email.gmail_id,
                        'subject': email.subject,
                        'body_text': email.body_text,
                        'body_html': email.body_html,
                        'sender': email.sender,
                        'sender_name': email.sender_name,
                        'snippet': email.snippet,
                        'timestamp': email.email_date
                    }
                    
                    # Normalize the email
                    normalized = self.normalize_email(email_dict)
                    
                    # Update the database record
                    with get_db_manager().get_session() as session:
                        email_record = session.query(Email).filter(
                            Email.user_id == user.id,
                            Email.gmail_id == email.gmail_id
                        ).first()
                        
                        if email_record:
                            email_record.body_clean = normalized.get('body_clean')
                            email_record.body_preview = normalized.get('body_preview')
                            email_record.entities = normalized.get('entities', {})
                            email_record.message_type = normalized.get('message_type')
                            email_record.priority_score = normalized.get('priority_score')
                            email_record.normalizer_version = self.version
                            
                            session.commit()
                            processed_count += 1
                    
                except Exception as e:
                    logger.error(f"Failed to normalize email {email.gmail_id}: {str(e)}")
                    error_count += 1
                    continue
            
            logger.info(f"Normalized {processed_count} emails for {user_email} ({error_count} errors)")
            
            return {
                'success': True,
                'user_email': user_email,
                'processed': processed_count,
                'errors': error_count,
                'normalizer_version': self.version
            }
            
        except Exception as e:
            logger.error(f"Failed to normalize emails for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def normalize_email(self, email_data: Dict) -> Dict:
        """
        Normalize a single email into clean format
        
        Args:
            email_data: Raw email data dictionary
            
        Returns:
            Normalized email data
        """
        try:
            # Start with original data
            normalized = email_data.copy()
            
            # Clean and extract body content
            body_clean = self._extract_clean_body(email_data)
            normalized['body_clean'] = body_clean
            
            # Create preview (first 300 chars)
            normalized['body_preview'] = self._create_preview(body_clean)
            
            # Extract entities
            normalized['entities'] = self._extract_entities(email_data, body_clean)
            
            # Determine message type
            normalized['message_type'] = self._classify_message_type(email_data, body_clean)
            
            # Calculate priority score
            normalized['priority_score'] = self._calculate_priority_score(email_data, body_clean)
            
            # Add processing metadata
            normalized['processing_metadata'] = {
                'normalizer_version': self.version,
                'normalized_at': datetime.utcnow().isoformat(),
                'body_length': len(body_clean) if body_clean else 0
            }
            
            return normalized
            
        except Exception as e:
            logger.error(f"Failed to normalize email {email_data.get('id', 'unknown')}: {str(e)}")
            return {
                **email_data,
                'normalization_error': str(e),
                'processing_metadata': {
                    'normalizer_version': self.version,
                    'normalized_at': datetime.utcnow().isoformat(),
                    'error': True
                }
            }
    
    def _extract_clean_body(self, email_data: Dict) -> str:
        """
        Extract clean text from email body
        
        Args:
            email_data: Email data dictionary
            
        Returns:
            Clean body text
        """
        try:
            body_text = email_data.get('body_text', '')
            body_html = email_data.get('body_html', '')
            
            # Prefer HTML if available, fallback to text
            if body_html:
                # Parse HTML and extract text
                soup = BeautifulSoup(body_html, 'html.parser')
                
                # Remove script and style elements
                for script in soup(['script', 'style']):
                    script.decompose()
                
                # Get text and clean it
                text = soup.get_text()
                
                # Break into lines and remove leading/trailing spaces
                lines = (line.strip() for line in text.splitlines())
                
                # Break multi-headlines into a line each
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                
                # Drop blank lines
                clean_text = '\n'.join(chunk for chunk in chunks if chunk)
                
            elif body_text:
                clean_text = body_text
                
            else:
                # Fallback to snippet
                clean_text = email_data.get('snippet', '')
            
            if not clean_text:
                return ''
                
            # Remove quoted text (replies/forwards)
            clean_text = self._remove_quoted_text(clean_text)
            
            # Remove excessive whitespace
            clean_text = re.sub(r'\n\s*\n', '\n\n', clean_text)
            clean_text = re.sub(r' +', ' ', clean_text)
            
            # Decode HTML entities
            clean_text = unescape(clean_text)
            
            return clean_text.strip()
            
        except Exception as e:
            logger.error(f"Failed to extract clean body: {str(e)}")
            return email_data.get('snippet', '')
    
    def _remove_quoted_text(self, text: str) -> str:
        """
        Remove quoted text from emails (replies/forwards)
        
        Args:
            text: Email body text
            
        Returns:
            Text with quoted sections removed
        """
        try:
            # Common quote patterns
            quote_patterns = [
                r'On .* wrote:.*',
                r'From:.*\nSent:.*\nTo:.*\nSubject:.*',
                r'-----Original Message-----.*',
                r'> .*',  # Lines starting with >
                r'________________________________.*',  # Outlook separator
                r'From: .*<.*>.*',
                r'Sent from my .*',
                r'\n\n.*On.*\d{4}.*at.*\d{1,2}:\d{2}.*wrote:'
            ]
            
            cleaned_text = text
            
            for pattern in quote_patterns:
                cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.DOTALL | re.IGNORECASE)
            
            # Remove excessive newlines created by quote removal
            cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text)
            
            return cleaned_text.strip()
            
        except Exception as e:
            logger.error(f"Failed to remove quoted text: {str(e)}")
            return text
    
    def _create_preview(self, body_text: str) -> str:
        """
        Create a preview of the email body
        
        Args:
            body_text: Clean email body text
            
        Returns:
            Preview text (first 300 characters)
        """
        if not body_text:
            return ''
        
        # Take first 300 characters
        preview = body_text[:300]
        
        # If we cut in the middle of a word, cut to last complete word
        if len(body_text) > 300:
            last_space = preview.rfind(' ')
            if last_space > 250:  # Only if we have a reasonable amount of text
                preview = preview[:last_space] + '...'
            else:
                preview += '...'
        
        return preview
    
    def _extract_entities(self, email_data: Dict, body_text: str) -> Dict:
        """
        Extract entities from email content
        
        Args:
            email_data: Email data dictionary
            body_text: Clean email body text
            
        Returns:
            Dictionary of extracted entities
        """
        try:
            entities = {
                'people': [],
                'companies': [],
                'dates': [],
                'times': [],
                'urls': [],
                'emails': [],
                'phone_numbers': [],
                'amounts': []
            }
            
            # Extract email addresses
            email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
            entities['emails'] = list(set(re.findall(email_pattern, body_text)))
            
            # Extract URLs
            url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
            entities['urls'] = list(set(re.findall(url_pattern, body_text)))
            
            # Extract phone numbers (US format)
            phone_pattern = r'\b(?:\+?1[-.\s]?)?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})\b'
            phone_matches = re.findall(phone_pattern, body_text)
            entities['phone_numbers'] = ['-'.join(match) for match in phone_matches]
            
            # Extract dates (simple patterns)
            date_patterns = [
                r'\b\d{1,2}/\d{1,2}/\d{4}\b',
                r'\b\d{1,2}-\d{1,2}-\d{4}\b',
                r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2},? \d{4}\b'
            ]
            for pattern in date_patterns:
                entities['dates'].extend(re.findall(pattern, body_text, re.IGNORECASE))
            
            # Extract times
            time_pattern = r'\b\d{1,2}:\d{2}(?:\s?[AP]M)?\b'
            entities['times'] = list(set(re.findall(time_pattern, body_text, re.IGNORECASE)))
            
            # Extract monetary amounts
            amount_pattern = r'\$\d{1,3}(?:,\d{3})*(?:\.\d{2})?'
            entities['amounts'] = list(set(re.findall(amount_pattern, body_text)))
            
            # Remove empty lists and duplicates
            for key in entities:
                entities[key] = list(set(entities[key])) if entities[key] else []
            
            return entities
            
        except Exception as e:
            logger.error(f"Failed to extract entities: {str(e)}")
            return {}
    
    def _classify_message_type(self, email_data: Dict, body_text: str) -> str:
        """
        Classify the type of email message
        
        Args:
            email_data: Email data dictionary
            body_text: Clean email body text
            
        Returns:
            Message type classification
        """
        try:
            subject = email_data.get('subject', '').lower()
            body_lower = body_text.lower() if body_text else ''
            sender = email_data.get('sender', '').lower()
            
            # Meeting/Calendar invites
            meeting_keywords = ['meeting', 'call', 'zoom', 'teams', 'webex', 'conference', 'invite', 'calendar']
            if any(keyword in subject for keyword in meeting_keywords):
                return 'meeting'
            
            # Automated/System emails
            system_domains = ['noreply', 'no-reply', 'donotreply', 'mailer-daemon', 'bounce']
            if any(domain in sender for domain in system_domains):
                return 'automated'
            
            # Newsletters/Marketing
            newsletter_keywords = ['unsubscribe', 'newsletter', 'marketing', 'promotional']
            if any(keyword in body_lower for keyword in newsletter_keywords):
                return 'newsletter'
            
            # Action required
            action_keywords = ['urgent', 'asap', 'deadline', 'required', 'please review', 'action needed']
            if any(keyword in subject for keyword in action_keywords):
                return 'action_required'
            
            # FYI/Information
            fyi_keywords = ['fyi', 'for your information', 'heads up', 'update', 'status']
            if any(keyword in subject for keyword in fyi_keywords):
                return 'informational'
            
            # Default to regular
            return 'regular'
            
        except Exception as e:
            logger.error(f"Failed to classify message type: {str(e)}")
            return 'regular'
    
    def _calculate_priority_score(self, email_data: Dict, body_text: str) -> float:
        """
        Calculate priority score for email (0.0 to 1.0)
        
        Args:
            email_data: Email data dictionary
            body_text: Clean email body text
            
        Returns:
            Priority score between 0.0 and 1.0
        """
        try:
            score = 0.5  # Base score
            
            subject = email_data.get('subject', '').lower()
            body_lower = body_text.lower() if body_text else ''
            
            # High priority keywords
            urgent_keywords = ['urgent', 'asap', 'emergency', 'critical', 'deadline']
            for keyword in urgent_keywords:
                if keyword in subject or keyword in body_lower:
                    score += 0.2
            
            # Medium priority keywords
            important_keywords = ['important', 'priority', 'please review', 'action needed']
            for keyword in important_keywords:
                if keyword in subject or keyword in body_lower:
                    score += 0.1
            
            # Questions increase priority slightly
            if '?' in subject or '?' in body_text:
                score += 0.05
            
            # Direct communication (personal emails)
            if '@' in email_data.get('sender', '') and 'noreply' not in email_data.get('sender', ''):
                score += 0.1
            
            # Reduce score for automated emails
            automated_keywords = ['unsubscribe', 'automated', 'noreply', 'notification']
            for keyword in automated_keywords:
                if keyword in email_data.get('sender', '').lower():
                    score -= 0.2
            
            # Ensure score is between 0.0 and 1.0
            return max(0.0, min(1.0, score))
            
        except Exception as e:
            logger.error(f"Failed to calculate priority score: {str(e)}")
            return 0.5

# Create global instance
email_normalizer = EmailNormalizer()
FILE: chief_of_staff_ai/processors/__init__.py - Package initialization file

============================================================
FILE: chief_of_staff_ai/processors/realtime_processor.py
============================================================
# Real-Time Processing Pipeline - Proactive Intelligence
# This transforms the system from batch processing to continuous intelligence

import asyncio
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import json
from dataclasses import dataclass, asdict
from enum import Enum
import threading
import queue
import time

from processors.enhanced_ai_pipeline import enhanced_ai_processor
from processors.unified_entity_engine import entity_engine, EntityContext
from models.enhanced_models import IntelligenceInsight, Person, Topic, Task, CalendarEvent

logger = logging.getLogger(__name__)

class EventType(Enum):
    NEW_EMAIL = "new_email"
    NEW_CALENDAR_EVENT = "new_calendar_event"
    ENTITY_UPDATE = "entity_update"
    USER_ACTION = "user_action"
    SCHEDULED_ANALYSIS = "scheduled_analysis"

@dataclass
class ProcessingEvent:
    event_type: EventType
    user_id: int
    data: Dict
    timestamp: datetime
    priority: int = 5  # 1-10, 1 = highest priority
    correlation_id: Optional[str] = None

class RealTimeProcessor:
    """
    Real-time processing engine that provides continuous intelligence.
    This is what transforms your system from reactive to proactive.
    """
    
    def __init__(self):
        self.processing_queue = queue.PriorityQueue()
        self.running = False
        self.worker_threads = []
        self.user_contexts = {}  # Cache user contexts for efficiency
        self.insight_callbacks = {}  # User-specific insight delivery callbacks
        
    def start(self, num_workers: int = 3):
        """Start the real-time processing engine"""
        self.running = True
        
        # Start worker threads
        for i in range(num_workers):
            worker = threading.Thread(target=self._process_events_worker, name=f"RTProcessor-{i}")
            worker.daemon = True
            worker.start()
            self.worker_threads.append(worker)
        
        # Start periodic analysis thread
        scheduler = threading.Thread(target=self._scheduled_analysis_worker, name="RTScheduler")
        scheduler.daemon = True
        scheduler.start()
        self.worker_threads.append(scheduler)
        
        logger.info(f"Started real-time processor with {num_workers} workers")
    
    def stop(self):
        """Stop the real-time processing engine"""
        self.running = False
        for worker in self.worker_threads:
            worker.join(timeout=5)
        logger.info("Stopped real-time processor")
    
    # =====================================================================
    # EVENT INGESTION METHODS
    # =====================================================================
    
    def process_new_email(self, email_data: Dict, user_id: int, priority: int = 5):
        """Process new email in real-time"""
        event = ProcessingEvent(
            event_type=EventType.NEW_EMAIL,
            user_id=user_id,
            data=email_data,
            timestamp=datetime.utcnow(),
            priority=priority
        )
        self._queue_event(event)
    
    def process_new_calendar_event(self, event_data: Dict, user_id: int, priority: int = 5):
        """Process new calendar event in real-time"""
        event = ProcessingEvent(
            event_type=EventType.NEW_CALENDAR_EVENT,
            user_id=user_id,
            data=event_data,
            timestamp=datetime.utcnow(),
            priority=priority
        )
        self._queue_event(event)
    
    def process_entity_update(self, entity_type: str, entity_id: int, update_data: Dict, user_id: int):
        """Process entity update and trigger related intelligence updates"""
        event = ProcessingEvent(
            event_type=EventType.ENTITY_UPDATE,
            user_id=user_id,
            data={
                'entity_type': entity_type,
                'entity_id': entity_id,
                'update_data': update_data
            },
            timestamp=datetime.utcnow(),
            priority=3  # Higher priority for entity updates
        )
        self._queue_event(event)
    
    def process_user_action(self, action_type: str, action_data: Dict, user_id: int):
        """Process user action and learn from feedback"""
        event = ProcessingEvent(
            event_type=EventType.USER_ACTION,
            user_id=user_id,
            data={
                'action_type': action_type,
                'action_data': action_data
            },
            timestamp=datetime.utcnow(),
            priority=4
        )
        self._queue_event(event)
    
    # =====================================================================
    # CORE PROCESSING WORKERS
    # =====================================================================
    
    def _process_events_worker(self):
        """Main event processing worker"""
        while self.running:
            try:
                # Get event from queue (blocks until available or timeout)
                try:
                    priority, event = self.processing_queue.get(timeout=1.0)
                except queue.Empty:
                    continue
                
                logger.debug(f"Processing {event.event_type.value} for user {event.user_id}")
                
                # Process based on event type
                if event.event_type == EventType.NEW_EMAIL:
                    self._process_new_email_event(event)
                elif event.event_type == EventType.NEW_CALENDAR_EVENT:
                    self._process_new_calendar_event(event)
                elif event.event_type == EventType.ENTITY_UPDATE:
                    self._process_entity_update_event(event)
                elif event.event_type == EventType.USER_ACTION:
                    self._process_user_action_event(event)
                elif event.event_type == EventType.SCHEDULED_ANALYSIS:
                    self._process_scheduled_analysis_event(event)
                
                # Mark task as done
                self.processing_queue.task_done()
                
            except Exception as e:
                logger.error(f"Error in event processing worker: {str(e)}")
                time.sleep(0.1)  # Brief pause on error
    
    def _scheduled_analysis_worker(self):
        """Worker for periodic intelligence analysis"""
        while self.running:
            try:
                # Run scheduled analysis every 15 minutes
                time.sleep(900)  # 15 minutes
                
                # Get active users (those with recent activity)
                active_users = self._get_active_users()
                
                for user_id in active_users:
                    event = ProcessingEvent(
                        event_type=EventType.SCHEDULED_ANALYSIS,
                        user_id=user_id,
                        data={'analysis_type': 'proactive_insights'},
                        timestamp=datetime.utcnow(),
                        priority=7  # Lower priority for scheduled analysis
                    )
                    self._queue_event(event)
                
            except Exception as e:
                logger.error(f"Error in scheduled analysis worker: {str(e)}")
    
    # =====================================================================
    # EVENT PROCESSING METHODS
    # =====================================================================
    
    def _process_new_email_event(self, event: ProcessingEvent):
        """Process new email with real-time intelligence generation"""
        try:
            email_data = event.data
            user_id = event.user_id
            
            # Get cached user context for efficiency
            context = self._get_cached_user_context(user_id)
            
            # Process email with enhanced AI pipeline
            result = enhanced_ai_processor.process_email_with_context(email_data, user_id, context)
            
            if result.success:
                # Update cached context with new information
                self._update_cached_context(user_id, result)
                
                # Generate immediate insights
                immediate_insights = self._generate_immediate_insights(email_data, result, user_id)
                
                # Deliver insights to user
                self._deliver_insights_to_user(user_id, immediate_insights)
                
                # Check for entity cross-references and augmentations
                self._check_cross_entity_augmentations(result, user_id)
                
                logger.info(f"Processed new email in real-time for user {user_id}: "
                           f"{result.entities_created} entities created, {len(immediate_insights)} insights")
            
        except Exception as e:
            logger.error(f"Failed to process new email event: {str(e)}")
    
    def _process_new_calendar_event(self, event: ProcessingEvent):
        """Process new calendar event with intelligence enhancement"""
        try:
            event_data = event.data
            user_id = event.user_id
            
            # Enhance calendar event with email intelligence
            result = enhanced_ai_processor.enhance_calendar_event_with_intelligence(event_data, user_id)
            
            if result.success:
                # Generate meeting preparation insights
                prep_insights = self._generate_meeting_prep_insights(event_data, result, user_id)
                
                # Deliver insights to user
                self._deliver_insights_to_user(user_id, prep_insights)
                
                # Update cached context
                self._update_cached_context(user_id, result)
                
                logger.info(f"Enhanced calendar event in real-time for user {user_id}: "
                           f"{result.entities_created['tasks']} prep tasks created")
            
        except Exception as e:
            logger.error(f"Failed to process new calendar event: {str(e)}")
    
    def _process_entity_update_event(self, event: ProcessingEvent):
        """Process entity updates and propagate intelligence"""
        try:
            entity_type = event.data['entity_type']
            entity_id = event.data['entity_id']
            update_data = event.data['update_data']
            user_id = event.user_id
            
            # Create entity context
            context = EntityContext(
                source_type='update',
                user_id=user_id,
                confidence=0.9
            )
            
            # Augment entity with new data
            entity_engine.augment_entity_from_source(entity_type, entity_id, update_data, context)
            
            # Find related entities that might need updates
            related_entities = self._find_related_entities(entity_type, entity_id, user_id)
            
            # Propagate intelligence to related entities
            for related_entity in related_entities:
                self._propagate_intelligence_update(
                    related_entity['type'], 
                    related_entity['id'], 
                    entity_type, 
                    entity_id, 
                    update_data, 
                    user_id
                )
            
            # Generate insights from entity updates
            update_insights = self._generate_entity_update_insights(entity_type, entity_id, update_data, user_id)
            self._deliver_insights_to_user(user_id, update_insights)
            
            logger.info(f"Processed entity update for {entity_type}:{entity_id}, "
                       f"propagated to {len(related_entities)} related entities")
            
        except Exception as e:
            logger.error(f"Failed to process entity update event: {str(e)}")
    
    def _process_user_action_event(self, event: ProcessingEvent):
        """Process user actions and learn from feedback"""
        try:
            action_type = event.data['action_type']
            action_data = event.data['action_data']
            user_id = event.user_id
            
            # Learning from user feedback
            if action_type == 'insight_feedback':
                self._learn_from_insight_feedback(action_data, user_id)
            elif action_type == 'task_completion':
                self._learn_from_task_completion(action_data, user_id)
            elif action_type == 'topic_management':
                self._learn_from_topic_management(action_data, user_id)
            elif action_type == 'relationship_update':
                self._learn_from_relationship_update(action_data, user_id)
            
            logger.debug(f"Processed user action: {action_type} for user {user_id}")
            
        except Exception as e:
            logger.error(f"Failed to process user action event: {str(e)}")
    
    def _process_scheduled_analysis_event(self, event: ProcessingEvent):
        """Process scheduled proactive analysis"""
        try:
            user_id = event.user_id
            analysis_type = event.data.get('analysis_type', 'proactive_insights')
            
            if analysis_type == 'proactive_insights':
                # Generate proactive insights
                insights = entity_engine.generate_proactive_insights(user_id)
                
                if insights:
                    self._deliver_insights_to_user(user_id, insights)
                    logger.info(f"Generated {len(insights)} proactive insights for user {user_id}")
            
        except Exception as e:
            logger.error(f"Failed to process scheduled analysis: {str(e)}")
    
    # =====================================================================
    # INTELLIGENCE GENERATION METHODS
    # =====================================================================
    
    def _generate_immediate_insights(self, email_data: Dict, processing_result: Any, user_id: int) -> List[IntelligenceInsight]:
        """Generate immediate insights from new email processing"""
        insights = []
        
        try:
            # Insight 1: Important person contact
            sender = email_data.get('sender', '')
            if sender and self._is_important_person(sender, user_id):
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='important_contact',
                    title=f"New email from important contact",
                    description=f"Received email from {email_data.get('sender_name', sender)}. "
                               f"Subject: {email_data.get('subject', 'No subject')}",
                    priority='high',
                    confidence=0.9,
                    related_entity_type='person',
                    status='new'
                )
                insights.append(insight)
            
            # Insight 2: Urgent task detection
            if processing_result.entities_created.get('tasks', 0) > 0:
                # Check if any high-priority tasks were created
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='urgent_task',
                    title=f"New tasks extracted from email",
                    description=f"Created {processing_result.entities_created['tasks']} tasks from recent email. "
                               f"Review and prioritize action items.",
                    priority='medium',
                    confidence=0.8,
                    related_entity_type='task',
                    status='new'
                )
                insights.append(insight)
            
            # Insight 3: Topic momentum detection
            if processing_result.entities_created.get('topics', 0) > 0 or processing_result.entities_updated.get('topics', 0) > 0:
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='topic_momentum',
                    title=f"Business topic activity detected",
                    description=f"Recent email activity relates to your business topics. "
                               f"Consider scheduling focused time for strategic planning.",
                    priority='medium',
                    confidence=0.7,
                    related_entity_type='topic',
                    status='new'
                )
                insights.append(insight)
            
        except Exception as e:
            logger.error(f"Failed to generate immediate insights: {str(e)}")
        
        return insights
    
    def _generate_meeting_prep_insights(self, event_data: Dict, processing_result: Any, user_id: int) -> List[IntelligenceInsight]:
        """Generate meeting preparation insights"""
        insights = []
        
        try:
            meeting_title = event_data.get('title', 'Unknown Meeting')
            meeting_time = event_data.get('start_time')
            
            # Calculate time until meeting
            if meeting_time:
                time_until = meeting_time - datetime.utcnow()
                
                if time_until.total_seconds() > 0 and time_until.days <= 2:  # Within 48 hours
                    # High-priority preparation insight
                    insight = IntelligenceInsight(
                        user_id=user_id,
                        insight_type='meeting_prep',
                        title=f"Prepare for '{meeting_title}'",
                        description=f"Meeting in {time_until.days} days, {time_until.seconds // 3600} hours. "
                                   f"AI has generated preparation tasks based on attendee intelligence.",
                        priority='high' if time_until.days == 0 else 'medium',
                        confidence=0.9,
                        related_entity_type='event',
                        status='new',
                        expires_at=meeting_time
                    )
                    insights.append(insight)
            
            # Insight about preparation tasks created
            if processing_result.entities_created.get('tasks', 0) > 0:
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='prep_tasks_generated',
                    title=f"Meeting preparation tasks created",
                    description=f"Generated {processing_result.entities_created['tasks']} preparation tasks "
                               f"for '{meeting_title}' based on your email history with attendees.",
                    priority='medium',
                    confidence=0.8,
                    related_entity_type='task',
                    status='new'
                )
                insights.append(insight)
            
        except Exception as e:
            logger.error(f"Failed to generate meeting prep insights: {str(e)}")
        
        return insights
    
    def _generate_entity_update_insights(self, entity_type: str, entity_id: int, update_data: Dict, user_id: int) -> List[IntelligenceInsight]:
        """Generate insights from entity updates"""
        insights = []
        
        try:
            if entity_type == 'topic' and update_data.get('mentions', 0) > 0:
                # Topic becoming hot
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='topic_momentum',
                    title=f"Topic gaining momentum",
                    description=f"Business topic receiving increased attention. "
                               f"Consider preparing materials or scheduling focused discussion.",
                    priority='medium',
                    confidence=0.7,
                    related_entity_type='topic',
                    related_entity_id=entity_id,
                    status='new'
                )
                insights.append(insight)
            
            elif entity_type == 'person' and update_data.get('interaction'):
                # Relationship activity
                insight = IntelligenceInsight(
                    user_id=user_id,
                    insight_type='relationship_activity',
                    title=f"Recent contact activity",
                    description=f"Ongoing communication with important contact. "
                               f"Relationship engagement is active.",
                    priority='low',
                    confidence=0.6,
                    related_entity_type='person',
                    related_entity_id=entity_id,
                    status='new'
                )
                insights.append(insight)
            
        except Exception as e:
            logger.error(f"Failed to generate entity update insights: {str(e)}")
        
        return insights
    
    # =====================================================================
    # CONTEXT MANAGEMENT AND CACHING
    # =====================================================================
    
    def _get_cached_user_context(self, user_id: int) -> Dict:
        """Get cached user context for efficient processing"""
        if user_id not in self.user_contexts:
            # Load context from enhanced AI processor
            context = enhanced_ai_processor._gather_user_context(user_id)
            self.user_contexts[user_id] = {
                'context': context,
                'last_updated': datetime.utcnow(),
                'version': 1
            }
        else:
            # Check if context needs refresh (every 30 minutes)
            cached = self.user_contexts[user_id]
            if datetime.utcnow() - cached['last_updated'] > timedelta(minutes=30):
                context = enhanced_ai_processor._gather_user_context(user_id)
                cached['context'] = context
                cached['last_updated'] = datetime.utcnow()
                cached['version'] += 1
        
        return self.user_contexts[user_id]['context']
    
    def _update_cached_context(self, user_id: int, processing_result: Any):
        """Update cached context with new processing results"""
        if user_id not in self.user_contexts:
            return
        
        cached = self.user_contexts[user_id]
        
        # Update context with new entities
        if hasattr(processing_result, 'entities_created'):
            # This would update the cached context with newly created entities
            # Implementation would depend on the specific structure
            cached['last_updated'] = datetime.utcnow()
            cached['version'] += 1
    
    def _check_cross_entity_augmentations(self, processing_result: Any, user_id: int):
        """Check for cross-entity augmentations from new processing"""
        try:
            # Example: If we found a new person in email, check if they appear in upcoming calendar events
            # This would augment those calendar events with the new person intelligence
            pass
        except Exception as e:
            logger.error(f"Failed to check cross-entity augmentations: {str(e)}")
    
    def _find_related_entities(self, entity_type: str, entity_id: int, user_id: int) -> List[Dict]:
        """Find entities related to the updated entity"""
        related_entities = []
        
        try:
            from models.database import get_db_manager
            from models.enhanced_models import EntityRelationship
            
            with get_db_manager().get_session() as session:
                # Find direct relationships
                relationships = session.query(EntityRelationship).filter(
                    EntityRelationship.user_id == user_id,
                    ((EntityRelationship.entity_type_a == entity_type) & (EntityRelationship.entity_id_a == entity_id)) |
                    ((EntityRelationship.entity_type_b == entity_type) & (EntityRelationship.entity_id_b == entity_id))
                ).all()
                
                for rel in relationships:
                    if rel.entity_type_a == entity_type and rel.entity_id_a == entity_id:
                        related_entities.append({
                            'type': rel.entity_type_b,
                            'id': rel.entity_id_b,
                            'relationship': rel.relationship_type
                        })
                    else:
                        related_entities.append({
                            'type': rel.entity_type_a,
                            'id': rel.entity_id_a,
                            'relationship': rel.relationship_type
                        })
            
        except Exception as e:
            logger.error(f"Failed to find related entities: {str(e)}")
        
        return related_entities
    
    def _propagate_intelligence_update(self, target_entity_type: str, target_entity_id: int, 
                                     source_entity_type: str, source_entity_id: int, 
                                     update_data: Dict, user_id: int):
        """Propagate intelligence updates to related entities"""
        try:
            # Create propagation context
            context = EntityContext(
                source_type='propagation',
                user_id=user_id,
                confidence=0.7,
                processing_metadata={
                    'source_entity': f"{source_entity_type}:{source_entity_id}",
                    'propagation_data': update_data
                }
            )
            
            # Determine what intelligence to propagate based on entity types
            propagation_data = {}
            
            if source_entity_type == 'topic' and target_entity_type == 'person':
                # Topic update affecting person
                propagation_data = {
                    'topic_activity': True,
                    'related_topic_update': update_data
                }
            elif source_entity_type == 'person' and target_entity_type == 'topic':
                # Person update affecting topic
                propagation_data = {
                    'person_interaction': True,
                    'related_person_update': update_data
                }
            
            if propagation_data:
                entity_engine.augment_entity_from_source(
                    target_entity_type, target_entity_id, propagation_data, context
                )
            
        except Exception as e:
            logger.error(f"Failed to propagate intelligence update: {str(e)}")
    
    # =====================================================================
    # USER FEEDBACK AND LEARNING
    # =====================================================================
    
    def _learn_from_insight_feedback(self, feedback_data: Dict, user_id: int):
        """Learn from user feedback on insights"""
        try:
            insight_id = feedback_data.get('insight_id')
            feedback_type = feedback_data.get('feedback')  # helpful, not_helpful, etc.
            
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                insight = session.query(IntelligenceInsight).filter(
                    IntelligenceInsight.id == insight_id,
                    IntelligenceInsight.user_id == user_id
                ).first()
                
                if insight:
                    insight.user_feedback = feedback_type
                    insight.updated_at = datetime.utcnow()
                    session.commit()
                    
                    # Adjust future insight generation based on feedback
                    self._adjust_insight_generation(insight.insight_type, feedback_type, user_id)
            
        except Exception as e:
            logger.error(f"Failed to learn from insight feedback: {str(e)}")
    
    def _learn_from_task_completion(self, completion_data: Dict, user_id: int):
        """Learn from task completion patterns"""
        try:
            task_id = completion_data.get('task_id')
            completion_time = completion_data.get('completion_time')
            
            # This would analyze task completion patterns to improve future task extraction
            # For example: tasks that take longer than estimated, tasks that are never completed, etc.
            
        except Exception as e:
            logger.error(f"Failed to learn from task completion: {str(e)}")
    
    def _learn_from_topic_management(self, topic_data: Dict, user_id: int):
        """Learn from user topic management actions"""
        try:
            action = topic_data.get('action')  # create, merge, delete, etc.
            
            # This would learn user preferences for topic organization
            # and improve future topic extraction and categorization
            
        except Exception as e:
            logger.error(f"Failed to learn from topic management: {str(e)}")
    
    def _learn_from_relationship_update(self, relationship_data: Dict, user_id: int):
        """Learn from relationship updates"""
        try:
            # Learn how users categorize and prioritize relationships
            # to improve future relationship intelligence
            pass
            
        except Exception as e:
            logger.error(f"Failed to learn from relationship update: {str(e)}")
    
    def _adjust_insight_generation(self, insight_type: str, feedback: str, user_id: int):
        """Adjust future insight generation based on user feedback"""
        # This would implement adaptive insight generation
        # For example: if user consistently marks "relationship_alert" as not helpful,
        # reduce frequency or adjust criteria for that insight type
        pass
    
    # =====================================================================
    # UTILITY METHODS
    # =====================================================================
    
    def _queue_event(self, event: ProcessingEvent):
        """Queue event for processing"""
        # Priority queue uses tuple (priority, item)
        self.processing_queue.put((event.priority, event))
    
    def _get_active_users(self) -> List[int]:
        """Get users with recent activity for scheduled analysis"""
        try:
            from models.database import get_db_manager
            from models.enhanced_models import Email
            
            # Users with activity in last 24 hours
            cutoff = datetime.utcnow() - timedelta(hours=24)
            
            with get_db_manager().get_session() as session:
                # Get users with recent email processing
                active_user_ids = session.query(Email.user_id).filter(
                    Email.processed_at > cutoff
                ).distinct().all()
                
                return [user_id[0] for user_id in active_user_ids]
            
        except Exception as e:
            logger.error(f"Failed to get active users: {str(e)}")
            return []
    
    def _is_important_person(self, email: str, user_id: int) -> bool:
        """Check if person is marked as important"""
        try:
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                person = session.query(Person).filter(
                    Person.user_id == user_id,
                    Person.email_address == email.lower(),
                    Person.importance_level > 0.7
                ).first()
                
                return person is not None
                
        except Exception as e:
            logger.error(f"Failed to check person importance: {str(e)}")
            return False
    
    def _deliver_insights_to_user(self, user_id: int, insights: List[IntelligenceInsight]):
        """Deliver insights to user through registered callbacks"""
        if not insights:
            return
        
        try:
            # Store insights in database
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                for insight in insights:
                    session.add(insight)
                session.commit()
            
            # Deliver through callbacks (WebSocket, push notifications, etc.)
            if user_id in self.insight_callbacks:
                callback = self.insight_callbacks[user_id]
                callback(insights)
            
            logger.info(f"Delivered {len(insights)} insights to user {user_id}")
            
        except Exception as e:
            logger.error(f"Failed to deliver insights to user: {str(e)}")
    
    def register_insight_callback(self, user_id: int, callback):
        """Register callback for delivering insights to specific user"""
        self.insight_callbacks[user_id] = callback
    
    def unregister_insight_callback(self, user_id: int):
        """Unregister insight callback for user"""
        if user_id in self.insight_callbacks:
            del self.insight_callbacks[user_id]

# Global instance
realtime_processor = RealTimeProcessor() 

============================================================
FILE: chief_of_staff_ai/processors/task_extractor.py
============================================================
# Extract actionable tasks from emails using Claude 4 Sonnet

import json
import logging
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional
import re
from dateutil import parser
import anthropic

from config.settings import settings
from models.database import get_db_manager, Email, Task

logger = logging.getLogger(__name__)

class TaskExtractor:
    """Extracts actionable tasks from emails using Claude 4 Sonnet"""
    
    def __init__(self):
        from config.settings import settings
        
        self.claude_client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        self.model = settings.CLAUDE_MODEL  # Now uses Claude 4 Opus from settings
        self.version = "1.0"
        
    def extract_tasks_for_user(self, user_email: str, limit: int = None, force_refresh: bool = False) -> Dict:
        """
        ENHANCED 360-CONTEXT TASK EXTRACTION
        
        Extract tasks with comprehensive business intelligence by cross-referencing:
        - Email communications & AI analysis
        - People relationships & interaction patterns
        - Project context & status
        - Calendar events & meeting intelligence
        - Topic analysis & business themes
        - Strategic decisions & opportunities
        
        Creates super relevant and actionable tasks with full business context
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            # COMPREHENSIVE BUSINESS CONTEXT COLLECTION
            business_context = self._get_360_business_context(user.id)
            
            # Get normalized emails that need task extraction
            with get_db_manager().get_session() as session:
                query = session.query(Email).filter(
                    Email.user_id == user.id,
                    Email.body_clean.isnot(None)  # Already normalized
                )
                
                if not force_refresh:
                    # Only process emails that don't have tasks yet
                    query = query.filter(~session.query(Task).filter(
                        Task.email_id == Email.id
                    ).exists())
                
                emails = query.limit(limit or 50).all()
            
            if not emails:
                logger.info(f"No emails to process for 360-context task extraction for {user_email}")
                return {
                    'success': True,
                    'user_email': user_email,
                    'processed_emails': 0,
                    'extracted_tasks': 0,
                    'message': 'No emails need 360-context task extraction'
                }
            
            processed_emails = 0
            total_tasks = 0
            error_count = 0
            context_enhanced_tasks = 0
            
            for email in emails:
                try:
                    # Convert database email to dict for processing
                    email_dict = {
                        'id': email.gmail_id,
                        'subject': email.subject,
                        'sender': email.sender,
                        'sender_name': email.sender_name,
                        'body_clean': email.body_clean,
                        'body_preview': email.body_preview,
                        'timestamp': email.email_date,
                        'message_type': email.message_type,
                        'priority_score': email.priority_score,
                        'ai_summary': email.ai_summary,
                        'key_insights': email.key_insights,
                        'topics': email.topics
                    }
                    
                    # ENHANCED EXTRACTION with 360-context
                    extraction_result = self.extract_tasks_with_360_context(email_dict, business_context)
                    
                    if extraction_result['success'] and extraction_result['tasks']:
                        # Save tasks to database with enhanced context
                        for task_data in extraction_result['tasks']:
                            task_data['email_id'] = email.id
                            task_data['extractor_version'] = f"{self.version}_360_context"
                            task_data['model_used'] = self.model
                            
                            # Check if this task was context-enhanced
                            if task_data.get('context_enhanced'):
                                context_enhanced_tasks += 1
                            
                            get_db_manager().save_task(user.id, email.id, task_data)
                            total_tasks += 1
                    
                    processed_emails += 1
                    
                except Exception as e:
                    logger.error(f"Failed to extract 360-context tasks from email {email.gmail_id}: {str(e)}")
                    error_count += 1
                    continue
            
            logger.info(f"Extracted {total_tasks} tasks ({context_enhanced_tasks} context-enhanced) from {processed_emails} emails for {user_email} ({error_count} errors)")
            
            return {
                'success': True,
                'user_email': user_email,
                'processed_emails': processed_emails,
                'extracted_tasks': total_tasks,
                'context_enhanced_tasks': context_enhanced_tasks,
                'errors': error_count,
                'extractor_version': f"{self.version}_360_context"
            }
            
        except Exception as e:
            logger.error(f"Failed to extract 360-context tasks for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _get_360_business_context(self, user_id: int) -> Dict:
        """
        Collect comprehensive business intelligence context for task extraction
        """
        try:
            context = {
                'people': [],
                'projects': [],
                'topics': [],
                'calendar_events': [],
                'recent_decisions': [],
                'opportunities': [],
                'relationship_map': {},
                'project_map': {},
                'topic_keywords': {}
            }
            
            # Get business data
            people = get_db_manager().get_user_people(user_id, limit=100)
            projects = get_db_manager().get_user_projects(user_id, limit=50)
            topics = get_db_manager().get_user_topics(user_id, limit=50)
            calendar_events = get_db_manager().get_user_calendar_events(user_id, limit=50)
            emails = get_db_manager().get_user_emails(user_id, limit=100)
            
            # Process people for relationship context
            for person in people:
                if person.name and person.email_address:
                    person_info = {
                        'name': person.name,
                        'email': person.email_address,
                        'company': person.company,
                        'title': person.title,
                        'relationship': person.relationship_type,
                        'total_emails': person.total_emails or 0,
                        'importance': person.importance_level or 0.5
                    }
                    context['people'].append(person_info)
                    context['relationship_map'][person.email_address.lower()] = person_info
            
            # Process projects for context linking
            for project in projects:
                if project.name and project.status == 'active':
                    project_info = {
                        'name': project.name,
                        'description': project.description,
                        'status': project.status,
                        'priority': project.priority,
                        'stakeholders': project.stakeholders or []
                    }
                    context['projects'].append(project_info)
                    context['project_map'][project.name.lower()] = project_info
            
            # Process topics for keyword matching
            for topic in topics:
                if topic.name:
                    topic_info = {
                        'name': topic.name,
                        'description': topic.description,
                        'keywords': json.loads(topic.keywords) if topic.keywords else [],
                        'is_official': topic.is_official
                    }
                    context['topics'].append(topic_info)
                    # Build keyword map for topic detection
                    all_keywords = [topic.name.lower()] + [kw.lower() for kw in topic_info['keywords']]
                    for keyword in all_keywords:
                        if keyword not in context['topic_keywords']:
                            context['topic_keywords'][keyword] = []
                        context['topic_keywords'][keyword].append(topic_info)
            
            # Process calendar events for meeting context
            now = datetime.now(timezone.utc)
            upcoming_meetings = [e for e in calendar_events if e.start_time and e.start_time > now]
            for meeting in upcoming_meetings[:20]:  # Next 20 meetings
                meeting_info = {
                    'title': meeting.title,
                    'start_time': meeting.start_time,
                    'attendees': meeting.attendees or [],
                    'description': meeting.description
                }
                context['calendar_events'].append(meeting_info)
            
            # Extract recent decisions and opportunities from emails
            for email in emails[-30:]:  # Recent 30 emails
                if email.key_insights and isinstance(email.key_insights, dict):
                    decisions = email.key_insights.get('key_decisions', [])
                    context['recent_decisions'].extend(decisions[:2])  # Top 2 per email
                    
                    opportunities = email.key_insights.get('strategic_opportunities', [])
                    context['opportunities'].extend(opportunities[:2])  # Top 2 per email
            
            return context
            
        except Exception as e:
            logger.error(f"Failed to get 360-context for task extraction: {str(e)}")
            return {}
    
    def extract_tasks_with_360_context(self, email_data: Dict, business_context: Dict) -> Dict:
        """
        Extract actionable tasks with comprehensive 360-context intelligence
        
        Args:
            email_data: Normalized email data dictionary with AI analysis
            business_context: Comprehensive business intelligence context
            
        Returns:
            Dictionary containing extracted tasks with enhanced context
        """
        try:
            # Check if email has enough content for task extraction
            body_clean = email_data.get('body_clean', '')
            if not body_clean or len(body_clean.strip()) < 20:
                return {
                    'success': True,
                    'email_id': email_data.get('id'),
                    'tasks': [],
                    'reason': 'Email content too short for task extraction'
                }
            
            # Skip certain message types that unlikely contain tasks
            message_type = email_data.get('message_type', 'regular')
            if message_type in ['newsletter', 'automated']:
                return {
                    'success': True,
                    'email_id': email_data.get('id'),
                    'tasks': [],
                    'reason': f'Message type "{message_type}" skipped for task extraction'
                }
            
            # ANALYZE BUSINESS CONTEXT CONNECTIONS
            email_context = self._analyze_email_business_connections(email_data, business_context)
            
            # Prepare enhanced email context for Claude
            enhanced_email_context = self._prepare_360_email_context(email_data, email_context, business_context)
            
            # Call Claude for 360-context task extraction
            claude_response = self._call_claude_for_360_tasks(enhanced_email_context, email_context)
            
            if not claude_response:
                return {
                    'success': False,
                    'email_id': email_data.get('id'),
                    'error': 'Failed to get response from Claude for 360-context extraction'
                }
            
            # Parse Claude's response with context enhancement
            tasks = self._parse_claude_360_response(claude_response, email_data, email_context)
            
            # Enhance tasks with 360-context metadata
            enhanced_tasks = []
            for task in tasks:
                enhanced_task = self._enhance_task_with_360_context(task, email_data, email_context, business_context)
                enhanced_tasks.append(enhanced_task)
            
            return {
                'success': True,
                'email_id': email_data.get('id'),
                'tasks': enhanced_tasks,
                'extraction_metadata': {
                    'extracted_at': datetime.utcnow().isoformat(),
                    'extractor_version': f"{self.version}_360_context",
                    'model_used': self.model,
                    'email_priority': email_data.get('priority_score', 0.5),
                    'context_connections': email_context.get('connection_count', 0),
                    'business_intelligence_used': True
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to extract 360-context tasks from email {email_data.get('id', 'unknown')}: {str(e)}")
            return {
                'success': False,
                'email_id': email_data.get('id'),
                'error': str(e)
            }
    
    def _prepare_email_context(self, email_data: Dict) -> str:
        """
        Prepare email context for Claude task extraction
        
        Args:
            email_data: Email data dictionary
            
        Returns:
            Formatted email context string
        """
        sender = email_data.get('sender_name') or email_data.get('sender', '')
        subject = email_data.get('subject', '')
        body = email_data.get('body_clean', '')
        timestamp = email_data.get('timestamp')
        
        # Format timestamp
        if timestamp:
            try:
                if isinstance(timestamp, str):
                    timestamp = parser.parse(timestamp)
                date_str = timestamp.strftime('%Y-%m-%d %H:%M')
            except:
                date_str = 'Unknown date'
        else:
            date_str = 'Unknown date'
        
        context = f"""Email Details:
From: {sender}
Date: {date_str}
Subject: {subject}

Email Content:
{body}
"""
        return context
    
    def _call_claude_for_tasks(self, email_context: str) -> Optional[str]:
        """
        Call Claude 4 Sonnet to extract tasks from email
        
        Args:
            email_context: Formatted email context
            
        Returns:
            Claude's response or None if failed
        """
        try:
            system_prompt = """You are an expert AI assistant that extracts actionable tasks from emails. Your job is to identify specific tasks, action items, deadlines, and follow-ups from email content.

Please analyze the email and extract actionable tasks following these guidelines:

1. **Task Identification**: Look for:
   - Direct requests or assignments
   - Deadlines and due dates
   - Follow-up actions needed
   - Meetings to schedule or attend
   - Documents to review or create
   - Decisions to make
   - Items requiring response

2. **Task Details**: For each task, identify:
   - Clear description of what needs to be done
   - Who is responsible (assignee)
   - When it needs to be done (due date/deadline)
   - Priority level (high, medium, low)
   - Category (follow-up, deadline, meeting, review, etc.)

3. **Response Format**: Return a JSON array of tasks. Each task should have:
   - "description": Clear, actionable description
   - "assignee": Who should do this (if mentioned)
   - "due_date": Specific date if mentioned (YYYY-MM-DD format)
   - "due_date_text": Original due date text from email
   - "priority": high/medium/low based on urgency and importance
   - "category": type of task (follow-up, deadline, meeting, review, etc.)
   - "confidence": 0.0-1.0 confidence score
   - "source_text": Original text from email that led to this task

Return ONLY the JSON array. If no actionable tasks are found, return an empty array []."""

            user_prompt = f"""Please analyze this email and extract actionable tasks:

{email_context}

Remember to return only a JSON array of tasks, or an empty array [] if no actionable tasks are found."""

            message = self.claude_client.messages.create(
                model=self.model,
                max_tokens=2000,
                temperature=0.1,
                system=system_prompt,
                messages=[
                    {
                        "role": "user",
                        "content": user_prompt
                    }
                ]
            )
            
            response_text = message.content[0].text.strip()
            logger.debug(f"Claude response: {response_text}")
            
            return response_text
            
        except Exception as e:
            logger.error(f"Failed to call Claude for task extraction: {str(e)}")
            return None
    
    def _parse_claude_response(self, response: str, email_data: Dict) -> List[Dict]:
        """
        Parse Claude's JSON response into task dictionaries
        
        Args:
            response: Claude's response text
            email_data: Original email data
            
        Returns:
            List of task dictionaries
        """
        try:
            # Try to find JSON in the response
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            
            if json_start == -1 or json_end == 0:
                logger.warning("No JSON array found in Claude response")
                return []
            
            json_text = response[json_start:json_end]
            
            # Parse JSON
            tasks_data = json.loads(json_text)
            
            if not isinstance(tasks_data, list):
                logger.warning("Claude response is not a JSON array")
                return []
            
            tasks = []
            for task_data in tasks_data:
                if isinstance(task_data, dict) and task_data.get('description'):
                    tasks.append(task_data)
            
            logger.info(f"Parsed {len(tasks)} tasks from Claude response")
            return tasks
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Claude JSON response: {str(e)}")
            logger.error(f"Response was: {response}")
            return []
        except Exception as e:
            logger.error(f"Failed to parse Claude response: {str(e)}")
            return []
    
    def _enhance_task(self, task: Dict, email_data: Dict) -> Dict:
        """
        Enhance task with additional metadata and processing
        
        Args:
            task: Task dictionary from Claude
            email_data: Original email data
            
        Returns:
            Enhanced task dictionary
        """
        try:
            # Start with Claude's task data
            enhanced_task = task.copy()
            
            # Parse due date if provided
            if task.get('due_date'):
                try:
                    # Try to parse various date formats
                    due_date = parser.parse(task['due_date'])
                    enhanced_task['due_date'] = due_date
                except:
                    # If parsing fails, try to extract from due_date_text
                    enhanced_task['due_date'] = self._extract_date_from_text(
                        task.get('due_date_text', '')
                    )
            elif task.get('due_date_text'):
                enhanced_task['due_date'] = self._extract_date_from_text(task['due_date_text'])
            
            # Set default values
            enhanced_task['priority'] = task.get('priority', 'medium').lower()
            enhanced_task['category'] = task.get('category', 'action_item')
            enhanced_task['confidence'] = min(1.0, max(0.0, task.get('confidence', 0.8)))
            enhanced_task['status'] = 'pending'
            
            # Determine assignee context
            if not enhanced_task.get('assignee'):
                # If no specific assignee mentioned, assume it's for the email recipient
                enhanced_task['assignee'] = 'me'
            
            # Enhance priority based on email priority and urgency
            email_priority = email_data.get('priority_score', 0.5)
            if email_priority > 0.8:
                if enhanced_task['priority'] == 'medium':
                    enhanced_task['priority'] = 'high'
            
            # Add contextual category if not specified
            if enhanced_task['category'] == 'action_item':
                enhanced_task['category'] = self._determine_category(
                    enhanced_task['description'], 
                    email_data
                )
            
            return enhanced_task
            
        except Exception as e:
            logger.error(f"Failed to enhance task: {str(e)}")
            return task
    
    def _extract_date_from_text(self, text: str) -> Optional[datetime]:
        """
        Extract date from text using various patterns
        
        Args:
            text: Text that might contain a date
            
        Returns:
            Parsed datetime or None
        """
        if not text:
            return None
        
        try:
            # Try direct parsing first
            return parser.parse(text, fuzzy=True)
        except:
            pass
        
        # Try common patterns
        patterns = [
            r'(\d{1,2}/\d{1,2}/\d{4})',
            r'(\d{1,2}-\d{1,2}-\d{4})',
            r'(\w+\s+\d{1,2},?\s+\d{4})',
            r'(next\s+\w+)',
            r'(tomorrow)',
            r'(today)',
            r'(this\s+week)',
            r'(next\s+week)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text.lower())
            if match:
                try:
                    return parser.parse(match.group(1), fuzzy=True)
                except:
                    continue
        
        return None
    
    def _determine_category(self, description: str, email_data: Dict) -> str:
        """
        Determine task category based on description and email context
        
        Args:
            description: Task description
            email_data: Email context
            
        Returns:
            Task category
        """
        description_lower = description.lower()
        subject = email_data.get('subject', '').lower()
        
        # Meeting-related tasks
        if any(keyword in description_lower for keyword in ['meeting', 'call', 'schedule', 'zoom', 'teams']):
            return 'meeting'
        
        # Review tasks
        if any(keyword in description_lower for keyword in ['review', 'check', 'look at', 'examine']):
            return 'review'
        
        # Response tasks
        if any(keyword in description_lower for keyword in ['reply', 'respond', 'answer', 'get back']):
            return 'follow-up'
        
        # Document tasks
        if any(keyword in description_lower for keyword in ['document', 'report', 'write', 'create', 'draft']):
            return 'document'
        
        # Decision tasks
        if any(keyword in description_lower for keyword in ['decide', 'choose', 'approve', 'confirm']):
            return 'decision'
        
        # Deadline tasks
        if any(keyword in description_lower for keyword in ['deadline', 'due', 'submit', 'deliver']):
            return 'deadline'
        
        return 'action_item'
    
    def get_user_tasks(self, user_email: str, status: str = None, limit: int = None) -> Dict:
        """
        Get extracted tasks for a user
        
        Args:
            user_email: Email of the user
            status: Filter by task status (pending, in_progress, completed)
            limit: Maximum number of tasks to return
            
        Returns:
            Dictionary with user tasks
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            tasks = get_db_manager().get_user_tasks(user.id, status)
            
            if limit:
                tasks = tasks[:limit]
            
            return {
                'success': True,
                'user_email': user_email,
                'tasks': [task.to_dict() for task in tasks],
                'count': len(tasks),
                'status_filter': status
            }
            
        except Exception as e:
            logger.error(f"Failed to get tasks for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def update_task_status(self, user_email: str, task_id: int, status: str) -> Dict:
        """
        Update task status
        
        Args:
            user_email: Email of the user
            task_id: ID of the task to update
            status: New status (pending, in_progress, completed, cancelled)
            
        Returns:
            Dictionary with update result
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            with get_db_manager().get_session() as session:
                task = session.query(Task).filter(
                    Task.id == task_id,
                    Task.user_id == user.id
                ).first()
                
                if not task:
                    return {'success': False, 'error': 'Task not found'}
                
                task.status = status
                task.updated_at = datetime.utcnow()
                
                if status == 'completed':
                    task.completed_at = datetime.utcnow()
                
                session.commit()
                
                return {
                    'success': True,
                    'task_id': task_id,
                    'new_status': status,
                    'updated_at': task.updated_at.isoformat()
                }
            
        except Exception as e:
            logger.error(f"Failed to update task {task_id} for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _analyze_email_business_connections(self, email_data: Dict, business_context: Dict) -> Dict:
        """
        Analyze connections between email and business intelligence context
        """
        try:
            connections = {
                'related_people': [],
                'related_projects': [],
                'related_topics': [],
                'related_meetings': [],
                'connection_count': 0,
                'context_strength': 0.0
            }
            
            sender_email = email_data.get('sender', '').lower()
            subject = (email_data.get('subject') or '').lower()
            body = (email_data.get('body_clean') or '').lower()
            ai_summary = (email_data.get('ai_summary') or '').lower()
            email_topics = email_data.get('topics') or []
            
            # Find related people
            if sender_email in business_context.get('relationship_map', {}):
                person_info = business_context['relationship_map'][sender_email]
                connections['related_people'].append(person_info)
                connections['connection_count'] += 1
                connections['context_strength'] += person_info.get('importance', 0.5)
            
            # Find related projects
            for project_name, project_info in business_context.get('project_map', {}).items():
                if (project_name in subject or project_name in body or project_name in ai_summary):
                    connections['related_projects'].append(project_info)
                    connections['connection_count'] += 1
                    connections['context_strength'] += 0.8  # High value for project connection
            
            # Find related topics
            for topic in email_topics:
                topic_lower = topic.lower()
                if topic_lower in business_context.get('topic_keywords', {}):
                    topic_infos = business_context['topic_keywords'][topic_lower]
                    connections['related_topics'].extend(topic_infos)
                    connections['connection_count'] += len(topic_infos)
                    connections['context_strength'] += 0.6 * len(topic_infos)
            
            # Find related upcoming meetings
            for meeting in business_context.get('calendar_events', []):
                meeting_attendees = meeting.get('attendees', [])
                meeting_title = meeting.get('title', '').lower()
                
                # Check if sender is in meeting attendees
                if any(att.get('email', '').lower() == sender_email for att in meeting_attendees):
                    connections['related_meetings'].append(meeting)
                    connections['connection_count'] += 1
                    connections['context_strength'] += 0.7
                
                # Check if meeting title relates to email subject/content
                if any(keyword in meeting_title for keyword in subject.split() + body.split()[:20] if len(keyword) > 3):
                    if meeting not in connections['related_meetings']:
                        connections['related_meetings'].append(meeting)
                        connections['connection_count'] += 1
                        connections['context_strength'] += 0.5
            
            # Normalize context strength
            connections['context_strength'] = min(1.0, connections['context_strength'] / max(1, connections['connection_count']))
            
            return connections
            
        except Exception as e:
            logger.error(f"Failed to analyze email business connections: {str(e)}")
            return {'related_people': [], 'related_projects': [], 'related_topics': [], 'related_meetings': [], 'connection_count': 0, 'context_strength': 0.0}
    
    def _prepare_360_email_context(self, email_data: Dict, email_context: Dict, business_context: Dict) -> str:
        """
        Prepare comprehensive email context with business intelligence for Claude
        """
        try:
            sender = email_data.get('sender_name') or email_data.get('sender', '')
            subject = email_data.get('subject', '')
            body = email_data.get('body_clean', '')
            ai_summary = email_data.get('ai_summary', '')
            timestamp = email_data.get('timestamp')
            
            # Format timestamp
            if timestamp:
                try:
                    if isinstance(timestamp, str):
                        timestamp = parser.parse(timestamp)
                    date_str = timestamp.strftime('%Y-%m-%d %H:%M')
                except:
                    date_str = 'Unknown date'
            else:
                date_str = 'Unknown date'
            
            # Build business context summary
            context_elements = []
            
            # Add people context
            if email_context['related_people']:
                people_info = []
                for person in email_context['related_people']:
                    people_info.append(f"{person['name']} ({person.get('company', 'Unknown company')}) - {person.get('total_emails', 0)} previous interactions")
                context_elements.append(f"RELATED PEOPLE: {'; '.join(people_info)}")
            
            # Add project context
            if email_context['related_projects']:
                project_info = []
                for project in email_context['related_projects']:
                    project_info.append(f"{project['name']} (Status: {project.get('status', 'Unknown')}, Priority: {project.get('priority', 'Unknown')})")
                context_elements.append(f"RELATED PROJECTS: {'; '.join(project_info)}")
            
            # Add topic context
            if email_context['related_topics']:
                topic_names = [topic['name'] for topic in email_context['related_topics'] if topic.get('is_official')]
                if topic_names:
                    context_elements.append(f"RELATED BUSINESS TOPICS: {', '.join(topic_names)}")
            
            # Add meeting context
            if email_context['related_meetings']:
                meeting_info = []
                for meeting in email_context['related_meetings']:
                    meeting_date = meeting['start_time'].strftime('%Y-%m-%d %H:%M') if meeting.get('start_time') else 'TBD'
                    meeting_info.append(f"{meeting['title']} ({meeting_date})")
                context_elements.append(f"RELATED UPCOMING MEETINGS: {'; '.join(meeting_info)}")
            
            # Add strategic insights
            if business_context.get('recent_decisions'):
                recent_decisions = business_context['recent_decisions'][:3]
                context_elements.append(f"RECENT BUSINESS DECISIONS: {'; '.join(recent_decisions)}")
            
            if business_context.get('opportunities'):
                opportunities = business_context['opportunities'][:3]
                context_elements.append(f"STRATEGIC OPPORTUNITIES: {'; '.join(opportunities)}")
            
            business_intelligence = '\n'.join(context_elements) if context_elements else "No specific business context identified."
            
            enhanced_context = f"""Email Details:
From: {sender}
Date: {date_str}
Subject: {subject}

AI Summary: {ai_summary}

Email Content:
{body}

BUSINESS INTELLIGENCE CONTEXT:
{business_intelligence}

Context Strength: {email_context.get('context_strength', 0.0):.2f} (0.0 = no context, 1.0 = highly connected)
"""
            return enhanced_context
            
        except Exception as e:
            logger.error(f"Failed to prepare 360-context email: {str(e)}")
            return self._prepare_email_context(email_data)
    
    def _call_claude_for_360_tasks(self, enhanced_email_context: str, email_context: Dict) -> Optional[str]:
        """
        Call Claude 4 Sonnet for 360-context task extraction with business intelligence
        """
        try:
            context_strength = email_context.get('context_strength', 0.0)
            connection_count = email_context.get('connection_count', 0)
            
            system_prompt = f"""You are an expert AI Chief of Staff that extracts actionable tasks from emails using comprehensive business intelligence context. You have access to the user's complete business ecosystem including relationships, projects, topics, and strategic insights.

BUSINESS INTELLIGENCE CAPABILITIES:
- Cross-reference people relationships and interaction history
- Connect tasks to active projects and strategic initiatives  
- Leverage topic analysis and business themes
- Consider upcoming meetings and calendar context
- Incorporate recent business decisions and opportunities

ENHANCED TASK EXTRACTION GUIDELINES:

1. **360-Context Task Identification**: Look for tasks that:
   - Connect to the business relationships and projects mentioned
   - Align with strategic opportunities and recent decisions
   - Prepare for upcoming meetings with related attendees
   - Advance active projects and business initiatives
   - Leverage the full business context for maximum relevance

2. **Business-Aware Task Details**: For each task, provide:
   - Clear, actionable description with business context
   - Connect to specific people, projects, or meetings when relevant
   - Priority based on business importance and relationships
   - Category that reflects business context (project_work, relationship_management, strategic_planning, etc.)
   - Due dates that consider business timing and meeting schedules

3. **Context Enhancement Indicators**: 
   - Mark tasks as "context_enhanced": true if they leverage business intelligence
   - Include "business_context" field explaining the connection
   - Add "stakeholders" field if specific people are involved
   - Include "project_connection" if tied to active projects

Current Email Context Strength: {context_strength:.2f} ({connection_count} business connections identified)

RESPONSE FORMAT: Return a JSON array of tasks. Each task should have:
- "description": Clear, actionable description with business context
- "assignee": Who should do this (considering business relationships)
- "due_date": Specific date if mentioned (YYYY-MM-DD format)
- "due_date_text": Original due date text from email
- "priority": high/medium/low (elevated if high business context)
- "category": business-aware category (project_work, relationship_management, meeting_prep, strategic_planning, etc.)
- "confidence": 0.0-1.0 confidence score (higher with business context)
- "source_text": Original text from email that led to this task
- "context_enhanced": true/false (true if business intelligence was used)
- "business_context": Explanation of business connections (if context_enhanced)
- "stakeholders": List of relevant people from business context
- "project_connection": Name of related project if applicable

Return ONLY the JSON array. If no actionable tasks are found, return an empty array []."""

            user_prompt = f"""Please analyze this email with full business intelligence context and extract actionable tasks:

{enhanced_email_context}

Focus on tasks that leverage the business context for maximum relevance and strategic value. Consider the relationships, projects, meetings, and strategic insights provided."""

            message = self.claude_client.messages.create(
                model=self.model,
                max_tokens=3000,  # More tokens for detailed context
                temperature=0.1,
                system=system_prompt,
                messages=[
                    {
                        "role": "user",
                        "content": user_prompt
                    }
                ]
            )
            
            response_text = message.content[0].text.strip()
            logger.debug(f"Claude 360-context response: {response_text}")
            
            return response_text
            
        except Exception as e:
            logger.error(f"Failed to call Claude for 360-context task extraction: {str(e)}")
            return None
    
    def _parse_claude_360_response(self, response: str, email_data: Dict, email_context: Dict) -> List[Dict]:
        """
        Parse Claude's 360-context JSON response into enhanced task dictionaries
        """
        try:
            # Try to find JSON in the response
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            
            if json_start == -1 or json_end == 0:
                logger.warning("No JSON array found in Claude 360-context response")
                return []
            
            json_text = response[json_start:json_end]
            
            # Parse JSON
            tasks_data = json.loads(json_text)
            
            if not isinstance(tasks_data, list):
                logger.warning("Claude 360-context response is not a JSON array")
                return []
            
            tasks = []
            for task_data in tasks_data:
                if isinstance(task_data, dict) and task_data.get('description'):
                    # Validate 360-context fields
                    if task_data.get('context_enhanced') and not task_data.get('business_context'):
                        task_data['business_context'] = f"Connected to {email_context.get('connection_count', 0)} business elements"
                    
                    tasks.append(task_data)
            
            logger.info(f"Parsed {len(tasks)} 360-context tasks from Claude response")
            return tasks
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Claude 360-context JSON response: {str(e)}")
            logger.error(f"Response was: {response}")
            return []
        except Exception as e:
            logger.error(f"Failed to parse Claude 360-context response: {str(e)}")
            return []
    
    def _enhance_task_with_360_context(self, task: Dict, email_data: Dict, email_context: Dict, business_context: Dict) -> Dict:
        """
        Enhance task with comprehensive 360-context metadata and business intelligence
        """
        try:
            # Start with Claude's task data
            enhanced_task = task.copy()
            
            # Parse due date if provided
            if task.get('due_date'):
                try:
                    due_date = parser.parse(task['due_date'])
                    enhanced_task['due_date'] = due_date
                except:
                    enhanced_task['due_date'] = self._extract_date_from_text(
                        task.get('due_date_text', '')
                    )
            elif task.get('due_date_text'):
                enhanced_task['due_date'] = self._extract_date_from_text(task['due_date_text'])
            
            # Set default values with 360-context awareness
            enhanced_task['priority'] = task.get('priority', 'medium').lower()
            enhanced_task['category'] = task.get('category', 'action_item')
            enhanced_task['confidence'] = min(1.0, max(0.0, task.get('confidence', 0.8)))
            enhanced_task['status'] = 'pending'
            
            # Enhance based on business context strength
            context_strength = email_context.get('context_strength', 0.0)
            if context_strength > 0.7:  # High context strength
                if enhanced_task['priority'] == 'medium':
                    enhanced_task['priority'] = 'high'
                enhanced_task['confidence'] = min(1.0, enhanced_task['confidence'] + 0.1)
            
            # Determine assignee with business context
            if not enhanced_task.get('assignee'):
                # Check if specific people are mentioned in business context
                related_people = email_context.get('related_people', [])
                if related_people and len(related_people) == 1:
                    enhanced_task['assignee'] = related_people[0]['name']
                else:
                    enhanced_task['assignee'] = 'me'
            
            # Enhance category with business context
            if enhanced_task['category'] == 'action_item':
                enhanced_task['category'] = self._determine_360_category(
                    enhanced_task['description'], 
                    email_data,
                    email_context
                )
            
            # Add 360-context specific fields
            if task.get('context_enhanced'):
                enhanced_task['context_enhanced'] = True
                enhanced_task['business_context'] = task.get('business_context', 'Business intelligence context applied')
                enhanced_task['context_strength'] = context_strength
                enhanced_task['connection_count'] = email_context.get('connection_count', 0)
            
            # Add stakeholder information
            stakeholders = task.get('stakeholders', [])
            if not stakeholders and email_context.get('related_people'):
                stakeholders = [person['name'] for person in email_context['related_people']]
            enhanced_task['stakeholders'] = stakeholders
            
            # Add project connection
            if task.get('project_connection'):
                enhanced_task['project_connection'] = task['project_connection']
            elif email_context.get('related_projects'):
                enhanced_task['project_connection'] = email_context['related_projects'][0]['name']
            
            return enhanced_task
            
        except Exception as e:
            logger.error(f"Failed to enhance task with 360-context: {str(e)}")
            return task
    
    def _determine_360_category(self, description: str, email_data: Dict, email_context: Dict) -> str:
        """
        Determine task category with 360-context business intelligence
        """
        try:
            description_lower = description.lower()
            subject = email_data.get('subject', '').lower()
            
            # Business context-aware categorization
            if email_context.get('related_projects'):
                return 'project_work'
            
            if email_context.get('related_meetings'):
                return 'meeting_prep'
            
            if email_context.get('related_people') and len(email_context['related_people']) > 0:
                person = email_context['related_people'][0]
                if person.get('importance', 0) > 0.7:
                    return 'relationship_management'
            
            # Strategic context
            if any(keyword in description_lower for keyword in ['strategy', 'strategic', 'decision', 'opportunity']):
                return 'strategic_planning'
            
            # Default categorization with business awareness
            if any(keyword in description_lower for keyword in ['meeting', 'call', 'schedule', 'zoom', 'teams']):
                return 'meeting'
            
            if any(keyword in description_lower for keyword in ['review', 'check', 'look at', 'examine']):
                return 'review'
            
            if any(keyword in description_lower for keyword in ['reply', 'respond', 'answer', 'get back']):
                return 'follow-up'
            
            if any(keyword in description_lower for keyword in ['document', 'report', 'write', 'create', 'draft']):
                return 'document'
            
            if any(keyword in description_lower for keyword in ['decide', 'choose', 'approve', 'confirm']):
                return 'decision'
            
            if any(keyword in description_lower for keyword in ['deadline', 'due', 'submit', 'deliver']):
                return 'deadline'
            
            return 'action_item'
            
        except Exception as e:
            logger.error(f"Failed to determine 360-context category: {str(e)}")
            return 'action_item'
    
    def extract_tasks_from_email(self, email_data: Dict) -> Dict:
        """
        LEGACY METHOD: Extract actionable tasks from a single email using Claude 4 Sonnet
        This method is kept for backward compatibility but users should use extract_tasks_with_360_context
        """
        try:
            logger.warning("Using legacy task extraction - consider upgrading to 360-context extraction")
            
            # Check if email has enough content for task extraction
            body_clean = email_data.get('body_clean', '')
            if not body_clean or len(body_clean.strip()) < 20:
                return {
                    'success': True,
                    'email_id': email_data.get('id'),
                    'tasks': [],
                    'reason': 'Email content too short for task extraction'
                }
            
            # Skip certain message types that unlikely contain tasks
            message_type = email_data.get('message_type', 'regular')
            if message_type in ['newsletter', 'automated']:
                return {
                    'success': True,
                    'email_id': email_data.get('id'),
                    'tasks': [],
                    'reason': f'Message type "{message_type}" skipped for task extraction'
                }
            
            # Prepare email context for Claude
            email_context = self._prepare_email_context(email_data)
            
            # Call Claude for task extraction
            claude_response = self._call_claude_for_tasks(email_context)
            
            if not claude_response:
                return {
                    'success': False,
                    'email_id': email_data.get('id'),
                    'error': 'Failed to get response from Claude'
                }
            
            # Parse Claude's response
            tasks = self._parse_claude_response(claude_response, email_data)
            
            # Enhance tasks with additional metadata
            enhanced_tasks = []
            for task in tasks:
                enhanced_task = self._enhance_task(task, email_data)
                enhanced_tasks.append(enhanced_task)
            
            return {
                'success': True,
                'email_id': email_data.get('id'),
                'tasks': enhanced_tasks,
                'extraction_metadata': {
                    'extracted_at': datetime.utcnow().isoformat(),
                    'extractor_version': self.version,
                    'model_used': self.model,
                    'email_priority': email_data.get('priority_score', 0.5)
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to extract tasks from email {email_data.get('id', 'unknown')}: {str(e)}")
            return {
                'success': False,
                'email_id': email_data.get('id'),
                'error': str(e)
            }

# Create global instance
task_extractor = TaskExtractor()

============================================================
FILE: chief_of_staff_ai/processors/knowledge_engine.py
============================================================
"""
Knowledge Engine - Core Processing for Knowledge Replacement System
==================================================================

This is the brain of the Knowledge-Centric Architecture. It handles:

1. Hierarchical Topic Tree Building (auto-generated + user-managed)
2. Multi-Source Knowledge Ingestion (email, slack, dropbox, etc.)
3. Bidirectional People-Topic Relationship Management
4. Source Traceability and Content Retrieval
5. Knowledge Evolution and Quality Management
6. Proactive Intelligence Generation for Auto-Response Capabilities

Goal: Build comprehensive knowledge to enable auto-response and decision-making
"""

import logging
from typing import Dict, List, Any, Optional, Tuple, Set
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, desc, func
import re
import json
from dataclasses import dataclass

from models.database import get_db_manager
from models.knowledge_models import (
    TopicHierarchy, PersonTopicRelationship, KnowledgeSource, 
    UnifiedKnowledgeGraph, ProactiveKnowledgeInsight, KnowledgeEvolutionLog,
    TopicType, SourceType, RelationshipType, KnowledgeConfidence,
    TopicSummary, PersonTopicContext, KnowledgeTraceability
)
from models.email import Email  # Add Email model import

logger = logging.getLogger(__name__)

@dataclass
class KnowledgeExtractionResult:
    """Result of knowledge extraction from content"""
    topics: List[Dict[str, Any]]
    people: List[Dict[str, Any]]
    relationships: List[Dict[str, Any]]
    tasks: List[Dict[str, Any]]
    insights: List[Dict[str, Any]]
    confidence: float
    source_reference: str

@dataclass
class TopicHierarchySuggestion:
    """AI suggestion for topic hierarchy placement"""
    topic_name: str
    suggested_parent: Optional[str]
    suggested_type: TopicType
    confidence: float
    reasoning: str

class KnowledgeEngine:
    """
    Core Knowledge Processing Engine
    
    This is the central intelligence that builds and maintains the knowledge base
    that can eventually replace the user's decision-making capabilities.
    """
    
    def __init__(self):
        self.db_manager = get_db_manager()
        
        # Configuration for topic hierarchy building
        self.TOPIC_CONFIDENCE_THRESHOLD = 0.6
        self.RELATIONSHIP_CONFIDENCE_THRESHOLD = 0.5
        self.HIERARCHY_MAX_DEPTH = 6
        self.AUTO_ORGANIZE_THRESHOLD = 10  # topics before auto-organizing
        
        # Knowledge quality thresholds
        self.MIN_EVIDENCE_COUNT = 2
        self.KNOWLEDGE_DECAY_DAYS = 30
        
    # ==========================================================================
    # TOPIC HIERARCHY MANAGEMENT
    # ==========================================================================
    
    def build_topic_hierarchy_from_content(self, user_id: int, source_content: List[Dict]) -> Dict[str, Any]:
        """
        Automatically build topic hierarchy from content analysis.
        This is core to making the system intelligent about business structure.
        """
        logger.info(f"  Building topic hierarchy for user {user_id} from {len(source_content)} content sources")
        
        try:
            with self.db_manager.get_session() as session:
                # Step 1: Extract all topics from content
                all_topics = self._extract_topics_from_content(source_content, user_id)
                
                # Step 2: Analyze and categorize topics
                categorized_topics = self._categorize_topics(all_topics)
                
                # Step 3: Build hierarchical structure
                hierarchy = self._build_hierarchy_structure(categorized_topics, session, user_id)
                
                # Step 4: Create/update topic records
                created_topics = self._create_topic_records(hierarchy, session, user_id)
                
                # Step 5: Update topic relationships and metadata
                self._update_topic_relationships(created_topics, session, user_id)
                
                session.commit()
                
                result = {
                    'success': True,
                    'topics_created': len(created_topics),
                    'hierarchy_depth': max([t.depth_level for t in created_topics]) if created_topics else 0,
                    'auto_generated': len([t for t in created_topics if t.auto_generated]),
                    'categories_found': len(set([t.topic_type for t in created_topics])),
                    'top_level_topics': [t.name for t in created_topics if t.depth_level == 0]
                }
                
                logger.info(f" Topic hierarchy built: {result}")
                return result
                
        except Exception as e:
            logger.error(f" Error building topic hierarchy: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _extract_topics_from_content(self, content: List[Dict], user_id: int) -> List[Dict]:
        """Extract topics from various content types"""
        topics = []
        
        for item in content:
            source_type = item.get('source_type', 'unknown')
            content_text = item.get('content', '') or item.get('body_text', '') or item.get('text', '')
            
            if not content_text:
                continue
            
            # Use AI to extract topics (would integrate with Claude here)
            extracted = self._ai_extract_topics(content_text, source_type)
            
            for topic in extracted:
                topics.append({
                    'name': topic['name'],
                    'confidence': topic['confidence'],
                    'source_type': source_type,
                    'source_id': item.get('id', 'unknown'),
                    'context': topic.get('context', ''),
                    'mentions': topic.get('mentions', 1),
                    'category_hints': topic.get('category_hints', [])
                })
        
        # Consolidate duplicate topics
        return self._consolidate_topics(topics)
    
    def _ai_extract_topics(self, content: str, source_type: str) -> List[Dict]:
        """
        AI-powered topic extraction from content.
        In production, this would use Claude API.
        """
        # Placeholder for AI extraction - would integrate with Claude
        # For now, use pattern matching for common business topics
        
        topics = []
        content_lower = content.lower()
        
        # Business structure patterns
        company_patterns = [
            r'\b([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*)\s+(?:company|corp|inc|ltd)\b',
            r'\b(?:company|organization|business)\s+([A-Z][a-zA-Z\s]+)\b'
        ]
        
        project_patterns = [
            r'\bproject\s+([A-Z][a-zA-Z\s]+)\b',
            r'\b([A-Z][a-zA-Z]+)\s+project\b',
            r'\binitiative\s+([A-Z][a-zA-Z\s]+)\b'
        ]
        
        product_patterns = [
            r'\b([A-Z][a-zA-Z]+)\s+(?:app|application|software|platform|system)\b',
            r'\bproduct\s+([A-Z][a-zA-Z\s]+)\b'
        ]
        
        # Extract patterns
        patterns = {
            'company': company_patterns,
            'project': project_patterns,
            'product': product_patterns
        }
        
        for category, pattern_list in patterns.items():
            for pattern in pattern_list:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    topic_name = match.strip() if isinstance(match, str) else ' '.join(match).strip()
                    if len(topic_name) > 2:  # Filter out very short matches
                        topics.append({
                            'name': topic_name,
                            'confidence': 0.7,
                            'context': f"Extracted from {source_type}",
                            'mentions': content_lower.count(topic_name.lower()),
                            'category_hints': [category]
                        })
        
        return topics
    
    def _consolidate_topics(self, topics: List[Dict]) -> List[Dict]:
        """Consolidate duplicate and similar topics"""
        consolidated = {}
        
        for topic in topics:
            name = topic['name'].lower().strip()
            
            if name in consolidated:
                # Merge with existing
                consolidated[name]['mentions'] += topic['mentions']
                consolidated[name]['confidence'] = max(consolidated[name]['confidence'], topic['confidence'])
                consolidated[name]['category_hints'].extend(topic['category_hints'])
                consolidated[name]['sources'] = consolidated[name].get('sources', []) + [topic.get('source_id', '')]
            else:
                topic['sources'] = [topic.get('source_id', '')]
                topic['category_hints'] = list(set(topic['category_hints']))
                consolidated[name] = topic
        
        return list(consolidated.values())
    
    def _categorize_topics(self, topics: List[Dict]) -> Dict[str, List[Dict]]:
        """Categorize topics into hierarchy levels"""
        categorized = {
            'company': [],
            'department': [],
            'product': [],
            'project': [],
            'feature': [],
            'custom': []
        }
        
        for topic in topics:
            category = self._determine_topic_category(topic)
            categorized[category].append(topic)
        
        return categorized
    
    def _determine_topic_category(self, topic: Dict) -> str:
        """Determine the category/type of a topic"""
        name = topic['name'].lower()
        hints = topic.get('category_hints', [])
        
        # Use hints if available
        if 'company' in hints:
            return 'company'
        elif 'project' in hints:
            return 'project'
        elif 'product' in hints:
            return 'product'
        
        # Pattern-based categorization
        if any(word in name for word in ['company', 'corp', 'inc', 'organization']):
            return 'company'
        elif any(word in name for word in ['department', 'team', 'division', 'group']):
            return 'department'
        elif any(word in name for word in ['app', 'application', 'platform', 'system', 'software']):
            return 'product'
        elif any(word in name for word in ['project', 'initiative', 'program']):
            return 'project'
        elif any(word in name for word in ['feature', 'component', 'module', 'function']):
            return 'feature'
        else:
            return 'custom'
    
    def _build_hierarchy_structure(self, categorized: Dict, session: Session, user_id: int) -> Dict[str, Any]:
        """Build the hierarchical structure"""
        hierarchy = {
            'root_topics': [],
            'relationships': [],
            'suggestions': []
        }
        
        # Create hierarchy: Company -> Department -> Product -> Project -> Feature
        depth_order = ['company', 'department', 'product', 'project', 'feature', 'custom']
        
        for depth, category in enumerate(depth_order):
            topics = categorized.get(category, [])
            
            for topic in topics:
                # Find potential parent based on content analysis
                parent = self._find_topic_parent(topic, hierarchy, depth, session, user_id)
                
                topic_record = {
                    'name': topic['name'],
                    'topic_type': category,
                    'depth_level': depth,
                    'parent': parent,
                    'confidence_score': topic['confidence'],
                    'mentions': topic['mentions'],
                    'auto_generated': True,
                    'sources': topic.get('sources', [])
                }
                
                if parent:
                    hierarchy['relationships'].append({
                        'child': topic['name'],
                        'parent': parent,
                        'confidence': topic['confidence']
                    })
                else:
                    hierarchy['root_topics'].append(topic_record)
        
        return hierarchy
    
    def _find_topic_parent(self, topic: Dict, hierarchy: Dict, depth: int, session: Session, user_id: int) -> Optional[str]:
        """Find the most likely parent for a topic"""
        if depth == 0:  # Root level
            return None
        
        # Look for parent relationships in content
        # This would use more sophisticated AI analysis in production
        
        # Simple heuristic: look for topics mentioned together
        topic_name = topic['name'].lower()
        
        # Check existing topics at higher levels
        existing_topics = session.query(TopicHierarchy).filter(
            and_(
                TopicHierarchy.depth_level < depth,
                TopicHierarchy.depth_level >= 0
            )
        ).all()
        
        best_parent = None
        best_score = 0
        
        for existing in existing_topics:
            # Calculate relationship score based on co-occurrence
            score = self._calculate_topic_relationship_score(topic_name, existing.name.lower())
            if score > best_score and score > 0.3:
                best_score = score
                best_parent = existing.name
        
        return best_parent
    
    def _calculate_topic_relationship_score(self, topic1: str, topic2: str) -> float:
        """Calculate how likely two topics are related"""
        # Placeholder for more sophisticated relationship scoring
        # Would analyze co-occurrence in content, semantic similarity, etc.
        
        # Simple word overlap scoring
        words1 = set(topic1.split())
        words2 = set(topic2.split())
        
        if len(words1.union(words2)) == 0:
            return 0.0
        
        overlap = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return overlap / union
    
    def _create_topic_records(self, hierarchy: Dict, session: Session, user_id: int) -> List[TopicHierarchy]:
        """Create topic records in database"""
        created_topics = []
        topic_map = {}  # name -> TopicHierarchy object
        
        # First pass: create all topics
        all_topics = hierarchy['root_topics'] + [
            rel for rel in hierarchy.get('relationships', [])
        ]
        
        for topic_data in hierarchy['root_topics']:
            topic = TopicHierarchy(
                name=topic_data['name'],
                topic_type=topic_data['topic_type'],
                depth_level=topic_data['depth_level'],
                confidence_score=topic_data['confidence_score'],
                mention_count=topic_data['mentions'],
                auto_generated=topic_data['auto_generated'],
                user_created=False,
                hierarchy_path=topic_data['name']
            )
            
            session.add(topic)
            session.flush()  # Get ID
            
            topic_map[topic.name] = topic
            created_topics.append(topic)
        
        # Second pass: establish parent-child relationships
        for rel in hierarchy.get('relationships', []):
            child_name = rel['child']
            parent_name = rel['parent']
            
            if parent_name in topic_map:
                parent = topic_map[parent_name]
                
                # Create child topic
                child_topic = next((t for t in all_topics if isinstance(t, dict) and t.get('name') == child_name), None)
                if child_topic:
                    child = TopicHierarchy(
                        name=child_name,
                        topic_type=child_topic.get('topic_type', 'custom'),
                        depth_level=parent.depth_level + 1,
                        parent_topic_id=parent.id,
                        confidence_score=rel['confidence'],
                        auto_generated=True,
                        hierarchy_path=f"{parent.hierarchy_path}/{child_name}"
                    )
                    
                    session.add(child)
                    session.flush()
                    
                    topic_map[child.name] = child
                    created_topics.append(child)
        
        return created_topics
    
    def _update_topic_relationships(self, topics: List[TopicHierarchy], session: Session, user_id: int):
        """Update topic relationships and cross-references"""
        # This would analyze content to establish topic relationships
        # and update the unified knowledge graph
        pass
    
    # ==========================================================================
    # MULTI-SOURCE KNOWLEDGE INGESTION
    # ==========================================================================
    
    def ingest_knowledge_from_source(self, source_type: SourceType, content: Dict, user_id: int) -> KnowledgeExtractionResult:
        """
        Ingest knowledge from any source type.
        This is the unified entry point for all knowledge ingestion.
        """
        logger.info(f" Ingesting knowledge from {source_type.value} for user {user_id}")
        
        try:
            with self.db_manager.get_session() as session:
                # Step 1: Store source content with full traceability
                source_record = self._store_source_content(content, source_type, session, user_id)
                
                # Step 2: Extract knowledge entities
                extraction_result = self._extract_knowledge_entities(content, source_type, user_id)
                
                # Step 3: Update knowledge graph
                self._update_knowledge_graph(extraction_result, source_record, session, user_id)
                
                # Step 4: Generate proactive insights
                insights = self._generate_proactive_insights(extraction_result, session, user_id)
                
                session.commit()
                
                logger.info(f" Knowledge ingestion complete: {len(extraction_result.topics)} topics, {len(extraction_result.people)} people")
                return extraction_result
                
        except Exception as e:
            logger.error(f" Knowledge ingestion error: {str(e)}")
            raise
    
    def _store_source_content(self, content: Dict, source_type: SourceType, session: Session, user_id: int) -> KnowledgeSource:
        """Store source content with full traceability"""
        source = KnowledgeSource(
            source_type=source_type.value,
            source_id=content.get('id', 'unknown'),
            raw_content=json.dumps(content.get('raw_content', content)),
            processed_content=content.get('processed_content', ''),
            content_summary=content.get('summary', ''),
            title=content.get('title', '') or content.get('subject', ''),
            author=content.get('author', '') or content.get('sender', ''),
            timestamp=datetime.fromisoformat(content.get('timestamp', datetime.utcnow().isoformat())),
            processing_status='pending'
        )
        
        session.add(source)
        session.flush()
        return source
    
    def _extract_knowledge_entities(self, content: Dict, source_type: SourceType, user_id: int) -> KnowledgeExtractionResult:
        """Extract all knowledge entities from content"""
        # This would integrate with Claude AI for sophisticated extraction
        # For now, using pattern-based extraction
        
        text = content.get('processed_content', '') or content.get('raw_content', '')
        
        return KnowledgeExtractionResult(
            topics=self._extract_topics_from_text(text),
            people=self._extract_people_from_text(text),
            relationships=self._extract_relationships_from_text(text),
            tasks=self._extract_tasks_from_text(text),
            insights=self._extract_insights_from_text(text),
            confidence=0.7,
            source_reference=content.get('id', 'unknown')
        )
    
    def _extract_topics_from_text(self, text: str) -> List[Dict]:
        """Extract topics from text"""
        # Placeholder - would use AI in production
        return []
    
    def _extract_people_from_text(self, text: str) -> List[Dict]:
        """Extract people mentions from text"""
        # Placeholder - would use AI in production
        return []
    
    def _extract_relationships_from_text(self, text: str) -> List[Dict]:
        """Extract relationships from text"""
        # Placeholder - would use AI in production
        return []
    
    def _extract_tasks_from_text(self, text: str) -> List[Dict]:
        """Extract tasks from text"""
        # Placeholder - would use AI in production
        return []
    
    def _extract_insights_from_text(self, text: str) -> List[Dict]:
        """Extract insights from text"""
        # Placeholder - would use AI in production
        return []
    
    def _update_knowledge_graph(self, extraction: KnowledgeExtractionResult, source: KnowledgeSource, session: Session, user_id: int):
        """Update the unified knowledge graph with new relationships"""
        # Update extraction results in source record
        source.extracted_topics = extraction.topics
        source.extracted_people = extraction.people
        source.extracted_tasks = extraction.tasks
        source.extracted_insights = extraction.insights
        source.processing_status = 'processed'
        
        # Create knowledge graph entries for relationships
        for relationship in extraction.relationships:
            graph_entry = UnifiedKnowledgeGraph(
                entity_type_1=relationship['entity_type_1'],
                entity_id_1=relationship['entity_id_1'],
                entity_type_2=relationship['entity_type_2'],
                entity_id_2=relationship['entity_id_2'],
                relationship_type=relationship['type'],
                relationship_strength=relationship.get('strength', 0.5),
                confidence=relationship.get('confidence', 0.5),
                evidence_sources=[source.id],
                evidence_count=1
            )
            session.add(graph_entry)
    
    def _generate_proactive_insights(self, extraction: KnowledgeExtractionResult, session: Session, user_id: int) -> List[Dict]:
        """Generate proactive insights from knowledge extraction"""
        insights = []
        
        # Analyze patterns and generate insights
        # This would use sophisticated AI analysis in production
        
        return insights
    
    # ==========================================================================
    # KNOWLEDGE RETRIEVAL AND TRACEABILITY
    # ==========================================================================
    
    def get_source_content(self, source_type: str, source_id: str, user_id: int) -> Optional[Dict]:
        """
        Retrieve full source content for traceability.
        Critical for user to verify AI decisions.
        """
        try:
            with self.db_manager.get_session() as session:
                source = session.query(KnowledgeSource).filter(
                    and_(
                        KnowledgeSource.source_type == source_type,
                        KnowledgeSource.source_id == source_id
                    )
                ).first()
                
                if source:
                    return {
                        'source_type': source.source_type,
                        'source_id': source.source_id,
                        'raw_content': json.loads(source.raw_content) if source.raw_content else {},
                        'processed_content': source.processed_content,
                        'summary': source.content_summary,
                        'title': source.title,
                        'author': source.author,
                        'timestamp': source.timestamp.isoformat() if source.timestamp else None,
                        'extraction_results': {
                            'topics': source.extracted_topics,
                            'people': source.extracted_people,
                            'tasks': source.extracted_tasks,
                            'insights': source.extracted_insights
                        }
                    }
                
                return None
                
        except Exception as e:
            logger.error(f"Error retrieving source content: {str(e)}")
            return None
    
    def get_knowledge_traceability(self, entity_type: str, entity_id: int, user_id: int) -> List[KnowledgeTraceability]:
        """
        Get complete traceability for any knowledge entity.
        Shows all sources that contributed to this knowledge.
        """
        try:
            with self.db_manager.get_session() as session:
                # Query knowledge source references
                references = session.query(KnowledgeSource).join(
                    # Would join through knowledge_source_association table
                ).filter(
                    # Filter by entity
                ).all()
                
                traceability = []
                for ref in references:
                    traceability.append(KnowledgeTraceability(
                        entity_type=entity_type,
                        entity_id=entity_id,
                        source_type=ref.source_type,
                        source_id=ref.source_id,
                        source_content_snippet=ref.content_summary[:200] if ref.content_summary else '',
                        confidence=0.8,  # Would calculate based on extraction confidence
                        timestamp=ref.timestamp,
                        can_access_full_content=True
                    ))
                
                return traceability
                
        except Exception as e:
            logger.error(f"Error getting knowledge traceability: {str(e)}")
            return []

    # ==========================================================================
    # KNOWLEDGE FOUNDATION BOOTSTRAPPING
    # ==========================================================================
    
    def build_knowledge_foundation_from_bulk_emails(self, user_id: int, months_back: int = 6) -> Dict[str, Any]:
        """
        Build comprehensive knowledge foundation from bulk historical emails.
        This creates the context skeleton that makes all future ingestion more accurate.
        
        This is the "Automatic Approach" - analyze large amounts of historical data
        to understand the user's complete business context before processing new content.
        """
        logger.info(f"  Building knowledge foundation from {months_back} months of historical emails for user {user_id}")
        
        try:
            with self.db_manager.get_session() as session:
                # Step 1: Get quality-filtered historical emails
                historical_emails = self._fetch_foundation_emails(user_id, months_back, session)
                
                if len(historical_emails) < 10:
                    return {
                        'success': False,
                        'error': 'Insufficient historical data for foundation building',
                        'recommendation': 'Use manual interview approach instead'
                    }
                
                # Step 2: Analyze complete corpus with Claude for comprehensive understanding
                foundation_analysis = self._analyze_complete_email_corpus(historical_emails, user_id)
                
                # Step 3: Build comprehensive hierarchical structure
                foundation_hierarchy = self._build_foundation_hierarchy(foundation_analysis, session, user_id)
                
                # Step 4: Create detailed topic records with rich context
                created_topics = self._create_foundation_topic_records(foundation_hierarchy, session, user_id)
                
                # Step 5: Build people-topic relationships from the foundation
                self._establish_foundation_relationships(foundation_analysis, created_topics, session, user_id)
                
                session.commit()
                
                result = {
                    'success': True,
                    'foundation_type': 'automatic_bulk_analysis',
                    'emails_analyzed': len(historical_emails),
                    'topics_created': len(created_topics),
                    'hierarchy_depth': max([t.depth_level for t in created_topics]) if created_topics else 0,
                    'business_areas_identified': len([t for t in created_topics if t.topic_type == 'department']),
                    'projects_identified': len([t for t in created_topics if t.topic_type == 'project']),
                    'people_connected': len(set([rel.person_id for rel in session.query(PersonTopicRelationship).all()])),
                    'foundation_quality_score': self._calculate_foundation_quality(created_topics, session)
                }
                
                logger.info(f" Knowledge foundation built: {result}")
                return result
                
        except Exception as e:
            logger.error(f" Error building knowledge foundation: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _fetch_foundation_emails(self, user_id: int, months_back: int, session: Session) -> List[Dict]:
        """Fetch quality-filtered historical emails for foundation building"""
        from chief_of_staff_ai.processors.email_quality_filter import email_quality_filter, ContactTier
        from datetime import datetime, timedelta
        
        # Get emails from the past N months
        cutoff_date = datetime.utcnow() - timedelta(days=months_back * 30)
        
        # Get all emails in date range
        all_emails = session.query(Email).filter(
            Email.user_id == user_id,
            Email.email_date >= cutoff_date,
            Email.ai_summary.isnot(None)  # Only processed emails
        ).order_by(Email.email_date.desc()).limit(1000).all()
        
        # Apply quality filtering - only include Tier 1 and Tier 2 contacts
        foundation_emails = []
        for email in all_emails:
            if email.sender:
                contact_stats = email_quality_filter._get_contact_stats(email.sender.lower(), user_id)
                if contact_stats.tier in [ContactTier.TIER_1, ContactTier.TIER_2]:
                    foundation_emails.append({
                        'id': email.gmail_id,
                        'subject': email.subject or '',
                        'sender': email.sender,
                        'body_text': email.body_text or '',
                        'ai_summary': email.ai_summary or '',
                        'date': email.email_date.isoformat() if email.email_date else '',
                        'contact_tier': contact_stats.tier.value,
                        'response_rate': contact_stats.response_rate
                    })
        
        logger.info(f" Foundation emails: {len(foundation_emails)} quality emails from {len(all_emails)} total")
        return foundation_emails
    
    def _analyze_complete_email_corpus(self, emails: List[Dict], user_id: int) -> Dict[str, Any]:
        """
        Send complete email corpus to Claude for comprehensive business analysis.
        This is where we get the "big picture" understanding.
        """
        # Prepare comprehensive prompt for Claude
        corpus_text = self._prepare_corpus_for_analysis(emails)
        
        # This would integrate with Claude API in production
        # For now, we'll simulate comprehensive analysis
        analysis = {
            'business_structure': {
                'company_focus': 'Technology consulting and software development',
                'departments': ['Engineering', 'Sales', 'Marketing', 'Operations'],
                'core_products': ['Mobile Apps', 'Web Platforms', 'API Services']
            },
            'project_hierarchy': {
                'major_initiatives': [
                    {
                        'name': 'Mobile Platform Redesign',
                        'department': 'Engineering',
                        'key_people': ['john@company.com', 'sarah@company.com'],
                        'sub_projects': ['iOS App', 'Android App', 'Backend API']
                    },
                    {
                        'name': 'Q1 Sales Campaign', 
                        'department': 'Sales',
                        'key_people': ['mike@company.com'],
                        'sub_projects': ['Lead Generation', 'Client Presentations']
                    }
                ]
            },
            'relationship_mapping': {
                'internal_team': ['john@company.com', 'sarah@company.com', 'mike@company.com'],
                'key_clients': ['client@bigcorp.com', 'contact@startup.com'],
                'external_partners': ['partner@vendor.com'],
                'decision_makers': ['ceo@company.com', 'cto@company.com']
            },
            'business_priorities': [
                'Product launch deadline: March 2024',
                'Client retention and satisfaction',
                'Team scaling and hiring',
                'Technology stack modernization'
            ],
            'topic_confidence_scores': {
                'Mobile Platform Redesign': 0.95,
                'Q1 Sales Campaign': 0.87,
                'Engineering': 0.92,
                'Sales': 0.88
            }
        }
        
        return analysis
    
    def _build_foundation_hierarchy(self, analysis: Dict, session: Session, user_id: int) -> Dict[str, Any]:
        """Build hierarchical topic structure from comprehensive analysis"""
        hierarchy = {
            'root_topics': [],
            'topic_relationships': [],
            'people_topic_connections': [],
            'business_context': analysis
        }
        
        # Build from business structure analysis
        business_structure = analysis.get('business_structure', {})
        
        # Create company/organization root
        company_topic = {
            'name': business_structure.get('company_focus', 'Business Operations'),
            'topic_type': 'company',
            'depth_level': 0,
            'confidence_score': 0.95,
            'description': 'Main business focus and operations',
            'children': []
        }
        
        # Add departments as children
        for dept in business_structure.get('departments', []):
            dept_topic = {
                'name': dept,
                'topic_type': 'department', 
                'depth_level': 1,
                'parent': company_topic['name'],
                'confidence_score': 0.9,
                'children': []
            }
            company_topic['children'].append(dept_topic)
        
        # Add projects under departments
        for project in analysis.get('project_hierarchy', {}).get('major_initiatives', []):
            project_topic = {
                'name': project['name'],
                'topic_type': 'project',
                'depth_level': 2,
                'parent': project.get('department', 'Operations'),
                'confidence_score': analysis.get('topic_confidence_scores', {}).get(project['name'], 0.8),
                'people': project.get('key_people', []),
                'children': []
            }
            
            # Add sub-projects
            for sub_project in project.get('sub_projects', []):
                sub_topic = {
                    'name': sub_project,
                    'topic_type': 'feature',
                    'depth_level': 3,
                    'parent': project['name'],
                    'confidence_score': 0.75
                }
                project_topic['children'].append(sub_topic)
            
            # Find parent department and add project
            for dept in company_topic['children']:
                if dept['name'] == project.get('department'):
                    dept['children'].append(project_topic)
                    break
        
        hierarchy['root_topics'] = [company_topic]
        return hierarchy
    
    def _create_foundation_topic_records(self, hierarchy: Dict, session: Session, user_id: int) -> List[TopicHierarchy]:
        """Create detailed topic records with foundation context"""
        created_topics = []
        
        def create_topic_recursive(topic_data, parent_id=None, parent_path=""):
            # Create topic record
            topic = TopicHierarchy(
                name=topic_data['name'],
                topic_type=topic_data['topic_type'],
                depth_level=topic_data['depth_level'],
                parent_topic_id=parent_id,
                confidence_score=topic_data['confidence_score'],
                auto_generated=True,
                user_created=False,
                status='active',
                priority='medium',
                description=topic_data.get('description', f"Auto-generated {topic_data['topic_type']} from email analysis"),
                hierarchy_path=f"{parent_path}/{topic_data['name']}" if parent_path else topic_data['name'],
                mention_count=0,  # Will be updated when we process the source emails
                strategic_importance=topic_data['confidence_score'],
                keywords=topic_data.get('keywords', []),
                related_entities=topic_data.get('people', [])
            )
            
            session.add(topic)
            session.flush()  # Get ID
            
            created_topics.append(topic)
            
            # Create child topics recursively
            for child in topic_data.get('children', []):
                create_topic_recursive(child, topic.id, topic.hierarchy_path)
            
            return topic
        
        # Create all topics from hierarchy
        for root_topic in hierarchy['root_topics']:
            create_topic_recursive(root_topic)
        
        return created_topics
    
    def _establish_foundation_relationships(self, analysis: Dict, topics: List[TopicHierarchy], session: Session, user_id: int):
        """Establish people-topic relationships from foundation analysis"""
        # This would create PersonTopicRelationship records based on the analysis
        # Implementation would map people from the relationship_mapping to topics
        pass
    
    def _calculate_foundation_quality(self, topics: List[TopicHierarchy], session: Session) -> float:
        """Calculate quality score for the foundation"""
        if not topics:
            return 0.0
        
        # Quality factors:
        # - Hierarchy depth (deeper = more detailed)
        # - Confidence scores
        # - Topic distribution across types
        # - People-topic connections
        
        avg_confidence = sum([t.confidence_score for t in topics]) / len(topics)
        max_depth = max([t.depth_level for t in topics])
        type_diversity = len(set([t.topic_type for t in topics]))
        
        quality_score = (avg_confidence * 0.4) + (min(max_depth / 5, 1.0) * 0.3) + (min(type_diversity / 6, 1.0) * 0.3)
        
        return round(quality_score, 2)
    
    def _prepare_corpus_for_analysis(self, emails: List[Dict]) -> str:
        """Prepare email corpus for Claude analysis"""
        corpus_parts = []
        
        for email in emails[:50]:  # Limit for token management
            corpus_parts.append(f"""
Email from {email['sender']} | Subject: {email['subject']}
Date: {email['date']} | Tier: {email['contact_tier']}
Summary: {email['ai_summary'][:200]}...
""")
        
        return "\n".join(corpus_parts)

# Global instance
knowledge_engine = KnowledgeEngine() 

============================================================
FILE: chief_of_staff_ai/processors/email_intelligence.py
============================================================
# Enhanced Email Intelligence Processor using Claude 4 Sonnet

import json
import logging
import re
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple
import anthropic
import time

from config.settings import settings
from models.database import get_db_manager, Email, Person, Project, Task, User

logger = logging.getLogger(__name__)

class EmailIntelligenceProcessor:
    """Advanced email intelligence using Claude 4 Sonnet for comprehensive understanding"""
    
    def __init__(self):
        self.claude_client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        self.model = settings.CLAUDE_MODEL  # Now uses Claude 4 Opus from settings
        self.version = "2.2"  # Debug version with relaxed filters
        
        # Quality filtering patterns (RELAXED FOR DEBUGGING)
        self.non_human_patterns = [
            'noreply', 'no-reply', 'donotreply', 'automated', 'newsletter',
            'unsubscribe', 'notification', 'system', 'support', 'help',
            'admin', 'contact', 'info', 'sales', 'marketing', 'hello',
            'team', 'notifications', 'alerts', 'updates', 'reports'
        ]
        
        # RELAXED quality thresholds to capture more content
        self.min_insight_length = 10  # Reduced from 15
        self.min_confidence_score = 0.4  # Reduced from 0.6 - be more inclusive
        
    def process_user_emails_intelligently(self, user_email: str, limit: int = None, force_refresh: bool = False) -> Dict:
        """
        Process user emails with Claude 4 Sonnet for high-quality business intelligence
        Enhanced with quality filtering and strategic insights
        """
        try:
            logger.info(f"Starting quality-focused email processing for {user_email}")
            
            # Get user and validate
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
                
            # Get business context for enhanced AI analysis
            user_context = self._get_user_business_context(user.id)
            
            # Get emails needing processing with quality pre-filtering
            emails_to_process = self._get_emails_needing_processing(user.id, limit or 100, force_refresh)
            
            # RELAXED: Filter for quality emails but be more inclusive
            quality_filtered_emails = self._filter_quality_emails_debug(emails_to_process, user_email)
            
            logger.info(f"Found {len(emails_to_process)} emails to process, {len(quality_filtered_emails)} passed quality filters")
            
            if not quality_filtered_emails:
                logger.warning(f"No emails passed quality filters for {user_email}")
                return {
                    'success': True,
                    'user_email': user_email,
                    'processed_emails': 0,
                    'high_quality_insights': 0,
                    'human_contacts_identified': 0,
                    'meaningful_projects': 0,
                    'actionable_tasks': 0,
                    'processor_version': self.version,
                    'debug_info': f"No emails passed filters out of {len(emails_to_process)} total emails"
                }
            
            # Limit to top quality emails for processing
            emails_to_process = quality_filtered_emails[:limit or 50]
            
            processed_count = 0
            insights_extracted = 0
            people_identified = 0
            projects_identified = 0
            tasks_created = 0
            
            for idx, email in enumerate(emails_to_process):
                try:
                    logger.info(f"Processing email {idx + 1}/{len(emails_to_process)} for {user_email}")
                    logger.debug(f"Email from: {email.sender}, subject: {email.subject}")
                    
                    # Skip if email has issues
                    if not email.body_clean and not email.snippet:
                        logger.warning(f"Skipping email {email.gmail_id} - no content")
                        continue
                    
                    # Get comprehensive email analysis from Claude with enhanced prompts
                    analysis = self._get_quality_focused_email_analysis(email, user, user_context)
                    
                    if analysis:
                        logger.debug(f"AI Analysis received for email {email.gmail_id}")
                        logger.debug(f"Strategic value score: {analysis.get('strategic_value_score', 'N/A')}")
                        logger.debug(f"Sender analysis: {analysis.get('sender_analysis', {})}")
                        logger.debug(f"People found: {len(analysis.get('people', []))}")
                        
                        if self._validate_analysis_quality_debug(analysis):
                            # Update email with insights
                            self._update_email_with_insights(email, analysis)
                            
                            # Extract and update people information (with human filtering)
                            if analysis.get('people') or analysis.get('sender_analysis'):
                                people_count = self._process_human_contacts_only_debug(user.id, analysis, email)
                                people_identified += people_count
                                logger.info(f"Extracted {people_count} people from email {email.gmail_id}")
                            
                            # Extract and update project information
                            if analysis.get('project') and self._validate_project_quality(analysis['project']):
                                project = self._process_project_insights(user.id, analysis['project'], email)
                                if project:
                                    projects_identified += 1
                                    email.project_id = project.id
                            
                            # Extract high-confidence tasks only
                            if analysis.get('tasks'):
                                tasks_count = self._process_high_quality_tasks(user.id, email.id, analysis['tasks'])
                                tasks_created += tasks_count
                            
                            insights_extracted += 1
                        else:
                            logger.info(f"Analysis for email {email.gmail_id} didn't meet quality thresholds")
                    else:
                        logger.warning(f"No analysis returned for email {email.gmail_id}")
                    
                    processed_count += 1
                    
                    # Add a small delay to prevent overwhelming the system
                    time.sleep(0.1)
                    
                except Exception as e:
                    logger.error(f"Failed to intelligently process email {email.gmail_id}: {str(e)}")
                    continue
            
            logger.info(f"Quality-focused processing: {processed_count} emails, {people_identified} people identified for {user_email}")
            
            return {
                'success': True,
                'user_email': user_email,
                'processed_emails': processed_count,
                'high_quality_insights': insights_extracted,
                'human_contacts_identified': people_identified,
                'meaningful_projects': projects_identified,
                'actionable_tasks': tasks_created,
                'processor_version': self.version,
                'debug_info': f"Processed {processed_count} emails, passed quality filters: {len(quality_filtered_emails)}"
            }
            
        except Exception as e:
            logger.error(f"Failed intelligent email processing for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _get_emails_needing_processing(self, user_id: int, limit: int, force_refresh: bool) -> List[Email]:
        """Get emails that need Claude analysis (generic filter)"""
        with get_db_manager().get_session() as session:
            query = session.query(Email).filter(
                Email.user_id == user_id,
                Email.body_clean.isnot(None)
            )
            
            if not force_refresh:
                query = query.filter(Email.ai_summary.is_(None))
            
            # Detach from session before returning to avoid issues
            emails = query.order_by(Email.email_date.desc()).limit(limit).all()
            session.expunge_all()
            return emails

    def _filter_unreplied_emails(self, emails: List[Email], user_email: str) -> List[Email]:
        """Filter a list of emails to find ones that are likely unreplied"""
        unreplied = []
        for email in emails:
            # If email is from the user themselves, skip
            if email.sender and user_email.lower() in email.sender.lower():
                continue

            # If email contains certain patterns suggesting it's automated, skip
            automated_patterns = [
                'noreply', 'no-reply', 'donotreply', 'automated', 'newsletter',
                'unsubscribe', 'notification only', 'system generated'
            ]
            sender_lower = (email.sender or '').lower()
            subject_lower = (email.subject or '').lower()
            if any(pattern in sender_lower or pattern in subject_lower for pattern in automated_patterns):
                continue

            # Default to including emails that seem personal/business oriented
            unreplied.append(email)
        return unreplied
    
    def _is_unreplied_email(self, email: Email, user_email: str) -> bool:
        """Determine if an email is unreplied using heuristics"""
        # If email is from the user themselves, skip
        if email.sender and user_email.lower() in email.sender.lower():
            return False
        
        # If email contains certain patterns suggesting it's automated, skip
        automated_patterns = [
            'noreply', 'no-reply', 'donotreply', 'automated', 'newsletter',
            'unsubscribe', 'notification only', 'system generated'
        ]
        
        sender_lower = (email.sender or '').lower()
        subject_lower = (email.subject or '').lower()
        
        if any(pattern in sender_lower or pattern in subject_lower for pattern in automated_patterns):
            return False
        
        # If email is marked as important or has action-oriented subject, include it
        action_words = ['review', 'approve', 'sign', 'confirm', 'urgent', 'asap', 'deadline', 'meeting']
        if any(word in subject_lower for word in action_words):
            return True
        
        # Default to including emails that seem personal/business oriented
        return True
    
    def _get_quality_focused_email_analysis(self, email: Email, user, user_context: Dict) -> Optional[Dict]:
        """Get quality-focused email analysis from Claude with enhanced business context"""
        try:
            # Safety check to prevent processing very large emails
            email_content = email.body_clean or email.snippet or ""
            if len(email_content) > 10000:  # Limit email content size
                logger.warning(f"Email {email.gmail_id} too large ({len(email_content)} chars), truncating")
                email_content = email_content[:10000] + "... [truncated]"
            
            if len(email_content) < 10:  # Skip very short emails
                logger.warning(f"Email {email.gmail_id} too short, skipping AI analysis")
                return None
            
            email_context = self._prepare_enhanced_email_context(email, user)
            
            # Limit context size to prevent API issues
            if len(email_context) > 15000:
                logger.warning(f"Email context too large for {email.gmail_id}, truncating")
                email_context = email_context[:15000] + "... [truncated]"
            
            # Enhanced system prompt with business context and quality requirements
            business_context_str = self._format_business_context(user_context)
            
            system_prompt = f"""You are an expert AI Chief of Staff that provides comprehensive email analysis for business intelligence and productivity. Be INCLUSIVE and extract valuable insights from business communications.

**YOUR MISSION:**
- Extract ALL valuable business intelligence, contacts, tasks, and insights
- Be inclusive rather than restrictive - capture business value wherever it exists
- Focus on building comprehensive knowledge about professional relationships and work

**BUSINESS CONTEXT FOR {user.email}:**
{business_context_str}

**ANALYSIS REQUIREMENTS:**

1. **EMAIL SUMMARY**: Clear description of the email's business purpose and content
2. **PEOPLE EXTRACTION**: Extract ALL human contacts with professional relevance (be generous!)
   - ALWAYS extract the sender if they're a real person
   - Extract anyone mentioned by name with business context
   - Include names even with limited contact information
3. **TASK IDENTIFICATION**: Find ANY actionable items or commitments mentioned
4. **BUSINESS INSIGHTS**: Extract any strategic value, opportunities, or challenges
5. **PROJECT CONTEXT**: Identify any work initiatives or business activities
6. **TOPIC EXTRACTION**: Identify business topics, project names, company names, technologies

**INCLUSIVE EXTRACTION GUIDELINES:**
- Extract people even if limited info is available (name + context is enough)
- Include tasks with clear actionable language, even if informal
- Capture business insights at any level (strategic, operational, or tactical)
- Process emails from colleagues, clients, partners, vendors - anyone professional
- Include follow-ups, scheduling, decisions, updates, and work discussions
- Extract topics like project names, company names, technologies, business areas
- Be generous with topic extraction - include any business-relevant subjects

Return a JSON object with this structure:
{{
    "summary": "Clear description of the email's business purpose and key content",
    "strategic_value_score": 0.7,  // Be generous - most business emails have value
    "sender_analysis": {{
        "name": "Sender's actual name (extract from signature or display name)",
        "role": "Their role/title if mentioned",
        "company": "Their company if identifiable",
        "relationship": "Professional relationship context",
        "is_human_contact": true,  // Default to true for most senders
        "business_relevance": "Why this person is professionally relevant"
    }},
    "people": [
        {{
            "name": "Full name of any person mentioned",
            "email": "their_email@example.com",
            "role": "Their role if mentioned",
            "company": "Company if mentioned", 
            "relationship": "Professional context",
            "business_relevance": "Why they're mentioned/relevant",
            "mentioned_context": "How they were mentioned in the email"
        }}
    ],
    "project": {{
        "name": "Project or initiative name",
        "description": "Description of the work or project",
        "category": "business/client_work/internal/operational",
        "priority": "high/medium/low",
        "status": "active/planning/discussed",
        "business_impact": "Potential impact or value",
        "key_stakeholders": ["person1", "person2"]
    }},
    "business_insights": {{
        "key_decisions": ["Any decisions mentioned or needed"],
        "strategic_opportunities": ["Opportunities or potential business value"],
        "business_challenges": ["Challenges or issues discussed"],
        "actionable_metrics": ["Any numbers or metrics mentioned"],
        "competitive_intelligence": ["Market or competitor information"],
        "partnership_opportunities": ["Collaboration potential"]
    }},
    "tasks": [
        {{
            "description": "Clear description of the actionable item",
            "assignee": "{user.email}",
            "due_date": "2025-02-15",
            "due_date_text": "deadline mentioned in email",
            "priority": "high/medium/low",
            "category": "action_item/follow_up/meeting/review",
            "confidence": 0.8,  // Be generous with confidence scores
            "business_context": "Why this task matters",
            "success_criteria": "What completion looks like"
        }}
    ],
    "topics": ["HitCraft", "board meeting", "fundraising", "AI in music", "certification", "business development"],  // Extract: project names, company names, technologies, business areas, meeting types
    "ai_category": "business_communication/client_work/project_coordination/operational"
}}

**IMPORTANT**: Extract value from most business emails. Only skip obvious spam or completely irrelevant content. Be generous with people extraction and task identification.
"""

            user_prompt = f"""Analyze this email comprehensively for business intelligence. Extract ALL valuable people, tasks, and insights:

{email_context}

Focus on building comprehensive business knowledge. Extract people and tasks generously - capture business value wherever it exists."""

            # Add timeout and retry protection
            max_retries = 2
            for attempt in range(max_retries):
                try:
                    logger.info(f"Calling Claude API for comprehensive analysis of email {email.gmail_id}, attempt {attempt + 1}")
                    
                    message = self.claude_client.messages.create(
                        model=self.model,
                        max_tokens=3000,
                        temperature=0.1,
                        system=system_prompt,
                        messages=[{"role": "user", "content": user_prompt}]
                    )
                    
                    response_text = message.content[0].text.strip()
                    
                    # Handle null responses (low-quality emails)
                    if response_text.lower().strip() in ['null', 'none', '{}', '']:
                        logger.info(f"Claude rejected email {email.gmail_id} as low-quality")
                        return None
                    
                    # Parse JSON response with better error handling
                    json_start = response_text.find('{')
                    json_end = response_text.rfind('}') + 1
                    
                    if json_start != -1 and json_end > json_start:
                        json_text = response_text[json_start:json_end]
                        try:
                            analysis = json.loads(json_text)
                            logger.info(f"Successfully analyzed email {email.gmail_id}")
                            return analysis
                        except json.JSONDecodeError as json_error:
                            logger.error(f"JSON parsing error for email {email.gmail_id}: {str(json_error)}")
                            if attempt < max_retries - 1:
                                time.sleep(1)  # Wait before retry
                                continue
                            return None
                    else:
                        logger.warning(f"No valid JSON found in Claude response for email {email.gmail_id}")
                        if attempt < max_retries - 1:
                            time.sleep(1)  # Wait before retry
                            continue
                        return None
                        
                except Exception as api_error:
                    logger.error(f"Claude API error for email {email.gmail_id}, attempt {attempt + 1}: {str(api_error)}")
                    if attempt < max_retries - 1:
                        time.sleep(2)  # Wait longer before retry
                        continue
                    return None
            
            logger.warning(f"Failed to analyze email {email.gmail_id} after {max_retries} attempts")
            return None
            
        except Exception as e:
            logger.error(f"Failed to get email analysis from Claude for {email.gmail_id}: {str(e)}")
            return None
    
    def _format_business_context(self, user_context: Dict) -> str:
        """Format business context for AI prompt"""
        context_parts = []
        
        if user_context.get('existing_projects'):
            context_parts.append(f"Current Projects: {', '.join(user_context['existing_projects'])}")
        
        if user_context.get('key_contacts'):
            context_parts.append(f"Key Business Contacts: {', '.join(user_context['key_contacts'][:5])}")  # Top 5
        
        if user_context.get('official_topics'):
            context_parts.append(f"Business Focus Areas: {', '.join(user_context['official_topics'])}")
        
        return '\n'.join(context_parts) if context_parts else "No existing business context available"
    
    def _validate_analysis_quality(self, analysis: Dict) -> bool:
        """Validate that the analysis meets quality standards - RELAXED VERSION"""
        try:
            # RELAXED: Check strategic value score - lowered threshold
            strategic_value = analysis.get('strategic_value_score', 0)
            if strategic_value < 0.5:  # Reduced from 0.6 to 0.5
                logger.info(f"Analysis rejected - low strategic value: {strategic_value}")
                return False
            
            # RELAXED: Check summary quality - reduced minimum length
            summary = analysis.get('summary', '')
            if len(summary) < self.min_insight_length:
                logger.info(f"Analysis rejected - summary too short: {len(summary)} chars")
                return False
            
            # RELAXED: More lenient trivial content detection
            trivial_phrases = [
                'thanks', 'thank you', 'got it', 'received', 'noted', 'okay', 'ok',
                'sounds good', 'will do', 'understood', 'acknowledged'
            ]
            
            # Only reject if it's VERY short AND contains only trivial phrases
            if any(phrase in summary.lower() for phrase in trivial_phrases) and len(summary) < 30:  # Reduced from 50
                logger.info(f"Analysis rejected - trivial content detected")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"Error validating analysis quality: {str(e)}")
            return False
    
    def _validate_project_quality(self, project_data: Dict) -> bool:
        """Validate that project data meets quality standards"""
        if not project_data or not project_data.get('name'):
            return False
        
        # Check project name is substantial
        if len(project_data['name']) < 5:
            return False
        
        # Check for meaningful description
        description = project_data.get('description', '')
        if len(description) < self.min_insight_length:
            return False
        
        return True
    
    def _process_human_contacts_only_debug(self, user_id: int, analysis: Dict, email: Email) -> int:
        """Process people information with COMPREHENSIVE RELATIONSHIP INTELLIGENCE GENERATION"""
        people_count = 0
        
        # Process sender first (with comprehensive relationship intelligence)
        sender_analysis = analysis.get('sender_analysis')
        if (sender_analysis and email.sender and 
            not self._is_obviously_non_human_contact(email.sender)):
            
            # Get existing person to accumulate knowledge
            existing_person = get_db_manager().find_person_by_email(user_id, email.sender)
            
            # Generate comprehensive relationship story and intelligence
            comprehensive_relationship_story = self._generate_comprehensive_relationship_story(sender_analysis, email, existing_person)
            relationship_insights = self._generate_relationship_insights(sender_analysis, email, existing_person)
            
            # Accumulate notes and context over time
            existing_notes = existing_person.notes if existing_person else ""
            new_relevance = sender_analysis.get('business_relevance', '')
            
            # Combine old and new notes intelligently
            accumulated_notes = existing_notes
            if new_relevance and new_relevance not in accumulated_notes:
                if accumulated_notes:
                    accumulated_notes += f"\n\nRecent Context: {new_relevance}"
                else:
                    accumulated_notes = new_relevance
            
            # Create enhanced person data with comprehensive intelligence
            person_data = {
                'email_address': email.sender,
                'name': sender_analysis.get('name', email.sender_name or email.sender.split('@')[0]),
                'title': sender_analysis.get('role'),
                'company': sender_analysis.get('company'),
                'relationship_type': sender_analysis.get('relationship', 'Contact'),
                'notes': accumulated_notes,  # Accumulated knowledge
                'importance_level': 0.8,  # Default importance
                'ai_version': self.version,
                'total_emails': (existing_person.total_emails if existing_person else 0) + 1,  # Increment email count
                
                # COMPREHENSIVE RELATIONSHIP INTELLIGENCE - These are the rich stories that make people clickable
                'comprehensive_relationship_story': comprehensive_relationship_story,
                'relationship_insights': relationship_insights,
                
                # Enhanced communication timeline
                'communication_timeline': self._generate_communication_timeline_entry(email),
                
                # Business intelligence metadata
                'relationship_intelligence': {
                    'context_story': comprehensive_relationship_story[:200] + '...' if len(comprehensive_relationship_story) > 200 else comprehensive_relationship_story,
                    'business_relevance': self._assess_business_relevance(sender_analysis, email),
                    'strategic_value': self._calculate_strategic_value(sender_analysis, email),
                    'recent_activity': 1,  # This email counts as recent activity
                    'communication_frequency': self._assess_communication_frequency(existing_person),
                    'last_strategic_topic': self._extract_strategic_topic_from_email(email),
                    'collaboration_score': self._calculate_collaboration_score(sender_analysis, email),
                    'expertise_areas': self._extract_expertise_areas(email, sender_analysis),
                    'meeting_participant': self._is_meeting_participant(email),
                    'communication_style': self._assess_communication_style(email),
                    'avg_urgency': self._assess_email_urgency(email)
                },
                
                # Enhanced business context
                'business_context': {
                    'strategic_topics': self._extract_strategic_topics_list(email),
                    'business_insights_count': 1,  # This email contributes insights
                    'communication_patterns': [self._categorize_communication_pattern(email)],
                    'project_involvement': self._extract_project_involvement(email, sender_analysis),
                    'has_strategic_communications': self._is_strategic_communication(email),
                    'last_strategic_communication': self._format_last_strategic_communication(email) if self._is_strategic_communication(email) else None,
                    'key_decisions_involved': self._extract_key_decisions(analysis, email),
                    'opportunities_discussed': self._extract_opportunities(analysis, email),
                    'challenges_mentioned': self._extract_challenges(analysis, email),
                    'collaboration_projects': self._extract_collaboration_projects(email, analysis),
                    'expertise_indicators': self._extract_expertise_indicators(email, sender_analysis),
                    'meeting_frequency': self._calculate_meeting_frequency(email),
                    'response_reliability': self._assess_response_reliability(existing_person)
                },
                
                # Enhanced relationship analytics
                'relationship_analytics': {
                    'total_interactions': (existing_person.total_emails if existing_person else 0) + 1,
                    'recent_interactions': 1,  # This is a recent interaction
                    'strategic_interactions': 1 if self._is_strategic_communication(email) else 0,
                    'avg_email_importance': self._calculate_avg_email_importance(email),
                    'relationship_trend': self._assess_relationship_trend(existing_person),
                    'engagement_level': self._assess_engagement_level(email, sender_analysis),
                    'communication_consistency': self._assess_communication_consistency(existing_person),
                    'business_value_score': self._calculate_business_value_score(sender_analysis, email),
                    'collaboration_strength': self._calculate_collaboration_strength(email, analysis),
                    'decision_influence': self._assess_decision_influence(analysis, email),
                    'topic_expertise_count': len(self._extract_expertise_areas(email, sender_analysis)),
                    'urgency_compatibility': self._assess_urgency_compatibility(email)
                }
            }
            
            get_db_manager().create_or_update_person(user_id, person_data)
            people_count += 1
            logger.info(f"Created/updated person with comprehensive intelligence: {person_data['name']} ({person_data['email_address']})")
        
        # Process mentioned people (also with comprehensive intelligence but lighter weight)
        people_mentioned = analysis.get('people', [])
        for person_info in people_mentioned:
            if (person_info.get('email') and 
                not self._is_obviously_non_human_contact(person_info['email'])):
                
                existing_person = get_db_manager().find_person_by_email(user_id, person_info['email'])
                
                # Generate lighter but still comprehensive relationship intelligence for mentioned people
                comprehensive_relationship_story = self._generate_mentioned_person_relationship_story(person_info, email, existing_person)
                relationship_insights = self._generate_mentioned_person_insights(person_info, email, existing_person)
                
                person_data = {
                    'email_address': person_info['email'],
                    'name': person_info.get('name', person_info['email'].split('@')[0]),
                    'title': person_info.get('role'),
                    'company': person_info.get('company'),
                    'relationship_type': person_info.get('relationship', 'Mentioned Contact'),
                    'notes': person_info.get('business_relevance', '') + f"\n\nMentioned in: {email.subject}",
                    'importance_level': 0.6,  # Lower importance for mentioned people
                    'ai_version': self.version,
                    'total_emails': (existing_person.total_emails if existing_person else 0),  # Don't increment for mentions
                    
                    # Comprehensive intelligence for mentioned people too
                    'comprehensive_relationship_story': comprehensive_relationship_story,
                    'relationship_insights': relationship_insights,
                    
                    # Lighter business intelligence for mentioned people
                    'relationship_intelligence': {
                        'context_story': f"Mentioned in discussion about {email.subject or 'business matters'}",
                        'business_relevance': 'mentioned_contact',
                        'strategic_value': 0.3,  # Lower strategic value for mentions
                        'recent_activity': 0,  # No direct activity
                        'communication_frequency': 'mentioned_only',
                        'last_strategic_topic': self._extract_strategic_topic_from_email(email),
                        'collaboration_score': 0.2,
                        'expertise_areas': [],
                        'meeting_participant': False,
                        'communication_style': 'unknown',
                        'avg_urgency': 0.5
                    }
                }
                
                get_db_manager().create_or_update_person(user_id, person_data)
                people_count += 1
                logger.info(f"Created/updated mentioned person: {person_data['name']} ({person_data['email_address']})")
        
        return people_count
    
    def _generate_comprehensive_relationship_story(self, sender_analysis: Dict, email: Email, existing_person=None) -> str:
        """Generate a comprehensive relationship story explaining the full context of this business relationship"""
        try:
            story_parts = []
            
            # Relationship introduction
            name = sender_analysis.get('name', email.sender_name or email.sender.split('@')[0] if email.sender else 'Contact')
            company = sender_analysis.get('company', 'Unknown Company')
            role = sender_analysis.get('role', 'Professional Contact')
            
            story_parts.append(f" **Professional Contact:** {name}")
            if role and role != 'Professional Contact':
                story_parts.append(f" **Role:** {role}")
            if company and company != 'Unknown Company':
                story_parts.append(f" **Company:** {company}")
            
            # Relationship context
            relationship = sender_analysis.get('relationship', 'Business Contact')
            business_relevance = sender_analysis.get('business_relevance', '')
            
            story_parts.append(f" **Relationship:** {relationship}")
            if business_relevance:
                story_parts.append(f" **Business Relevance:** {business_relevance}")
            
            # Current communication context
            if email.subject:
                story_parts.append(f" **Current Discussion:** '{email.subject}'")
            
            if email.ai_summary and len(email.ai_summary) > 20:
                story_parts.append(f" **Latest Communication:** {email.ai_summary}")
            
            # Historical context if available
            if existing_person:
                total_emails = existing_person.total_emails or 0
                if total_emails > 1:
                    story_parts.append(f" **Communication History:** {total_emails} previous email exchanges")
                
                if existing_person.last_interaction:
                    from datetime import datetime, timezone
                    days_since = (datetime.now(timezone.utc) - existing_person.last_interaction).days
                    if days_since < 7:
                        story_parts.append(f" **Recent Activity:** Last contacted {days_since} days ago")
                    elif days_since < 30:
                        story_parts.append(f" **Regular Contact:** Last contacted {days_since} days ago")
                    else:
                        story_parts.append(f" **Reconnection:** Last contacted {days_since} days ago")
            else:
                story_parts.append(" **New Contact:** First recorded interaction")
            
            # Business impact assessment
            if self._is_strategic_communication(email):
                story_parts.append(" **Strategic Importance:** This communication has high strategic value")
            
            return "\n".join(story_parts)
            
        except Exception as e:
            logger.error(f"Error generating comprehensive relationship story: {str(e)}")
            return f"Professional contact: {sender_analysis.get('name', 'Business Contact')}"
    
    def _generate_relationship_insights(self, sender_analysis: Dict, email: Email, existing_person=None) -> str:
        """Generate actionable relationship insights and recommendations"""
        try:
            insights = []
            
            # Relationship strength assessment
            name = sender_analysis.get('name', 'Contact')
            
            if existing_person and existing_person.total_emails and existing_person.total_emails > 5:
                insights.append(f" **Strong Relationship:** You've exchanged {existing_person.total_emails} emails with {name}, indicating an established professional relationship.")
            elif existing_person and existing_person.total_emails and existing_person.total_emails > 1:
                insights.append(f" **Developing Relationship:** Building relationship with {name} through ongoing communication.")
            else:
                insights.append(f" **New Connection:** This is your first recorded interaction with {name}.")
            
            # Business value insights
            business_relevance = sender_analysis.get('business_relevance', '')
            if business_relevance:
                insights.append(f" **Business Value:** {business_relevance}")
            
            # Communication pattern insights
            if self._is_strategic_communication(email):
                insights.append(" **Strategic Relevance:** This person is involved in strategic business discussions.")
            
            # Company/role insights
            company = sender_analysis.get('company')
            role = sender_analysis.get('role')
            if company:
                insights.append(f" **Company Intelligence:** {name} works at {company}, potentially opening collaboration opportunities.")
            if role:
                insights.append(f" **Role Intelligence:** As {role}, they may have decision-making authority or specialized expertise.")
            
            # Engagement recommendations
            if existing_person and existing_person.last_interaction:
                from datetime import datetime, timezone
                days_since = (datetime.now(timezone.utc) - existing_person.last_interaction).days
                if days_since > 60:
                    insights.append(f" **Engagement Opportunity:** Consider reaching out to {name} to maintain the relationship.")
                elif days_since < 7:
                    insights.append(f" **Active Relationship:** Regular communication with {name} indicates strong engagement.")
            
            # Topic-based insights
            strategic_topic = self._extract_strategic_topic_from_email(email)
            if strategic_topic:
                insights.append(f" **Topic Expertise:** {name} is engaged in discussions about {strategic_topic}.")
            
            return "\n".join(insights)
            
        except Exception as e:
            logger.error(f"Error generating relationship insights: {str(e)}")
            return f"Professional relationship with valuable business context."
    
    def _generate_mentioned_person_relationship_story(self, person_info: Dict, email: Email, existing_person=None) -> str:
        """Generate relationship story for people mentioned in emails"""
        try:
            story_parts = []
            
            name = person_info.get('name', person_info.get('email', 'Unknown').split('@')[0])
            story_parts.append(f" **Mentioned Contact:** {name}")
            
            # Context of mention
            mentioned_context = person_info.get('mentioned_context', '')
            if mentioned_context:
                story_parts.append(f" **Mentioned In Context:** {mentioned_context}")
            
            # Business relevance
            business_relevance = person_info.get('business_relevance', '')
            if business_relevance:
                story_parts.append(f" **Business Relevance:** {business_relevance}")
            
            # Email context
            if email.subject:
                story_parts.append(f" **Discussion Context:** Mentioned in '{email.subject}'")
            
            # Historical context if available
            if existing_person and existing_person.total_emails:
                story_parts.append(f" **Previous Contact:** {existing_person.total_emails} direct communications on record")
            else:
                story_parts.append(" **Indirect Contact:** Known through mentions in communications")
            
            return "\n".join(story_parts)
            
        except Exception as e:
            logger.error(f"Error generating mentioned person story: {str(e)}")
            return f"Contact mentioned in business communications."
    
    def _generate_mentioned_person_insights(self, person_info: Dict, email: Email, existing_person=None) -> str:
        """Generate insights for people mentioned in emails"""
        try:
            insights = []
            
            name = person_info.get('name', 'Contact')
            
            # Mention analysis
            insights.append(f" **Indirect Intelligence:** {name} was mentioned in business communications, indicating relevance to your work.")
            
            # Business context
            business_relevance = person_info.get('business_relevance', '')
            if business_relevance:
                insights.append(f" **Strategic Value:** {business_relevance}")
            
            # Potential for direct engagement
            if person_info.get('email'):
                insights.append(f" **Engagement Opportunity:** Consider direct outreach to {name} for collaboration or information.")
            
            # Company intelligence
            company = person_info.get('company')
            if company:
                insights.append(f" **Company Connection:** {name} at {company} may represent partnership or business opportunities.")
            
            return "\n".join(insights)
            
        except Exception as e:
            logger.error(f"Error generating mentioned person insights: {str(e)}")
            return f"Mentioned in business context with potential strategic value."
    
    def _process_high_quality_tasks(self, user_id: int, email_id: int, tasks_data: List[Dict]) -> int:
        """Process and create high-quality tasks with COMPREHENSIVE CONTEXT STORIES"""
        tasks_created = 0
        
        # Get the source email for rich context generation
        with get_db_manager().get_session() as session:
            source_email = session.query(Email).filter(Email.id == email_id).first()
            if not source_email:
                logger.error(f"Could not find source email with ID {email_id}")
                return 0
        
        for task_data in tasks_data:
            try:
                # Enhanced quality validation
                description = task_data.get('description', '').strip()
                confidence = task_data.get('confidence', 0.0)
                
                if len(description) < 5 or confidence < self.min_confidence_score:
                    logger.debug(f"Task rejected: description='{description}', confidence={confidence}")
                    continue
                
                # GENERATE COMPREHENSIVE CONTEXT STORY
                comprehensive_context_story = self._generate_comprehensive_task_context_story(task_data, source_email)
                
                # GENERATE DETAILED TASK MEANING
                detailed_task_meaning = self._generate_detailed_task_meaning(task_data, source_email)
                
                # GENERATE COMPREHENSIVE IMPORTANCE ANALYSIS  
                comprehensive_importance_analysis = self._generate_comprehensive_importance_analysis(task_data, source_email)
                
                # GENERATE COMPREHENSIVE ORIGIN DETAILS
                comprehensive_origin_details = self._generate_comprehensive_origin_details(task_data, source_email)
                
                # Parse due date with better handling
                due_date = None
                due_date_text = task_data.get('due_date_text', '')
                if task_data.get('due_date'):
                    due_date = self._parse_due_date(task_data['due_date'])
                
                # Create enhanced task with comprehensive context
                enhanced_task_data = {
                    'description': description,
                    'assignee': task_data.get('assignee'),
                    'due_date': due_date,
                    'due_date_text': due_date_text,
                    'priority': task_data.get('priority', 'medium'),
                    'status': 'pending',
                    'category': task_data.get('category', 'action_item'),
                    'confidence': confidence,
                    'source_text': task_data.get('business_context', ''),
                    'context': task_data.get('business_context', ''),
                    'email_id': email_id,
                    'source_email_subject': source_email.subject,
                    'ai_version': self.version,
                    
                    # COMPREHENSIVE CONTEXT FIELDS - This is what makes tasks rich and detailed
                    'comprehensive_context_story': comprehensive_context_story,
                    'detailed_task_meaning': detailed_task_meaning,
                    'comprehensive_importance_analysis': comprehensive_importance_analysis,
                    'comprehensive_origin_details': comprehensive_origin_details,
                    
                    # Enhanced metadata for frontend intelligence
                    'business_intelligence': {
                        'entity_connections': 1 if source_email.sender else 0,
                        'source_quality': 'high' if source_email.ai_summary else 'medium',
                        'ai_confidence': confidence,
                        'cross_referenced': True,
                        'relationship_strength': 1,  # Will be enhanced based on sender frequency
                        'business_impact_score': confidence * 0.8,  # Strategic importance estimate
                        'action_clarity': 'high' if len(description) > 20 else 'medium',
                        'contextual_richness': 'comprehensive'
                    }
                }
                
                task = get_db_manager().create_or_update_task(user_id, enhanced_task_data)
                if task:
                    tasks_created += 1
                    logger.info(f"Created comprehensive task: {description[:50]}...")
                
            except Exception as e:
                logger.error(f"Failed to create enhanced task: {str(e)}")
                continue
        
        return tasks_created
    
    def _generate_comprehensive_task_context_story(self, task_data: Dict, source_email: Email) -> str:
        """Generate a comprehensive context story explaining the full background of this task"""
        try:
            # Build rich narrative about the task context
            story_parts = []
            
            # Email source context
            if source_email:
                sender_name = source_email.sender_name or source_email.sender.split('@')[0] if source_email.sender else "someone"
                
                # Timing context
                if source_email.email_date:
                    from datetime import datetime, timezone
                    days_ago = (datetime.now(timezone.utc) - source_email.email_date).days
                    if days_ago == 0:
                        timing = f"today ({source_email.email_date.strftime('%I:%M %p')})"
                    elif days_ago == 1:
                        timing = f"yesterday ({source_email.email_date.strftime('%I:%M %p')})"
                    elif days_ago < 7:
                        timing = f"{days_ago} days ago ({source_email.email_date.strftime('%A, %I:%M %p')})"
                    else:
                        timing = source_email.email_date.strftime('%B %d at %I:%M %p')
                else:
                    timing = "recently"
                
                story_parts.append(f" **Email from:** {sender_name}")
                story_parts.append(f" **Timing:** Received {timing}")
                
                if source_email.subject:
                    story_parts.append(f" **Subject:** '{source_email.subject}'")
                
                # Email content context
                if source_email.ai_summary and len(source_email.ai_summary) > 20:
                    story_parts.append(f" **Email Summary:** {source_email.ai_summary}")
                
                # Business category if available
                if hasattr(source_email, 'ai_category') and source_email.ai_category:
                    story_parts.append(f" **Business Category:** {source_email.ai_category}")
            
            # Task-specific context
            business_context = task_data.get('business_context', '')
            if business_context:
                story_parts.append(f" **Business Impact:** {business_context}")
            
            # Success criteria if available
            success_criteria = task_data.get('success_criteria', '')
            if success_criteria:
                story_parts.append(f" **Success Criteria:** {success_criteria}")
            
            return "\n".join(story_parts)
            
        except Exception as e:
            logger.error(f"Error generating comprehensive task context: {str(e)}")
            return f"Task from email communication requiring attention."
    
    def _generate_detailed_task_meaning(self, task_data: Dict, source_email: Email) -> str:
        """Generate detailed explanation of what this task actually means and how to complete it"""
        try:
            explanation_parts = []
            description = task_data.get('description', '')
            description_lower = description.lower()
            
            # Analyze the type of action required
            if any(word in description_lower for word in ['call', 'phone', 'ring']):
                explanation_parts.append(" **Action Type:** You need to make a phone call")
                explanation_parts.append(f" **Specific Task:** {description}")
                explanation_parts.append(" **Steps to Complete:**")
                explanation_parts.append("   1. Find contact information")
                explanation_parts.append("   2. Prepare talking points based on email context")
                explanation_parts.append("   3. Make the call")
                explanation_parts.append("   4. Follow up if needed")
                
            elif any(word in description_lower for word in ['email', 'send', 'reply', 'respond']):
                explanation_parts.append(" **Action Type:** You need to send an email or document")
                explanation_parts.append(f" **Specific Task:** {description}")
                explanation_parts.append(" **Steps to Complete:**")
                explanation_parts.append("   1. Review the original email context")
                explanation_parts.append("   2. Draft your response/email")
                explanation_parts.append("   3. Include relevant information or attachments")
                explanation_parts.append("   4. Send and track response")
                
            elif any(word in description_lower for word in ['schedule', 'book', 'meeting', 'appointment']):
                explanation_parts.append(" **Action Type:** You need to arrange a meeting or appointment")
                explanation_parts.append(f" **Specific Task:** {description}")
                explanation_parts.append(" **Steps to Complete:**")
                explanation_parts.append("   1. Check your calendar availability")
                explanation_parts.append("   2. Propose meeting times")
                explanation_parts.append("   3. Send calendar invite")
                explanation_parts.append("   4. Confirm attendance")
                
            elif any(word in description_lower for word in ['review', 'check', 'examine', 'evaluate']):
                explanation_parts.append(" **Action Type:** You need to examine or evaluate something")
                explanation_parts.append(f" **Specific Task:** {description}")
                
            elif any(word in description_lower for word in ['follow up', 'followup', 'follow-up']):
                explanation_parts.append(" **Action Type:** You need to check back on something or continue a conversation")
                explanation_parts.append(f" **Specific Task:** {description}")
                
            elif any(word in description_lower for word in ['complete', 'finish', 'deliver', 'submit']):
                explanation_parts.append(" **Action Type:** You need to complete or deliver something")
                explanation_parts.append(f" **Specific Task:** {description}")
                
            else:
                explanation_parts.append(" **Action Type:** General business action required")
                explanation_parts.append(f" **Specific Task:** {description}")
            
            # Add email context for better understanding
            if source_email and source_email.ai_summary:
                explanation_parts.append(f" **Background Context:** {source_email.ai_summary}")
            
            return "\n".join(explanation_parts)
            
        except Exception as e:
            logger.error(f"Error generating detailed task meaning: {str(e)}")
            return f"Action required: {task_data.get('description', 'Task completion needed')}"
    
    def _generate_comprehensive_importance_analysis(self, task_data: Dict, source_email: Email) -> str:
        """Generate comprehensive analysis of why this task is important"""
        try:
            importance_factors = []
            
            # Priority analysis
            priority = task_data.get('priority', 'medium')
            if priority == 'high':
                importance_factors.append(" **Priority Level:** This is marked as HIGH PRIORITY - requires immediate attention")
            elif priority == 'medium':
                importance_factors.append(" **Priority Level:** This is medium priority - should be completed soon")
            else:
                importance_factors.append(" **Priority Level:** This is standard priority")
            
            # AI confidence analysis
            confidence = task_data.get('confidence', 0.0)
            if confidence > 0.9:
                importance_factors.append(" **AI Confidence:** VERY HIGH (95%+) - This is definitely a real action item")
            elif confidence > 0.8:
                importance_factors.append(" **AI Confidence:** HIGH (80%+) - This is very likely a real action item")
            elif confidence > 0.6:
                importance_factors.append(" **AI Confidence:** MEDIUM (60%+) - This appears to be an action item")
            
            # Business impact
            business_context = task_data.get('business_context', '')
            if business_context:
                importance_factors.append(f" **Business Impact:** {business_context}")
            
            # Email source importance
            if source_email:
                if hasattr(source_email, 'strategic_importance') and source_email.strategic_importance and source_email.strategic_importance > 0.7:
                    importance_factors.append(" **Strategic Value:** The source email was marked as strategically important")
                
                if hasattr(source_email, 'urgency_score') and source_email.urgency_score and source_email.urgency_score > 0.7:
                    importance_factors.append(" **Business Impact:** The original email was marked as urgent")
                
                if hasattr(source_email, 'action_required') and source_email.action_required:
                    importance_factors.append(" **Business Impact:** The original email explicitly requested action")
            
            # Due date urgency
            due_date_text = task_data.get('due_date_text', '')
            if due_date_text:
                importance_factors.append(f" **Timing:** Deadline mentioned: {due_date_text}")
            
            return "\n".join(importance_factors)
            
        except Exception as e:
            logger.error(f"Error generating importance analysis: {str(e)}")
            return "Standard business task requiring attention."
    
    def _generate_comprehensive_origin_details(self, task_data: Dict, source_email: Email) -> str:
        """Generate comprehensive details about where this task originated"""
        try:
            origin_details = []
            
            if source_email:
                # Source identification
                if source_email.sender_name and source_email.sender:
                    origin_details.append(f" **Original Email From:** {source_email.sender_name} ({source_email.sender})")
                elif source_email.sender:
                    origin_details.append(f" **Original Email From:** {source_email.sender}")
                
                # Email details
                if source_email.subject:
                    origin_details.append(f" **Email Subject:** '{source_email.subject}'")
                
                if source_email.email_date:
                    origin_details.append(f" **Received:** {source_email.email_date.strftime('%A, %B %d, %Y at %I:%M %p')}")
                
                # Email content preview
                if hasattr(source_email, 'body_text') and source_email.body_text:
                    preview = source_email.body_text[:300].strip()
                    if len(source_email.body_text) > 300:
                        preview += "..."
                    origin_details.append(f" **Email Content Preview:** {preview}")
                elif source_email.ai_summary:
                    origin_details.append(f" **Email Content Summary:** {source_email.ai_summary}")
                
                # Processing details
                if source_email.processed_at:
                    origin_details.append(f" **AI Processed:** {source_email.processed_at.strftime('%Y-%m-%d at %I:%M %p')}")
                
                # Email metadata
                metadata = []
                if hasattr(source_email, 'thread_id') and source_email.thread_id:
                    metadata.append("Part of email thread")
                if hasattr(source_email, 'labels') and source_email.labels:
                    metadata.append(f"Gmail labels: {', '.join(source_email.labels[:3])}")
                if metadata:
                    origin_details.append(f" **Email Metadata:** {'; '.join(metadata)}")
            else:
                origin_details.append(" **Task Origin:** Created manually or from unknown source")
                origin_details.append(" **Note:** This task was not automatically extracted from an email")
            
            return "\n".join(origin_details)
            
        except Exception as e:
            logger.error(f"Error generating origin details: {str(e)}")
            return "Task created from email communication."
    
    def _prepare_enhanced_email_context(self, email: Email, user) -> str:
        """Prepare comprehensive email context for Claude analysis"""
        timestamp = email.email_date.strftime('%Y-%m-%d %H:%M') if email.email_date else 'Unknown'
        
        context = f"""EMAIL ANALYSIS REQUEST

Recipient: {user.email} ({user.name})
From: {email.sender_name or 'Unknown'} <{email.sender}>
Date: {timestamp}
Subject: {email.subject}

Email Content:
{email.body_clean or email.snippet}

Additional Context:
- Recipients: {', '.join(email.recipient_emails) if email.recipient_emails else 'Not specified'}
- Thread ID: {email.thread_id}
- Email Labels: {', '.join(email.labels) if email.labels else 'None'}
- Message Type: {email.message_type or 'Unknown'}
- Priority Score: {email.priority_score or 'Not calculated'}
"""
        return context
    
    def _update_email_with_insights(self, email: Email, analysis: Dict):
        """Update email record with Claude insights"""
        with get_db_manager().get_session() as session:
            email_record = session.query(Email).filter(Email.id == email.id).first()
            if email_record:
                email_record.ai_summary = analysis.get('summary')
                email_record.ai_category = analysis.get('ai_category')
                email_record.sentiment_score = analysis.get('sentiment_score')
                email_record.urgency_score = analysis.get('urgency_score')
                email_record.key_insights = analysis.get('business_insights')
                email_record.topics = analysis.get('topics')
                email_record.action_required = analysis.get('action_required', False)
                email_record.follow_up_required = analysis.get('follow_up_required', False)
                
                session.commit()
    
    def _process_project_insights(self, user_id: int, project_data: Dict, email: Email) -> Optional[Project]:
        """Process and update project information - SAFE VERSION"""
        if not project_data or not project_data.get('name'):
            return None
        
        try:
            project_info = {
                'name': project_data['name'],
                'slug': self._create_slug(project_data['name']),
                'description': project_data.get('description'),
                'category': project_data.get('category'),
                'priority': project_data.get('priority', 'medium'),
                'status': project_data.get('status', 'active'),
                'key_topics': project_data.get('key_topics', []),
                'stakeholders': project_data.get('stakeholders', []),
                'ai_version': self.version
            }
            
            return get_db_manager().create_or_update_project(user_id, project_info)
            
        except Exception as e:
            logger.error(f"Error processing project insights: {str(e)}")
            return None
    
    def _create_slug(self, name: str) -> str:
        """Create URL-friendly slug from name"""
        return re.sub(r'[^a-zA-Z0-9]+', '-', name.lower()).strip('-')
    
    def _parse_due_date(self, date_str: str) -> Optional[datetime]:
        """Parse due date string into datetime"""
        if not date_str:
            return None
        
        try:
            return datetime.strptime(date_str, '%Y-%m-%d')
        except:
            return None
    
    def get_business_knowledge_summary(self, user_email: str) -> Dict:
        """Get comprehensive business knowledge summary with quality synthesis"""
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            # Get all processed emails with quality filtering
            emails = get_db_manager().get_user_emails(user.id, limit=1000)
            projects = get_db_manager().get_user_projects(user.id, limit=200)
            people = get_db_manager().get_user_people(user.id, limit=500)
            
            # Filter for high-quality insights only
            quality_emails = [e for e in emails if e.ai_summary and len(e.ai_summary) > self.min_insight_length]
            human_contacts = [p for p in people if not self._is_non_human_contact(p.email_address or '')]
            substantial_projects = [p for p in projects if p.description and len(p.description) > self.min_insight_length]
            
            # Synthesize high-quality business insights
            strategic_decisions = []
            business_opportunities = []
            key_challenges = []
            competitive_insights = []
            
            for email in quality_emails:
                if email.key_insights and isinstance(email.key_insights, dict):
                    insights = email.key_insights
                    
                    # Extract strategic-level insights only
                    decisions = insights.get('key_decisions', [])
                    strategic_decisions.extend([d for d in decisions if len(d) > self.min_insight_length])
                    
                    opportunities = insights.get('strategic_opportunities', insights.get('opportunities', []))
                    business_opportunities.extend([o for o in opportunities if len(o) > self.min_insight_length])
                    
                    challenges = insights.get('business_challenges', insights.get('challenges', []))
                    key_challenges.extend([c for c in challenges if len(c) > self.min_insight_length])
                    
                    competitive = insights.get('competitive_intelligence', [])
                    competitive_insights.extend([ci for ci in competitive if len(ci) > self.min_insight_length])
            
            # Get meaningful topics
            topics = get_db_manager().get_user_topics(user.id, limit=1000)
            business_topics = [topic.name for topic in topics if topic.is_official or 
                              (topic.description and len(topic.description) > 10)]
            
            return {
                'success': True,
                'user_email': user_email,
                'business_knowledge': {
                    'summary_stats': {
                        'quality_emails_analyzed': len(quality_emails),
                        'human_contacts': len(human_contacts),
                        'substantial_projects': len(substantial_projects),
                        'strategic_insights': len(strategic_decisions) + len(business_opportunities) + len(key_challenges)
                    },
                    'strategic_intelligence': {
                        'key_decisions': self._deduplicate_and_rank(strategic_decisions)[:8],  # Top 8 strategic decisions
                        'business_opportunities': self._deduplicate_and_rank(business_opportunities)[:8],
                        'key_challenges': self._deduplicate_and_rank(key_challenges)[:8],
                        'competitive_intelligence': self._deduplicate_and_rank(competitive_insights)[:5]
                    },
                    'business_topics': business_topics[:15],  # Top 15 business topics
                    'network_intelligence': {
                        'total_human_contacts': len(human_contacts),
                        'active_projects': len([p for p in substantial_projects if p.status == 'active']),
                        'project_categories': list(set([p.category for p in substantial_projects if p.category]))
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to get business knowledge for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}

    def get_chat_knowledge_summary(self, user_email: str) -> Dict:
        """Get comprehensive knowledge summary for chat interface with enhanced context"""
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            # Get all processed data with quality filters
            emails = get_db_manager().get_user_emails(user.id, limit=1000)
            projects = get_db_manager().get_user_projects(user.id, limit=200)
            people = get_db_manager().get_user_people(user.id, limit=500)
            topics = get_db_manager().get_user_topics(user.id, limit=1000)
            
            # GET CALENDAR EVENTS FOR KNOWLEDGE BASE
            now = datetime.now(timezone.utc)
            calendar_events = get_db_manager().get_user_calendar_events(
                user.id, 
                start_date=now - timedelta(days=30),  # Past 30 days for context
                end_date=now + timedelta(days=60),    # Next 60 days for planning
                limit=200
            )
            
            # Filter for high-quality content
            quality_emails = [e for e in emails if e.ai_summary and len(e.ai_summary) > self.min_insight_length]
            human_contacts = [p for p in people if not self._is_non_human_contact(p.email_address or '') and p.name]
            
            # Compile rich contacts with enhanced professional context
            rich_contacts = []
            for person in human_contacts[:15]:  # Top 15 human contacts
                # Create rich professional story
                professional_story = self._create_professional_story(person, quality_emails)
                
                contact_info = {
                    'name': person.name,
                    'email': person.email_address,
                    'title': person.title or person.role,
                    'company': person.company,
                    'relationship': person.relationship_type,
                    'story': professional_story,
                    'total_emails': person.total_emails or 0,
                    'last_interaction': person.last_interaction.isoformat() if person.last_interaction else None,
                    'importance_score': person.importance_level or 0.5
                }
                rich_contacts.append(contact_info)
            
            # Enhanced business intelligence compilation
            business_decisions = []
            opportunities = []
            challenges = []
            
            for email in quality_emails:
                if email.key_insights and isinstance(email.key_insights, dict):
                    insights = email.key_insights
                    
                    # Enhanced insight extraction with context
                    decisions = insights.get('key_decisions', [])
                    for decision in decisions:
                        if len(decision) > self.min_insight_length:
                            business_decisions.append({
                                'decision': decision,
                                'context': email.ai_summary,
                                'sender': email.sender_name or email.sender,
                                'date': email.email_date.isoformat() if email.email_date else None
                            })
                    
                    opps = insights.get('strategic_opportunities', insights.get('opportunities', []))
                    for opp in opps:
                        if len(opp) > self.min_insight_length:
                            opportunities.append({
                                'opportunity': opp,
                                'context': email.ai_summary,
                                'source': email.sender_name or email.sender,
                                'date': email.email_date.isoformat() if email.email_date else None
                            })
                    
                    chals = insights.get('business_challenges', insights.get('challenges', []))
                    for chal in chals:
                        if len(chal) > self.min_insight_length:
                            challenges.append({
                                'challenge': chal,
                                'context': email.ai_summary,
                                'source': email.sender_name or email.sender,
                                'date': email.email_date.isoformat() if email.email_date else None
                            })
            
            # Enhanced topic knowledge with rich contexts
            topic_knowledge = {
                'all_topics': [topic.name for topic in topics if topic.is_official or 
                              (topic.description and len(topic.description) > 10)],
                'official_topics': [topic.name for topic in topics if topic.is_official],
                'topic_contexts': {}
            }
            
            for topic in topics:
                if topic.is_official or (topic.description and len(topic.description) > 10):
                    topic_emails = [email for email in quality_emails if email.topics and topic.name in email.topics]
                    contexts = []
                    for email in topic_emails[:3]:  # Top 3 emails per topic
                        if email.ai_summary:
                            contexts.append({
                                'summary': email.ai_summary,
                                'sender': email.sender_name or email.sender,
                                'date': email.email_date.isoformat() if email.email_date else None,
                                'email_subject': email.subject
                            })
                    topic_knowledge['topic_contexts'][topic.name] = contexts
            
            # Enhanced statistics
            summary_stats = {
                'total_emails_analyzed': len(quality_emails),
                'rich_contacts': len(rich_contacts),
                'business_decisions': len(business_decisions),
                'opportunities_identified': len(opportunities),
                'challenges_tracked': len(challenges),
                'active_projects': len([p for p in projects if p.status == 'active']),
                'official_topics': len([t for t in topics if t.is_official]),
                'calendar_events': len(calendar_events),
                'upcoming_meetings': len([e for e in calendar_events if e.start_time and e.start_time > now]),
                'recent_meetings': len([e for e in calendar_events if e.start_time and e.start_time < now])
            }
            
            # Process calendar intelligence for knowledge base
            calendar_intelligence = self._extract_calendar_intelligence(calendar_events, people, now)
            
            return {
                'success': True,
                'user_email': user_email,
                'knowledge_base': {
                    'summary_stats': summary_stats,
                    'rich_contacts': rich_contacts,
                    'business_intelligence': {
                        'recent_decisions': sorted(business_decisions, 
                                                 key=lambda x: x['date'] or '', reverse=True)[:8],
                        'top_opportunities': sorted(opportunities,
                                                  key=lambda x: x['date'] or '', reverse=True)[:8],
                        'current_challenges': sorted(challenges,
                                                   key=lambda x: x['date'] or '', reverse=True)[:8]
                    },
                    'topic_knowledge': topic_knowledge,
                    'projects_summary': [
                        {
                            'name': project.name,
                            'description': project.description,
                            'status': project.status,
                            'priority': project.priority,
                            'stakeholders': project.stakeholders or [],
                            'key_topics': project.key_topics or []
                        }
                        for project in projects if project.description and len(project.description) > self.min_insight_length
                    ][:10],  # Top 10 substantial projects
                    'calendar_events': calendar_events,
                    'calendar_intelligence': calendar_intelligence
                }
            }
        
        except Exception as e:
            logger.error(f"Failed to get chat knowledge for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _create_professional_story(self, person: Person, emails: List[Email]) -> str:
        """Create a rich professional story for a contact based on email interactions"""
        try:
            # Find emails from this person
            person_emails = [e for e in emails if e.sender and person.email_address and 
                           e.sender.lower() == person.email_address.lower()]
            
            if not person_emails:
                return f"Professional contact with {person.relationship_type or 'business'} relationship."
            
            # Analyze communication patterns and content
            total_emails = len(person_emails)
            recent_emails = sorted(person_emails, key=lambda x: x.email_date or datetime.min, reverse=True)[:3]
            
            # Extract key themes from their communication
            themes = []
            for email in recent_emails:
                if email.ai_summary and len(email.ai_summary) > 20:
                    themes.append(email.ai_summary)
            
            # Create professional narrative
            story_parts = []
            
            if person.company and person.title:
                story_parts.append(f"{person.title} at {person.company}")
            elif person.company:
                story_parts.append(f"Works at {person.company}")
            elif person.title:
                story_parts.append(f"{person.title}")
            
            if total_emails > 1:
                story_parts.append(f"Active correspondence with {total_emails} substantive emails")
            
            if themes:
                story_parts.append(f"Recent discussions: {'; '.join(themes[:2])}")
            
            if person.relationship_type:
                story_parts.append(f"Relationship: {person.relationship_type}")
            
            return '. '.join(story_parts) if story_parts else "Professional business contact"
            
        except Exception as e:
            logger.error(f"Error creating professional story: {str(e)}")
            return "Professional business contact"
    
    def _deduplicate_and_rank(self, items: List[str]) -> List[str]:
        """Deduplicate similar items and rank by relevance"""
        if not items:
            return []
        
        # Simple deduplication by similarity (basic approach)
        unique_items = []
        for item in items:
            # Check if this item is too similar to existing ones
            is_duplicate = False
            for existing in unique_items:
                # Simple similarity check - if 70% of words overlap, consider duplicate
                item_words = set(item.lower().split())
                existing_words = set(existing.lower().split())
                
                if len(item_words) > 0 and len(existing_words) > 0:
                    overlap = len(item_words & existing_words)
                    similarity = overlap / min(len(item_words), len(existing_words))
                    if similarity > 0.7:
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                unique_items.append(item)
        
        # Rank by length and specificity (longer, more specific items are often better)
        unique_items.sort(key=lambda x: (len(x), len(x.split())), reverse=True)
        
        return unique_items

    def _get_user_business_context(self, user_id: int) -> Dict:
        """Get existing business context to enhance AI analysis"""
        try:
            # Get existing high-quality projects
            projects = get_db_manager().get_user_projects(user_id, limit=50)
            project_context = [p.name for p in projects if p.description and len(p.description) > 20]
            
            # Get existing high-quality people
            people = get_db_manager().get_user_people(user_id, limit=100)
            people_context = [f"{p.name} ({p.role or 'Unknown role'}) at {p.company or 'Unknown company'}" 
                             for p in people if p.name and not self._is_non_human_contact(p.email_address or '')]
            
            # Get existing topics
            topics = get_db_manager().get_user_topics(user_id, limit=100)
            topic_context = [t.name for t in topics if t.is_official]
            
            return {
                'existing_projects': project_context[:10],  # Top 10 projects
                'key_contacts': people_context[:20],  # Top 20 human contacts
                'official_topics': topic_context[:15]  # Top 15 official topics
            }
        except Exception as e:
            logger.error(f"Failed to get user business context: {str(e)}")
            return {'existing_projects': [], 'key_contacts': [], 'official_topics': []}

    def _is_non_human_contact(self, email_address: str) -> bool:
        """Determine if an email address belongs to a non-human sender - BALANCED VERSION"""
        if not email_address:
            return True
            
        email_lower = email_address.lower()
        
        # FOCUSED: Only filter obvious automation, preserve business contacts
        definite_non_human_patterns = [
            'noreply', 'no-reply', 'donotreply', 'do-not-reply', 
            'mailer-daemon', 'postmaster@', 'daemon@', 'bounce@',
            'robot@', 'bot@', 'automated@', 'system@notification',
            'newsletter@', 'digest@', 'updates@notifications'
        ]
        
        # Check against definite non-human patterns only
        for pattern in definite_non_human_patterns:
            if pattern in email_lower:
                return True
        
        # SPECIFIC: Only filter major newsletter/automation services
        automation_domains = [
            'substack.com', 'beehiiv.com', 'mailchimp.com', 'constantcontact.com',
            'campaign-archive.com', 'sendgrid.net', 'mailgun.org', 'mandrill.com'
        ]
        
        for domain in automation_domains:
            if domain in email_lower:
                return True
        
        # PRESERVE: Keep business contacts that might use standard business email patterns
        # Removed: 'admin@', 'info@', 'contact@', 'help@', 'service@', 'team@', 'hello@', 'hi@'
        # Removed: 'linkedin.com', 'facebook.com', etc. - people use these for business
                
        return False

    def _filter_quality_emails_debug(self, emails: List[Email], user_email: str) -> List[Email]:
        """Enhanced filtering for quality-focused email processing - ULTRA PERMISSIVE DEBUG VERSION"""
        quality_emails = []
        
        for email in emails:
            logger.debug(f"Evaluating email from {email.sender} with subject: {email.subject}")
            
            # Skip emails from the user themselves - check both email and name
            if email.sender and user_email.lower() in email.sender.lower():
                logger.debug(f"Skipping email from user themselves: {email.sender}")
                continue
            
            # ULTRA PERMISSIVE: Accept almost all emails for debugging
            # Only skip completely empty emails
            content = email.body_clean or email.snippet or email.subject or ''
            if len(content.strip()) < 3:  # Ultra permissive - just need any content
                logger.debug(f"Skipping email with no content: {len(content)} chars")
                continue
                
            logger.debug(f"Email passed ultra-permissive quality filters: {email.sender}")
            quality_emails.append(email)
        
        logger.info(f"Quality filtering (DEBUG MODE): {len(quality_emails)} emails passed out of {len(emails)} total")
        return quality_emails

    def _is_obviously_non_human_contact(self, email_address: str) -> bool:
        """RELAXED: Only filter obviously non-human contacts - for debugging"""
        if not email_address:
            return True
            
        email_lower = email_address.lower()
        
        # Only the most obvious non-human patterns
        obvious_non_human_patterns = [
            'noreply', 'no-reply', 'donotreply', 'mailer-daemon',
            'postmaster@', 'daemon@', 'bounce@', 'automated@',
            'robot@', 'bot@'
        ]
        
        # Check against obvious non-human patterns only
        for pattern in obvious_non_human_patterns:
            if pattern in email_lower:
                logger.debug(f"Obvious non-human pattern detected: {pattern} in {email_address}")
                return True
                
        return False

    def _is_obvious_newsletter_or_promotional(self, email: Email) -> bool:
        """RELAXED: Only filter obvious newsletters - for debugging"""
        if not email:
            return True
            
        sender = (email.sender or '').lower()
        subject = (email.subject or '').lower()
        content = (email.body_clean or email.snippet or '').lower()
        
        # Only check for very obvious newsletter patterns
        obvious_newsletter_patterns = [
            'substack.com', 'mailchimp.com', 'beehiiv.com',
            'unsubscribe', 'view in browser', 'manage preferences'
        ]
        
        # Check domain patterns
        for pattern in obvious_newsletter_patterns:
            if pattern in sender or pattern in content:
                logger.debug(f"Obvious newsletter pattern detected: {pattern}")
                return True
                
        return False

    def _validate_analysis_quality_debug(self, analysis: Dict) -> bool:
        """Validate that the analysis meets quality standards - DEBUG VERSION (more permissive)"""
        try:
            # Check strategic value score - very relaxed threshold
            strategic_value = analysis.get('strategic_value_score', 0)
            if strategic_value < 0.3:  # Very relaxed - was 0.5
                logger.debug(f"Analysis rejected - low strategic value: {strategic_value}")
                return False
            
            # Check summary quality - very short minimum
            summary = analysis.get('summary', '')
            if len(summary) < 5:  # Very short minimum
                logger.debug(f"Analysis rejected - summary too short: {len(summary)} chars")
                return False
            
            # Very relaxed trivial content detection
            if len(summary) < 15 and summary.lower().strip() in ['ok', 'thanks', 'got it', 'noted']:
                logger.debug(f"Analysis rejected - trivial content detected: {summary}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"Error validating analysis quality: {str(e)}")
            return False
    
    # Helper methods for comprehensive relationship intelligence
    def _generate_communication_timeline_entry(self, email: Email) -> Dict:
        """Generate a timeline entry for this communication"""
        return {
            'date': email.email_date.isoformat() if email.email_date else None,
            'subject': email.subject,
            'summary': email.ai_summary[:100] + '...' if email.ai_summary and len(email.ai_summary) > 100 else email.ai_summary,
            'urgency': self._assess_email_urgency(email),
            'action_required': self._has_action_required(email),
            'business_category': self._categorize_communication_pattern(email)
        }
    
    def _assess_business_relevance(self, sender_analysis: Dict, email: Email) -> str:
        """Assess the business relevance of this relationship"""
        business_relevance = sender_analysis.get('business_relevance', '')
        if business_relevance:
            return business_relevance
        
        # Fallback assessment based on email content
        if self._is_strategic_communication(email):
            return 'high'
        elif sender_analysis.get('company'):
            return 'medium'
        else:
            return 'standard'
    
    def _calculate_strategic_value(self, sender_analysis: Dict, email: Email) -> float:
        """Calculate strategic value of this relationship"""
        value = 0.5  # Base value
        
        # Company factor
        if sender_analysis.get('company'):
            value += 0.2
        
        # Role factor
        role = sender_analysis.get('role', '').lower()
        if any(keyword in role for keyword in ['director', 'manager', 'ceo', 'founder', 'vp', 'head', 'lead']):
            value += 0.2
        
        # Strategic communication factor
        if self._is_strategic_communication(email):
            value += 0.3
        
        return min(1.0, value)
    
    def _assess_communication_frequency(self, existing_person) -> str:
        """Assess communication frequency pattern"""
        if not existing_person or not existing_person.total_emails:
            return 'new'
        
        total_emails = existing_person.total_emails
        if total_emails >= 20:
            return 'frequent'
        elif total_emails >= 5:
            return 'regular'
        elif total_emails >= 2:
            return 'occasional'
        else:
            return 'minimal'
    
    def _extract_strategic_topic_from_email(self, email: Email) -> str:
        """Extract the main strategic topic from email"""
        if email.subject:
            # Simple extraction of key terms
            subject_lower = email.subject.lower()
            strategic_keywords = ['project', 'meeting', 'proposal', 'partnership', 'deal', 'contract', 'strategy', 'funding', 'launch']
            for keyword in strategic_keywords:
                if keyword in subject_lower:
                    return keyword.capitalize()
        
        # Fallback to business category
        return self._categorize_communication_pattern(email)
    
    def _calculate_collaboration_score(self, sender_analysis: Dict, email: Email) -> float:
        """Calculate collaboration potential score"""
        score = 0.3  # Base score
        
        # Project involvement
        if any(word in (email.ai_summary or '').lower() for word in ['project', 'collaboration', 'work together', 'partnership']):
            score += 0.4
        
        # Role-based collaboration potential
        role = sender_analysis.get('role', '').lower()
        if any(keyword in role for keyword in ['manager', 'director', 'lead', 'coordinator']):
            score += 0.3
        
        return min(1.0, score)
    
    def _extract_expertise_areas(self, email: Email, sender_analysis: Dict) -> List[str]:
        """Extract areas of expertise from communication"""
        expertise_areas = []
        
        # From role
        role = sender_analysis.get('role', '')
        if role:
            expertise_areas.append(role)
        
        # From email content
        if email.ai_summary:
            technical_terms = ['AI', 'machine learning', 'technology', 'software', 'development', 'marketing', 'sales', 'finance', 'strategy']
            content_lower = email.ai_summary.lower()
            for term in technical_terms:
                if term.lower() in content_lower:
                    expertise_areas.append(term)
        
        return list(set(expertise_areas))[:3]  # Limit to top 3
    
    def _is_meeting_participant(self, email: Email) -> bool:
        """Check if this person is likely a meeting participant"""
        if email.subject:
            meeting_indicators = ['meeting', 'call', 'zoom', 'teams', 'conference', 'discussion']
            return any(indicator in email.subject.lower() for indicator in meeting_indicators)
        return False
    
    def _assess_communication_style(self, email: Email) -> str:
        """Assess communication style"""
        if email.subject:
            subject_lower = email.subject.lower()
            if any(word in subject_lower for word in ['urgent', 'asap', 'immediately']):
                return 'urgent'
            elif any(word in subject_lower for word in ['follow up', 'checking in', 'update']):
                return 'collaborative'
            elif any(word in subject_lower for word in ['meeting', 'call', 'discussion']):
                return 'meeting-focused'
        return 'professional'
    
    def _assess_email_urgency(self, email: Email) -> float:
        """Assess urgency of email communication"""
        if email.subject:
            urgent_keywords = ['urgent', 'asap', 'immediately', 'critical', 'emergency']
            subject_lower = email.subject.lower()
            if any(keyword in subject_lower for keyword in urgent_keywords):
                return 0.9
            elif any(keyword in subject_lower for keyword in ['important', 'priority']):
                return 0.7
        return 0.5  # Default moderate urgency
    
    def _extract_strategic_topics_list(self, email: Email) -> List[str]:
        """Extract list of strategic topics from email"""
        topics = []
        
        # From subject
        if email.subject:
            # Extract project names, company names, etc.
            words = email.subject.split()
            for word in words:
                if len(word) > 3 and word[0].isupper():  # Likely proper noun
                    topics.append(word)
        
        # From business categories
        category = self._categorize_communication_pattern(email)
        if category:
            topics.append(category)
        
        return list(set(topics))[:5]  # Limit to top 5
    
    def _categorize_communication_pattern(self, email: Email) -> str:
        """Categorize the communication pattern"""
        if email.subject:
            subject_lower = email.subject.lower()
            if any(word in subject_lower for word in ['meeting', 'call', 'zoom']):
                return 'meeting_coordination'
            elif any(word in subject_lower for word in ['project', 'task', 'deliverable']):
                return 'project_management'
            elif any(word in subject_lower for word in ['follow up', 'status', 'update']):
                return 'status_update'
            elif any(word in subject_lower for word in ['proposal', 'contract', 'agreement']):
                return 'business_development'
        return 'general_business'
    
    def _extract_project_involvement(self, email: Email, sender_analysis: Dict) -> List[Dict]:
        """Extract project involvement information"""
        projects = []
        
        if email.ai_summary:
            # Simple project detection
            summary_lower = email.ai_summary.lower()
            if 'project' in summary_lower:
                projects.append({
                    'project': 'Ongoing Project',
                    'email_date': email.email_date.isoformat() if email.email_date else None,
                    'role': 'collaborator',
                    'context': email.ai_summary[:100] + '...' if len(email.ai_summary) > 100 else email.ai_summary
                })
        
        return projects
    
    def _is_strategic_communication(self, email: Email) -> bool:
        """Check if this is a strategic communication"""
        if hasattr(email, 'strategic_importance') and email.strategic_importance:
            return email.strategic_importance > 0.7
        
        # Fallback analysis
        if email.subject:
            strategic_keywords = ['strategy', 'strategic', 'important', 'critical', 'partnership', 'deal', 'funding', 'board']
            return any(keyword in email.subject.lower() for keyword in strategic_keywords)
        
        return False
    
    def _format_last_strategic_communication(self, email: Email) -> Dict:
        """Format strategic communication details"""
        return {
            'date': email.email_date.isoformat() if email.email_date else None,
            'subject': email.subject,
            'summary': email.ai_summary,
            'importance': getattr(email, 'strategic_importance', 0.8),
            'category': self._categorize_communication_pattern(email),
            'action_required': self._has_action_required(email),
            'urgency': self._assess_email_urgency(email)
        }
    
    def _extract_key_decisions(self, analysis: Dict, email: Email) -> List[str]:
        """Extract key decisions from analysis"""
        decisions = []
        business_insights = analysis.get('business_insights', {})
        if isinstance(business_insights, dict):
            decisions.extend(business_insights.get('key_decisions', []))
        return decisions[:3]  # Limit to top 3
    
    def _extract_opportunities(self, analysis: Dict, email: Email) -> List[str]:
        """Extract opportunities from analysis"""
        opportunities = []
        business_insights = analysis.get('business_insights', {})
        if isinstance(business_insights, dict):
            opportunities.extend(business_insights.get('strategic_opportunities', []))
        return opportunities[:3]  # Limit to top 3
    
    def _extract_challenges(self, analysis: Dict, email: Email) -> List[str]:
        """Extract challenges from analysis"""
        challenges = []
        business_insights = analysis.get('business_insights', {})
        if isinstance(business_insights, dict):
            challenges.extend(business_insights.get('business_challenges', []))
        return challenges[:2]  # Limit to top 2
    
    def _extract_collaboration_projects(self, email: Email, analysis: Dict) -> List[str]:
        """Extract collaboration projects"""
        projects = []
        
        # From project analysis
        project_data = analysis.get('project', {})
        if project_data and project_data.get('name'):
            projects.append(project_data['name'])
        
        # From email content
        if email.subject and 'project' in email.subject.lower():
            projects.append(email.subject)
        
        return list(set(projects))[:3]
    
    def _extract_expertise_indicators(self, email: Email, sender_analysis: Dict) -> List[str]:
        """Extract expertise indicators"""
        indicators = []
        
        # From role
        role = sender_analysis.get('role', '')
        if role:
            indicators.append(role)
        
        # From company
        company = sender_analysis.get('company', '')
        if company:
            indicators.append(f"{company} expertise")
        
        return indicators[:2]
    
    def _calculate_meeting_frequency(self, email: Email) -> int:
        """Calculate meeting frequency indicator"""
        if self._is_meeting_participant(email):
            return 1
        return 0
    
    def _assess_response_reliability(self, existing_person) -> str:
        """Assess response reliability"""
        if not existing_person:
            return 'unknown'
        
        # Simple assessment based on email frequency
        if existing_person.total_emails and existing_person.total_emails >= 5:
            return 'reliable'
        elif existing_person.total_emails and existing_person.total_emails >= 2:
            return 'moderate'
        else:
            return 'limited'
    
    def _calculate_avg_email_importance(self, email: Email) -> float:
        """Calculate average email importance"""
        if hasattr(email, 'strategic_importance') and email.strategic_importance:
            return email.strategic_importance
        
        # Fallback calculation
        if self._is_strategic_communication(email):
            return 0.8
        else:
            return 0.5
    
    def _assess_relationship_trend(self, existing_person) -> str:
        """Assess relationship trend"""
        if not existing_person:
            return 'new'
        
        # Simple trend assessment
        if existing_person.total_emails and existing_person.total_emails >= 10:
            return 'stable'
        elif existing_person.total_emails and existing_person.total_emails >= 3:
            return 'growing'
        else:
            return 'developing'
    
    def _assess_engagement_level(self, email: Email, sender_analysis: Dict) -> str:
        """Assess engagement level"""
        # High engagement indicators
        if self._is_strategic_communication(email):
            return 'high'
        elif sender_analysis.get('company') and sender_analysis.get('role'):
            return 'medium'
        else:
            return 'standard'
    
    def _assess_communication_consistency(self, existing_person) -> str:
        """Assess communication consistency"""
        if not existing_person:
            return 'new'
        
        if existing_person.total_emails and existing_person.total_emails >= 15:
            return 'consistent'
        elif existing_person.total_emails and existing_person.total_emails >= 5:
            return 'regular'
        else:
            return 'sporadic'
    
    def _calculate_business_value_score(self, sender_analysis: Dict, email: Email) -> float:
        """Calculate business value score"""
        value = 0.3  # Base value
        
        # Strategic communication adds value
        if self._is_strategic_communication(email):
            value += 0.4
        
        # Company affiliation adds value
        if sender_analysis.get('company'):
            value += 0.2
        
        # Role authority adds value
        role = sender_analysis.get('role', '').lower()
        if any(keyword in role for keyword in ['director', 'manager', 'ceo', 'vp']):
            value += 0.3
        
        return min(1.0, value)
    
    def _calculate_collaboration_strength(self, email: Email, analysis: Dict) -> float:
        """Calculate collaboration strength"""
        strength = 0.2  # Base strength
        
        # Project involvement
        if analysis.get('project'):
            strength += 0.5
        
        # Meeting coordination
        if self._is_meeting_participant(email):
            strength += 0.3
        
        return min(1.0, strength)
    
    def _assess_decision_influence(self, analysis: Dict, email: Email) -> float:
        """Assess decision influence"""
        influence = 0.3  # Base influence
        
        # Key decisions mentioned
        business_insights = analysis.get('business_insights', {})
        if isinstance(business_insights, dict) and business_insights.get('key_decisions'):
            influence += 0.4
        
        # Strategic communication
        if self._is_strategic_communication(email):
            influence += 0.3
        
        return min(1.0, influence)
    
    def _assess_urgency_compatibility(self, email: Email) -> str:
        """Assess urgency compatibility"""
        urgency = self._assess_email_urgency(email)
        
        if urgency > 0.7:
            return 'high-urgency'
        elif urgency > 0.4:
            return 'moderate-urgency'
        else:
            return 'low-urgency'
    
    def _has_action_required(self, email: Email) -> bool:
        """Check if email has action required"""
        if hasattr(email, 'action_required') and email.action_required:
            return True
        
        # Fallback analysis
        if email.subject:
            action_keywords = ['please', 'need', 'require', 'action', 'respond', 'reply', 'confirm']
            return any(keyword in email.subject.lower() for keyword in action_keywords)
        
        return False

# Global instance
email_intelligence = EmailIntelligenceProcessor() 

============================================================
FILE: chief_of_staff_ai/processors/email_quality_filter.py
============================================================
"""
 Email Quality Filter - Intelligent Email Injection System
==========================================================

This module implements a sophisticated email quality filtering system based on user engagement patterns.
The system categorizes contacts into tiers and filters email injection accordingly.

TIER SYSTEM:
- Tier 1: People you respond to regularly (HIGH QUALITY - Always process)
- Tier 2: New contacts or occasional contacts (MEDIUM QUALITY - Process with caution)
- Tier LAST: Contacts you never respond to (LOW QUALITY - Ignore completely)

Author: AI Chief of Staff
Created: December 2024
"""

import logging
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Set, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import json
import re
from collections import defaultdict, Counter
from email.utils import parseaddr

from models.database import get_db_manager
from models.enhanced_models import Email, Person, Task
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, desc, func

logger = logging.getLogger(__name__)

class ContactTier(Enum):
    """Contact tier classifications based on engagement patterns"""
    TIER_1 = "tier_1"           # High engagement - always respond to
    TIER_2 = "tier_2"           # Medium engagement - new or occasional 
    TIER_LAST = "tier_last"     # No engagement - consistently ignore
    UNCLASSIFIED = "unclassified"  # Not yet analyzed

@dataclass
class ContactEngagementStats:
    """Statistics for contact engagement analysis"""
    email_address: str
    name: Optional[str]
    emails_received: int
    emails_responded_to: int
    last_email_date: datetime
    first_email_date: datetime
    response_rate: float
    days_since_last_email: int
    avg_days_between_emails: float
    tier: ContactTier
    tier_reason: str
    should_process: bool

@dataclass
class EmailQualityResult:
    """Result of email quality assessment"""
    should_process: bool
    tier: ContactTier
    reason: str
    sender_stats: Optional[ContactEngagementStats]
    confidence: float

class EmailQualityFilter:
    """
    Intelligent email quality filtering system that categorizes contacts
    based on engagement patterns and filters email injection accordingly.
    """
    
    def __init__(self):
        """Initialize the EmailQualityFilter with configuration"""
        from models.database import get_db_manager
        
        self.db_manager = get_db_manager()
        self._contact_tiers: Dict[str, ContactEngagementStats] = {}
        self._last_tier_update: Optional[datetime] = None
        
        # Configuration for tier classification thresholds
        self.TIER_1_MIN_RESPONSE_RATE = 0.5  # 50% response rate for Tier 1
        self.TIER_LAST_MAX_RESPONSE_RATE = 0.1  # 10% max for Tier LAST
        self.TIER_LAST_MIN_EMAILS = 5  # Need at least 5 emails to classify as Tier LAST
        self.MIN_EMAILS_FOR_CLASSIFICATION = 3  # Minimum emails needed for tier classification
        self.NEW_CONTACT_GRACE_PERIOD = 30  # Days to give new contacts grace period
        self.MONTHLY_REVIEW_DAYS = 30  # Review tiers every 30 days
        
        # Clear any corrupted cache data on startup
        self.clear_corrupted_cache()
        
    def analyze_email_quality(self, email_data: Dict, user_id: int) -> EmailQualityResult:
        """
        Main entry point: Analyze if an email should be processed based on sender quality.
        
        Args:
            email_data: Email data dictionary with sender, subject, body, etc.
            user_id: User ID for analysis
            
        Returns:
            EmailQualityResult with processing decision and reasoning
        """
        try:
            sender_email = self._extract_sender_email(email_data)
            if not sender_email:
                return EmailQualityResult(
                    should_process=False,
                    tier=ContactTier.UNCLASSIFIED,
                    reason="No valid sender email found",
                    sender_stats=None,
                    confidence=1.0
                )
            
            # Check if we need to update contact tiers
            if self._should_refresh_tiers():
                logger.info(f" Refreshing contact tiers for user {user_id}")
                self._analyze_all_contacts(user_id)
            
            # Get sender engagement stats
            sender_stats = self._get_contact_stats(sender_email, user_id)
            
            # Make processing decision based on tier
            should_process, reason, confidence = self._make_processing_decision(sender_stats, email_data)
            
            logger.info(f" Email quality check: {sender_email} -> {sender_stats.tier.value} -> {'PROCESS' if should_process else 'SKIP'}")
            
            return EmailQualityResult(
                should_process=should_process,
                tier=sender_stats.tier,
                reason=reason,
                sender_stats=sender_stats,
                confidence=confidence
            )
            
        except Exception as e:
            logger.error(f" Email quality analysis error: {str(e)}")
            # Fail open - process email if analysis fails
            return EmailQualityResult(
                should_process=True,
                tier=ContactTier.UNCLASSIFIED,
                reason=f"Analysis error: {str(e)}",
                sender_stats=None,
                confidence=0.0
            )
    
    def _extract_sender_email(self, email_data: Dict) -> Optional[str]:
        """Extract and normalize sender email address"""
        sender = email_data.get('sender') or email_data.get('from') or email_data.get('From')
        if not sender:
            return None
        
        # Extract email from various formats: "Name <email@domain.com>" or "email@domain.com"
        email_match = re.search(r'[\w\.-]+@[\w\.-]+\.\w+', sender)
        if email_match:
            return email_match.group(0).lower().strip()
        
        return None
    
    def _should_refresh_tiers(self) -> bool:
        """Check if contact tiers need to be refreshed"""
        if not self._last_tier_update:
            return True
        
        days_since_update = (datetime.now() - self._last_tier_update).days
        return days_since_update >= self.MONTHLY_REVIEW_DAYS
    
    def _analyze_all_contacts(self, user_id: int):
        """
        Comprehensive analysis of all contacts to determine tiers.
        This is the core intelligence that implements your engagement-based tiering.
        """
        logger.info(f" Running comprehensive contact tier analysis for user {user_id}")
        
        with self.db_manager.get_session() as session:
            # Get all emails for analysis
            all_emails = session.query(Email).filter(Email.user_id == user_id).all()
            
            # Get user's sent emails to identify who they respond to
            sent_emails = [email for email in all_emails if self._is_sent_email(email)]
            received_emails = [email for email in all_emails if not self._is_sent_email(email)]
            
            logger.info(f" Analyzing {len(received_emails)} received emails and {len(sent_emails)} sent emails")
            
            # Build engagement statistics
            contact_stats = self._build_engagement_statistics(received_emails, sent_emails)
            
            # Classify contacts into tiers
            self._classify_contacts_into_tiers(contact_stats)
            
            # Cache results
            self._contact_tiers = {stats.email_address: stats for stats in contact_stats.values()}
            self._last_tier_update = datetime.now()
            
            # Log tier summary
            self._log_tier_summary(contact_stats)
    
    def _is_sent_email(self, email: Email) -> bool:
        """Determine if an email was sent by the user"""
        # Heuristics to identify sent emails
        if hasattr(email, 'is_sent') and email.is_sent:
            return True
        
        # Check common sent folder indicators
        if hasattr(email, 'folder') and email.folder:
            sent_indicators = ['sent', 'outbox', 'drafts']
            return any(indicator in email.folder.lower() for indicator in sent_indicators)
        
        # Check subject for "Re:" or "Fwd:" patterns and check if it's a response
        if hasattr(email, 'subject') and email.subject:
            # This is a simplified heuristic - in real implementation you'd want more sophisticated detection
            return False
        
        return False
    
    def _build_engagement_statistics(self, received_emails: List[Email], sent_emails: List[Email]) -> Dict[str, ContactEngagementStats]:
        """Build comprehensive engagement statistics for all contacts"""
        contact_stats = {}
        
        # Group received emails by sender
        emails_by_sender = defaultdict(list)
        for email in received_emails:
            sender = self._extract_sender_email({'sender': email.sender})
            if sender:
                emails_by_sender[sender].append(email)
        
        # Build sent email lookup for response detection
        sent_subjects = set()
        sent_recipients = set()
        for email in sent_emails:
            if hasattr(email, 'recipient_emails') and email.recipient_emails:
                # Extract recipients from sent emails
                recipients = self._extract_email_addresses(email.recipient_emails)
                sent_recipients.update(recipients)
            
            if hasattr(email, 'subject') and email.subject:
                sent_subjects.add(email.subject.lower().strip())
        
        # Analyze each contact
        for sender_email, sender_emails in emails_by_sender.items():
            if len(sender_emails) == 0:
                continue
            
            # Calculate basic stats
            emails_received = len(sender_emails)
            first_email_date = min(email.email_date for email in sender_emails if email.email_date)
            last_email_date = max(email.email_date for email in sender_emails if email.email_date)
            
            # Calculate response rate (sophisticated heuristic)
            emails_responded_to = self._calculate_response_count(sender_emails, sent_subjects, sent_recipients, sender_email)
            response_rate = emails_responded_to / emails_received if emails_received > 0 else 0.0
            
            # Calculate timing statistics
            days_since_last = (datetime.now() - last_email_date).days if last_email_date else 999
            total_days = (last_email_date - first_email_date).days if first_email_date and last_email_date else 1
            avg_days_between = total_days / max(1, emails_received - 1) if emails_received > 1 else 0
            
            # Get contact name
            contact_name = sender_emails[0].sender_name if hasattr(sender_emails[0], 'sender_name') else None
            
            stats = ContactEngagementStats(
                email_address=sender_email,
                name=contact_name,
                emails_received=emails_received,
                emails_responded_to=emails_responded_to,
                last_email_date=last_email_date,
                first_email_date=first_email_date,
                response_rate=response_rate,
                days_since_last_email=days_since_last,
                avg_days_between_emails=avg_days_between,
                tier=ContactTier.UNCLASSIFIED,  # Will be set in classification step
                tier_reason="",
                should_process=True
            )
            
            contact_stats[sender_email] = stats
        
        return contact_stats
    
    def _extract_email_addresses(self, recipients_string: str) -> List[str]:
        """Extract email addresses from recipients string"""
        if not recipients_string:
            return []
        
        emails = re.findall(r'[\w\.-]+@[\w\.-]+\.\w+', recipients_string)
        return [email.lower().strip() for email in emails]
    
    def _calculate_response_count(self, sender_emails: List[Email], sent_subjects: Set[str], sent_recipients: Set[str], sender_email: str) -> int:
        """
        Calculate how many emails from this sender we responded to.
        Uses sophisticated heuristics to detect responses.
        """
        responses = 0
        
        # Check if we ever sent emails to this sender
        if sender_email in sent_recipients:
            responses += 1  # Basic engagement indicator
        
        # Check for subject-based response patterns
        for email in sender_emails:
            if not email.subject:
                continue
            
            subject = email.subject.lower().strip()
            
            # Look for response patterns in sent emails
            response_patterns = [
                f"re: {subject}",
                f"re:{subject}",
                subject  # Exact match might indicate a response
            ]
            
            for pattern in response_patterns:
                if pattern in sent_subjects:
                    responses += 1
                    break
        
        return min(responses, len(sender_emails))  # Cap at number of emails received
    
    def _classify_contacts_into_tiers(self, contact_stats: Dict[str, ContactEngagementStats]):
        """
        Classify contacts into tiers based on engagement patterns.
        This implements your core tiering logic.
        """
        for email_address, stats in contact_stats.items():
            tier, reason, should_process = self._determine_contact_tier(stats)
            
            stats.tier = tier
            stats.tier_reason = reason
            stats.should_process = should_process
    
    def _determine_contact_tier(self, stats: ContactEngagementStats) -> Tuple[ContactTier, str, bool]:
        """
        Core logic to determine contact tier based on engagement statistics.
        Implements your specified tiering rules.
        """
        # Get user's email addresses
        from models.database import get_db_manager
        user = get_db_manager().get_user_by_email(stats.email_address)
        
        # If this is one of the user's own email addresses, always Tier 1
        if user:
            return ContactTier.TIER_1, "User's own email address", True
            
        # Check for common variations of the user's email
        if user and stats.email_address.split('@')[0] in ['sandman', 'oudi', 'oudiantebi']:
            return ContactTier.TIER_1, "User's alias email address", True
        
        # NEW: Check if this contact is from sent emails (TrustedContact) - these are automatically Tier 1
        try:
            with get_db_manager().get_session() as session:
                from models.database import TrustedContact
                trusted_contact = session.query(TrustedContact).filter(
                    TrustedContact.email_address == stats.email_address
                ).first()
                
                if trusted_contact:
                    return ContactTier.TIER_1, f"Contact from sent emails (engagement: {trusted_contact.engagement_score:.1f})", True
        except Exception as e:
            logger.warning(f"Could not check TrustedContact for {stats.email_address}: {e}")
        
        # Tier 1: People you respond to regularly (HIGH QUALITY)
        if stats.response_rate >= self.TIER_1_MIN_RESPONSE_RATE:
            return ContactTier.TIER_1, f"High response rate ({stats.response_rate:.1%})", True
        
        # Tier LAST: People you consistently ignore (LOW QUALITY)
        if (stats.emails_received >= self.TIER_LAST_MIN_EMAILS and 
            stats.response_rate <= self.TIER_LAST_MAX_RESPONSE_RATE and
            stats.days_since_last_email <= 60):  # Still actively emailing
            return ContactTier.TIER_LAST, f"Low response rate ({stats.response_rate:.1%}) with {stats.emails_received} emails", False
        
        # New contacts (grace period)
        if stats.days_since_last_email <= self.NEW_CONTACT_GRACE_PERIOD:
            return ContactTier.TIER_2, "New contact (grace period)", True
        
        # Insufficient data for classification
        if stats.emails_received < self.MIN_EMAILS_FOR_CLASSIFICATION:
            return ContactTier.TIER_2, f"Insufficient data ({stats.emails_received} emails)", True
        
        # Default to Tier 2 (MEDIUM QUALITY)
        return ContactTier.TIER_2, f"Medium engagement ({stats.response_rate:.1%})", True
    
    def _log_tier_summary(self, contact_stats: Dict[str, ContactEngagementStats]):
        """Log summary of tier classification results"""
        tier_counts = Counter(stats.tier for stats in contact_stats.values())
        
        logger.info(" Contact Tier Classification Summary:")
        logger.info(f"    Tier 1 (High Quality): {tier_counts[ContactTier.TIER_1]} contacts")
        logger.info(f"     Tier 2 (Medium Quality): {tier_counts[ContactTier.TIER_2]} contacts")
        logger.info(f"     Tier LAST (Low Quality): {tier_counts[ContactTier.TIER_LAST]} contacts")
        logger.info(f"    Unclassified: {tier_counts[ContactTier.UNCLASSIFIED]} contacts")
        
        # Log some examples
        tier_1_examples = [stats.email_address for stats in contact_stats.values() if stats.tier == ContactTier.TIER_1][:3]
        tier_last_examples = [stats.email_address for stats in contact_stats.values() if stats.tier == ContactTier.TIER_LAST][:3]
        
        if tier_1_examples:
            logger.info(f"    Tier 1 examples: {', '.join(tier_1_examples)}")
        if tier_last_examples:
            logger.info(f"     Tier LAST examples: {', '.join(tier_last_examples)}")
    
    def _get_contact_stats(self, sender_email: str, user_id: int) -> ContactEngagementStats:
        """Get cached contact statistics or analyze on-demand"""
        
        # Return cached stats if available
        if sender_email in self._contact_tiers:
            return self._contact_tiers[sender_email]
        
        # Analyze this specific contact on-demand
        logger.info(f" On-demand analysis for new contact: {sender_email}")
        
        with self.db_manager.get_session() as session:
            # Get emails from this sender
            sender_emails = session.query(Email).filter(
                and_(Email.user_id == user_id, Email.sender.ilike(f"%{sender_email}%"))
            ).all()
            
            if not sender_emails:
                # New contact - no history
                stats = ContactEngagementStats(
                    email_address=sender_email,
                    name=None,
                    emails_received=0,
                    emails_responded_to=0,
                    last_email_date=datetime.now(),
                    first_email_date=datetime.now(),
                    response_rate=0.0,
                    days_since_last_email=0,
                    avg_days_between_emails=0.0,
                    tier=ContactTier.TIER_2,
                    tier_reason="New contact",
                    should_process=True
                )
            else:
                # Quick analysis for this contact
                stats = self._quick_contact_analysis(sender_emails, sender_email, user_id)
            
            # Cache the result
            self._contact_tiers[sender_email] = stats
            return stats
    
    def _quick_contact_analysis(self, sender_emails: List[Email], sender_email: str, user_id: int) -> ContactEngagementStats:
        """Perform quick analysis for a single contact"""
        
        emails_received = len(sender_emails)
        first_email_date = min(email.email_date for email in sender_emails if email.email_date)
        last_email_date = max(email.email_date for email in sender_emails if email.email_date)
        
        # Quick response rate estimation (simplified)
        with self.db_manager.get_session() as session:
            sent_to_sender = session.query(Email).filter(
                and_(
                    Email.user_id == user_id,
                    Email.recipient_emails.ilike(f"%{sender_email}%")
                )
            ).count()
        
        response_rate = min(1.0, sent_to_sender / emails_received) if emails_received > 0 else 0.0
        
        days_since_last = (datetime.now() - last_email_date).days if last_email_date else 0
        
        stats = ContactEngagementStats(
            email_address=sender_email,
            name=sender_emails[0].sender_name if hasattr(sender_emails[0], 'sender_name') else None,
            emails_received=emails_received,
            emails_responded_to=sent_to_sender,
            last_email_date=last_email_date,
            first_email_date=first_email_date,
            response_rate=response_rate,
            days_since_last_email=days_since_last,
            avg_days_between_emails=0.0,
            tier=ContactTier.UNCLASSIFIED,
            tier_reason="Quick analysis",
            should_process=True
        )
        
        tier, reason, should_process = self._determine_contact_tier(stats)
        stats.tier = tier
        stats.tier_reason = reason
        stats.should_process = should_process
        
        return stats
    
    def _make_processing_decision(self, sender_stats: ContactEngagementStats, email_data: Dict) -> Tuple[bool, str, float]:
        """
        Make the final decision on whether to process this email based on sender tier
        and additional email characteristics.
        """
        
        # Base decision on sender tier
        base_decision = sender_stats.should_process
        base_reason = f"Sender tier: {sender_stats.tier.value} - {sender_stats.tier_reason}"
        
        # Additional quality checks
        confidence = 0.8
        
        # Check for obvious spam/promotional indicators
        subject = email_data.get('subject', '').lower()
        spam_indicators = ['unsubscribe', 'marketing', 'promotion', 'sale', 'deal', 'offer', '% off']
        
        if any(indicator in subject for indicator in spam_indicators):
            if sender_stats.tier != ContactTier.TIER_1:  # Don't filter Tier 1 contacts
                return False, f"{base_reason} + Spam indicators detected", 0.9
        
        # Check for very short or empty content
        body = email_data.get('body', '') or email_data.get('body_text', '')
        if len(body.strip()) < 50 and sender_stats.tier == ContactTier.TIER_LAST:
            return False, f"{base_reason} + Low content quality", 0.85
        
        return base_decision, base_reason, confidence
    
    def force_tier_refresh(self, user_id: int):
        """Force a refresh of contact tiers (useful for manual testing)"""
        logger.info(f" Forcing contact tier refresh for user {user_id}")
        self._contact_tiers.clear()
        self._last_tier_update = None
        self._analyze_all_contacts(user_id)

    def clear_corrupted_cache(self):
        """Clear any corrupted cache data and reinitialize"""
        logger.info(" Clearing potentially corrupted contact tier cache")
        
        # Check for corrupted objects in cache
        corrupted_keys = []
        for email_address, obj in self._contact_tiers.items():
            if not isinstance(obj, ContactEngagementStats):
                logger.warning(f" Found corrupted object in cache: {email_address} -> {type(obj)}")
                corrupted_keys.append(email_address)
        
        # Remove corrupted entries
        for key in corrupted_keys:
            del self._contact_tiers[key]
        
        if corrupted_keys:
            logger.info(f" Removed {len(corrupted_keys)} corrupted cache entries")
        
        # Reset timestamps to force fresh analysis
        self._last_tier_update = None
    
    def get_contact_tier_summary(self, user_id: int) -> Dict:
        """Get a summary of contact tiers for reporting/debugging"""
        if not self._contact_tiers:
            self._analyze_all_contacts(user_id)
        
        tier_summary = {
            'total_contacts': len(self._contact_tiers),
            'last_updated': self._last_tier_update.isoformat() if self._last_tier_update else None,
            'tier_counts': {},
            'examples': {}
        }
        
        # Count by tier
        for tier in ContactTier:
            contacts_in_tier = [stats for stats in self._contact_tiers.values() if stats.tier == tier]
            tier_summary['tier_counts'][tier.value] = len(contacts_in_tier)
            
            # Add examples
            examples = [stats.email_address for stats in contacts_in_tier[:5]]
            tier_summary['examples'][tier.value] = examples
        
        return tier_summary
    
    def override_contact_tier(self, email_address: str, new_tier: ContactTier, reason: str = "Manual override"):
        """Allow manual override of contact tier (for edge cases)"""
        email_address = email_address.lower().strip()
        
        if email_address in self._contact_tiers:
            # Safety check: ensure we have a ContactEngagementStats object
            contact_stats = self._contact_tiers[email_address]
            if not isinstance(contact_stats, ContactEngagementStats):
                logger.error(f" Invalid object type in contact tiers: {type(contact_stats)} for {email_address}")
                # Create a proper ContactEngagementStats object
                contact_stats = ContactEngagementStats(
                    email_address=email_address,
                    name=None,
                    emails_received=0,
                    emails_responded_to=0,
                    last_email_date=datetime.now(),
                    first_email_date=datetime.now(),
                    response_rate=0.0,
                    days_since_last_email=0,
                    avg_days_between_emails=0.0,
                    tier=new_tier,
                    tier_reason=reason,
                    should_process=new_tier != ContactTier.TIER_LAST
                )
                self._contact_tiers[email_address] = contact_stats
            else:
                old_tier = contact_stats.tier
                contact_stats.tier = new_tier
                contact_stats.tier_reason = reason
                contact_stats.should_process = new_tier != ContactTier.TIER_LAST
                
                logger.info(f"  Manual tier override: {email_address} {old_tier.value} -> {new_tier.value}")
        else:
            # Create new contact stats for unknown contact
            contact_stats = ContactEngagementStats(
                email_address=email_address,
                name=None,
                emails_received=0,
                emails_responded_to=0,
                last_email_date=datetime.now(),
                first_email_date=datetime.now(),
                response_rate=0.0,
                days_since_last_email=0,
                avg_days_between_emails=0.0,
                tier=new_tier,
                tier_reason=reason,
                should_process=new_tier != ContactTier.TIER_LAST
            )
            self._contact_tiers[email_address] = contact_stats
            logger.info(f"  Created new contact with tier: {email_address} -> {new_tier.value}")

    def cleanup_existing_low_quality_data(self, user_id: int) -> Dict[str, Any]:
        """
        Clean up existing database records that came from Tier LAST contacts.
        This removes emails, tasks, and insights generated from low-quality contacts.
        
        Args:
            user_id: User ID to clean up data for
            
        Returns:
            Dictionary with cleanup statistics
        """
        try:
            from models.database import get_db_manager, Email, Task, Person
            
            logger.info(f" Starting cleanup of low-quality data for user {user_id}")
            
            # Get contact tier summary to identify Tier LAST contacts
            tier_summary = self.get_contact_tier_summary(user_id)
            
            cleanup_stats = {
                'emails_removed': 0,
                'tasks_removed': 0,
                'people_removed': 0,
                'insights_cleaned': 0,
                'tier_last_contacts': 0
            }
            
            with get_db_manager().get_session() as session:
                # Get all people for this user
                all_people = session.query(Person).filter(Person.user_id == user_id).all()
                
                tier_last_emails = set()
                tier_last_people_ids = []
                
                for person in all_people:
                    if person.email_address:
                        contact_stats = self._get_contact_stats(person.email_address.lower(), user_id)
                        
                        if contact_stats.tier == ContactTier.TIER_LAST:
                            tier_last_emails.add(person.email_address.lower())
                            tier_last_people_ids.append(person.id)
                            cleanup_stats['tier_last_contacts'] += 1
                
                logger.info(f"  Found {len(tier_last_emails)} Tier LAST contacts to clean up")
                
                # Remove emails from Tier LAST contacts
                if tier_last_emails:
                    emails_to_remove = session.query(Email).filter(
                        Email.user_id == user_id,
                        Email.sender.ilike_any([f"%{email}%" for email in tier_last_emails])
                    ).all()
                    
                    for email in emails_to_remove:
                        session.delete(email)
                        cleanup_stats['emails_removed'] += 1
                
                # Remove tasks that might have been generated from these emails
                # This is approximate - we can't definitively trace task origin
                if tier_last_people_ids:
                    # Remove tasks that mention these people in description
                    all_tasks = session.query(Task).filter(Task.user_id == user_id).all()
                    
                    for task in all_tasks:
                        if task.description:
                            # Check if task mentions any Tier LAST contact
                            task_desc_lower = task.description.lower()
                            for person_id in tier_last_people_ids:
                                person = session.query(Person).get(person_id)
                                if person and person.name:
                                    if person.name.lower() in task_desc_lower:
                                        session.delete(task)
                                        cleanup_stats['tasks_removed'] += 1
                                        break
                
                # Optionally remove Tier LAST people entirely (uncomment if desired)
                # for person_id in tier_last_people_ids:
                #     person = session.query(Person).get(person_id)
                #     if person:
                #         session.delete(person)
                #         cleanup_stats['people_removed'] += 1
                
                session.commit()
                
            logger.info(f" Cleanup complete: {cleanup_stats}")
            
            return {
                'success': True,
                'cleanup_stats': cleanup_stats,
                'message': f"Cleaned up {cleanup_stats['emails_removed']} emails and {cleanup_stats['tasks_removed']} tasks from {cleanup_stats['tier_last_contacts']} Tier LAST contacts"
            }
            
        except Exception as e:
            logger.error(f" Cleanup error: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }

    def set_all_contacts_tier_1(self, user_email: str):
        """Set all contacts from sent emails to Tier 1"""
        from models.database import get_db_manager
        
        try:
            # Get user from database
            db_user = get_db_manager().get_user_by_email(user_email)
            if not db_user:
                logger.error(f"User {user_email} not found")
                return False
            
            # Get all sent emails
            with get_db_manager().get_session() as session:
                sent_emails = session.query(Email).filter(
                    Email.user_id == db_user.id,
                    Email.sender.ilike(f'%{user_email}%')  # Emails sent by the user
                ).all()
                
                # Extract all unique recipients
                recipients = set()
                for email in sent_emails:
                    # Add the user's own email addresses
                    if email.sender:
                        sender_email = parseaddr(email.sender)[1].lower()
                        if sender_email and '@' in sender_email:
                            recipients.add(sender_email)
                    
                    # Add recipients
                    if email.recipient_emails:
                        if isinstance(email.recipient_emails, str):
                            try:
                                recipient_list = json.loads(email.recipient_emails)
                            except:
                                recipient_list = [email.recipient_emails]
                        else:
                            recipient_list = email.recipient_emails
                            
                        for recipient in recipient_list:
                            email_addr = parseaddr(recipient)[1].lower()
                            if email_addr and '@' in email_addr:
                                recipients.add(email_addr)
                
                # Set all recipients to Tier 1 with proper stats
                for recipient in recipients:
                    stats = ContactEngagementStats(
                        email_address=recipient,
                        name=None,  # We don't have the name here
                        emails_received=1,  # Placeholder value
                        emails_responded_to=1,  # Assume responded since it's from sent emails
                        last_email_date=datetime.now(timezone.utc),
                        first_email_date=datetime.now(timezone.utc),
                        response_rate=1.0,  # Perfect response rate for Tier 1
                        days_since_last_email=0,
                        avg_days_between_emails=0,
                        tier=ContactTier.TIER_1,
                        tier_reason="Sent email contact",
                        should_process=True
                    )
                    self._contact_tiers[recipient] = stats
                
                logger.info(f" Set {len(recipients)} contacts to Tier 1 for {user_email}")
                return True
                
        except Exception as e:
            logger.error(f"Failed to set contacts to Tier 1: {str(e)}")
            return False

# Global instance
email_quality_filter = EmailQualityFilter()

def analyze_email_quality(email_data: Dict, user_id: int) -> EmailQualityResult:
    """
    Convenience function for email quality analysis.
    
    Usage:
        result = analyze_email_quality(email_data, user_id)
        if result.should_process:
            # Process the email
            pass
    """
    return email_quality_filter.analyze_email_quality(email_data, user_id)

def force_refresh_contact_tiers(user_id: int):
    """Force refresh of contact tiers (useful for monthly review)"""
    email_quality_filter.force_tier_refresh(user_id)

def get_contact_tier_summary(user_id: int) -> Dict:
    """Get summary of contact tiers for debugging/monitoring"""
    return email_quality_filter.get_contact_tier_summary(user_id) 

============================================================
FILE: chief_of_staff_ai/processors/enhanced_processors/enhanced_data_normalizer.py
============================================================
# Enhanced Data Normalizer - Stub Implementation
import logging
from typing import Dict, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class NormalizationResult:
    success: bool = True
    normalized_data: Dict = None
    quality_score: float = 0.8
    issues_found: list = None
    processing_notes: list = None

class EnhancedDataNormalizer:
    """Stub implementation of enhanced data normalizer"""
    
    def normalize_email_data(self, email_data: Dict) -> NormalizationResult:
        """Normalize email data"""
        try:
            # Basic normalization - just pass through the data
            return NormalizationResult(
                success=True,
                normalized_data=email_data,
                quality_score=0.8,
                issues_found=[],
                processing_notes=["Stub normalizer - basic pass-through"]
            )
        except Exception as e:
            logger.error(f"Error in email normalization: {str(e)}")
            return NormalizationResult(
                success=False,
                normalized_data={},
                quality_score=0.0,
                issues_found=[str(e)],
                processing_notes=[]
            )
    
    def normalize_calendar_data(self, calendar_data: Dict) -> NormalizationResult:
        """Normalize calendar data"""
        try:
            # Basic normalization - just pass through the data
            return NormalizationResult(
                success=True,
                normalized_data=calendar_data,
                quality_score=0.8,
                issues_found=[],
                processing_notes=["Stub normalizer - basic pass-through"]
            )
        except Exception as e:
            logger.error(f"Error in calendar normalization: {str(e)}")
            return NormalizationResult(
                success=False,
                normalized_data={},
                quality_score=0.0,
                issues_found=[str(e)],
                processing_notes=[]
            )

# Global instance
enhanced_data_normalizer = EnhancedDataNormalizer() 
FILE: chief_of_staff_ai/processors/enhanced_processors/__init__.py - Package initialization file

============================================================
FILE: chief_of_staff_ai/processors/enhanced_processors/enhanced_email_processor.py
============================================================
# Enhanced Email Processor - Entity-Centric Email Intelligence
# This replaces the old email_intelligence.py with unified entity engine integration

import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
import json
import re

from processors.unified_entity_engine import entity_engine, EntityContext
from processors.enhanced_ai_pipeline import enhanced_ai_processor
from processors.realtime_processor import realtime_processor
from models.enhanced_models import Email, Person, Topic, Task, CalendarEvent

logger = logging.getLogger(__name__)

class EnhancedEmailProcessor:
    """
    Enhanced email processor that leverages the unified entity engine and real-time processing.
    This replaces the old email_intelligence.py with context-aware, entity-integrated email analysis.
    """
    
    def __init__(self):
        self.entity_engine = entity_engine
        self.ai_processor = enhanced_ai_processor
        self.realtime_processor = realtime_processor
        
    # =====================================================================
    # MAIN EMAIL PROCESSING METHODS
    # =====================================================================
    
    def process_email_comprehensive(self, email_data: Dict, user_id: int, 
                                   real_time: bool = True) -> Dict[str, Any]:
        """
        Comprehensive email processing with entity creation and relationship building.
        This is the main entry point that replaces old email processing.
        """
        try:
            logger.info(f"Processing email comprehensively for user {user_id}")
            
            # Step 1: Normalize email data
            normalized_email = self._normalize_email_data(email_data)
            
            # Step 2: Check for duplicates
            if self._is_duplicate_email(normalized_email, user_id):
                logger.info(f"Duplicate email detected, skipping processing")
                return {'success': True, 'result': {'status': 'duplicate', 'processed': False}}
            
            # Step 3: Use enhanced AI pipeline for comprehensive processing
            if real_time:
                # Queue for real-time processing
                self.realtime_processor.process_new_email(normalized_email, user_id, priority=5)
                
                return {
                    'success': True, 
                    'result': {
                        'status': 'queued_for_realtime',
                        'processed': True,
                        'message': 'Email queued for real-time intelligence processing'
                    }
                }
            else:
                # Process immediately
                result = self.ai_processor.process_email_with_context(normalized_email, user_id)
                
                if result.success:
                    # Extract comprehensive processing summary
                    summary = {
                        'email_id': normalized_email.get('gmail_id'),
                        'processing_summary': {
                            'entities_created': result.entities_created,
                            'entities_updated': result.entities_updated,
                            'processing_time': result.processing_time,
                            'insights_generated': len(result.insights_generated)
                        },
                        'intelligence_summary': self._create_intelligence_summary(result, normalized_email),
                        'entity_relationships': self._extract_entity_relationships(result),
                        'action_items': self._extract_action_items(result),
                        'strategic_insights': result.insights_generated
                    }
                    
                    logger.info(f"Successfully processed email: {summary['processing_summary']}")
                    return {'success': True, 'result': summary}
                else:
                    logger.error(f"Failed to process email: {result.error}")
                    return {'success': False, 'error': result.error}
                    
        except Exception as e:
            logger.error(f"Error in comprehensive email processing: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def process_email_batch(self, email_list: List[Dict], user_id: int, 
                           batch_size: int = 10) -> Dict[str, Any]:
        """
        Process multiple emails in batches with efficiency optimizations.
        """
        try:
            logger.info(f"Processing batch of {len(email_list)} emails for user {user_id}")
            
            # Get user context once for the entire batch
            user_context = self.ai_processor._gather_user_context(user_id)
            
            results = {
                'total_emails': len(email_list),
                'processed': 0,
                'failed': 0,
                'duplicates': 0,
                'batch_summary': {
                    'total_entities_created': {'people': 0, 'topics': 0, 'tasks': 0, 'projects': 0},
                    'total_insights': 0,
                    'processing_time': 0.0
                },
                'individual_results': []
            }
            
            # Process in batches
            for i in range(0, len(email_list), batch_size):
                batch = email_list[i:i + batch_size]
                batch_results = self._process_email_batch_chunk(batch, user_id, user_context)
                
                # Aggregate results
                for result in batch_results:
                    results['individual_results'].append(result)
                    
                    if result['success']:
                        if result['result'].get('status') == 'duplicate':
                            results['duplicates'] += 1
                        else:
                            results['processed'] += 1
                            # Aggregate batch summary
                            processing_summary = result['result'].get('processing_summary', {})
                            entities_created = processing_summary.get('entities_created', {})
                            
                            for entity_type, count in entities_created.items():
                                results['batch_summary']['total_entities_created'][entity_type] += count
                                
                            results['batch_summary']['total_insights'] += processing_summary.get('insights_generated', 0)
                            results['batch_summary']['processing_time'] += processing_summary.get('processing_time', 0)
                    else:
                        results['failed'] += 1
            
            logger.info(f"Batch processing complete: {results['processed']} processed, "
                       f"{results['failed']} failed, {results['duplicates']} duplicates")
            
            return {'success': True, 'result': results}
            
        except Exception as e:
            logger.error(f"Error in batch email processing: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def analyze_email_patterns(self, user_id: int, days_back: int = 30) -> Dict[str, Any]:
        """
        Analyze email communication patterns and generate insights.
        """
        try:
            from models.database import get_db_manager
            
            cutoff_date = datetime.utcnow() - timedelta(days=days_back)
            
            with get_db_manager().get_session() as session:
                emails = session.query(Email).filter(
                    Email.user_id == user_id,
                    Email.email_date > cutoff_date
                ).all()
                
                patterns = {
                    'total_emails': len(emails),
                    'communication_patterns': self._analyze_communication_patterns(emails),
                    'topic_trends': self._analyze_topic_trends(emails, user_id),
                    'relationship_activity': self._analyze_relationship_activity(emails, user_id),
                    'business_intelligence': self._generate_business_intelligence(emails, user_id),
                    'productivity_insights': self._generate_productivity_insights_from_emails(emails),
                    'strategic_recommendations': self._generate_strategic_recommendations(emails, user_id)
                }
                
                return {'success': True, 'result': patterns}
                
        except Exception as e:
            logger.error(f"Error analyzing email patterns: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    # =====================================================================
    # EMAIL INTELLIGENCE EXTRACTION
    # =====================================================================
    
    def extract_meeting_requests(self, email_data: Dict, user_id: int) -> Dict[str, Any]:
        """
        Extract meeting requests and create calendar preparation tasks.
        """
        try:
            # Check if email contains meeting-related content
            email_content = email_data.get('body_clean', '')
            subject = email_data.get('subject', '')
            
            meeting_indicators = [
                'meeting', 'call', 'discussion', 'catch up', 'sync',
                'available', 'schedule', 'calendar', 'time', 'when'
            ]
            
            has_meeting_content = any(indicator in email_content.lower() or 
                                    indicator in subject.lower() 
                                    for indicator in meeting_indicators)
            
            if not has_meeting_content:
                return {'success': True, 'result': {'has_meeting_request': False}}
            
            # Use AI to extract meeting details
            meeting_extraction_prompt = self._create_meeting_extraction_prompt(email_data)
            
            # This would call Claude to extract meeting details
            # For now, return a structured response
            meeting_info = {
                'has_meeting_request': True,
                'meeting_type': 'discussion',
                'suggested_participants': [email_data.get('sender')],
                'topic_hints': self._extract_topic_hints_from_content(email_content),
                'urgency_level': self._assess_meeting_urgency(email_content, subject),
                'preparation_tasks': self._generate_meeting_prep_tasks(email_data, user_id)
            }
            
            return {'success': True, 'result': meeting_info}
            
        except Exception as e:
            logger.error(f"Error extracting meeting requests: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def extract_business_context(self, email_data: Dict, user_id: int) -> Dict[str, Any]:
        """
        Extract business context and strategic intelligence from email.
        """
        try:
            # Get user context for enhanced analysis
            user_context = self.ai_processor._gather_user_context(user_id)
            
            context_analysis = {
                'business_category': self._categorize_business_content(email_data),
                'strategic_importance': self._assess_strategic_importance(email_data, user_context),
                'stakeholder_analysis': self._analyze_stakeholders(email_data, user_id),
                'project_connections': self._identify_project_connections(email_data, user_context),
                'decision_points': self._extract_decision_points(email_data),
                'follow_up_requirements': self._identify_follow_up_requirements(email_data),
                'competitive_intelligence': self._extract_competitive_intelligence(email_data),
                'opportunity_signals': self._detect_opportunity_signals(email_data, user_context)
            }
            
            return {'success': True, 'result': context_analysis}
            
        except Exception as e:
            logger.error(f"Error extracting business context: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def enhance_with_historical_context(self, email_data: Dict, user_id: int) -> Dict[str, Any]:
        """
        Enhance email analysis with historical communication context.
        """
        try:
            sender_email = email_data.get('sender', '')
            if not sender_email:
                return {'success': True, 'result': {'has_history': False}}
            
            # Get historical communication with this sender
            historical_context = self._get_sender_history(sender_email, user_id)
            
            if not historical_context:
                return {'success': True, 'result': {'has_history': False}}
            
            # Analyze communication patterns
            enhancement = {
                'has_history': True,
                'communication_frequency': historical_context['frequency'],
                'relationship_strength': historical_context['strength'],
                'common_topics': historical_context['topics'],
                'interaction_patterns': historical_context['patterns'],
                'relationship_trajectory': self._analyze_relationship_trajectory(historical_context),
                'contextual_insights': self._generate_contextual_insights(email_data, historical_context),
                'recommended_response_tone': self._recommend_response_tone(historical_context),
                'priority_adjustment': self._adjust_priority_with_history(email_data, historical_context)
            }
            
            return {'success': True, 'result': enhancement}
            
        except Exception as e:
            logger.error(f"Error enhancing with historical context: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    # =====================================================================
    # HELPER METHODS
    # =====================================================================
    
    def _normalize_email_data(self, email_data: Dict) -> Dict:
        """Normalize email data for consistent processing"""
        normalized = email_data.copy()
        
        # Ensure required fields exist
        required_fields = ['gmail_id', 'subject', 'sender', 'body_clean', 'email_date']
        for field in required_fields:
            if field not in normalized:
                normalized[field] = ''
        
        # Clean and normalize text fields
        if normalized.get('subject'):
            normalized['subject'] = self._clean_text(normalized['subject'])
        
        if normalized.get('body_clean'):
            normalized['body_clean'] = self._clean_text(normalized['body_clean'])
        
        # Normalize sender email
        if normalized.get('sender'):
            normalized['sender'] = normalized['sender'].lower().strip()
        
        return normalized
    
    def _is_duplicate_email(self, email_data: Dict, user_id: int) -> bool:
        """Check if email has already been processed with AI"""
        try:
            from models.database import get_db_manager
            
            gmail_id = email_data.get('gmail_id')
            if not gmail_id:
                return False
            
            with get_db_manager().get_session() as session:
                existing = session.query(Email).filter(
                    Email.user_id == user_id,
                    Email.gmail_id == gmail_id,
                    Email.ai_summary.isnot(None)  # Only consider it duplicate if AI-processed
                ).first()
                
                return existing is not None
                
        except Exception as e:
            logger.error(f"Error checking for duplicate email: {str(e)}")
            return False
    
    def _process_email_batch_chunk(self, email_batch: List[Dict], user_id: int, user_context: Dict) -> List[Dict]:
        """Process a chunk of emails in a batch"""
        results = []
        
        for email_data in email_batch:
            try:
                # Use cached context for efficiency
                result = self.ai_processor.process_email_with_context(
                    email_data, user_id, user_context
                )
                
                if result.success:
                    summary = {
                        'email_id': email_data.get('gmail_id'),
                        'processing_summary': {
                            'entities_created': result.entities_created,
                            'entities_updated': result.entities_updated,
                            'processing_time': result.processing_time,
                            'insights_generated': len(result.insights_generated)
                        }
                    }
                    results.append({'success': True, 'result': summary})
                else:
                    results.append({'success': False, 'error': result.error})
                    
            except Exception as e:
                results.append({'success': False, 'error': str(e)})
        
        return results
    
    def _create_intelligence_summary(self, result: Any, email_data: Dict) -> Dict:
        """Create intelligence summary from processing result"""
        return {
            'business_summary': 'Email processed with entity-centric intelligence',
            'key_entities': {
                'people_mentioned': result.entities_created.get('people', 0),
                'topics_discussed': result.entities_created.get('topics', 0),
                'tasks_extracted': result.entities_created.get('tasks', 0),
                'projects_referenced': result.entities_created.get('projects', 0)
            },
            'strategic_value': 'Medium',  # This would be calculated
            'follow_up_required': result.entities_created.get('tasks', 0) > 0
        }
    
    def _extract_entity_relationships(self, result: Any) -> List[Dict]:
        """Extract entity relationships from processing result"""
        # This would extract actual relationships
        # For now return placeholder
        return [
            {
                'relationship_type': 'person_discusses_topic',
                'entities': ['person:1', 'topic:2'],
                'strength': 0.8
            }
        ]
    
    def _extract_action_items(self, result: Any) -> List[Dict]:
        """Extract action items from processing result"""
        action_items = []
        
        # Tasks created are action items
        if result.entities_created.get('tasks', 0) > 0:
            action_items.append({
                'type': 'tasks_created',
                'count': result.entities_created['tasks'],
                'description': f"Created {result.entities_created['tasks']} tasks from email analysis"
            })
        
        # People to follow up with
        if result.entities_created.get('people', 0) > 0:
            action_items.append({
                'type': 'relationship_update',
                'count': result.entities_created['people'],
                'description': f"Updated {result.entities_created['people']} person profiles"
            })
        
        return action_items
    
    def _clean_text(self, text: str) -> str:
        """Clean and normalize text content"""
        if not text:
            return ''
        
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove HTML entities
        text = re.sub(r'&[a-zA-Z0-9#]+;', '', text)
        
        return text.strip()
    
    # =====================================================================
    # ANALYSIS METHODS
    # =====================================================================
    
    def _analyze_communication_patterns(self, emails: List[Email]) -> Dict:
        """Analyze communication patterns from emails"""
        patterns = {
            'emails_per_day': len(emails) / 30,  # Assuming 30-day period
            'top_senders': self._get_top_senders(emails),
            'response_time_analysis': self._analyze_response_times(emails),
            'communication_times': self._analyze_communication_times(emails),
            'email_categories': self._categorize_emails(emails)
        }
        return patterns
    
    def _analyze_topic_trends(self, emails: List[Email], user_id: int) -> Dict:
        """Analyze topic trends from email communications"""
        try:
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                # Get topics mentioned in recent emails
                topic_mentions = {}
                
                for email in emails:
                    if hasattr(email, 'primary_topic') and email.primary_topic:
                        topic_name = email.primary_topic.name
                        if topic_name not in topic_mentions:
                            topic_mentions[topic_name] = 0
                        topic_mentions[topic_name] += 1
                
                # Sort by frequency
                trending_topics = sorted(topic_mentions.items(), key=lambda x: x[1], reverse=True)
                
                return {
                    'trending_topics': trending_topics[:10],
                    'total_topics_discussed': len(topic_mentions),
                    'topic_distribution': topic_mentions
                }
                
        except Exception as e:
            logger.error(f"Error analyzing topic trends: {str(e)}")
            return {}
    
    def _analyze_relationship_activity(self, emails: List[Email], user_id: int) -> Dict:
        """Analyze relationship activity from emails"""
        sender_activity = {}
        
        for email in emails:
            sender = email.sender
            if sender not in sender_activity:
                sender_activity[sender] = {
                    'email_count': 0,
                    'last_contact': None,
                    'avg_importance': 0
                }
            
            sender_activity[sender]['email_count'] += 1
            sender_activity[sender]['last_contact'] = email.email_date
            
            if email.strategic_importance:
                current_avg = sender_activity[sender]['avg_importance']
                count = sender_activity[sender]['email_count']
                sender_activity[sender]['avg_importance'] = (
                    (current_avg * (count - 1) + email.strategic_importance) / count
                )
        
        # Sort by activity level
        active_relationships = sorted(
            sender_activity.items(), 
            key=lambda x: x[1]['email_count'], 
            reverse=True
        )
        
        return {
            'most_active_contacts': active_relationships[:10],
            'total_unique_contacts': len(sender_activity),
            'relationship_distribution': sender_activity
        }
    
    def _generate_business_intelligence(self, emails: List[Email], user_id: int) -> Dict:
        """Generate business intelligence from email patterns"""
        intelligence = {
            'communication_health': self._assess_communication_health(emails),
            'business_momentum': self._assess_business_momentum(emails),
            'opportunity_indicators': self._detect_opportunity_indicators(emails),
            'risk_signals': self._detect_risk_signals(emails),
            'strategic_priorities': self._identify_strategic_priorities(emails)
        }
        return intelligence
    
    def _generate_productivity_insights_from_emails(self, emails: List[Email]) -> List[str]:
        """Generate productivity insights from email analysis"""
        insights = []
        
        # Email volume analysis
        daily_average = len(emails) / 30
        if daily_average > 50:
            insights.append("High email volume detected. Consider email management strategies.")
        elif daily_average < 10:
            insights.append("Low email volume. Good email management or potential communication gaps.")
        
        # Response time analysis
        urgent_emails = [e for e in emails if e.urgency_score and e.urgency_score > 0.7]
        if urgent_emails:
            insights.append(f"{len(urgent_emails)} urgent emails detected. Prioritize timely responses.")
        
        return insights
    
    def _generate_strategic_recommendations(self, emails: List[Email], user_id: int) -> List[str]:
        """Generate strategic recommendations from email analysis"""
        recommendations = []
        
        # Analyze communication patterns for recommendations
        high_importance_emails = [e for e in emails if e.strategic_importance and e.strategic_importance > 0.7]
        
        if high_importance_emails:
            recommendations.append(
                f"Focus on {len(high_importance_emails)} high-importance communications for strategic impact."
            )
        
        # Analyze relationship building opportunities
        unique_senders = len(set(e.sender for e in emails))
        if unique_senders > 20:
            recommendations.append(
                "Consider consolidating communications or delegating to manage relationship bandwidth."
            )
        
        return recommendations
    
    # =====================================================================
    # UTILITY METHODS
    # =====================================================================
    
    def _get_top_senders(self, emails: List[Email]) -> List[Dict]:
        """Get top email senders by frequency"""
        sender_counts = {}
        for email in emails:
            sender = email.sender
            if sender not in sender_counts:
                sender_counts[sender] = 0
            sender_counts[sender] += 1
        
        sorted_senders = sorted(sender_counts.items(), key=lambda x: x[1], reverse=True)
        return [{'sender': sender, 'count': count} for sender, count in sorted_senders[:10]]
    
    def _analyze_response_times(self, emails: List[Email]) -> Dict:
        """Analyze email response time patterns"""
        # This would analyze response times between emails in threads
        return {
            'avg_response_time_hours': 4.5,
            'fastest_response_minutes': 15,
            'slowest_response_days': 3
        }
    
    def _analyze_communication_times(self, emails: List[Email]) -> Dict:
        """Analyze when communications typically happen"""
        hour_distribution = {}
        
        for email in emails:
            if email.email_date:
                hour = email.email_date.hour
                if hour not in hour_distribution:
                    hour_distribution[hour] = 0
                hour_distribution[hour] += 1
        
        return {
            'peak_hours': sorted(hour_distribution.items(), key=lambda x: x[1], reverse=True)[:3],
            'hourly_distribution': hour_distribution
        }
    
    def _categorize_emails(self, emails: List[Email]) -> Dict:
        """Categorize emails by business type"""
        categories = {}
        
        for email in emails:
            category = email.business_category or 'uncategorized'
            if category not in categories:
                categories[category] = 0
            categories[category] += 1
        
        return categories
    
    def _assess_communication_health(self, emails: List[Email]) -> str:
        """Assess overall communication health"""
        if len(emails) > 100:
            return "High activity"
        elif len(emails) > 50:
            return "Moderate activity"
        else:
            return "Low activity"
    
    def _assess_business_momentum(self, emails: List[Email]) -> str:
        """Assess business momentum from email patterns"""
        high_importance_count = len([e for e in emails if e.strategic_importance and e.strategic_importance > 0.7])
        
        if high_importance_count > 20:
            return "High momentum"
        elif high_importance_count > 10:
            return "Moderate momentum"
        else:
            return "Low momentum"
    
    def _detect_opportunity_indicators(self, emails: List[Email]) -> List[str]:
        """Detect opportunity indicators from emails"""
        indicators = []
        
        # Look for specific keywords or patterns
        opportunity_keywords = ['opportunity', 'partnership', 'proposal', 'deal', 'collaboration']
        
        for email in emails:
            content = (email.ai_summary or '').lower()
            for keyword in opportunity_keywords:
                if keyword in content:
                    indicators.append(f"Opportunity signal: {keyword} mentioned")
                    break
        
        return indicators[:5]  # Limit to top 5
    
    def _detect_risk_signals(self, emails: List[Email]) -> List[str]:
        """Detect risk signals from emails"""
        signals = []
        
        risk_keywords = ['concern', 'issue', 'problem', 'delay', 'budget', 'urgent']
        
        for email in emails:
            content = (email.ai_summary or '').lower()
            for keyword in risk_keywords:
                if keyword in content:
                    signals.append(f"Risk signal: {keyword} mentioned")
                    break
        
        return signals[:5]  # Limit to top 5
    
    def _identify_strategic_priorities(self, emails: List[Email]) -> List[str]:
        """Identify strategic priorities from email patterns"""
        priorities = []
        
        # Analyze high-importance topics
        high_importance_emails = [e for e in emails if e.strategic_importance and e.strategic_importance > 0.7]
        
        if high_importance_emails:
            priorities.append(f"Focus on {len(high_importance_emails)} high-strategic-value communications")
        
        return priorities

# Global instance for easy import
enhanced_email_processor = EnhancedEmailProcessor() 

============================================================
FILE: chief_of_staff_ai/processors/enhanced_processors/enhanced_task_processor.py
============================================================
# Enhanced Task Processor - Entity-Centric Task Management
# This replaces the old task_extractor.py with unified entity engine integration

import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
import json

from processors.unified_entity_engine import entity_engine, EntityContext
from processors.enhanced_ai_pipeline import enhanced_ai_processor
from models.enhanced_models import Task, Person, Topic, Email, CalendarEvent

logger = logging.getLogger(__name__)

class EnhancedTaskProcessor:
    """
    Enhanced task processor that leverages the unified entity engine.
    This replaces the old task_extractor.py with context-aware, entity-integrated task management.
    """
    
    def __init__(self):
        self.entity_engine = entity_engine
        self.ai_processor = enhanced_ai_processor
        
    # =====================================================================
    # MAIN TASK PROCESSING METHODS
    # =====================================================================
    
    def process_tasks_from_email(self, email_data: Dict, user_id: int) -> Dict[str, Any]:
        """
        Process tasks from email using enhanced AI pipeline and entity context.
        This replaces the old scattered task extraction with unified processing.
        """
        try:
            logger.info(f"Processing tasks from email for user {user_id}")
            
            # Use enhanced AI pipeline for comprehensive processing
            context = EntityContext(
                source_type='email',
                source_id=email_data.get('id'),
                user_id=user_id,
                confidence=0.8
            )
            
            # Single AI call that handles tasks, entities, and relationships
            result = self.ai_processor.process_email_with_context(email_data, user_id)
            
            if result.success:
                # Extract task-specific information from the comprehensive result
                task_summary = {
                    'tasks_created': result.entities_created.get('tasks', 0),
                    'task_details': self._extract_task_details_from_result(result, user_id),
                    'related_entities': {
                        'people': result.entities_created.get('people', 0),
                        'topics': result.entities_created.get('topics', 0),
                        'projects': result.entities_created.get('projects', 0)
                    },
                    'processing_time': result.processing_time,
                    'insights': result.insights_generated
                }
                
                logger.info(f"Successfully processed {task_summary['tasks_created']} tasks with full context")
                return {'success': True, 'result': task_summary}
            else:
                logger.error(f"Failed to process email tasks: {result.error}")
                return {'success': False, 'error': result.error}
                
        except Exception as e:
            logger.error(f"Error in enhanced task processing: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def process_tasks_from_calendar_event(self, event_data: Dict, user_id: int) -> Dict[str, Any]:
        """
        Process preparation tasks from calendar events with attendee intelligence.
        """
        try:
            logger.info(f"Processing meeting prep tasks for user {user_id}")
            
            # Use enhanced AI pipeline for meeting preparation
            result = self.ai_processor.enhance_calendar_event_with_intelligence(event_data, user_id)
            
            if result.success:
                task_summary = {
                    'prep_tasks_created': result.entities_created.get('tasks', 0),
                    'task_details': self._extract_prep_task_details(result, event_data, user_id),
                    'meeting_intelligence': {
                        'attendee_analysis': result.entities_updated.get('people', 0),
                        'business_context': 'Meeting context enhanced with email intelligence'
                    },
                    'insights': result.insights_generated
                }
                
                logger.info(f"Created {task_summary['prep_tasks_created']} preparation tasks")
                return {'success': True, 'result': task_summary}
            else:
                return {'success': False, 'error': result.error}
                
        except Exception as e:
            logger.error(f"Error in calendar task processing: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def create_manual_task_with_context(self, task_description: str, 
                                      assignee_email: str = None,
                                      topic_names: List[str] = None,
                                      project_name: str = None,
                                      due_date: datetime = None,
                                      priority: str = 'medium',
                                      user_id: int = None) -> Dict[str, Any]:
        """
        Create manual task with full entity context and relationships.
        This provides the same functionality as the old system but with entity integration.
        """
        try:
            context = EntityContext(
                source_type='manual',
                user_id=user_id,
                confidence=1.0  # High confidence for manual tasks
            )
            
            # Use unified entity engine for task creation with full context
            task = entity_engine.create_task_with_full_context(
                description=task_description,
                assignee_email=assignee_email,
                topic_names=topic_names or [],
                context=context,
                due_date=due_date,
                priority=priority
            )
            
            if task:
                # Create project relationship if specified
                if project_name:
                    self._link_task_to_project(task, project_name, user_id)
                
                task_details = {
                    'task_id': task.id,
                    'description': task.description,
                    'context_story': task.context_story,
                    'assignee': assignee_email,
                    'priority': priority,
                    'due_date': due_date.isoformat() if due_date else None,
                    'related_topics': topic_names or [],
                    'related_project': project_name
                }
                
                logger.info(f"Created manual task with full context: {task.description[:50]}...")
                return {'success': True, 'result': task_details}
            else:
                return {'success': False, 'error': 'Failed to create task'}
                
        except Exception as e:
            logger.error(f"Error creating manual task: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    # =====================================================================
    # TASK MANAGEMENT AND UPDATES
    # =====================================================================
    
    def update_task_status(self, task_id: int, new_status: str, user_id: int, 
                          completion_notes: str = None) -> Dict[str, Any]:
        """
        Update task status with intelligence propagation to related entities.
        """
        try:
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                task = session.query(Task).filter(
                    Task.id == task_id,
                    Task.user_id == user_id
                ).first()
                
                if not task:
                    return {'success': False, 'error': 'Task not found'}
                
                old_status = task.status
                task.status = new_status
                task.updated_at = datetime.utcnow()
                
                if new_status == 'completed':
                    task.completed_at = datetime.utcnow()
                
                # Add completion notes if provided
                if completion_notes:
                    task.context_story = f"{task.context_story}. Completed: {completion_notes}"
                
                session.commit()
                
                # Propagate task completion intelligence to related entities
                self._propagate_task_status_update(task, old_status, new_status, user_id)
                
                # Generate insights from task completion patterns
                if new_status == 'completed':
                    self._analyze_task_completion_patterns(task, user_id)
                
                result = {
                    'task_id': task_id,
                    'old_status': old_status,
                    'new_status': new_status,
                    'updated_at': task.updated_at.isoformat(),
                    'completed_at': task.completed_at.isoformat() if task.completed_at else None
                }
                
                logger.info(f"Updated task {task_id} status: {old_status} -> {new_status}")
                return {'success': True, 'result': result}
                
        except Exception as e:
            logger.error(f"Error updating task status: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def get_user_tasks_with_context(self, user_id: int, 
                                  status_filter: str = None,
                                  priority_filter: str = None,
                                  limit: int = 100) -> Dict[str, Any]:
        """
        Get user tasks with full entity context and relationships.
        """
        try:
            from models.database import get_db_manager
            
            with get_db_manager().get_session() as session:
                query = session.query(Task).filter(Task.user_id == user_id)
                
                if status_filter:
                    query = query.filter(Task.status == status_filter)
                if priority_filter:
                    query = query.filter(Task.priority == priority_filter)
                
                tasks = query.order_by(Task.created_at.desc()).limit(limit).all()
                
                # Enrich tasks with entity context
                enriched_tasks = []
                for task in tasks:
                    task_data = {
                        'id': task.id,
                        'description': task.description,
                        'context_story': task.context_story,
                        'status': task.status,
                        'priority': task.priority,
                        'confidence': task.confidence,
                        'created_at': task.created_at.isoformat(),
                        'updated_at': task.updated_at.isoformat(),
                        'due_date': task.due_date.isoformat() if task.due_date else None,
                        'completed_at': task.completed_at.isoformat() if task.completed_at else None,
                        
                        # Entity relationships
                        'assignee': self._get_task_assignee_info(task),
                        'related_topics': self._get_task_topic_info(task),
                        'source_context': self._get_task_source_context(task),
                        'entity_relationships': self._get_task_entity_relationships(task)
                    }
                    enriched_tasks.append(task_data)
                
                result = {
                    'total_tasks': len(enriched_tasks),
                    'filtered_by': {
                        'status': status_filter,
                        'priority': priority_filter
                    },
                    'tasks': enriched_tasks
                }
                
                return {'success': True, 'result': result}
                
        except Exception as e:
            logger.error(f"Error getting user tasks: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def analyze_task_patterns(self, user_id: int, days_back: int = 30) -> Dict[str, Any]:
        """
        Analyze user task patterns for productivity insights.
        """
        try:
            from models.database import get_db_manager
            
            cutoff_date = datetime.utcnow() - timedelta(days=days_back)
            
            with get_db_manager().get_session() as session:
                tasks = session.query(Task).filter(
                    Task.user_id == user_id,
                    Task.created_at > cutoff_date
                ).all()
                
                # Analyze patterns
                patterns = {
                    'total_tasks': len(tasks),
                    'completion_rate': self._calculate_completion_rate(tasks),
                    'priority_distribution': self._analyze_priority_distribution(tasks),
                    'topic_frequency': self._analyze_topic_frequency(tasks),
                    'source_breakdown': self._analyze_task_sources(tasks),
                    'productivity_trends': self._analyze_productivity_trends(tasks),
                    'insights': self._generate_productivity_insights(tasks, user_id)
                }
                
                return {'success': True, 'result': patterns}
                
        except Exception as e:
            logger.error(f"Error analyzing task patterns: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    # =====================================================================
    # HELPER METHODS
    # =====================================================================
    
    def _extract_task_details_from_result(self, result: Any, user_id: int) -> List[Dict]:
        """Extract task details from enhanced AI processing result"""
        try:
            from models.database import get_db_manager
            
            # Get recently created tasks for this user
            with get_db_manager().get_session() as session:
                recent_tasks = session.query(Task).filter(
                    Task.user_id == user_id,
                    Task.created_at > datetime.utcnow() - timedelta(minutes=5)
                ).order_by(Task.created_at.desc()).limit(10).all()
                
                task_details = []
                for task in recent_tasks:
                    task_details.append({
                        'id': task.id,
                        'description': task.description,
                        'context_story': task.context_story,
                        'priority': task.priority,
                        'confidence': task.confidence,
                        'assignee_id': task.assignee_id,
                        'source_email_id': task.source_email_id,
                        'created_at': task.created_at.isoformat()
                    })
                
                return task_details
                
        except Exception as e:
            logger.error(f"Error extracting task details: {str(e)}")
            return []
    
    def _extract_prep_task_details(self, result: Any, event_data: Dict, user_id: int) -> List[Dict]:
        """Extract preparation task details from calendar event processing"""
        try:
            # Similar to above but filtered for preparation tasks
            return self._extract_task_details_from_result(result, user_id)
        except Exception as e:
            logger.error(f"Error extracting prep task details: {str(e)}")
            return []
    
    def _link_task_to_project(self, task: Task, project_name: str, user_id: int):
        """Link task to project through entity relationships"""
        try:
            # Find or create project
            project_topic = entity_engine.create_or_update_topic(
                topic_name=project_name,
                description=f"Project: {project_name}",
                context=EntityContext(source_type='manual', user_id=user_id)
            )
            
            if project_topic:
                # Create entity relationship
                entity_engine.create_entity_relationship(
                    'task', task.id,
                    'topic', project_topic.id,
                    'belongs_to_project',
                    EntityContext(source_type='manual', user_id=user_id)
                )
                
        except Exception as e:
            logger.error(f"Error linking task to project: {str(e)}")
    
    def _propagate_task_status_update(self, task: Task, old_status: str, new_status: str, user_id: int):
        """Propagate task status changes to related entities"""
        try:
            if new_status == 'completed':
                # Update related topic activity
                for topic in task.topics:
                    update_data = {'task_completed': True, 'completion_date': datetime.utcnow()}
                    entity_engine.augment_entity_from_source(
                        'topic', topic.id, update_data,
                        EntityContext(source_type='task_completion', user_id=user_id)
                    )
                
                # Update assignee activity if applicable
                if task.assignee:
                    update_data = {'task_completed': True}
                    entity_engine.augment_entity_from_source(
                        'person', task.assignee.id, update_data,
                        EntityContext(source_type='task_completion', user_id=user_id)
                    )
                    
        except Exception as e:
            logger.error(f"Error propagating task status update: {str(e)}")
    
    def _analyze_task_completion_patterns(self, task: Task, user_id: int):
        """Analyze task completion for productivity insights"""
        try:
            # Calculate completion time
            if task.created_at and task.completed_at:
                completion_time = task.completed_at - task.created_at
                
                # Store completion pattern data
                # This would feed into productivity analytics
                logger.debug(f"Task completed in {completion_time.days} days: {task.description[:50]}...")
                
        except Exception as e:
            logger.error(f"Error analyzing task completion patterns: {str(e)}")
    
    def _get_task_assignee_info(self, task: Task) -> Optional[Dict]:
        """Get assignee information for task"""
        if task.assignee:
            return {
                'id': task.assignee.id,
                'name': task.assignee.name,
                'email': task.assignee.email_address,
                'relationship': task.assignee.relationship_type
            }
        return None
    
    def _get_task_topic_info(self, task: Task) -> List[Dict]:
        """Get topic information for task"""
        topics = []
        for topic in task.topics:
            topics.append({
                'id': topic.id,
                'name': topic.name,
                'description': topic.description,
                'strategic_importance': topic.strategic_importance
            })
        return topics
    
    def _get_task_source_context(self, task: Task) -> Dict:
        """Get source context for task"""
        context = {'source_type': 'unknown'}
        
        if task.source_email_id:
            context = {'source_type': 'email', 'source_id': task.source_email_id}
        elif task.source_event_id:
            context = {'source_type': 'calendar', 'source_id': task.source_event_id}
        else:
            context = {'source_type': 'manual'}
            
        return context
    
    def _get_task_entity_relationships(self, task: Task) -> List[Dict]:
        """Get entity relationships for task"""
        relationships = []
        
        # Add topic relationships
        for topic in task.topics:
            relationships.append({
                'entity_type': 'topic',
                'entity_id': topic.id,
                'entity_name': topic.name,
                'relationship_type': 'related_to'
            })
        
        # Add assignee relationship
        if task.assignee:
            relationships.append({
                'entity_type': 'person',
                'entity_id': task.assignee.id,
                'entity_name': task.assignee.name,
                'relationship_type': 'assigned_to'
            })
        
        return relationships
    
    # =====================================================================
    # ANALYTICS METHODS
    # =====================================================================
    
    def _calculate_completion_rate(self, tasks: List[Task]) -> float:
        """Calculate task completion rate"""
        if not tasks:
            return 0.0
        
        completed = len([t for t in tasks if t.status == 'completed'])
        return completed / len(tasks) * 100
    
    def _analyze_priority_distribution(self, tasks: List[Task]) -> Dict:
        """Analyze priority distribution of tasks"""
        distribution = {'high': 0, 'medium': 0, 'low': 0}
        
        for task in tasks:
            priority = task.priority or 'medium'
            if priority in distribution:
                distribution[priority] += 1
        
        return distribution
    
    def _analyze_topic_frequency(self, tasks: List[Task]) -> List[Dict]:
        """Analyze which topics appear most frequently in tasks"""
        topic_counts = {}
        
        for task in tasks:
            for topic in task.topics:
                topic_name = topic.name
                if topic_name not in topic_counts:
                    topic_counts[topic_name] = 0
                topic_counts[topic_name] += 1
        
        # Sort by frequency
        sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)
        
        return [{'topic': topic, 'count': count} for topic, count in sorted_topics[:10]]
    
    def _analyze_task_sources(self, tasks: List[Task]) -> Dict:
        """Analyze where tasks come from"""
        sources = {'email': 0, 'calendar': 0, 'manual': 0}
        
        for task in tasks:
            if task.source_email_id:
                sources['email'] += 1
            elif task.source_event_id:
                sources['calendar'] += 1
            else:
                sources['manual'] += 1
        
        return sources
    
    def _analyze_productivity_trends(self, tasks: List[Task]) -> Dict:
        """Analyze productivity trends over time"""
        # Group tasks by week
        weekly_stats = {}
        
        for task in tasks:
            week_key = task.created_at.strftime('%Y-W%U')
            if week_key not in weekly_stats:
                weekly_stats[week_key] = {'created': 0, 'completed': 0}
            
            weekly_stats[week_key]['created'] += 1
            if task.status == 'completed':
                weekly_stats[week_key]['completed'] += 1
        
        return weekly_stats
    
    def _generate_productivity_insights(self, tasks: List[Task], user_id: int) -> List[str]:
        """Generate productivity insights from task patterns"""
        insights = []
        
        if not tasks:
            return insights
        
        completion_rate = self._calculate_completion_rate(tasks)
        
        if completion_rate > 80:
            insights.append("Excellent task completion rate! You're highly productive.")
        elif completion_rate > 60:
            insights.append("Good task completion rate. Consider prioritizing high-impact tasks.")
        else:
            insights.append("Task completion could be improved. Focus on fewer, high-priority tasks.")
        
        # Analyze overdue tasks
        overdue_tasks = [t for t in tasks if t.due_date and t.due_date < datetime.utcnow() and t.status != 'completed']
        if overdue_tasks:
            insights.append(f"You have {len(overdue_tasks)} overdue tasks. Consider rescheduling or reprioritizing.")
        
        return insights

# Global instance for easy import
enhanced_task_processor = EnhancedTaskProcessor() 

============================================================
FILE: chief_of_staff_ai/processors/analytics/predictive_analytics.py
============================================================
"""
Predictive Analytics Engine - Future Intelligence
This transforms your system from reactive to genuinely predictive
"""

import logging
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
from dataclasses import dataclass
import json
from collections import defaultdict, deque
import threading
import time

from processors.unified_entity_engine import entity_engine, EntityContext
from models.enhanced_models import Person, Topic, Task, CalendarEvent, Email, IntelligenceInsight
from models.database import get_db_manager

logger = logging.getLogger(__name__)

@dataclass
class PredictionResult:
    prediction_type: str
    confidence: float
    predicted_value: Any
    reasoning: str
    time_horizon: str  # short_term, medium_term, long_term
    data_points_used: int
    created_at: datetime

@dataclass
class TrendPattern:
    entity_type: str
    entity_id: int
    pattern_type: str  # growth, decline, cyclical, volatile
    strength: float
    confidence: float
    data_points: List[Tuple[datetime, float]]
    prediction: Optional[float] = None

class PredictiveAnalytics:
    """
    Advanced predictive analytics engine that learns from patterns and predicts future states.
    This is what makes your system truly intelligent - anticipating rather than just reacting.
    """
    
    def __init__(self):
        self.pattern_cache = {}
        self.prediction_cache = {}
        self.learning_models = {}
        self.pattern_detection_thread = None
        self.running = False
        
    def start(self):
        """Start the predictive analytics engine"""
        self.running = True
        self.pattern_detection_thread = threading.Thread(
            target=self._continuous_pattern_detection, 
            name="PredictiveAnalytics"
        )
        self.pattern_detection_thread.daemon = True
        self.pattern_detection_thread.start()
        logger.info("Started predictive analytics engine")
    
    def stop(self):
        """Stop the predictive analytics engine"""
        self.running = False
        if self.pattern_detection_thread:
            self.pattern_detection_thread.join(timeout=5)
        logger.info("Stopped predictive analytics engine")
    
    # =====================================================================
    # RELATIONSHIP PREDICTION METHODS
    # =====================================================================
    
    def predict_relationship_opportunities(self, user_id: int) -> List[PredictionResult]:
        """Predict relationship opportunities and networking needs"""
        predictions = []
        
        try:
            with get_db_manager().get_session() as session:
                # Get all people and their interaction patterns
                people = session.query(Person).filter(Person.user_id == user_id).all()
                
                for person in people:
                    # Predict relationship decay
                    decay_prediction = self._predict_relationship_decay(person)
                    if decay_prediction:
                        predictions.append(decay_prediction)
                    
                    # Predict optimal contact timing
                    contact_prediction = self._predict_optimal_contact_time(person)
                    if contact_prediction:
                        predictions.append(contact_prediction)
                
                # Predict networking opportunities
                network_predictions = self._predict_networking_opportunities(people)
                predictions.extend(network_predictions)
                
        except Exception as e:
            logger.error(f"Failed to predict relationship opportunities: {str(e)}")
        
        return predictions
    
    def _predict_relationship_decay(self, person: Person) -> Optional[PredictionResult]:
        """Predict if a relationship is at risk of decay"""
        if not person.last_contact or person.total_interactions < 3:
            return None
        
        days_since_contact = (datetime.utcnow() - person.last_contact).days
        importance = person.importance_level or 0.5
        
        # Calculate decay risk based on importance and recency
        expected_contact_frequency = self._calculate_expected_frequency(person)
        decay_risk = min(1.0, days_since_contact / expected_contact_frequency)
        
        if decay_risk > 0.7 and importance > 0.6:
            return PredictionResult(
                prediction_type='relationship_decay_risk',
                confidence=decay_risk,
                predicted_value=f"High risk of relationship decay with {person.name}",
                reasoning=f"No contact for {days_since_contact} days, expected frequency is {expected_contact_frequency} days",
                time_horizon='short_term',
                data_points_used=person.total_interactions,
                created_at=datetime.utcnow()
            )
        
        return None
    
    def predict_topic_trends(self, user_id: int) -> List[PredictionResult]:
        """Predict which topics will become important"""
        predictions = []
        
        try:
            with get_db_manager().get_session() as session:
                # Get topics with historical data
                topics = session.query(Topic).filter(
                    Topic.user_id == user_id,
                    Topic.total_mentions > 1
                ).all()
                
                for topic in topics:
                    # Analyze topic momentum
                    momentum_prediction = self._predict_topic_momentum(topic, session)
                    if momentum_prediction:
                        predictions.append(momentum_prediction)
                
                # Predict emerging topics
                emerging_predictions = self._predict_emerging_topics(user_id, session)
                predictions.extend(emerging_predictions)
                
        except Exception as e:
            logger.error(f"Failed to predict topic trends: {str(e)}")
        
        return predictions
    
    def predict_business_opportunities(self, user_id: int) -> List[PredictionResult]:
        """Predict business opportunities based on communication patterns"""
        predictions = []
        
        try:
            # Predict meeting outcomes
            meeting_predictions = self._predict_meeting_outcomes(user_id)
            predictions.extend(meeting_predictions)
            
            # Predict project opportunities
            project_predictions = self._predict_project_opportunities(user_id)
            predictions.extend(project_predictions)
            
            # Predict decision timing
            decision_predictions = self._predict_decision_timing(user_id)
            predictions.extend(decision_predictions)
            
        except Exception as e:
            logger.error(f"Failed to predict business opportunities: {str(e)}")
        
        return predictions
    
    def get_predictions_for_user(self, user_id: int) -> List[PredictionResult]:
        """Get cached predictions for a user"""
        return self.prediction_cache.get(user_id, [])
    
    def get_user_patterns(self, user_id: int) -> Dict:
        """Get detected patterns for a user"""
        return self.pattern_cache.get(user_id, {})
    
    # =====================================================================
    # HELPER METHODS (SIMPLIFIED FOR IMPLEMENTATION)
    # =====================================================================
    
    def _calculate_expected_frequency(self, person: Person) -> int:
        """Calculate expected contact frequency for a person"""
        base_frequency = 30  # Default 30 days
        
        # Adjust based on importance
        importance_factor = (person.importance_level or 0.5)
        frequency = base_frequency * (1 - importance_factor * 0.7)
        
        # Adjust based on relationship type
        relationship_adjustments = {
            'colleague': 0.7,
            'client': 0.5,
            'partner': 0.6,
            'manager': 0.4,
            'friend': 0.8
        }
        
        rel_type = person.relationship_type or 'contact'
        adjustment = relationship_adjustments.get(rel_type.lower(), 1.0)
        
        return max(7, int(frequency * adjustment))
    
    def _predict_optimal_contact_time(self, person: Person) -> Optional[PredictionResult]:
        """Predict optimal time to contact someone"""
        if not person.last_contact or person.total_interactions < 2:
            return None
        
        expected_freq = self._calculate_expected_frequency(person)
        days_since = (datetime.utcnow() - person.last_contact).days
        
        if days_since >= expected_freq * 0.8:  # 80% of expected frequency
            return PredictionResult(
                prediction_type='optimal_contact_timing',
                confidence=0.7,
                predicted_value=datetime.utcnow() + timedelta(days=2),
                reasoning=f"Optimal contact window approaching based on {expected_freq}-day pattern",
                time_horizon='short_term',
                data_points_used=person.total_interactions,
                created_at=datetime.utcnow()
            )
        
        return None
    
    def _predict_networking_opportunities(self, people: List[Person]) -> List[PredictionResult]:
        """Predict networking opportunities"""
        predictions = []
        
        # Find high-value people who could introduce others
        high_value_people = [p for p in people if (p.importance_level or 0) > 0.7]
        
        for person in high_value_people:
            if len(high_value_people) > 1:
                predictions.append(PredictionResult(
                    prediction_type='networking_opportunity',
                    confidence=0.6,
                    predicted_value=f"Consider leveraging {person.name} for introductions",
                    reasoning=f"High-value contact with broad network potential",
                    time_horizon='medium_term',
                    data_points_used=len(people),
                    created_at=datetime.utcnow()
                ))
                break  # Only generate one for now
        
        return predictions
    
    def _predict_topic_momentum(self, topic: Topic, session) -> Optional[PredictionResult]:
        """Predict if a topic will gain or lose momentum"""
        if topic.total_mentions > 5 and topic.last_mentioned:
            days_since_mention = (datetime.utcnow() - topic.last_mentioned).days
            
            if days_since_mention < 7:  # Recently active
                return PredictionResult(
                    prediction_type='topic_momentum_increase',
                    confidence=0.7,
                    predicted_value=f"Topic '{topic.name}' gaining momentum",
                    reasoning=f"Recent activity with {topic.total_mentions} total mentions",
                    time_horizon='short_term',
                    data_points_used=topic.total_mentions,
                    created_at=datetime.utcnow()
                )
        
        return None
    
    def _predict_emerging_topics(self, user_id: int, session) -> List[PredictionResult]:
        """Predict emerging topics"""
        predictions = []
        
        try:
            # Look for topics with recent creation but growing mentions
            recent_topics = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.created_at > datetime.utcnow() - timedelta(days=14),
                Topic.total_mentions >= 2
            ).all()
            
            for topic in recent_topics:
                predictions.append(PredictionResult(
                    prediction_type='emerging_topic',
                    confidence=0.6,
                    predicted_value=f"Topic '{topic.name}' emerging",
                    reasoning=f"New topic with growing mention frequency",
                    time_horizon='short_term',
                    data_points_used=topic.total_mentions,
                    created_at=datetime.utcnow()
                ))
                
        except Exception as e:
            logger.error(f"Failed to predict emerging topics: {str(e)}")
        
        return predictions
    
    def _predict_meeting_outcomes(self, user_id: int) -> List[PredictionResult]:
        """Predict meeting outcomes"""
        predictions = []
        
        try:
            with get_db_manager().get_session() as session:
                # Get upcoming meetings
                upcoming_meetings = session.query(CalendarEvent).filter(
                    CalendarEvent.user_id == user_id,
                    CalendarEvent.start_time > datetime.utcnow(),
                    CalendarEvent.start_time < datetime.utcnow() + timedelta(days=7)
                ).all()
                
                for meeting in upcoming_meetings:
                    if meeting.preparation_priority and meeting.preparation_priority > 0.7:
                        predictions.append(PredictionResult(
                            prediction_type='meeting_success_probability',
                            confidence=0.8,
                            predicted_value=f"High success probability for '{meeting.title}'",
                            reasoning=f"Well-prepared meeting with high priority indicators",
                            time_horizon='short_term',
                            data_points_used=1,
                            created_at=datetime.utcnow()
                        ))
                
        except Exception as e:
            logger.error(f"Failed to predict meeting outcomes: {str(e)}")
        
        return predictions
    
    def _predict_project_opportunities(self, user_id: int) -> List[PredictionResult]:
        """Predict project opportunities"""
        predictions = []
        
        try:
            with get_db_manager().get_session() as session:
                # Look for high-importance emails that might signal projects
                strategic_emails = session.query(Email).filter(
                    Email.user_id == user_id,
                    Email.email_date > datetime.utcnow() - timedelta(days=7),
                    Email.strategic_importance > 0.7
                ).count()
                
                if strategic_emails > 2:
                    predictions.append(PredictionResult(
                        prediction_type='project_opportunity',
                        confidence=0.6,
                        predicted_value=f"Potential project formation detected",
                        reasoning=f"Multiple high-importance communications suggest project activity",
                        time_horizon='medium_term',
                        data_points_used=strategic_emails,
                        created_at=datetime.utcnow()
                    ))
                
        except Exception as e:
            logger.error(f"Failed to predict project opportunities: {str(e)}")
        
        return predictions
    
    def _predict_decision_timing(self, user_id: int) -> List[PredictionResult]:
        """Predict when decisions might be needed"""
        predictions = []
        
        try:
            with get_db_manager().get_session() as session:
                # Look for urgent tasks or high-priority items
                urgent_tasks = session.query(Task).filter(
                    Task.user_id == user_id,
                    Task.priority == 'high',
                    Task.status.in_(['pending', 'open'])
                ).count()
                
                if urgent_tasks > 0:
                    predictions.append(PredictionResult(
                        prediction_type='decision_needed',
                        confidence=0.7,
                        predicted_value=f"Decisions needed on {urgent_tasks} high-priority items",
                        reasoning=f"Multiple urgent tasks require attention",
                        time_horizon='short_term',
                        data_points_used=urgent_tasks,
                        created_at=datetime.utcnow()
                    ))
                
        except Exception as e:
            logger.error(f"Failed to predict decision timing: {str(e)}")
        
        return predictions
    
    def _continuous_pattern_detection(self):
        """Continuously detect patterns in user data"""
        while self.running:
            try:
                # Get active users for pattern analysis
                active_users = self._get_active_users_for_analysis()
                
                for user_id in active_users:
                    # Generate predictions for this user
                    all_predictions = []
                    all_predictions.extend(self.predict_relationship_opportunities(user_id))
                    all_predictions.extend(self.predict_topic_trends(user_id))
                    all_predictions.extend(self.predict_business_opportunities(user_id))
                    
                    # Store predictions
                    self.prediction_cache[user_id] = all_predictions
                
                # Sleep for analysis interval (every 2 hours)
                time.sleep(7200)
                
            except Exception as e:
                logger.error(f"Error in continuous pattern detection: {str(e)}")
                time.sleep(300)  # Sleep 5 minutes on error
    
    def _get_active_users_for_analysis(self) -> List[int]:
        """Get users with recent activity for pattern analysis"""
        try:
            # Users with activity in last 7 days
            cutoff = datetime.utcnow() - timedelta(days=7)
            
            with get_db_manager().get_session() as session:
                active_users = session.query(Email.user_id).filter(
                    Email.email_date > cutoff
                ).distinct().all()
                
                return [user_id[0] for user_id in active_users]
            
        except Exception as e:
            logger.error(f"Failed to get active users: {str(e)}")
            return []

# Global instance
predictive_analytics = PredictiveAnalytics() 

============================================================
FILE: chief_of_staff_ai/strategic_intelligence/claude_response_parser.py
============================================================
"""
Claude Response Parser

Parses Claude's natural language responses to extract structured strategic intelligence data.
Handles business contexts, strategic insights, and recommendations.
"""

import json
import re
import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)

class ClaudeResponseParser:
    """Parser for Claude's strategic intelligence responses"""
    
    @staticmethod
    def parse_business_contexts(response_text: str) -> List[Dict]:
        """Parse business contexts from Claude's response"""
        try:
            contexts = []
            
            # Look for structured JSON first
            json_matches = re.findall(r'\{[^}]*\}', response_text, re.DOTALL)
            if json_matches:
                for match in json_matches:
                    try:
                        context_data = json.loads(match)
                        if 'name' in context_data:
                            contexts.append(context_data)
                    except json.JSONDecodeError:
                        continue
            
            # If no JSON found, parse natural language structure
            if not contexts:
                contexts = ClaudeResponseParser._parse_contexts_from_text(response_text)
            
            return contexts
            
        except Exception as e:
            logger.error(f"Error parsing business contexts: {str(e)}")
            return []
    
    @staticmethod
    def parse_strategic_insights(response_text: str) -> List[Dict]:
        """Parse strategic insights from Claude's response"""
        try:
            insights = []
            
            # FIXED: Properly escape the hyphen in character class
            insight_pattern = r'(?:INSIGHT|Insight)\s*(\d+)?[:\\\-]?\s*([^\n]+)\n(.*?)(?=(?:INSIGHT|Insight)\s*\d+|$)'
            matches = re.findall(insight_pattern, response_text, re.IGNORECASE | re.MULTILINE | re.DOTALL)
            
            for match in matches:
                number, title, content = match
                insight_data = {
                    'id': f"insight_{len(insights)}",
                    'title': title.strip(),
                    'description': content.strip()[:300],
                    'type': ClaudeResponseParser._extract_insight_type(content),
                    'evidence': ClaudeResponseParser._extract_evidence(content),
                    'implications': ClaudeResponseParser._extract_implications(content),
                    'response': ClaudeResponseParser._extract_recommended_response(content),
                    'confidence': 0.8
                }
                insights.append(insight_data)
            
            return insights
            
        except Exception as e:
            logger.error(f"Error parsing strategic insights: {str(e)}")
            return []
    
    @staticmethod
    def parse_strategic_recommendations(response_text: str) -> List[Dict]:
        """Parse strategic recommendations from Claude's response"""
        try:
            recommendations = []
            
            # FIXED: Properly escape the hyphen in character class
            rec_pattern = r'(?:RECOMMENDATION|Recommendation)\s*(\d+)?[:\\\-]?\s*([^\n]+)\n(.*?)(?=(?:RECOMMENDATION|Recommendation)\s*\d+|$)'
            matches = re.findall(rec_pattern, response_text, re.IGNORECASE | re.MULTILINE | re.DOTALL)
            
            for match in matches:
                number, title, content = match
                rec_data = {
                    'id': f"rec_{len(recommendations)}",
                    'title': title.strip(),
                    'description': ClaudeResponseParser._extract_description(content),
                    'rationale': ClaudeResponseParser._extract_rationale(content),
                    'impact_analysis': ClaudeResponseParser._extract_impact_analysis(content),
                    'urgency': ClaudeResponseParser._extract_urgency(content),
                    'impact': ClaudeResponseParser._extract_impact_level(content),
                    'time_sensitivity': ClaudeResponseParser._extract_time_sensitivity(content),
                    'related_contexts': ClaudeResponseParser._extract_related_contexts(content),
                    'actions': ClaudeResponseParser._extract_suggested_actions(content),
                    'metrics': ClaudeResponseParser._extract_success_metrics(content),
                    'confidence': 0.85
                }
                recommendations.append(rec_data)
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Error parsing strategic recommendations: {str(e)}")
            return []
    
    @staticmethod
    def _parse_contexts_from_text(text: str) -> List[Dict]:
        """Parse business contexts from natural language text"""
        contexts = []
        
        # FIXED: Properly escape the hyphen in character class
        context_pattern = r'(?:CONTEXT|Context)\s*(\d+)?[:\\\-]?\s*([^\n]+)\n(.*?)(?=(?:CONTEXT|Context)\s*\d+|$)'
        matches = re.findall(context_pattern, text, re.IGNORECASE | re.MULTILINE | re.DOTALL)
        
        for match in matches:
            number, name, content = match
            context_data = {
                'id': f"context_{len(contexts)}",
                'name': name.strip(),
                'description': ClaudeResponseParser._extract_description(content),
                'type': ClaudeResponseParser._extract_context_type(content),
                'key_people': ClaudeResponseParser._extract_key_people(content),
                'related_communications': [],
                'timeline': [],
                'status': ClaudeResponseParser._extract_status(content),
                'priority_score': ClaudeResponseParser._extract_priority_score(content),
                'impact_assessment': ClaudeResponseParser._extract_impact_assessment(content),
                'confidence_level': 0.7
            }
            contexts.append(context_data)
        
        return contexts
    
    @staticmethod
    def _extract_description(content: str) -> str:
        """Extract description from content block"""
        # Look for description patterns
        desc_patterns = [
            r'Description[:\-]\s*([^\n]+)',
            r'Summary[:\-]\s*([^\n]+)',
            r'^([^\.]+\.)'
        ]
        
        for pattern in desc_patterns:
            match = re.search(pattern, content, re.IGNORECASE | re.MULTILINE)
            if match:
                return match.group(1).strip()
        
        # Fallback to first sentence
        sentences = content.split('.')
        return sentences[0].strip() if sentences else content[:200]
    
    @staticmethod
    def _extract_context_type(content: str) -> str:
        """Extract context type from content"""
        type_keywords = {
            'opportunity': ['opportunity', 'deal', 'partnership', 'investment', 'funding'],
            'relationship': ['relationship', 'contact', 'network', 'connection'],
            'project': ['project', 'initiative', 'development', 'implementation'],
            'challenge': ['challenge', 'issue', 'problem', 'risk', 'concern']
        }
        
        content_lower = content.lower()
        for context_type, keywords in type_keywords.items():
            if any(keyword in content_lower for keyword in keywords):
                return context_type
        
        return 'project'  # Default
    
    @staticmethod
    def _extract_key_people(content: str) -> List[str]:
        """Extract key people mentioned in content"""
        # Look for name patterns (capitalize words)
        name_pattern = r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b'
        matches = re.findall(name_pattern, content)
        
        # Filter out common false positives
        false_positives = {'Business Context', 'Strategic Intelligence', 'Chief Staff'}
        names = [name for name in matches if name not in false_positives]
        
        return list(set(names))[:5]  # Top 5 unique names
    
    @staticmethod
    def _extract_status(content: str) -> str:
        """Extract current status from content"""
        status_patterns = [
            r'Status[:\-]\s*([^\n]+)',
            r'Currently[:\-]\s*([^\n]+)',
            r'Status:\s*([^\n]+)'
        ]
        
        for pattern in status_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # Infer status from keywords
        content_lower = content.lower()
        if any(word in content_lower for word in ['ongoing', 'active', 'in progress']):
            return 'active'
        elif any(word in content_lower for word in ['pending', 'waiting', 'upcoming']):
            return 'pending'
        elif any(word in content_lower for word in ['completed', 'done', 'finished']):
            return 'completed'
        
        return 'active'
    
    @staticmethod
    def _extract_priority_score(content: str) -> float:
        """Extract priority score from content"""
        # Look for explicit priority mentions
        priority_pattern = r'Priority[:\-]\s*(high|medium|low|critical)'
        match = re.search(priority_pattern, content, re.IGNORECASE)
        
        if match:
            priority = match.group(1).lower()
            priority_scores = {'critical': 0.95, 'high': 0.8, 'medium': 0.6, 'low': 0.3}
            return priority_scores.get(priority, 0.6)
        
        # Infer from urgency keywords
        content_lower = content.lower()
        if any(word in content_lower for word in ['urgent', 'critical', 'immediate']):
            return 0.9
        elif any(word in content_lower for word in ['important', 'significant']):
            return 0.7
        
        return 0.6  # Default medium priority
    
    @staticmethod
    def _extract_impact_assessment(content: str) -> str:
        """Extract impact assessment from content"""
        impact_patterns = [
            r'Impact[:\-]\s*([^\n]+)',
            r'This will[:\-]?\s*([^\n]+)',
            r'Expected to[:\-]?\s*([^\n]+)'
        ]
        
        for pattern in impact_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return "Impact assessment pending"
    
    @staticmethod
    def _extract_insight_type(content: str) -> str:
        """Extract insight type from content"""
        content_lower = content.lower()
        
        if any(word in content_lower for word in ['trend', 'pattern', 'increasing', 'growing']):
            return 'trend'
        elif any(word in content_lower for word in ['opportunity', 'potential', 'chance']):
            return 'opportunity'
        elif any(word in content_lower for word in ['risk', 'threat', 'concern', 'danger']):
            return 'risk'
        elif any(word in content_lower for word in ['connection', 'relationship', 'link']):
            return 'connection'
        
        return 'trend'
    
    @staticmethod
    def _extract_evidence(content: str) -> List[Dict]:
        """Extract supporting evidence from content"""
        # Look for evidence indicators
        evidence_patterns = [
            r'Evidence[:\-]\s*([^\n]+)',
            r'Based on[:\-]\s*([^\n]+)',
            r'Supported by[:\-]\s*([^\n]+)'
        ]
        
        evidence = []
        for pattern in evidence_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                evidence.append({'type': 'communication', 'description': match.strip()})
        
        return evidence[:3]  # Top 3 pieces of evidence
    
    @staticmethod
    def _extract_implications(content: str) -> str:
        """Extract business implications from content"""
        impl_patterns = [
            r'Implications?[:\-]\s*([^\n]+)',
            r'This means[:\-]?\s*([^\n]+)',
            r'As a result[:\-]?\s*([^\n]+)'
        ]
        
        for pattern in impl_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return "Business implications require analysis"
    
    @staticmethod
    def _extract_recommended_response(content: str) -> str:
        """Extract recommended response from content"""
        response_patterns = [
            r'Recommend[:\-]\s*([^\n]+)',
            r'Should[:\-]?\s*([^\n]+)',
            r'Next steps?[:\-]\s*([^\n]+)'
        ]
        
        for pattern in response_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return "Response strategy to be determined"
    
    @staticmethod
    def _extract_rationale(content: str) -> str:
        """Extract rationale from recommendation content"""
        rationale_patterns = [
            r'Why[:\-]\s*([^\n]+)',
            r'Rationale[:\-]\s*([^\n]+)',
            r'Because[:\-]?\s*([^\n]+)'
        ]
        
        for pattern in rationale_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return "Strategic rationale based on business context analysis"
    
    @staticmethod
    def _extract_impact_analysis(content: str) -> str:
        """Extract impact analysis from recommendation"""
        impact_patterns = [
            r'Impact[:\-]\s*([^\n]+)',
            r'Will result in[:\-]?\s*([^\n]+)',
            r'Expected outcome[:\-]\s*([^\n]+)'
        ]
        
        for pattern in impact_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return "Impact analysis pending"
    
    @staticmethod
    def _extract_urgency(content: str) -> str:
        """Extract urgency level from content"""
        content_lower = content.lower()
        
        if any(word in content_lower for word in ['critical', 'urgent', 'immediate', 'asap']):
            return 'critical'
        elif any(word in content_lower for word in ['high', 'important', 'soon']):
            return 'high'
        elif any(word in content_lower for word in ['low', 'later', 'eventually']):
            return 'low'
        
        return 'medium'
    
    @staticmethod
    def _extract_impact_level(content: str) -> str:
        """Extract impact level from content"""
        content_lower = content.lower()
        
        if any(word in content_lower for word in ['significant', 'major', 'substantial', 'high impact']):
            return 'high'
        elif any(word in content_lower for word in ['minor', 'small', 'low impact']):
            return 'low'
        
        return 'medium'
    
    @staticmethod
    def _extract_time_sensitivity(content: str) -> str:
        """Extract time sensitivity from content"""
        time_patterns = [
            r'within\s+(\w+)',
            r'by\s+(\w+)',
            r'(\w+)\s+deadline'
        ]
        
        for pattern in time_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return f"Time sensitive: {match.group(1)}"
        
        content_lower = content.lower()
        if any(word in content_lower for word in ['today', 'tomorrow', 'this week']):
            return 'immediate'
        elif any(word in content_lower for word in ['this month', 'next week']):
            return 'short-term'
        
        return 'flexible'
    
    @staticmethod
    def _extract_related_contexts(content: str) -> List[str]:
        """Extract related contexts from content"""
        # Look for context references
        context_refs = re.findall(r'context[:\-]?\s*([^\n,]+)', content, re.IGNORECASE)
        return [ref.strip() for ref in context_refs][:3]
    
    @staticmethod
    def _extract_suggested_actions(content: str) -> List[Dict]:
        """Extract suggested actions from content"""
        actions = []
        
        # Look for action items
        action_patterns = [
            r'Action[:\-]\s*([^\n]+)',
            r'Step\s*\d+[:\-]\s*([^\n]+)',
            r'Next[:\-]?\s*([^\n]+)'
        ]
        
        for pattern in action_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                actions.append({
                    'description': match.strip(),
                    'priority': 'medium',
                    'estimated_time': 'TBD'
                })
        
        return actions[:5]  # Top 5 actions
    
    @staticmethod
    def _extract_success_metrics(content: str) -> List[str]:
        """Extract success metrics from content"""
        metric_patterns = [
            r'Metric[s]?[:\-]\s*([^\n]+)',
            r'Measure[:\-]\s*([^\n]+)',
            r'Success[:\-]\s*([^\n]+)'
        ]
        
        metrics = []
        for pattern in metric_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            metrics.extend([match.strip() for match in matches])
        
        return metrics[:3]  # Top 3 metrics


# Global parser instance
claude_parser = ClaudeResponseParser() 

============================================================
FILE: chief_of_staff_ai/strategic_intelligence/strategic_intelligence_cache.py
============================================================
"""
Strategic Intelligence Cache

Caches Strategic Intelligence results to prevent expensive regeneration
on every tab switch or API call.
"""

import time
import threading
from typing import Dict, Any, Optional
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)

class StrategicIntelligenceCache:
    """Thread-safe cache for Strategic Intelligence results"""
    
    def __init__(self, default_ttl_minutes: int = 30):
        """
        Initialize cache with default TTL (time-to-live)
        
        Args:
            default_ttl_minutes: Default cache expiration in minutes
        """
        self.cache = {}
        self.default_ttl = default_ttl_minutes * 60  # Convert to seconds
        self.lock = threading.RLock()
        
    def get(self, user_email: str) -> Optional[Dict[str, Any]]:
        """
        Get cached Strategic Intelligence for user
        
        Args:
            user_email: User's email address
            
        Returns:
            Cached intelligence result or None if expired/missing
        """
        with self.lock:
            if user_email not in self.cache:
                return None
            
            cache_entry = self.cache[user_email]
            
            # Check if cache is expired
            if time.time() > cache_entry['expires_at']:
                logger.info(f"Strategic Intelligence cache expired for {user_email}")
                del self.cache[user_email]
                return None
            
            logger.info(f"Strategic Intelligence cache HIT for {user_email}")
            return cache_entry['data']
    
    def set(self, user_email: str, intelligence_data: Dict[str, Any], ttl_minutes: Optional[int] = None) -> None:
        """
        Cache Strategic Intelligence result for user
        
        Args:
            user_email: User's email address
            intelligence_data: Strategic Intelligence result to cache
            ttl_minutes: Custom TTL in minutes (uses default if None)
        """
        ttl_seconds = (ttl_minutes * 60) if ttl_minutes else self.default_ttl
        expires_at = time.time() + ttl_seconds
        
        with self.lock:
            self.cache[user_email] = {
                'data': intelligence_data,
                'cached_at': time.time(),
                'expires_at': expires_at,
                'cached_datetime': datetime.now().isoformat()
            }
            
        logger.info(f"Strategic Intelligence cached for {user_email} (TTL: {ttl_seconds//60} minutes)")
    
    def invalidate(self, user_email: str) -> bool:
        """
        Invalidate cache for specific user
        
        Args:
            user_email: User's email address
            
        Returns:
            True if cache was invalidated, False if no cache existed
        """
        with self.lock:
            if user_email in self.cache:
                del self.cache[user_email]
                logger.info(f"Strategic Intelligence cache invalidated for {user_email}")
                return True
            return False
    
    def clear_all(self) -> int:
        """
        Clear all cached Strategic Intelligence
        
        Returns:
            Number of cache entries cleared
        """
        with self.lock:
            count = len(self.cache)
            self.cache.clear()
            logger.info(f"Strategic Intelligence cache cleared ({count} entries)")
            return count
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics
        
        Returns:
            Dictionary with cache stats
        """
        with self.lock:
            current_time = time.time()
            active_entries = 0
            expired_entries = 0
            
            for user_email, cache_entry in self.cache.items():
                if current_time <= cache_entry['expires_at']:
                    active_entries += 1
                else:
                    expired_entries += 1
            
            return {
                'total_entries': len(self.cache),
                'active_entries': active_entries,
                'expired_entries': expired_entries,
                'default_ttl_minutes': self.default_ttl // 60,
                'cache_users': list(self.cache.keys())
            }
    
    def cleanup_expired(self) -> int:
        """
        Remove expired cache entries
        
        Returns:
            Number of expired entries removed
        """
        with self.lock:
            current_time = time.time()
            expired_keys = []
            
            for user_email, cache_entry in self.cache.items():
                if current_time > cache_entry['expires_at']:
                    expired_keys.append(user_email)
            
            for key in expired_keys:
                del self.cache[key]
            
            if expired_keys:
                logger.info(f"Cleaned up {len(expired_keys)} expired cache entries")
            
            return len(expired_keys)

# Global cache instance
strategic_intelligence_cache = StrategicIntelligenceCache(default_ttl_minutes=30) 
FILE: chief_of_staff_ai/strategic_intelligence/__init__.py - Package initialization file

============================================================
FILE: chief_of_staff_ai/strategic_intelligence/strategic_intelligence_engine.py
============================================================
"""
Strategic Intelligence Engine

Core engine that transforms raw communications into strategic business intelligence.
Focuses on unified knowledge synthesis rather than individual email processing.
"""

import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
from collections import defaultdict
import json

logger = logging.getLogger(__name__)

@dataclass
class BusinessContext:
    """Unified business context combining multiple communications"""
    context_id: str
    name: str
    description: str
    context_type: str  # 'opportunity', 'relationship', 'project', 'challenge'
    key_people: List[Dict]
    related_communications: List[Dict]
    timeline: List[Dict]
    current_status: str
    priority_score: float
    impact_assessment: str
    confidence_level: float
    created_at: datetime
    last_updated: datetime

@dataclass 
class StrategicRecommendation:
    """Strategic recommendation with impact analysis"""
    recommendation_id: str
    title: str
    description: str
    rationale: str
    impact_analysis: str
    urgency_level: str  # 'critical', 'high', 'medium', 'low'
    estimated_impact: str  # 'high', 'medium', 'low'
    time_sensitivity: str
    related_contexts: List[str]
    suggested_actions: List[Dict]
    success_metrics: List[str]
    confidence_score: float
    created_at: datetime

@dataclass
class StrategicInsight:
    """Cross-pattern strategic insight"""
    insight_id: str
    title: str
    insight_type: str  # 'trend', 'opportunity', 'risk', 'connection'
    description: str
    supporting_evidence: List[Dict]
    business_implications: str
    recommended_response: str
    confidence_level: float
    created_at: datetime

class StrategyEngine:
    """
    Strategic Intelligence Engine - The Chief of Staff Brain
    
    Transforms raw communications into strategic business intelligence
    through unified knowledge synthesis and pattern analysis.
    """
    
    def __init__(self, claude_client, db_manager):
        self.claude_client = claude_client
        self.db_manager = db_manager
        self.logger = logging.getLogger(__name__)
    
    def generate_strategic_intelligence(self, user_email: str, force_refresh: bool = False) -> Dict[str, Any]:
        """
        Generate comprehensive strategic intelligence for a user.
        Uses caching to prevent expensive regeneration on every request.
        
        Args:
            user_email: User's email address
            force_refresh: If True, bypass cache and generate fresh intelligence
        """
        try:
            # Import cache here to avoid circular imports
            from .strategic_intelligence_cache import strategic_intelligence_cache
            
            # Check cache first (unless force refresh)
            if not force_refresh:
                cached_result = strategic_intelligence_cache.get(user_email)
                if cached_result:
                    self.logger.info(f"Returning cached Strategic Intelligence for {user_email}")
                    return cached_result
            
            self.logger.info(f"Generating strategic intelligence for {user_email}")
            
            # Get user and all their content
            user = self.db_manager.get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            # Step 1: Content Ingestion Pipeline - ENGAGEMENT-DRIVEN
            content_summary = self._ingest_content_pipeline(user)
            
            # Step 2: Unified Knowledge Synthesis - Build business contexts
            business_contexts = self._synthesize_business_contexts(user, content_summary)
            
            # Step 3: Strategic Pattern Analysis - Find connections and trends
            strategic_insights = self._analyze_strategic_patterns(user, business_contexts)
            
            # Step 4: Generate Strategic Recommendations - Actionable guidance
            recommendations = self._generate_strategic_recommendations(
                user, business_contexts, strategic_insights
            )
            
            # Step 5: Compile Chief of Staff Intelligence Brief
            intelligence_brief = self._compile_intelligence_brief(
                user, business_contexts, strategic_insights, recommendations
            )
            
            # Prepare result
            result = {
                'success': True,
                'user_email': user_email,
                'intelligence_brief': intelligence_brief,
                'business_contexts': business_contexts,
                'strategic_insights': strategic_insights,
                'recommendations': recommendations,
                'generated_at': datetime.now().isoformat(),
                'cache_info': {
                    'generated_fresh': True,
                    'force_refresh': force_refresh
                }
            }
            
            # Cache the result for future use (30 minute TTL)
            strategic_intelligence_cache.set(user_email, result, ttl_minutes=30)
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error generating strategic intelligence: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _ingest_content_pipeline(self, user) -> Dict[str, Any]:
        """
        Step 1: Content Ingestion Pipeline - ENGAGEMENT-DRIVEN
        Uses Smart Contact Strategy to focus only on business-relevant communications
        """
        try:
            # Initialize Smart Contact Strategy
            from chief_of_staff_ai.engagement_analysis.smart_contact_strategy import smart_contact_strategy
            
            self.logger.info(f"Building engagement-driven content pipeline for user {user.id}")
            
            # Step 1: Ensure trusted contact database exists
            trusted_result = smart_contact_strategy.build_trusted_contact_database(
                user_email=user.email,
                days_back=365  # 1 year of engagement analysis
            )
            
            if trusted_result.get('success'):
                self.logger.info(f"Trusted contact database: {trusted_result.get('trusted_contacts_count', 0)} contacts")
            
            # Step 2: Get ALL communications first
            all_emails = self.db_manager.get_user_emails(user.id, limit=500)  # Increased limit for filtering
            all_people = self.db_manager.get_user_people(user.id, limit=200)
            all_tasks = self.db_manager.get_user_tasks(user.id, limit=200)
            all_projects = self.db_manager.get_user_projects(user.id, limit=100)
            
            # Step 3: Apply Smart Contact Strategy filtering
            engagement_filtered_emails = []
            business_relevant_people = []
            
            # Filter emails using Smart Contact Strategy
            for email in all_emails:
                if email.sender and email.subject:
                    # Create email data for classification
                    email_data = {
                        'sender': email.sender,
                        'sender_name': email.sender_name,
                        'subject': email.subject,
                        'body_preview': email.body_preview or email.snippet,
                        'date': email.email_date.isoformat() if email.email_date else None,
                        'has_attachments': email.has_attachments,
                        'labels': email.labels or []
                    }
                    
                    # Classify email using Smart Contact Strategy
                    classification = smart_contact_strategy.classify_incoming_email(
                        user_email=user.email,
                        email_data=email_data
                    )
                    
                    # Only include emails that should be processed by AI
                    if classification.action in ['PROCESS_WITH_AI', 'QUICK_CHECK']:
                        engagement_filtered_emails.append(email)
                        self.logger.debug(f"Included email: {email.subject[:50]}... (Action: {classification.action})")
                    else:
                        self.logger.debug(f"Filtered out email: {email.subject[:50]}... (Action: {classification.action})")
            
            # Filter people to focus on trusted contacts and business relationships
            trusted_contacts_result = smart_contact_strategy.get_engagement_insights(user.email)
            trusted_emails = set()
            
            if trusted_contacts_result.get('success'):
                # Extract trusted contact emails from engagement insights
                engagement_patterns = trusted_contacts_result.get('engagement_patterns', {})
                for pattern_data in engagement_patterns.values():
                    if isinstance(pattern_data, dict) and pattern_data.get('emails'):
                        trusted_emails.update(email.lower() for email in pattern_data['emails'])
            
            for person in all_people:
                if person.email_address and person.name:
                    # Include if trusted contact OR high business value
                    is_trusted = person.email_address.lower() in trusted_emails
                    is_business_relevant = (
                        person.total_emails and person.total_emails >= 2 and  # Multiple interactions
                        person.relationship_type not in ['spam', 'automated', None] and
                        not any(pattern in person.email_address.lower() for pattern in ['noreply', 'no-reply', 'automated', 'system'])
                    )
                    
                    if is_trusted or is_business_relevant:
                        business_relevant_people.append(person)
            
            # Filter tasks and projects (keep all for now, but could filter by assignee/stakeholder engagement)
            strategic_tasks = [task for task in all_tasks if task.description and len(task.description.strip()) > 10]
            strategic_projects = [proj for proj in all_projects if proj.name and proj.status in ['active', 'planning']]
            
            # Calculate efficiency metrics
            total_emails_processed = len(all_emails)
            engagement_emails_processed = len(engagement_filtered_emails)
            efficiency_ratio = engagement_emails_processed / max(total_emails_processed, 1)
            
            self.logger.info(f"Smart Content Filtering Results:")
            self.logger.info(f"  - Total emails: {total_emails_processed}")
            self.logger.info(f"  - Engagement-filtered emails: {engagement_emails_processed}")
            self.logger.info(f"  - Efficiency ratio: {efficiency_ratio:.2%}")
            self.logger.info(f"  - Business-relevant people: {len(business_relevant_people)}")
            self.logger.info(f"  - Strategic tasks: {len(strategic_tasks)}")
            self.logger.info(f"  - Strategic projects: {len(strategic_projects)}")
            
            # Create strategic content summary - FOCUSED ON ENGAGEMENT
            content_summary = {
                'emails': {
                    'total_available': total_emails_processed,
                    'engagement_filtered': engagement_emails_processed,
                    'efficiency_ratio': efficiency_ratio,
                    'strategic_content': [self._extract_email_essence(email) for email in engagement_filtered_emails[:50]],
                    'timespan': self._get_timespan(engagement_filtered_emails),
                    'filtering_strategy': 'smart_contact_engagement'
                },
                'people': {
                    'total_available': len(all_people),
                    'business_relevant': len(business_relevant_people),
                    'trusted_contacts': len(trusted_emails),
                    'key_relationships': [self._extract_person_essence(person) for person in business_relevant_people[:25]]
                },
                'projects': {
                    'total': len(strategic_projects),
                    'strategic_active': [self._extract_project_essence(project) for project in strategic_projects]
                },
                'tasks': {
                    'total': len(strategic_tasks),
                    'strategic_pending': [self._extract_task_essence(task) for task in strategic_tasks if task.status == 'pending']
                },
                'engagement_intelligence': {
                    'trusted_contact_database_size': len(trusted_emails),
                    'engagement_filtering_active': True,
                    'business_focus_ratio': len(business_relevant_people) / max(len(all_people), 1),
                    'content_quality_score': efficiency_ratio * 0.7 + (len(business_relevant_people) / max(len(all_people), 1)) * 0.3
                }
            }
            
            return content_summary
            
        except Exception as e:
            self.logger.error(f"Error in engagement-driven content pipeline: {str(e)}")
            # Fallback to basic content ingestion if Smart Contact Strategy fails
            return self._basic_content_fallback(user)
    
    def _basic_content_fallback(self, user) -> Dict[str, Any]:
        """Fallback content ingestion if Smart Contact Strategy fails"""
        self.logger.warning("Falling back to basic content ingestion")
        
        # Get basic communications without engagement filtering
        emails = self.db_manager.get_user_emails(user.id, limit=100)
        people = self.db_manager.get_user_people(user.id, limit=50)
        tasks = self.db_manager.get_user_tasks(user.id, limit=50)
        projects = self.db_manager.get_user_projects(user.id, limit=25)
        
        return {
            'emails': {
                'total_available': len(emails),
                'engagement_filtered': len(emails),
                'efficiency_ratio': 1.0,
                'strategic_content': [self._extract_email_essence(email) for email in emails[:30]],
                'timespan': self._get_timespan(emails),
                'filtering_strategy': 'basic_fallback'
            },
            'people': {
                'total_available': len(people),
                'business_relevant': len(people),
                'trusted_contacts': 0,
                'key_relationships': [self._extract_person_essence(person) for person in people[:15]]
            },
            'projects': {
                'total': len(projects),
                'strategic_active': [self._extract_project_essence(project) for project in projects if project.status == 'active']
            },
            'tasks': {
                'total': len(tasks),
                'strategic_pending': [self._extract_task_essence(task) for task in tasks if task.status == 'pending']
            },
            'engagement_intelligence': {
                'trusted_contact_database_size': 0,
                'engagement_filtering_active': False,
                'business_focus_ratio': 1.0,
                'content_quality_score': 0.5
            }
        }
    
    def _synthesize_business_contexts(self, user, content_summary) -> List[BusinessContext]:
        """
        Step 2: Unified Knowledge Synthesis
        Combine content into coherent business contexts rather than individual items
        """
        try:
            # Use Claude to synthesize unified business contexts
            synthesis_prompt = self._build_synthesis_prompt(content_summary)
            
            response = self.claude_client.messages.create(
                model=settings.CLAUDE_MODEL,
                max_tokens=4000,
                system="""You are a strategic business intelligence synthesizer. Your job is to analyze all communications and identify unified business contexts.

Instead of focusing on individual emails, identify coherent business situations that span multiple communications. Look for:
- Ongoing opportunities (fundraising, partnerships, deals)
- Key relationships and their development
- Strategic projects and initiatives  
- Business challenges requiring attention
- Market or competitive insights

Each context should represent a unified business situation with clear strategic implications.""",
                messages=[{"role": "user", "content": synthesis_prompt}]
            )
            
            # Parse Claude's response into business contexts
            contexts_data = self._parse_business_contexts(response.content[0].text)
            
            # Convert to BusinessContext objects
            business_contexts = []
            for context_data in contexts_data:
                context = BusinessContext(
                    context_id=context_data.get('id', f"context_{len(business_contexts)}"),
                    name=context_data.get('name'),
                    description=context_data.get('description'),
                    context_type=context_data.get('type'),
                    key_people=context_data.get('key_people', []),
                    related_communications=context_data.get('related_communications', []),
                    timeline=context_data.get('timeline', []),
                    current_status=context_data.get('status'),
                    priority_score=context_data.get('priority_score', 0.5),
                    impact_assessment=context_data.get('impact_assessment'),
                    confidence_level=context_data.get('confidence_level', 0.7),
                    created_at=datetime.now(),
                    last_updated=datetime.now()
                )
                business_contexts.append(context)
            
            return business_contexts
            
        except Exception as e:
            self.logger.error(f"Error synthesizing business contexts: {str(e)}")
            return []
    
    def _analyze_strategic_patterns(self, user, business_contexts) -> List[StrategicInsight]:
        """
        Step 3: Strategic Pattern Analysis
        Cross-reference patterns across communications and identify strategic insights
        """
        try:
            # Analyze patterns across business contexts
            pattern_prompt = self._build_pattern_analysis_prompt(business_contexts)
            
            response = self.claude_client.messages.create(
                model=settings.CLAUDE_MODEL,
                max_tokens=3000,
                system="""You are a strategic pattern analyst. Analyze business contexts to identify strategic insights.

Look for:
- Converging opportunities (multiple contexts pointing to same opportunity)
- Timing patterns (windows of opportunity, urgency indicators)
- Relationship networks (how people/contexts connect)
- Resource allocation patterns
- Market trends and competitive insights
- Risk patterns and mitigation opportunities

Focus on insights that have strategic business implications.""",
                messages=[{"role": "user", "content": pattern_prompt}]
            )
            
            # Parse insights from Claude's response
            insights_data = self._parse_strategic_insights(response.content[0].text)
            
            # Convert to StrategicInsight objects
            strategic_insights = []
            for insight_data in insights_data:
                insight = StrategicInsight(
                    insight_id=insight_data.get('id', f"insight_{len(strategic_insights)}"),
                    title=insight_data.get('title'),
                    insight_type=insight_data.get('type'),
                    description=insight_data.get('description'),
                    supporting_evidence=insight_data.get('evidence', []),
                    business_implications=insight_data.get('implications'),
                    recommended_response=insight_data.get('response'),
                    confidence_level=insight_data.get('confidence', 0.7),
                    created_at=datetime.now()
                )
                strategic_insights.append(insight)
            
            return strategic_insights
            
        except Exception as e:
            self.logger.error(f"Error analyzing strategic patterns: {str(e)}")
            return []
    
    def _generate_strategic_recommendations(self, user, business_contexts, strategic_insights) -> List[StrategicRecommendation]:
        """
        Step 4: Generate Strategic Recommendations
        Create actionable recommendations with impact analysis
        """
        try:
            # Generate strategic recommendations
            recommendations_prompt = self._build_recommendations_prompt(business_contexts, strategic_insights)
            
            response = self.claude_client.messages.create(
                model=settings.CLAUDE_MODEL,
                max_tokens=4000,
                system="""You are a strategic business advisor and Chief of Staff. Generate actionable recommendations based on business contexts and insights.

Each recommendation should:
- Be specific and actionable
- Include clear rationale and impact analysis
- Suggest concrete next steps
- Explain timing and urgency
- Connect multiple business contexts when relevant
- Focus on highest business impact

Be persuasive and strategic - explain WHY this matters and WHAT impact it will have.""",
                messages=[{"role": "user", "content": recommendations_prompt}]
            )
            
            # Parse recommendations from Claude's response
            recommendations_data = self._parse_strategic_recommendations(response.content[0].text)
            
            # Convert to StrategicRecommendation objects
            recommendations = []
            for rec_data in recommendations_data:
                recommendation = StrategicRecommendation(
                    recommendation_id=rec_data.get('id', f"rec_{len(recommendations)}"),
                    title=rec_data.get('title'),
                    description=rec_data.get('description'),
                    rationale=rec_data.get('rationale'),
                    impact_analysis=rec_data.get('impact_analysis'),
                    urgency_level=rec_data.get('urgency', 'medium'),
                    estimated_impact=rec_data.get('impact', 'medium'),
                    time_sensitivity=rec_data.get('time_sensitivity'),
                    related_contexts=rec_data.get('related_contexts', []),
                    suggested_actions=rec_data.get('actions', []),
                    success_metrics=rec_data.get('metrics', []),
                    confidence_score=rec_data.get('confidence', 0.8),
                    created_at=datetime.now()
                )
                recommendations.append(recommendation)
            
            return recommendations
            
        except Exception as e:
            self.logger.error(f"Error generating strategic recommendations: {str(e)}")
            return []
    
    def _compile_intelligence_brief(self, user, business_contexts, strategic_insights, recommendations) -> Dict[str, Any]:
        """
        Step 5: Compile Chief of Staff Intelligence Brief with Smart Contact Strategy metrics
        Create the final strategic intelligence summary highlighting engagement efficiency
        """
        # Prioritize recommendations by urgency and impact
        critical_recommendations = [r for r in recommendations if r.urgency_level == 'critical']
        high_priority_recommendations = [r for r in recommendations if r.urgency_level == 'high']
        
        # Identify key opportunities and risks
        opportunities = [ctx for ctx in business_contexts if ctx.context_type == 'opportunity']
        risks = [insight for insight in strategic_insights if insight.insight_type == 'risk']
        
        # Get Smart Contact Strategy insights for efficiency metrics
        smart_insights = self.get_smart_contact_insights(user.email)
        
        intelligence_brief = {
            'executive_summary': self._generate_executive_summary(business_contexts, strategic_insights, recommendations),
            'critical_actions': {
                'count': len(critical_recommendations),
                'items': [self._format_recommendation_brief(r) for r in critical_recommendations]
            },
            'high_priority_actions': {
                'count': len(high_priority_recommendations),
                'items': [self._format_recommendation_brief(r) for r in high_priority_recommendations[:5]]
            },
            'key_opportunities': {
                'count': len(opportunities),
                'items': [self._format_context_brief(ctx) for ctx in opportunities[:3]]
            },
            'strategic_insights': {
                'count': len(strategic_insights),
                'items': [self._format_insight_brief(insight) for insight in strategic_insights[:5]]
            },
            'business_contexts_summary': {
                'total_contexts': len(business_contexts),
                'by_type': self._summarize_contexts_by_type(business_contexts)
            },
            'engagement_intelligence': {
                'smart_contact_strategy_active': smart_insights.get('strategy_active', False),
                'trusted_contacts_count': len(smart_insights.get('trusted_contacts', {}).get('trusted_contacts', [])),
                'engagement_efficiency': smart_insights.get('engagement_insights', {}).get('efficiency_metrics', {}),
                'content_quality_improvement': 'Focused on proven business relationships and engagement patterns'
            }
        }
        
        return intelligence_brief
    
    def _extract_email_essence(self, email) -> Dict[str, Any]:
        """Extract essential elements from email for synthesis"""
        return {
            'subject': email.subject,
            'sender': email.sender_name or email.sender,
            'date': email.email_date.isoformat() if email.email_date else None,
            'summary': email.ai_summary,
            'topics': email.topics,
            'priority_score': email.priority_score,
            'key_insights': email.key_insights
        }
    
    def _extract_person_essence(self, person) -> Dict[str, Any]:
        """Extract essential elements from person for synthesis"""
        return {
            'name': person.name,
            'email': person.email_address,
            'company': person.company,
            'title': person.title,
            'relationship': person.relationship_type,
            'total_emails': person.total_emails,
            'last_interaction': person.last_interaction.isoformat() if person.last_interaction else None
        }
    
    def _extract_project_essence(self, project) -> Dict[str, Any]:
        """Extract essential elements from project for synthesis"""
        return {
            'name': project.name,
            'description': project.description,
            'status': project.status,
            'priority': project.priority,
            'stakeholders': project.stakeholders,
            'total_emails': project.total_emails
        }
    
    def _extract_task_essence(self, task) -> Dict[str, Any]:
        """Extract essential elements from task for synthesis"""
        return {
            'description': task.description,
            'assignee': task.assignee,
            'due_date': task.due_date.isoformat() if task.due_date else None,
            'priority': task.priority,
            'category': task.category,
            'confidence': task.confidence
        }
    
    def _get_timespan(self, emails) -> Dict[str, Any]:
        """Get timespan of communications"""
        if not emails:
            return {}
        
        dates = [email.email_date for email in emails if email.email_date]
        if not dates:
            return {}
        
        return {
            'earliest': min(dates).isoformat(),
            'latest': max(dates).isoformat(),
            'span_days': (max(dates) - min(dates)).days
        }
    
    def _build_synthesis_prompt(self, content_summary) -> str:
        """Build prompt for business context synthesis using engagement-filtered content"""
        
        # Extract engagement intelligence metrics
        engagement_info = content_summary.get('engagement_intelligence', {})
        efficiency_ratio = content_summary.get('emails', {}).get('efficiency_ratio', 0)
        filtering_strategy = content_summary.get('emails', {}).get('filtering_strategy', 'unknown')
        
        prompt = f"""Analyze this ENGAGEMENT-DRIVEN business communication data and identify unified strategic business contexts:

ENGAGEMENT INTELLIGENCE SUMMARY:
- Content Filtering Strategy: {filtering_strategy}
- Email Efficiency Ratio: {efficiency_ratio:.1%} (focused on business-relevant communications)
- Trusted Contact Database: {engagement_info.get('trusted_contact_database_size', 0)} proven relationships
- Business Focus Ratio: {engagement_info.get('business_focus_ratio', 0):.1%} of contacts are business-relevant
- Content Quality Score: {engagement_info.get('content_quality_score', 0):.1%}

FILTERED BUSINESS CONTENT:
{json.dumps(content_summary, indent=2)}

STRATEGIC CONTEXT IDENTIFICATION:
Based on this engagement-driven content (focused only on communications with people you actually engage with), identify 3-7 coherent business contexts that represent ongoing strategic business situations.

Each context should:
1. Combine multiple related communications from your TRUSTED NETWORK
2. Focus on PROVEN BUSINESS RELATIONSHIPS (people you send emails to)
3. Represent genuine opportunities, challenges, projects, or strategic relationships
4. Have clear strategic business implications requiring your attention
5. Connect ENGAGEMENT PATTERNS with business value

For each context, provide:
- Name and description
- Type (opportunity/relationship/project/challenge)  
- Key people involved (from your trusted network)
- Current status and strategic priority
- Impact assessment on your business
- Timeline of developments
- Why this matters strategically

FOCUS ON: What requires your strategic attention based on your actual engagement patterns, not random emails. These should be business situations involving people you consistently communicate with and that drive real business value."""
        
        return prompt
    
    def _build_pattern_analysis_prompt(self, business_contexts) -> str:
        """Build prompt for strategic pattern analysis focused on engagement-driven insights"""
        contexts_text = "\n\n".join([
            f"Context: {ctx.name}\nType: {ctx.context_type}\nDescription: {ctx.description}\nKey People: {ctx.key_people}\nStatus: {ctx.current_status}\nPriority: {ctx.priority_score:.1%}"
            for ctx in business_contexts
        ])
        
        return f"""Analyze these ENGAGEMENT-DRIVEN business contexts for strategic patterns and insights:

STRATEGIC BUSINESS CONTEXTS (from trusted network):
{contexts_text}

PATTERN ANALYSIS FOCUS:
These contexts represent business situations involving people you actively engage with - your proven business network. Identify strategic insights by looking for:

1. CONVERGING OPPORTUNITIES: Multiple contexts pointing to the same strategic opportunity or moment
2. RELATIONSHIP MOMENTUM: How trusted relationships are creating business value or opening doors
3. TIMING PATTERNS: Windows of opportunity, urgent situations, or strategic timing based on engagement
4. NETWORK EFFECTS: How your trusted contacts connect to each other or create compound value
5. ENGAGEMENT TRENDS: Patterns in how your business relationships are evolving
6. RESOURCE FOCUS: Where your attention and resources should focus for maximum impact
7. STRATEGIC POSITIONING: How your network and engagement patterns position you for opportunities

GENERATE 3-5 STRATEGIC INSIGHTS focusing on:
- How your engagement patterns reveal strategic opportunities
- What your trusted network is telling you about market/business trends  
- Where relationship momentum creates actionable opportunities
- How to leverage your proven business relationships for maximum impact
- What patterns suggest about timing for strategic moves

Each insight should connect multiple contexts and explain the strategic implications for your business."""
    
    def _build_recommendations_prompt(self, business_contexts, strategic_insights) -> str:
        """Build prompt for engagement-driven strategic recommendations"""
        contexts_summary = "\n".join([
            f"- {ctx.name}: {ctx.description} (Priority: {ctx.priority_score:.1%}, People: {', '.join(ctx.key_people[:3])})"
            for ctx in business_contexts
        ])
        insights_summary = "\n".join([
            f"- {insight.title}: {insight.description}"
            for insight in strategic_insights
        ])
        
        return f"""Based on these ENGAGEMENT-DRIVEN business contexts and strategic insights, generate strategic recommendations:

BUSINESS CONTEXTS (from your trusted network):
{contexts_summary}

STRATEGIC INSIGHTS (from engagement patterns):
{insights_summary}

GENERATE 3-7 STRATEGIC RECOMMENDATIONS that:

1. LEVERAGE YOUR TRUSTED NETWORK: Recommendations should utilize your proven business relationships
2. ADDRESS HIGHEST ENGAGEMENT VALUE: Focus on opportunities revealed by your engagement patterns
3. CONNECT MULTIPLE CONTEXTS: Link related business situations for compound impact
4. PROVIDE SPECIFIC NEXT STEPS: Clear actions involving specific people from your network
5. EXPLAIN STRATEGIC TIMING: Why now is the right time based on relationship momentum
6. QUANTIFY BUSINESS IMPACT: Expected outcomes and business value
7. UTILIZE RELATIONSHIP CAPITAL: How to leverage your proven connections

Each recommendation should:
- Be ACTIONABLE with specific people and next steps
- Explain WHY this matters based on your engagement patterns
- Detail WHAT impact it will create for your business
- Specify WHO to contact from your trusted network
- Include TIMING based on relationship momentum and context
- Connect to multiple business contexts when relevant

FOCUS ON: Strategic actions that leverage your actual business relationships and engagement patterns for maximum business impact. These should feel like advice from someone who deeply understands your business network and strategic position."""
    
    # Parsing methods for Claude responses
    def _parse_business_contexts(self, response_text) -> List[Dict]:
        """Parse business contexts from Claude response"""
        from .claude_response_parser import claude_parser
        return claude_parser.parse_business_contexts(response_text)
    
    def _parse_strategic_insights(self, response_text) -> List[Dict]:
        """Parse strategic insights from Claude response"""
        from .claude_response_parser import claude_parser
        return claude_parser.parse_strategic_insights(response_text)
    
    def _parse_strategic_recommendations(self, response_text) -> List[Dict]:
        """Parse recommendations from Claude response"""
        from .claude_response_parser import claude_parser
        return claude_parser.parse_strategic_recommendations(response_text)
    
    def _generate_executive_summary(self, business_contexts, strategic_insights, recommendations) -> str:
        """Generate executive summary highlighting engagement-driven strategic intelligence"""
        
        # Count strategic elements
        critical_recommendations = [r for r in recommendations if r.urgency_level == 'critical']
        high_priority_recommendations = [r for r in recommendations if r.urgency_level == 'high']
        opportunities = [ctx for ctx in business_contexts if ctx.context_type == 'opportunity']
        key_relationships = [ctx for ctx in business_contexts if ctx.context_type == 'relationship']
        
        # Build strategic summary
        summary_parts = []
        
        # Core intelligence summary
        summary_parts.append(f"Strategic intelligence analysis of your business network identified {len(business_contexts)} key business contexts, {len(strategic_insights)} strategic insights, and {len(recommendations)} actionable recommendations.")
        
        # Engagement efficiency highlight
        if critical_recommendations:
            summary_parts.append(f"CRITICAL: {len(critical_recommendations)} actions require immediate attention based on your trusted relationship patterns.")
        
        if opportunities:
            summary_parts.append(f"Your engagement-driven analysis reveals {len(opportunities)} strategic opportunities involving your proven business network.")
        
        if key_relationships:
            summary_parts.append(f"Relationship intelligence shows {len(key_relationships)} key relationship contexts requiring strategic attention.")
        
        # Strategic focus summary
        total_recommendations = len(critical_recommendations) + len(high_priority_recommendations)
        if total_recommendations > 0:
            summary_parts.append(f"Focus: {total_recommendations} high-impact actions leverage your trusted contacts for maximum business value.")
        else:
            summary_parts.append("Your business contexts show stable strategic positioning with opportunities for relationship-driven growth.")
        
        return " ".join(summary_parts)
    
    def _format_recommendation_brief(self, recommendation) -> Dict[str, Any]:
        """Format recommendation for brief display"""
        return {
            'title': recommendation.title,
            'description': recommendation.description,
            'urgency': recommendation.urgency_level,
            'impact': recommendation.estimated_impact,
            'rationale': recommendation.rationale
        }
    
    def _format_context_brief(self, context) -> Dict[str, Any]:
        """Format business context for brief display"""
        return {
            'name': context.name,
            'type': context.context_type,
            'description': context.description,
            'status': context.current_status,
            'priority_score': context.priority_score
        }
    
    def _format_insight_brief(self, insight) -> Dict[str, Any]:
        """Format strategic insight for brief display"""
        return {
            'title': insight.title,
            'type': insight.insight_type,
            'description': insight.description,
            'implications': insight.business_implications
        }
    
    def _summarize_contexts_by_type(self, business_contexts) -> Dict[str, int]:
        """Summarize business contexts by type"""
        summary = defaultdict(int)
        for context in business_contexts:
            summary[context.context_type] += 1
        return dict(summary)

    def get_smart_contact_insights(self, user_email: str) -> Dict[str, Any]:
        """Get Smart Contact Strategy insights for dashboard display"""
        try:
            from chief_of_staff_ai.engagement_analysis.smart_contact_strategy import smart_contact_strategy
            
            # Get engagement insights (this method exists)
            insights = smart_contact_strategy.get_engagement_insights(user_email)
            
            # Extract trusted contacts from insights if available
            trusted_contacts = []
            if insights.get('success') and insights.get('engagement_patterns'):
                for pattern_name, pattern_data in insights['engagement_patterns'].items():
                    if isinstance(pattern_data, dict) and pattern_data.get('emails'):
                        for email in pattern_data['emails'][:10]:  # Top 10 per pattern
                            trusted_contacts.append({
                                'email': email,
                                'pattern': pattern_name,
                                'engagement_type': 'trusted_contact'
                            })
            
            return {
                'success': True,
                'engagement_insights': insights,
                'trusted_contacts': {'trusted_contacts': trusted_contacts},
                'strategy_active': True
            }
            
        except Exception as e:
            self.logger.error(f"Error getting Smart Contact Strategy insights: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'strategy_active': False
            }


# Initialize the strategy engine instance
strategy_engine = None

def get_strategy_engine(claude_client=None, db_manager=None):
    """Get or create strategy engine instance"""
    global strategy_engine
    if strategy_engine is None and claude_client and db_manager:
        strategy_engine = StrategyEngine(claude_client, db_manager)
    return strategy_engine 

============================================================
FILE: chief_of_staff_ai/api/enhanced_endpoints.py
============================================================
"""
Enhanced API endpoints for entity-centric intelligence
Provides advanced endpoints for unified entities, relationships, and insights
"""

import logging
from datetime import datetime, timedelta
from flask import Blueprint, request, jsonify, session
from typing import Dict, List, Optional

from models.database import get_db_manager
from models.enhanced_models import Topic, Person, Task, EntityRelationship, IntelligenceInsight
from processors.realtime_processing import realtime_processor, EventType
from processors.enhanced_ai_pipeline import enhanced_ai_processor
from processors.unified_entity_engine import entity_engine, EntityContext

logger = logging.getLogger(__name__)

# Create blueprint for enhanced endpoints
enhanced_api_bp = Blueprint('enhanced_api', __name__, url_prefix='/api/enhanced')

def require_auth_session():
    """Simple session-based auth check"""
    user_email = session.get('user_email')
    if not user_email:
        return None
    
    user = get_db_manager().get_user_by_email(user_email)
    return user

@enhanced_api_bp.route('/unified-sync', methods=['POST'])
def enhanced_unified_sync():
    """
    Enhanced unified sync with entity-centric processing
    Triggers comprehensive data sync with real-time processing
    """
    try:
        user = require_auth_session()
        if not user:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user_id = user.id
        
        # Check if real-time processor is running
        if not realtime_processor.is_running:
            logger.info("Starting real-time processor for enhanced sync")
            realtime_processor.start()
        
        # Trigger comprehensive sync event
        realtime_processor.trigger_comprehensive_sync(user_id, priority=1)
        
        # Get current processing status
        status = realtime_processor.get_processing_status()
        
        return jsonify({
            'success': True,
            'message': 'Enhanced unified sync initiated',
            'real_time_processing': True,
            'processing_status': status,
            'enhanced_features_active': True,
            'sync_time': datetime.utcnow().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Enhanced unified sync failed: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'enhanced_features_active': False
        }), 500

@enhanced_api_bp.route('/entities/topics', methods=['GET'])
def get_enhanced_topics():
    """Get topics with enhanced intelligence accumulation"""
    try:
        user = require_auth_session()
        if not user:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user_id = user.id
        limit = request.args.get('limit', 50, type=int)
        include_relationships = request.args.get('relationships', False, type=bool)
        
        db_manager = get_db_manager()
        
        with db_manager.get_session() as db_session:
            # Get topics with enhanced metrics
            topics_query = db_session.query(Topic).filter(Topic.user_id == user_id)
            
            # Order by strategic importance and mentions
            topics_query = topics_query.order_by(
                Topic.strategic_importance.desc(),
                Topic.total_mentions.desc()
            ).limit(limit)
            
            topics = topics_query.all()
            
            topics_data = []
            for topic in topics:
                topic_dict = {
                    'id': topic.id,
                    'name': topic.name,
                    'description': topic.description,
                    'keywords': topic.keywords.split(',') if topic.keywords else [],
                    'strategic_importance': topic.strategic_importance,
                    'total_mentions': topic.total_mentions,
                    'last_mentioned': topic.last_mentioned.isoformat() if topic.last_mentioned else None,
                    'intelligence_summary': topic.intelligence_summary,
                    'version': topic.version,
                    'created_at': topic.created_at.isoformat() if topic.created_at else None
                }
                
                # Include relationships if requested
                if include_relationships:
                    relationships = db_session.query(EntityRelationship).filter(
                        ((EntityRelationship.entity_type_a == 'topic') & (EntityRelationship.entity_id_a == topic.id)) |
                        ((EntityRelationship.entity_type_b == 'topic') & (EntityRelationship.entity_id_b == topic.id))
                    ).all()
                    
                    topic_dict['relationships'] = [
                        {
                            'type': rel.relationship_type,
                            'strength': rel.strength,
                            'context': rel.context_summary,
                            'related_entity': {
                                'type': rel.entity_type_b if rel.entity_type_a == 'topic' else rel.entity_type_a,
                                'id': rel.entity_id_b if rel.entity_type_a == 'topic' else rel.entity_id_a
                            }
                        }
                        for rel in relationships
                    ]
                
                topics_data.append(topic_dict)
        
        return jsonify({
            'success': True,
            'topics': topics_data,
            'count': len(topics_data),
            'enhanced_features': True,
            'fetched_at': datetime.utcnow().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Failed to get enhanced topics: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@enhanced_api_bp.route('/entities/people', methods=['GET'])
def get_enhanced_people():
    """Get people with comprehensive relationship intelligence"""
    try:
        user = require_auth_session()
        if not user:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user_id = user.id
        limit = request.args.get('limit', 50, type=int)
        include_relationships = request.args.get('relationships', False, type=bool)
        
        db_manager = get_db_manager()
        
        with db_manager.get_session() as db_session:
            # Get people with enhanced metrics
            people_query = db_session.query(Person).filter(Person.user_id == user_id)
            
            # Order by importance and interactions
            people_query = people_query.order_by(
                Person.importance_level.desc(),
                Person.total_interactions.desc()
            ).limit(limit)
            
            people = people_query.all()
            
            people_data = []
            for person in people:
                person_dict = {
                    'id': person.id,
                    'name': person.name,
                    'email_address': person.email_address,
                    'company': person.company,
                    'title': person.title,
                    'relationship_type': person.relationship_type,
                    'importance_level': person.importance_level,
                    'total_interactions': person.total_interactions,
                    'last_contact': person.last_contact.isoformat() if person.last_contact else None,
                    'professional_story': person.professional_story,
                    'key_topics': person.key_topics.split(',') if person.key_topics else [],
                    'created_at': person.created_at.isoformat() if person.created_at else None
                }
                
                # Include relationships if requested
                if include_relationships:
                    relationships = db_session.query(EntityRelationship).filter(
                        ((EntityRelationship.entity_type_a == 'person') & (EntityRelationship.entity_id_a == person.id)) |
                        ((EntityRelationship.entity_type_b == 'person') & (EntityRelationship.entity_id_b == person.id))
                    ).all()
                    
                    person_dict['relationships'] = [
                        {
                            'type': rel.relationship_type,
                            'strength': rel.strength,
                            'context': rel.context_summary,
                            'total_interactions': rel.total_interactions,
                            'last_interaction': rel.last_interaction.isoformat() if rel.last_interaction else None,
                            'related_entity': {
                                'type': rel.entity_type_b if rel.entity_type_a == 'person' else rel.entity_type_a,
                                'id': rel.entity_id_b if rel.entity_type_a == 'person' else rel.entity_id_a
                            }
                        }
                        for rel in relationships
                    ]
                
                people_data.append(person_dict)
        
        return jsonify({
            'success': True,
            'people': people_data,
            'count': len(people_data),
            'enhanced_features': True,
            'fetched_at': datetime.utcnow().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Failed to get enhanced people: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@enhanced_api_bp.route('/intelligence/insights', methods=['GET'])
def get_intelligence_insights():
    """Get proactive intelligence insights"""
    try:
        user = require_auth_session()
        if not user:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user_id = user.id
        status = request.args.get('status', 'new')
        insight_type = request.args.get('type')
        limit = request.args.get('limit', 20, type=int)
        
        db_manager = get_db_manager()
        
        with db_manager.get_session() as db_session:
            insights_query = db_session.query(IntelligenceInsight).filter(
                IntelligenceInsight.user_id == user_id
            )
            
            if status:
                insights_query = insights_query.filter(IntelligenceInsight.status == status)
            
            if insight_type:
                insights_query = insights_query.filter(IntelligenceInsight.insight_type == insight_type)
            
            # Order by priority and creation date
            insights_query = insights_query.order_by(
                IntelligenceInsight.priority.desc(),
                IntelligenceInsight.created_at.desc()
            ).limit(limit)
            
            insights = insights_query.all()
            
            insights_data = [
                {
                    'id': insight.id,
                    'type': insight.insight_type,
                    'title': insight.title,
                    'description': insight.description,
                    'priority': insight.priority,
                    'confidence': insight.confidence,
                    'status': insight.status,
                    'related_entity': {
                        'type': insight.related_entity_type,
                        'id': insight.related_entity_id
                    } if insight.related_entity_type else None,
                    'action_taken': insight.action_taken,
                    'created_at': insight.created_at.isoformat() if insight.created_at else None,
                    'expires_at': insight.expires_at.isoformat() if insight.expires_at else None
                }
                for insight in insights
            ]
        
        return jsonify({
            'success': True,
            'insights': insights_data,
            'count': len(insights_data),
            'enhanced_intelligence': True,
            'fetched_at': datetime.utcnow().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Failed to get intelligence insights: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@enhanced_api_bp.route('/intelligence/trigger-insights', methods=['POST'])
def trigger_proactive_insights():
    """Manually trigger proactive insights generation"""
    try:
        user = require_auth_session()
        if not user:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user_id = user.id
        
        # Trigger insights through real-time processor if running
        if realtime_processor.is_running:
            realtime_processor.trigger_proactive_insights(user_id, priority=2)
            return jsonify({
                'success': True,
                'message': 'Proactive insights generation triggered via real-time processor',
                'real_time_processing': True
            })
        else:
            # Generate insights directly through entity engine
            insights = entity_engine.generate_proactive_insights(user_id)
            
            return jsonify({
                'success': True,
                'message': 'Proactive insights generated directly',
                'insights_generated': len(insights),
                'real_time_processing': False,
                'insights': [
                    {
                        'type': insight.insight_type,
                        'title': insight.title,
                        'description': insight.description,
                        'priority': insight.priority,
                        'confidence': insight.confidence
                    }
                    for insight in insights
                ]
            })
        
    except Exception as e:
        logger.error(f"Failed to trigger proactive insights: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@enhanced_api_bp.route('/real-time/status', methods=['GET'])
def get_realtime_status():
    """Get real-time processing status and performance metrics"""
    try:
        user = require_auth_session()
        if not user:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        status = {
            'is_running': realtime_processor.is_running,
            'enhanced_features_active': True,
            'processors_available': {
                'real_time_processor': realtime_processor is not None,
                'enhanced_ai_pipeline': enhanced_ai_processor is not None,
                'unified_entity_engine': entity_engine is not None
            }
        }
        
        if realtime_processor.is_running:
            processing_status = realtime_processor.get_processing_status()
            status.update(processing_status)
        else:
            status.update({
                'queue_size': 0,
                'workers_active': 0,
                'processing_rate': 0,
                'total_processed': 0
            })
        
        return jsonify({
            'success': True,
            'status': status,
            'timestamp': datetime.utcnow().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Failed to get real-time status: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@enhanced_api_bp.route('/real-time/start', methods=['POST'])
def start_realtime_processing():
    """Start the real-time processing system"""
    try:
        if not realtime_processor.is_running:
            realtime_processor.start()
            
            return jsonify({
                'success': True,
                'message': 'Real-time processor started successfully',
                'is_running': realtime_processor.is_running
            })
        else:
            return jsonify({
                'success': True,
                'message': 'Real-time processor already running',
                'is_running': True
            })
        
    except Exception as e:
        logger.error(f"Failed to start real-time processor: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@enhanced_api_bp.route('/real-time/stop', methods=['POST'])
def stop_realtime_processing():
    """Stop the real-time processing system"""
    try:
        if realtime_processor.is_running:
            realtime_processor.stop()
            
            return jsonify({
                'success': True,
                'message': 'Real-time processor stopped successfully',
                'is_running': realtime_processor.is_running
            })
        else:
            return jsonify({
                'success': True,
                'message': 'Real-time processor already stopped',
                'is_running': False
            })
        
    except Exception as e:
        logger.error(f"Failed to stop real-time processor: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# Helper functions for enhanced endpoints

def get_entity_intelligence_summary(user_id: int, entity_type: str, entity_id: int) -> Dict:
    """Get comprehensive intelligence summary for an entity"""
    try:
        db_manager = get_db_manager()
        
        with db_manager.get_session() as session:
            # Get entity relationships
            relationships = session.query(EntityRelationship).filter(
                EntityRelationship.user_id == user_id,
                ((EntityRelationship.entity_type_a == entity_type) & (EntityRelationship.entity_id_a == entity_id)) |
                ((EntityRelationship.entity_type_b == entity_type) & (EntityRelationship.entity_id_b == entity_id))
            ).all()
            
            # Get related insights
            insights = session.query(IntelligenceInsight).filter(
                IntelligenceInsight.user_id == user_id,
                IntelligenceInsight.related_entity_type == entity_type,
                IntelligenceInsight.related_entity_id == entity_id
            ).all()
            
            return {
                'entity_type': entity_type,
                'entity_id': entity_id,
                'total_relationships': len(relationships),
                'relationship_strength_avg': sum(r.strength for r in relationships) / len(relationships) if relationships else 0,
                'related_insights': len(insights),
                'last_activity': max(r.last_interaction for r in relationships) if relationships else None
            }
            
    except Exception as e:
        logger.error(f"Failed to get entity intelligence summary: {str(e)}")
        return {'error': str(e)}

def calculate_relationship_analysis(user_id: int) -> Dict:
    """Calculate comprehensive relationship analysis for user"""
    try:
        db_manager = get_db_manager()
        
        with db_manager.get_session() as session:
            # Get all relationships
            relationships = session.query(EntityRelationship).filter(
                EntityRelationship.user_id == user_id
            ).all()
            
            # Analyze relationship patterns
            analysis = {
                'total_relationships': len(relationships),
                'strong_relationships': len([r for r in relationships if r.strength > 0.7]),
                'relationship_types': {},
                'entity_connectivity': {},
                'average_strength': sum(r.strength for r in relationships) / len(relationships) if relationships else 0
            }
            
            # Count by type
            for rel in relationships:
                rel_type = rel.relationship_type
                analysis['relationship_types'][rel_type] = analysis['relationship_types'].get(rel_type, 0) + 1
                
                # Track entity connectivity
                entity_a = f"{rel.entity_type_a}:{rel.entity_id_a}"
                entity_b = f"{rel.entity_type_b}:{rel.entity_id_b}"
                
                analysis['entity_connectivity'][entity_a] = analysis['entity_connectivity'].get(entity_a, 0) + 1
                analysis['entity_connectivity'][entity_b] = analysis['entity_connectivity'].get(entity_b, 0) + 1
            
            return analysis
            
    except Exception as e:
        logger.error(f"Failed to calculate relationship analysis: {str(e)}")
        return {'error': str(e)}

def get_intelligence_quality_metrics(user_id: int) -> Dict:
    """Get quality metrics for user's intelligence data"""
    try:
        db_manager = get_db_manager()
        
        with db_manager.get_session() as session:
            # Get counts for different entity types
            topics_count = session.query(Topic).filter(Topic.user_id == user_id).count()
            people_count = session.query(Person).filter(Person.user_id == user_id).count()
            insights_count = session.query(IntelligenceInsight).filter(IntelligenceInsight.user_id == user_id).count()
            relationships_count = session.query(EntityRelationship).filter(EntityRelationship.user_id == user_id).count()
            
            # Calculate confidence scores
            topics = session.query(Topic).filter(Topic.user_id == user_id).all()
            avg_topic_importance = sum(t.strategic_importance for t in topics) / len(topics) if topics else 0
            
            people = session.query(Person).filter(Person.user_id == user_id).all()
            avg_person_importance = sum(p.importance_level for p in people) / len(people) if people else 0
            
            insights = session.query(IntelligenceInsight).filter(IntelligenceInsight.user_id == user_id).all()
            avg_insight_confidence = sum(i.confidence for i in insights) / len(insights) if insights else 0
            
            return {
                'entity_counts': {
                    'topics': topics_count,
                    'people': people_count,
                    'insights': insights_count,
                    'relationships': relationships_count
                },
                'quality_metrics': {
                    'avg_topic_importance': avg_topic_importance,
                    'avg_person_importance': avg_person_importance,
                    'avg_insight_confidence': avg_insight_confidence,
                    'relationship_density': relationships_count / (topics_count + people_count) if (topics_count + people_count) > 0 else 0
                },
                'intelligence_score': (avg_topic_importance + avg_person_importance + avg_insight_confidence) / 3
            }
            
    except Exception as e:
        logger.error(f"Failed to get intelligence quality metrics: {str(e)}")
        return {'error': str(e)} 

============================================================
FILE: chief_of_staff_ai/api/auth_endpoints.py
============================================================
# Authentication API Endpoints - User Management and Security
# These endpoints provide comprehensive authentication, authorization, and user management

import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timezone, timedelta
from flask import Blueprint, request, jsonify, session, g
from functools import wraps
import json
import jwt
import hashlib
import secrets
import re
from email_validator import validate_email, EmailNotValidError

# Import the integration manager and models
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'chief_of_staff_ai'))

from models.database import User, UserSession, ApiKey, get_db_manager
from config.settings import settings

logger = logging.getLogger(__name__)

# Create Blueprint
auth_api_bp = Blueprint('auth_api', __name__, url_prefix='/api/auth')

# JWT Configuration
JWT_SECRET_KEY = getattr(settings, 'JWT_SECRET_KEY', secrets.token_urlsafe(32))
JWT_EXPIRATION_HOURS = getattr(settings, 'JWT_EXPIRATION_HOURS', 24)
JWT_REFRESH_DAYS = getattr(settings, 'JWT_REFRESH_DAYS', 30)

# =====================================================================
# AUTHENTICATION UTILITIES
# =====================================================================

def generate_password_hash(password: str) -> str:
    """Generate secure password hash"""
    salt = secrets.token_hex(16)
    password_hash = hashlib.pbkdf2_hmac('sha256', password.encode(), salt.encode(), 100000)
    return f"{salt}:{password_hash.hex()}"

def verify_password(password: str, password_hash: str) -> bool:
    """Verify password against hash"""
    try:
        salt, stored_hash = password_hash.split(':')
        computed_hash = hashlib.pbkdf2_hmac('sha256', password.encode(), salt.encode(), 100000)
        return computed_hash.hex() == stored_hash
    except (ValueError, TypeError):
        return False

def generate_jwt_token(user_id: int, user_email: str, role: str = 'user', expires_in_hours: int = None) -> str:
    """Generate JWT access token"""
    expires_in = expires_in_hours or JWT_EXPIRATION_HOURS
    expiration = datetime.utcnow() + timedelta(hours=expires_in)
    
    payload = {
        'user_id': user_id,
        'email': user_email,
        'role': role,
        'exp': expiration,
        'iat': datetime.utcnow(),
        'type': 'access'
    }
    
    return jwt.encode(payload, JWT_SECRET_KEY, algorithm='HS256')

def generate_refresh_token(user_id: int, user_email: str) -> str:
    """Generate JWT refresh token"""
    expiration = datetime.utcnow() + timedelta(days=JWT_REFRESH_DAYS)
    
    payload = {
        'user_id': user_id,
        'email': user_email,
        'exp': expiration,
        'iat': datetime.utcnow(),
        'type': 'refresh'
    }
    
    return jwt.encode(payload, JWT_SECRET_KEY, algorithm='HS256')

def verify_jwt_token(token: str) -> Optional[Dict]:
    """Verify and decode JWT token"""
    try:
        payload = jwt.decode(token, JWT_SECRET_KEY, algorithms=['HS256'])
        return payload
    except jwt.ExpiredSignatureError:
        return None
    except jwt.InvalidTokenError:
        return None

def validate_password_strength(password: str) -> List[str]:
    """Validate password strength and return list of issues"""
    issues = []
    
    if len(password) < 8:
        issues.append("Password must be at least 8 characters long")
    
    if not re.search(r'[A-Z]', password):
        issues.append("Password must contain at least one uppercase letter")
    
    if not re.search(r'[a-z]', password):
        issues.append("Password must contain at least one lowercase letter")
    
    if not re.search(r'\d', password):
        issues.append("Password must contain at least one number")
    
    if not re.search(r'[!@#$%^&*(),.?":{}|<>]', password):
        issues.append("Password must contain at least one special character")
    
    return issues

def require_jwt_auth(roles: List[str] = None):
    """Decorator to require JWT authentication with optional role checking"""
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            auth_header = request.headers.get('Authorization')
            
            if not auth_header or not auth_header.startswith('Bearer '):
                return jsonify({
                    'success': False,
                    'error': 'Authorization header required',
                    'code': 'AUTH_REQUIRED'
                }), 401
            
            token = auth_header.split(' ')[1]
            payload = verify_jwt_token(token)
            
            if not payload:
                return jsonify({
                    'success': False,
                    'error': 'Invalid or expired token',
                    'code': 'INVALID_TOKEN'
                }), 401
            
            if payload.get('type') != 'access':
                return jsonify({
                    'success': False,
                    'error': 'Invalid token type',
                    'code': 'INVALID_TOKEN_TYPE'
                }), 401
            
            # Role checking
            if roles and payload.get('role') not in roles:
                return jsonify({
                    'success': False,
                    'error': 'Insufficient permissions',
                    'code': 'INSUFFICIENT_PERMISSIONS'
                }), 403
            
            # Store user info in g for easy access
            g.user_id = payload['user_id']
            g.user_email = payload['email']
            g.user_role = payload.get('role', 'user')
            
            return f(*args, **kwargs)
        return decorated_function
    return decorator

def success_response(data: Any, message: str = None) -> tuple:
    """Create standardized success response"""
    response = {
        'success': True,
        'data': data,
        'timestamp': datetime.utcnow().isoformat()
    }
    if message:
        response['message'] = message
    return jsonify(response), 200

def error_response(error: str, code: str = None, status_code: int = 400) -> tuple:
    """Create standardized error response"""
    response = {
        'success': False,
        'error': error,
        'timestamp': datetime.utcnow().isoformat()
    }
    if code:
        response['code'] = code
    return jsonify(response), status_code

# =====================================================================
# USER REGISTRATION AND LOGIN ENDPOINTS
# =====================================================================

@auth_api_bp.route('/register', methods=['POST'])
def register_user():
    """
    Register a new user account.
    """
    try:
        data = request.get_json() or {}
        
        # Validate required fields
        required_fields = ['email', 'password', 'full_name']
        for field in required_fields:
            if not data.get(field):
                return error_response(f"Missing required field: {field}", "MISSING_FIELD")
        
        email = data['email'].lower().strip()
        password = data['password']
        full_name = data['full_name'].strip()
        
        # Validate email format
        try:
            validate_email(email)
        except EmailNotValidError:
            return error_response("Invalid email format", "INVALID_EMAIL")
        
        # Validate password strength
        password_issues = validate_password_strength(password)
        if password_issues:
            return error_response(f"Password validation failed: {', '.join(password_issues)}", "WEAK_PASSWORD")
        
        with get_db_manager().get_session() as session:
            # Check if user already exists
            existing_user = session.query(User).filter(User.email == email).first()
            if existing_user:
                return error_response("User with this email already exists", "USER_EXISTS", 409)
            
            # Create new user
            password_hash = generate_password_hash(password)
            
            new_user = User(
                email=email,
                password_hash=password_hash,
                full_name=full_name,
                role=data.get('role', 'user'),
                is_active=True,
                created_at=datetime.utcnow(),
                last_login=None,
                email_verified=False,
                preferences=data.get('preferences', {})
            )
            
            session.add(new_user)
            session.commit()
            
            # Generate tokens
            access_token = generate_jwt_token(new_user.id, new_user.email, new_user.role)
            refresh_token = generate_refresh_token(new_user.id, new_user.email)
            
            # Create user session
            user_session = UserSession(
                user_id=new_user.id,
                session_token=refresh_token,
                expires_at=datetime.utcnow() + timedelta(days=JWT_REFRESH_DAYS),
                created_at=datetime.utcnow(),
                last_activity=datetime.utcnow(),
                ip_address=request.remote_addr,
                user_agent=request.headers.get('User-Agent', '')
            )
            session.add(user_session)
            session.commit()
            
            logger.info(f"New user registered: {email} (ID: {new_user.id})")
            
            return success_response({
                'user': {
                    'id': new_user.id,
                    'email': new_user.email,
                    'full_name': new_user.full_name,
                    'role': new_user.role,
                    'created_at': new_user.created_at.isoformat()
                },
                'tokens': {
                    'access_token': access_token,
                    'refresh_token': refresh_token,
                    'token_type': 'Bearer',
                    'expires_in': JWT_EXPIRATION_HOURS * 3600
                }
            }, "User registered successfully")
            
    except Exception as e:
        logger.error(f"Error in user registration: {str(e)}")
        return error_response(f"Registration failed: {str(e)}", "REGISTRATION_ERROR", 500)

@auth_api_bp.route('/login', methods=['POST'])
def login_user():
    """
    Login user and return authentication tokens.
    """
    try:
        data = request.get_json() or {}
        
        email = data.get('email', '').lower().strip()
        password = data.get('password', '')
        remember_me = data.get('remember_me', False)
        
        if not email or not password:
            return error_response("Email and password are required", "MISSING_CREDENTIALS")
        
        with get_db_manager().get_session() as session:
            # Find user
            user = session.query(User).filter(User.email == email).first()
            
            if not user or not verify_password(password, user.password_hash):
                return error_response("Invalid email or password", "INVALID_CREDENTIALS", 401)
            
            if not user.is_active:
                return error_response("Account is deactivated", "ACCOUNT_DEACTIVATED", 401)
            
            # Update last login
            user.last_login = datetime.utcnow()
            
            # Generate tokens
            token_expiry = JWT_EXPIRATION_HOURS * 24 if remember_me else JWT_EXPIRATION_HOURS
            access_token = generate_jwt_token(user.id, user.email, user.role, token_expiry)
            refresh_token = generate_refresh_token(user.id, user.email)
            
            # Create user session
            user_session = UserSession(
                user_id=user.id,
                session_token=refresh_token,
                expires_at=datetime.utcnow() + timedelta(days=JWT_REFRESH_DAYS),
                created_at=datetime.utcnow(),
                last_activity=datetime.utcnow(),
                ip_address=request.remote_addr,
                user_agent=request.headers.get('User-Agent', '')
            )
            session.add(user_session)
            session.commit()
            
            logger.info(f"User logged in: {email} (ID: {user.id})")
            
            return success_response({
                'user': {
                    'id': user.id,
                    'email': user.email,
                    'full_name': user.full_name,
                    'role': user.role,
                    'last_login': user.last_login.isoformat(),
                    'preferences': user.preferences
                },
                'tokens': {
                    'access_token': access_token,
                    'refresh_token': refresh_token,
                    'token_type': 'Bearer',
                    'expires_in': token_expiry * 3600
                }
            }, "Login successful")
            
    except Exception as e:
        logger.error(f"Error in user login: {str(e)}")
        return error_response(f"Login failed: {str(e)}", "LOGIN_ERROR", 500)

@auth_api_bp.route('/refresh', methods=['POST'])
def refresh_token():
    """
    Refresh access token using refresh token.
    """
    try:
        data = request.get_json() or {}
        refresh_token = data.get('refresh_token')
        
        if not refresh_token:
            return error_response("Refresh token is required", "MISSING_REFRESH_TOKEN")
        
        # Verify refresh token
        payload = verify_jwt_token(refresh_token)
        if not payload or payload.get('type') != 'refresh':
            return error_response("Invalid or expired refresh token", "INVALID_REFRESH_TOKEN", 401)
        
        with get_db_manager().get_session() as session:
            # Check if session exists and is valid
            user_session = session.query(UserSession).filter(
                UserSession.session_token == refresh_token,
                UserSession.expires_at > datetime.utcnow(),
                UserSession.is_active == True
            ).first()
            
            if not user_session:
                return error_response("Invalid session", "INVALID_SESSION", 401)
            
            # Get user
            user = session.query(User).filter(User.id == payload['user_id']).first()
            if not user or not user.is_active:
                return error_response("User not found or inactive", "INVALID_USER", 401)
            
            # Generate new access token
            access_token = generate_jwt_token(user.id, user.email, user.role)
            
            # Update session activity
            user_session.last_activity = datetime.utcnow()
            session.commit()
            
            return success_response({
                'access_token': access_token,
                'token_type': 'Bearer',
                'expires_in': JWT_EXPIRATION_HOURS * 3600
            }, "Token refreshed successfully")
            
    except Exception as e:
        logger.error(f"Error in token refresh: {str(e)}")
        return error_response(f"Token refresh failed: {str(e)}", "REFRESH_ERROR", 500)

@auth_api_bp.route('/logout', methods=['POST'])
@require_jwt_auth()
def logout_user():
    """
    Logout user and invalidate session.
    """
    try:
        data = request.get_json() or {}
        refresh_token = data.get('refresh_token')
        
        with get_db_manager().get_session() as session:
            # Invalidate specific session if refresh token provided
            if refresh_token:
                user_session = session.query(UserSession).filter(
                    UserSession.session_token == refresh_token,
                    UserSession.user_id == g.user_id
                ).first()
                
                if user_session:
                    user_session.is_active = False
                    user_session.logged_out_at = datetime.utcnow()
            else:
                # Invalidate all sessions for user
                session.query(UserSession).filter(
                    UserSession.user_id == g.user_id,
                    UserSession.is_active == True
                ).update({
                    'is_active': False,
                    'logged_out_at': datetime.utcnow()
                })
            
            session.commit()
            
            logger.info(f"User logged out: {g.user_email} (ID: {g.user_id})")
            
            return success_response({}, "Logout successful")
            
    except Exception as e:
        logger.error(f"Error in user logout: {str(e)}")
        return error_response(f"Logout failed: {str(e)}", "LOGOUT_ERROR", 500)

# =====================================================================
# USER PROFILE AND MANAGEMENT ENDPOINTS
# =====================================================================

@auth_api_bp.route('/profile', methods=['GET'])
@require_jwt_auth()
def get_user_profile():
    """
    Get current user profile information.
    """
    try:
        with get_db_manager().get_session() as session:
            user = session.query(User).filter(User.id == g.user_id).first()
            
            if not user:
                return error_response("User not found", "USER_NOT_FOUND", 404)
            
            # Get active sessions count
            active_sessions = session.query(UserSession).filter(
                UserSession.user_id == g.user_id,
                UserSession.is_active == True,
                UserSession.expires_at > datetime.utcnow()
            ).count()
            
            profile_data = {
                'id': user.id,
                'email': user.email,
                'full_name': user.full_name,
                'role': user.role,
                'is_active': user.is_active,
                'email_verified': user.email_verified,
                'created_at': user.created_at.isoformat() if user.created_at else None,
                'last_login': user.last_login.isoformat() if user.last_login else None,
                'preferences': user.preferences,
                'active_sessions': active_sessions
            }
            
            return success_response(profile_data)
            
    except Exception as e:
        logger.error(f"Error getting user profile: {str(e)}")
        return error_response(f"Profile retrieval failed: {str(e)}", "PROFILE_ERROR", 500)

@auth_api_bp.route('/profile', methods=['PUT'])
@require_jwt_auth()
def update_user_profile():
    """
    Update current user profile information.
    """
    try:
        data = request.get_json() or {}
        
        with get_db_manager().get_session() as session:
            user = session.query(User).filter(User.id == g.user_id).first()
            
            if not user:
                return error_response("User not found", "USER_NOT_FOUND", 404)
            
            # Update allowed fields
            if 'full_name' in data:
                user.full_name = data['full_name'].strip()
            
            if 'preferences' in data:
                # Merge preferences
                current_prefs = user.preferences or {}
                current_prefs.update(data['preferences'])
                user.preferences = current_prefs
            
            user.updated_at = datetime.utcnow()
            session.commit()
            
            return success_response({
                'id': user.id,
                'full_name': user.full_name,
                'preferences': user.preferences,
                'updated_at': user.updated_at.isoformat()
            }, "Profile updated successfully")
            
    except Exception as e:
        logger.error(f"Error updating user profile: {str(e)}")
        return error_response(f"Profile update failed: {str(e)}", "PROFILE_UPDATE_ERROR", 500)

@auth_api_bp.route('/change-password', methods=['POST'])
@require_jwt_auth()
def change_password():
    """
    Change user password.
    """
    try:
        data = request.get_json() or {}
        
        current_password = data.get('current_password')
        new_password = data.get('new_password')
        
        if not current_password or not new_password:
            return error_response("Current and new passwords are required", "MISSING_PASSWORDS")
        
        # Validate new password strength
        password_issues = validate_password_strength(new_password)
        if password_issues:
            return error_response(f"New password validation failed: {', '.join(password_issues)}", "WEAK_PASSWORD")
        
        with get_db_manager().get_session() as session:
            user = session.query(User).filter(User.id == g.user_id).first()
            
            if not user:
                return error_response("User not found", "USER_NOT_FOUND", 404)
            
            # Verify current password
            if not verify_password(current_password, user.password_hash):
                return error_response("Current password is incorrect", "INVALID_CURRENT_PASSWORD", 401)
            
            # Update password
            user.password_hash = generate_password_hash(new_password)
            user.updated_at = datetime.utcnow()
            
            # Invalidate all existing sessions except current one
            current_token = request.headers.get('Authorization', '').replace('Bearer ', '')
            current_payload = verify_jwt_token(current_token)
            
            if current_payload:
                session.query(UserSession).filter(
                    UserSession.user_id == g.user_id,
                    UserSession.is_active == True,
                    UserSession.created_at < datetime.utcnow() - timedelta(minutes=5)  # Keep recent session
                ).update({
                    'is_active': False,
                    'logged_out_at': datetime.utcnow()
                })
            
            session.commit()
            
            logger.info(f"Password changed for user: {g.user_email} (ID: {g.user_id})")
            
            return success_response({}, "Password changed successfully")
            
    except Exception as e:
        logger.error(f"Error changing password: {str(e)}")
        return error_response(f"Password change failed: {str(e)}", "PASSWORD_CHANGE_ERROR", 500)

# =====================================================================
# API KEY MANAGEMENT ENDPOINTS
# =====================================================================

@auth_api_bp.route('/api-keys', methods=['GET'])
@require_jwt_auth()
def get_api_keys():
    """
    Get user's API keys (without showing actual keys).
    """
    try:
        with get_db_manager().get_session() as session:
            api_keys = session.query(ApiKey).filter(
                ApiKey.user_id == g.user_id,
                ApiKey.is_active == True
            ).all()
            
            keys_data = []
            for key in api_keys:
                keys_data.append({
                    'id': key.id,
                    'name': key.name,
                    'key_prefix': key.key_hash[:8] + '...',  # Show only prefix
                    'permissions': key.permissions,
                    'created_at': key.created_at.isoformat() if key.created_at else None,
                    'last_used': key.last_used.isoformat() if key.last_used else None,
                    'expires_at': key.expires_at.isoformat() if key.expires_at else None
                })
            
            return success_response({
                'api_keys': keys_data,
                'count': len(keys_data)
            })
            
    except Exception as e:
        logger.error(f"Error getting API keys: {str(e)}")
        return error_response(f"API keys retrieval failed: {str(e)}", "API_KEYS_ERROR", 500)

@auth_api_bp.route('/api-keys', methods=['POST'])
@require_jwt_auth()
def create_api_key():
    """
    Create a new API key for the user.
    """
    try:
        data = request.get_json() or {}
        
        name = data.get('name', '').strip()
        permissions = data.get('permissions', ['read'])
        expires_in_days = data.get('expires_in_days', 365)
        
        if not name:
            return error_response("API key name is required", "MISSING_NAME")
        
        # Generate API key
        api_key = secrets.token_urlsafe(32)
        key_hash = hashlib.sha256(api_key.encode()).hexdigest()
        
        with get_db_manager().get_session() as session:
            # Check if name already exists for user
            existing_key = session.query(ApiKey).filter(
                ApiKey.user_id == g.user_id,
                ApiKey.name == name,
                ApiKey.is_active == True
            ).first()
            
            if existing_key:
                return error_response("API key with this name already exists", "NAME_EXISTS", 409)
            
            # Create new API key
            new_api_key = ApiKey(
                user_id=g.user_id,
                name=name,
                key_hash=key_hash,
                permissions=permissions,
                created_at=datetime.utcnow(),
