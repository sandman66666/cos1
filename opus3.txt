
      "dependencies": {
        "call-bind": "^1.0.7",
        "define-properties": "^1.2.1",
        "es-object-atoms": "^1.0.0"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/stringify-object": {
      "version": "3.3.0",
      "resolved": "https://registry.npmjs.org/stringify-object/-/stringify-object-3.3.0.tgz",
      "integrity": "sha512-rHqiFh1elqCQ9WPLIC8I0Q/g/wj5J1eMkyoiD6eoQApWHP0FtlK7rqnhmabL5VUY9JQCcqwwvlOaSuutekgyrw==",
      "license": "BSD-2-Clause",
      "dependencies": {
        "get-own-enumerable-property-symbols": "^3.0.0",
        "is-obj": "^1.0.1",
        "is-regexp": "^1.0.0"
      },
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/strip-ansi": {
      "version": "6.0.1",
      "resolved": "https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz",
      "integrity": "sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==",
      "license": "MIT",
      "dependencies": {
        "ansi-regex": "^5.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/strip-ansi-cjs": {
      "name": "strip-ansi",
      "version": "6.0.1",
      "resolved": "https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz",
      "integrity": "sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==",
      "license": "MIT",
      "dependencies": {
        "ansi-regex": "^5.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/strip-bom": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/strip-bom/-/strip-bom-4.0.0.tgz",
      "integrity": "sha512-3xurFv5tEgii33Zi8Jtp55wEIILR9eh34FAW00PZf+JnSsTmV/ioewSgQl97JHvgjoRGwPShsWm+IdrxB35d0w==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/strip-comments": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/strip-comments/-/strip-comments-2.0.1.tgz",
      "integrity": "sha512-ZprKx+bBLXv067WTCALv8SSz5l2+XhpYCsVtSqlMnkAXMWDq+/ekVbl1ghqP9rUHTzv6sm/DwCOiYutU/yp1fw==",
      "license": "MIT",
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/strip-final-newline": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/strip-final-newline/-/strip-final-newline-2.0.0.tgz",
      "integrity": "sha512-BrpvfNAE3dcvq7ll3xVumzjKjZQ5tI1sEUIKr3Uoks0XUl45St3FlatVqef9prk4jRDzhW6WZg+3bk93y6pLjA==",
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/strip-indent": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/strip-indent/-/strip-indent-3.0.0.tgz",
      "integrity": "sha512-laJTa3Jb+VQpaC6DseHhF7dXVqHTfJPCRDaEbid/drOhgitgYku/letMUqOXFoWV0zIIUbjpdH2t+tYj4bQMRQ==",
      "license": "MIT",
      "dependencies": {
        "min-indent": "^1.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/strip-json-comments": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/strip-json-comments/-/strip-json-comments-3.1.1.tgz",
      "integrity": "sha512-6fPc+R4ihwqP6N/aIv2f1gMH8lOVtWQHoqC4yK6oSDVVocumAsfCqjkXnqiYMhmMwS/mEHLp7Vehlt3ql6lEig==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/style-loader": {
      "version": "3.3.4",
      "resolved": "https://registry.npmjs.org/style-loader/-/style-loader-3.3.4.tgz",
      "integrity": "sha512-0WqXzrsMTyb8yjZJHDqwmnwRJvhALK9LfRtRc6B4UTWe8AijYLZYZ9thuJTZc2VfQWINADW/j+LiJnfy2RoC1w==",
      "license": "MIT",
      "engines": {
        "node": ">= 12.13.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/webpack"
      },
      "peerDependencies": {
        "webpack": "^5.0.0"
      }
    },
    "node_modules/stylehacks": {
      "version": "5.1.1",
      "resolved": "https://registry.npmjs.org/stylehacks/-/stylehacks-5.1.1.tgz",
      "integrity": "sha512-sBpcd5Hx7G6seo7b1LkpttvTz7ikD0LlH5RmdcBNb6fFR0Fl7LQwHDFr300q4cwUqi+IYrFGmsIHieMBfnN/Bw==",
      "license": "MIT",
      "dependencies": {
        "browserslist": "^4.21.4",
        "postcss-selector-parser": "^6.0.4"
      },
      "engines": {
        "node": "^10 || ^12 || >=14.0"
      },
      "peerDependencies": {
        "postcss": "^8.2.15"
      }
    },
    "node_modules/sucrase": {
      "version": "3.35.0",
      "resolved": "https://registry.npmjs.org/sucrase/-/sucrase-3.35.0.tgz",
      "integrity": "sha512-8EbVDiu9iN/nESwxeSxDKe0dunta1GOlHufmSSXxMD2z2/tMZpDMpvXQGsc+ajGo8y2uYUmixaSRUc/QPoQ0GA==",
      "license": "MIT",
      "dependencies": {
        "@jridgewell/gen-mapping": "^0.3.2",
        "commander": "^4.0.0",
        "glob": "^10.3.10",
        "lines-and-columns": "^1.1.6",
        "mz": "^2.7.0",
        "pirates": "^4.0.1",
        "ts-interface-checker": "^0.1.9"
      },
      "bin": {
        "sucrase": "bin/sucrase",
        "sucrase-node": "bin/sucrase-node"
      },
      "engines": {
        "node": ">=16 || 14 >=14.17"
      }
    },
    "node_modules/sucrase/node_modules/brace-expansion": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/brace-expansion/-/brace-expansion-2.0.2.tgz",
      "integrity": "sha512-Jt0vHyM+jmUBqojB7E1NIYadt0vI0Qxjxd2TErW94wDz+E2LAm5vKMXXwg6ZZBTHPuUlDgQHKXvjGBdfcF1ZDQ==",
      "license": "MIT",
      "dependencies": {
        "balanced-match": "^1.0.0"
      }
    },
    "node_modules/sucrase/node_modules/commander": {
      "version": "4.1.1",
      "resolved": "https://registry.npmjs.org/commander/-/commander-4.1.1.tgz",
      "integrity": "sha512-NOKm8xhkzAjzFx8B2v5OAHT+u5pRQc2UCa2Vq9jYL/31o2wi9mxBA7LIFs3sV5VSC49z6pEhfbMULvShKj26WA==",
      "license": "MIT",
      "engines": {
        "node": ">= 6"
      }
    },
    "node_modules/sucrase/node_modules/glob": {
      "version": "10.4.5",
      "resolved": "https://registry.npmjs.org/glob/-/glob-10.4.5.tgz",
      "integrity": "sha512-7Bv8RF0k6xjo7d4A/PxYLbUCfb6c+Vpd2/mB2yRDlew7Jb5hEXiCD9ibfO7wpk8i4sevK6DFny9h7EYbM3/sHg==",
      "license": "ISC",
      "dependencies": {
        "foreground-child": "^3.1.0",
        "jackspeak": "^3.1.2",
        "minimatch": "^9.0.4",
        "minipass": "^7.1.2",
        "package-json-from-dist": "^1.0.0",
        "path-scurry": "^1.11.1"
      },
      "bin": {
        "glob": "dist/esm/bin.mjs"
      },
      "funding": {
        "url": "https://github.com/sponsors/isaacs"
      }
    },
    "node_modules/sucrase/node_modules/minimatch": {
      "version": "9.0.5",
      "resolved": "https://registry.npmjs.org/minimatch/-/minimatch-9.0.5.tgz",
      "integrity": "sha512-G6T0ZX48xgozx7587koeX9Ys2NYy6Gmv//P89sEte9V9whIapMNF4idKxnW2QtCcLiTWlb/wfCabAtAFWhhBow==",
      "license": "ISC",
      "dependencies": {
        "brace-expansion": "^2.0.1"
      },
      "engines": {
        "node": ">=16 || 14 >=14.17"
      },
      "funding": {
        "url": "https://github.com/sponsors/isaacs"
      }
    },
    "node_modules/supports-color": {
      "version": "7.2.0",
      "resolved": "https://registry.npmjs.org/supports-color/-/supports-color-7.2.0.tgz",
      "integrity": "sha512-qpCAvRl9stuOHveKsn7HncJRvv501qIacKzQlO/+Lwxc9+0q2wLyv4Dfvt80/DPn2pqOBsJdDiogXGR9+OvwRw==",
      "license": "MIT",
      "dependencies": {
        "has-flag": "^4.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/supports-hyperlinks": {
      "version": "2.3.0",
      "resolved": "https://registry.npmjs.org/supports-hyperlinks/-/supports-hyperlinks-2.3.0.tgz",
      "integrity": "sha512-RpsAZlpWcDwOPQA22aCH4J0t7L8JmAvsCxfOSEwm7cQs3LshN36QaTkwd70DnBOXDWGssw2eUoc8CaRWT0XunA==",
      "license": "MIT",
      "dependencies": {
        "has-flag": "^4.0.0",
        "supports-color": "^7.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/supports-preserve-symlinks-flag": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/supports-preserve-symlinks-flag/-/supports-preserve-symlinks-flag-1.0.0.tgz",
      "integrity": "sha512-ot0WnXS9fgdkgIcePe6RHNk1WA8+muPa6cSjeR3V8K27q9BB1rTE3R1p7Hv0z1ZyAc8s6Vvv8DIyWf681MAt0w==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/svg-parser": {
      "version": "2.0.4",
      "resolved": "https://registry.npmjs.org/svg-parser/-/svg-parser-2.0.4.tgz",
      "integrity": "sha512-e4hG1hRwoOdRb37cIMSgzNsxyzKfayW6VOflrwvR+/bzrkyxY/31WkbgnQpgtrNp1SdpJvpUAGTa/ZoiPNDuRQ==",
      "license": "MIT"
    },
    "node_modules/svgo": {
      "version": "1.3.2",
      "resolved": "https://registry.npmjs.org/svgo/-/svgo-1.3.2.tgz",
      "integrity": "sha512-yhy/sQYxR5BkC98CY7o31VGsg014AKLEPxdfhora76l36hD9Rdy5NZA/Ocn6yayNPgSamYdtX2rFJdcv07AYVw==",
      "deprecated": "This SVGO version is no longer supported. Upgrade to v2.x.x.",
      "license": "MIT",
      "dependencies": {
        "chalk": "^2.4.1",
        "coa": "^2.0.2",
        "css-select": "^2.0.0",
        "css-select-base-adapter": "^0.1.1",
        "css-tree": "1.0.0-alpha.37",
        "csso": "^4.0.2",
        "js-yaml": "^3.13.1",
        "mkdirp": "~0.5.1",
        "object.values": "^1.1.0",
        "sax": "~1.2.4",
        "stable": "^0.1.8",
        "unquote": "~1.1.1",
        "util.promisify": "~1.0.0"
      },
      "bin": {
        "svgo": "bin/svgo"
      },
      "engines": {
        "node": ">=4.0.0"
      }
    },
    "node_modules/svgo/node_modules/ansi-styles": {
      "version": "3.2.1",
      "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-3.2.1.tgz",
      "integrity": "sha512-VT0ZI6kZRdTh8YyJw3SMbYm/u+NqfsAxEpWO0Pf9sq8/e94WxxOpPKx9FR1FlyCtOVDNOQ+8ntlqFxiRc+r5qA==",
      "license": "MIT",
      "dependencies": {
        "color-convert": "^1.9.0"
      },
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/svgo/node_modules/chalk": {
      "version": "2.4.2",
      "resolved": "https://registry.npmjs.org/chalk/-/chalk-2.4.2.tgz",
      "integrity": "sha512-Mti+f9lpJNcwF4tWV8/OrTTtF1gZi+f8FqlyAdouralcFWFQWF2+NgCHShjkCb+IFBLq9buZwE1xckQU4peSuQ==",
      "license": "MIT",
      "dependencies": {
        "ansi-styles": "^3.2.1",
        "escape-string-regexp": "^1.0.5",
        "supports-color": "^5.3.0"
      },
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/svgo/node_modules/color-convert": {
      "version": "1.9.3",
      "resolved": "https://registry.npmjs.org/color-convert/-/color-convert-1.9.3.tgz",
      "integrity": "sha512-QfAUtd+vFdAtFQcC8CCyYt1fYWxSqAiK2cSD6zDB8N3cpsEBAvRxp9zOGg6G/SHHJYAT88/az/IuDGALsNVbGg==",
      "license": "MIT",
      "dependencies": {
        "color-name": "1.1.3"
      }
    },
    "node_modules/svgo/node_modules/color-name": {
      "version": "1.1.3",
      "resolved": "https://registry.npmjs.org/color-name/-/color-name-1.1.3.tgz",
      "integrity": "sha512-72fSenhMw2HZMTVHeCA9KCmpEIbzWiQsjN+BHcBbS9vr1mtt+vJjPdksIBNUmKAW8TFUDPJK5SUU3QhE9NEXDw==",
      "license": "MIT"
    },
    "node_modules/svgo/node_modules/css-select": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/css-select/-/css-select-2.1.0.tgz",
      "integrity": "sha512-Dqk7LQKpwLoH3VovzZnkzegqNSuAziQyNZUcrdDM401iY+R5NkGBXGmtO05/yaXQziALuPogeG0b7UAgjnTJTQ==",
      "license": "BSD-2-Clause",
      "dependencies": {
        "boolbase": "^1.0.0",
        "css-what": "^3.2.1",
        "domutils": "^1.7.0",
        "nth-check": "^1.0.2"
      }
    },
    "node_modules/svgo/node_modules/css-what": {
      "version": "3.4.2",
      "resolved": "https://registry.npmjs.org/css-what/-/css-what-3.4.2.tgz",
      "integrity": "sha512-ACUm3L0/jiZTqfzRM3Hi9Q8eZqd6IK37mMWPLz9PJxkLWllYeRf+EHUSHYEtFop2Eqytaq1FizFVh7XfBnXCDQ==",
      "license": "BSD-2-Clause",
      "engines": {
        "node": ">= 6"
      },
      "funding": {
        "url": "https://github.com/sponsors/fb55"
      }
    },
    "node_modules/svgo/node_modules/dom-serializer": {
      "version": "0.2.2",
      "resolved": "https://registry.npmjs.org/dom-serializer/-/dom-serializer-0.2.2.tgz",
      "integrity": "sha512-2/xPb3ORsQ42nHYiSunXkDjPLBaEj/xTwUO4B7XCZQTRk7EBtTOPaygh10YAAh2OI1Qrp6NWfpAhzswj0ydt9g==",
      "license": "MIT",
      "dependencies": {
        "domelementtype": "^2.0.1",
        "entities": "^2.0.0"
      }
    },
    "node_modules/svgo/node_modules/domutils": {
      "version": "1.7.0",
      "resolved": "https://registry.npmjs.org/domutils/-/domutils-1.7.0.tgz",
      "integrity": "sha512-Lgd2XcJ/NjEw+7tFvfKxOzCYKZsdct5lczQ2ZaQY8Djz7pfAD3Gbp8ySJWtreII/vDlMVmxwa6pHmdxIYgttDg==",
      "license": "BSD-2-Clause",
      "dependencies": {
        "dom-serializer": "0",
        "domelementtype": "1"
      }
    },
    "node_modules/svgo/node_modules/domutils/node_modules/domelementtype": {
      "version": "1.3.1",
      "resolved": "https://registry.npmjs.org/domelementtype/-/domelementtype-1.3.1.tgz",
      "integrity": "sha512-BSKB+TSpMpFI/HOxCNr1O8aMOTZ8hT3pM3GQ0w/mWRmkhEDSFJkkyzz4XQsBV44BChwGkrDfMyjVD0eA2aFV3w==",
      "license": "BSD-2-Clause"
    },
    "node_modules/svgo/node_modules/escape-string-regexp": {
      "version": "1.0.5",
      "resolved": "https://registry.npmjs.org/escape-string-regexp/-/escape-string-regexp-1.0.5.tgz",
      "integrity": "sha512-vbRorB5FUQWvla16U8R/qgaFIya2qGzwDrNmCZuYKrbdSUMG6I1ZCGQRefkRVhuOkIGVne7BQ35DSfo1qvJqFg==",
      "license": "MIT",
      "engines": {
        "node": ">=0.8.0"
      }
    },
    "node_modules/svgo/node_modules/has-flag": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/has-flag/-/has-flag-3.0.0.tgz",
      "integrity": "sha512-sKJf1+ceQBr4SMkvQnBDNDtf4TXpVhVGateu0t918bl30FnbE2m4vNLX+VWe/dpjlb+HugGYzW7uQXH98HPEYw==",
      "license": "MIT",
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/svgo/node_modules/nth-check": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/nth-check/-/nth-check-1.0.2.tgz",
      "integrity": "sha512-WeBOdju8SnzPN5vTUJYxYUxLeXpCaVP5i5e0LF8fg7WORF2Wd7wFX/pk0tYZk7s8T+J7VLy0Da6J1+wCT0AtHg==",
      "license": "BSD-2-Clause",
      "dependencies": {
        "boolbase": "~1.0.0"
      }
    },
    "node_modules/svgo/node_modules/supports-color": {
      "version": "5.5.0",
      "resolved": "https://registry.npmjs.org/supports-color/-/supports-color-5.5.0.tgz",
      "integrity": "sha512-QjVjwdXIt408MIiAqCX4oUKsgU2EqAGzs2Ppkm4aQYbjm+ZEWEcW4SfFNTr4uMNZma0ey4f5lgLrkB0aX0QMow==",
      "license": "MIT",
      "dependencies": {
        "has-flag": "^3.0.0"
      },
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/symbol-tree": {
      "version": "3.2.4",
      "resolved": "https://registry.npmjs.org/symbol-tree/-/symbol-tree-3.2.4.tgz",
      "integrity": "sha512-9QNk5KwDF+Bvz+PyObkmSYjI5ksVUYtjW7AU22r2NKcfLJcXp96hkDWU3+XndOsUb+AQ9QhfzfCT2O+CNWT5Tw==",
      "license": "MIT"
    },
    "node_modules/tailwindcss": {
      "version": "3.4.17",
      "resolved": "https://registry.npmjs.org/tailwindcss/-/tailwindcss-3.4.17.tgz",
      "integrity": "sha512-w33E2aCvSDP0tW9RZuNXadXlkHXqFzSkQew/aIa2i/Sj8fThxwovwlXHSPXTbAHwEIhBFXAedUhP2tueAKP8Og==",
      "license": "MIT",
      "dependencies": {
        "@alloc/quick-lru": "^5.2.0",
        "arg": "^5.0.2",
        "chokidar": "^3.6.0",
        "didyoumean": "^1.2.2",
        "dlv": "^1.1.3",
        "fast-glob": "^3.3.2",
        "glob-parent": "^6.0.2",
        "is-glob": "^4.0.3",
        "jiti": "^1.21.6",
        "lilconfig": "^3.1.3",
        "micromatch": "^4.0.8",
        "normalize-path": "^3.0.0",
        "object-hash": "^3.0.0",
        "picocolors": "^1.1.1",
        "postcss": "^8.4.47",
        "postcss-import": "^15.1.0",
        "postcss-js": "^4.0.1",
        "postcss-load-config": "^4.0.2",
        "postcss-nested": "^6.2.0",
        "postcss-selector-parser": "^6.1.2",
        "resolve": "^1.22.8",
        "sucrase": "^3.35.0"
      },
      "bin": {
        "tailwind": "lib/cli.js",
        "tailwindcss": "lib/cli.js"
      },
      "engines": {
        "node": ">=14.0.0"
      }
    },
    "node_modules/tailwindcss/node_modules/lilconfig": {
      "version": "3.1.3",
      "resolved": "https://registry.npmjs.org/lilconfig/-/lilconfig-3.1.3.tgz",
      "integrity": "sha512-/vlFKAoH5Cgt3Ie+JLhRbwOsCQePABiU3tJ1egGvyQ+33R/vcwM2Zl2QR/LzjsBeItPt3oSVXapn+m4nQDvpzw==",
      "license": "MIT",
      "engines": {
        "node": ">=14"
      },
      "funding": {
        "url": "https://github.com/sponsors/antonk52"
      }
    },
    "node_modules/tapable": {
      "version": "2.2.2",
      "resolved": "https://registry.npmjs.org/tapable/-/tapable-2.2.2.tgz",
      "integrity": "sha512-Re10+NauLTMCudc7T5WLFLAwDhQ0JWdrMK+9B2M8zR5hRExKmsRDCBA7/aV/pNJFltmBFO5BAMlQFi/vq3nKOg==",
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/tar": {
      "version": "7.4.3",
      "resolved": "https://registry.npmjs.org/tar/-/tar-7.4.3.tgz",
      "integrity": "sha512-5S7Va8hKfV7W5U6g3aYxXmlPoZVAwUMy9AOKyF2fVuZa2UD3qZjg578OrLRt8PcNN1PleVaL/5/yYATNL0ICUw==",
      "license": "ISC",
      "dependencies": {
        "@isaacs/fs-minipass": "^4.0.0",
        "chownr": "^3.0.0",
        "minipass": "^7.1.2",
        "minizlib": "^3.0.1",
        "mkdirp": "^3.0.1",
        "yallist": "^5.0.0"
      },
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/tar/node_modules/mkdirp": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/mkdirp/-/mkdirp-3.0.1.tgz",
      "integrity": "sha512-+NsyUUAZDmo6YVHzL/stxSu3t9YS1iljliy3BSDrXJ/dkn1KYdmtZODGGjLcc9XLgVVpH4KshHB8XmZgMhaBXg==",
      "license": "MIT",
      "bin": {
        "mkdirp": "dist/cjs/src/bin.js"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/isaacs"
      }
    },
    "node_modules/tar/node_modules/yallist": {
      "version": "5.0.0",
      "resolved": "https://registry.npmjs.org/yallist/-/yallist-5.0.0.tgz",
      "integrity": "sha512-YgvUTfwqyc7UXVMrB+SImsVYSmTS8X/tSrtdNZMImM+n7+QTriRXyXim0mBrTXNeqzVF0KWGgHPeiyViFFrNDw==",
      "license": "BlueOak-1.0.0",
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/temp-dir": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/temp-dir/-/temp-dir-2.0.0.tgz",
      "integrity": "sha512-aoBAniQmmwtcKp/7BzsH8Cxzv8OL736p7v1ihGb5e9DJ9kTwGWHrQrVB5+lfVDzfGrdRzXch+ig7LHaY1JTOrg==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/tempy": {
      "version": "0.6.0",
      "resolved": "https://registry.npmjs.org/tempy/-/tempy-0.6.0.tgz",
      "integrity": "sha512-G13vtMYPT/J8A4X2SjdtBTphZlrp1gKv6hZiOjw14RCWg6GbHuQBGtjlx75xLbYV/wEc0D7G5K4rxKP/cXk8Bw==",
      "license": "MIT",
      "dependencies": {
        "is-stream": "^2.0.0",
        "temp-dir": "^2.0.0",
        "type-fest": "^0.16.0",
        "unique-string": "^2.0.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/tempy/node_modules/type-fest": {
      "version": "0.16.0",
      "resolved": "https://registry.npmjs.org/type-fest/-/type-fest-0.16.0.tgz",
      "integrity": "sha512-eaBzG6MxNzEn9kiwvtre90cXaNLkmadMWa1zQMs3XORCXNbsH/OewwbxC5ia9dCxIxnTAsSxXJaa/p5y8DlvJg==",
      "license": "(MIT OR CC0-1.0)",
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/terminal-link": {
      "version": "2.1.1",
      "resolved": "https://registry.npmjs.org/terminal-link/-/terminal-link-2.1.1.tgz",
      "integrity": "sha512-un0FmiRUQNr5PJqy9kP7c40F5BOfpGlYTrxonDChEZB7pzZxRNp/bt+ymiy9/npwXya9KH99nJ/GXFIiUkYGFQ==",
      "license": "MIT",
      "dependencies": {
        "ansi-escapes": "^4.2.1",
        "supports-hyperlinks": "^2.0.0"
      },
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/terser": {
      "version": "5.42.0",
      "resolved": "https://registry.npmjs.org/terser/-/terser-5.42.0.tgz",
      "integrity": "sha512-UYCvU9YQW2f/Vwl+P0GfhxJxbUGLwd+5QrrGgLajzWAtC/23AX0vcise32kkP7Eu0Wu9VlzzHAXkLObgjQfFlQ==",
      "license": "BSD-2-Clause",
      "dependencies": {
        "@jridgewell/source-map": "^0.3.3",
        "acorn": "^8.14.0",
        "commander": "^2.20.0",
        "source-map-support": "~0.5.20"
      },
      "bin": {
        "terser": "bin/terser"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/terser-webpack-plugin": {
      "version": "5.3.14",
      "resolved": "https://registry.npmjs.org/terser-webpack-plugin/-/terser-webpack-plugin-5.3.14.tgz",
      "integrity": "sha512-vkZjpUjb6OMS7dhV+tILUW6BhpDR7P2L/aQSAv+Uwk+m8KATX9EccViHTJR2qDtACKPIYndLGCyl3FMo+r2LMw==",
      "license": "MIT",
      "dependencies": {
        "@jridgewell/trace-mapping": "^0.3.25",
        "jest-worker": "^27.4.5",
        "schema-utils": "^4.3.0",
        "serialize-javascript": "^6.0.2",
        "terser": "^5.31.1"
      },
      "engines": {
        "node": ">= 10.13.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/webpack"
      },
      "peerDependencies": {
        "webpack": "^5.1.0"
      },
      "peerDependenciesMeta": {
        "@swc/core": {
          "optional": true
        },
        "esbuild": {
          "optional": true
        },
        "uglify-js": {
          "optional": true
        }
      }
    },
    "node_modules/terser/node_modules/commander": {
      "version": "2.20.3",
      "resolved": "https://registry.npmjs.org/commander/-/commander-2.20.3.tgz",
      "integrity": "sha512-GpVkmM8vF2vQUkj2LvZmD35JxeJOLCwJ9cUkugyk2nuhbv3+mJvpLYYt+0+USMxE+oj+ey/lJEnhZw75x/OMcQ==",
      "license": "MIT"
    },
    "node_modules/test-exclude": {
      "version": "6.0.0",
      "resolved": "https://registry.npmjs.org/test-exclude/-/test-exclude-6.0.0.tgz",
      "integrity": "sha512-cAGWPIyOHU6zlmg88jwm7VRyXnMN7iV68OGAbYDk/Mh/xC/pzVPlQtY6ngoIH/5/tciuhGfvESU8GrHrcxD56w==",
      "license": "ISC",
      "dependencies": {
        "@istanbuljs/schema": "^0.1.2",
        "glob": "^7.1.4",
        "minimatch": "^3.0.4"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/text-table": {
      "version": "0.2.0",
      "resolved": "https://registry.npmjs.org/text-table/-/text-table-0.2.0.tgz",
      "integrity": "sha512-N+8UisAXDGk8PFXP4HAzVR9nbfmVJ3zYLAWiTIoqC5v5isinhr+r5uaO8+7r3BMfuNIufIsA7RdpVgacC2cSpw==",
      "license": "MIT"
    },
    "node_modules/thenify": {
      "version": "3.3.1",
      "resolved": "https://registry.npmjs.org/thenify/-/thenify-3.3.1.tgz",
      "integrity": "sha512-RVZSIV5IG10Hk3enotrhvz0T9em6cyHBLkH/YAZuKqd8hRkKhSfCGIcP2KUY0EPxndzANBmNllzWPwak+bheSw==",
      "license": "MIT",
      "dependencies": {
        "any-promise": "^1.0.0"
      }
    },
    "node_modules/thenify-all": {
      "version": "1.6.0",
      "resolved": "https://registry.npmjs.org/thenify-all/-/thenify-all-1.6.0.tgz",
      "integrity": "sha512-RNxQH/qI8/t3thXJDwcstUO4zeqo64+Uy/+sNVRBx4Xn2OX+OZ9oP+iJnNFqplFra2ZUVeKCSa2oVWi3T4uVmA==",
      "license": "MIT",
      "dependencies": {
        "thenify": ">= 3.1.0 < 4"
      },
      "engines": {
        "node": ">=0.8"
      }
    },
    "node_modules/throat": {
      "version": "6.0.2",
      "resolved": "https://registry.npmjs.org/throat/-/throat-6.0.2.tgz",
      "integrity": "sha512-WKexMoJj3vEuK0yFEapj8y64V0A6xcuPuK9Gt1d0R+dzCSJc0lHqQytAbSB4cDAK0dWh4T0E2ETkoLE2WZ41OQ==",
      "license": "MIT"
    },
    "node_modules/thunky": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/thunky/-/thunky-1.1.0.tgz",
      "integrity": "sha512-eHY7nBftgThBqOyHGVN+l8gF0BucP09fMo0oO/Lb0w1OF80dJv+lDVpXG60WMQvkcxAkNybKsrEIE3ZtKGmPrA==",
      "license": "MIT"
    },
    "node_modules/tmpl": {
      "version": "1.0.5",
      "resolved": "https://registry.npmjs.org/tmpl/-/tmpl-1.0.5.tgz",
      "integrity": "sha512-3f0uOEAQwIqGuWW2MVzYg8fV/QNnc/IpuJNG837rLuczAaLVHslWHZQj4IGiEl5Hs3kkbhwL9Ab7Hrsmuj+Smw==",
      "license": "BSD-3-Clause"
    },
    "node_modules/to-regex-range": {
      "version": "5.0.1",
      "resolved": "https://registry.npmjs.org/to-regex-range/-/to-regex-range-5.0.1.tgz",
      "integrity": "sha512-65P7iz6X5yEr1cwcgvQxbbIw7Uk3gOy5dIdtZ4rDveLqhrdJP+Li/Hx6tyK0NEb+2GCyneCMJiGqrADCSNk8sQ==",
      "license": "MIT",
      "dependencies": {
        "is-number": "^7.0.0"
      },
      "engines": {
        "node": ">=8.0"
      }
    },
    "node_modules/toidentifier": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/toidentifier/-/toidentifier-1.0.1.tgz",
      "integrity": "sha512-o5sSPKEkg/DIQNmH43V0/uerLrpzVedkUh8tGNvaeXpfpuwjKenlSox/2O/BTlZUtEe+JG7s5YhEz608PlAHRA==",
      "license": "MIT",
      "engines": {
        "node": ">=0.6"
      }
    },
    "node_modules/tough-cookie": {
      "version": "4.1.4",
      "resolved": "https://registry.npmjs.org/tough-cookie/-/tough-cookie-4.1.4.tgz",
      "integrity": "sha512-Loo5UUvLD9ScZ6jh8beX1T6sO1w2/MpCRpEP7V280GKMVUQ0Jzar2U3UJPsrdbziLEMMhu3Ujnq//rhiFuIeag==",
      "license": "BSD-3-Clause",
      "dependencies": {
        "psl": "^1.1.33",
        "punycode": "^2.1.1",
        "universalify": "^0.2.0",
        "url-parse": "^1.5.3"
      },
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/tough-cookie/node_modules/universalify": {
      "version": "0.2.0",
      "resolved": "https://registry.npmjs.org/universalify/-/universalify-0.2.0.tgz",
      "integrity": "sha512-CJ1QgKmNg3CwvAv/kOFmtnEN05f0D/cn9QntgNOQlQF9dgvVTHj3t+8JPdjqawCHk7V/KA+fbUqzZ9XWhcqPUg==",
      "license": "MIT",
      "engines": {
        "node": ">= 4.0.0"
      }
    },
    "node_modules/tr46": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/tr46/-/tr46-2.1.0.tgz",
      "integrity": "sha512-15Ih7phfcdP5YxqiB+iDtLoaTz4Nd35+IiAv0kQ5FNKHzXgdWqPoTIqEDDJmXceQt4JZk6lVPT8lnDlPpGDppw==",
      "license": "MIT",
      "dependencies": {
        "punycode": "^2.1.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/tryer": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/tryer/-/tryer-1.0.1.tgz",
      "integrity": "sha512-c3zayb8/kWWpycWYg87P71E1S1ZL6b6IJxfb5fvsUgsf0S2MVGaDhDXXjDMpdCpfWXqptc+4mXwmiy1ypXqRAA==",
      "license": "MIT"
    },
    "node_modules/ts-interface-checker": {
      "version": "0.1.13",
      "resolved": "https://registry.npmjs.org/ts-interface-checker/-/ts-interface-checker-0.1.13.tgz",
      "integrity": "sha512-Y/arvbn+rrz3JCKl9C4kVNfTfSm2/mEp5FSz5EsZSANGPSlQrpRI5M4PKF+mJnE52jOO90PnPSc3Ur3bTQw0gA==",
      "license": "Apache-2.0"
    },
    "node_modules/tsconfig-paths": {
      "version": "3.15.0",
      "resolved": "https://registry.npmjs.org/tsconfig-paths/-/tsconfig-paths-3.15.0.tgz",
      "integrity": "sha512-2Ac2RgzDe/cn48GvOe3M+o82pEFewD3UPbyoUHHdKasHwJKjds4fLXWf/Ux5kATBKN20oaFGu+jbElp1pos0mg==",
      "license": "MIT",
      "dependencies": {
        "@types/json5": "^0.0.29",
        "json5": "^1.0.2",
        "minimist": "^1.2.6",
        "strip-bom": "^3.0.0"
      }
    },
    "node_modules/tsconfig-paths/node_modules/json5": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/json5/-/json5-1.0.2.tgz",
      "integrity": "sha512-g1MWMLBiz8FKi1e4w0UyVL3w+iJceWAFBAaBnnGKOpNa5f8TLktkbre1+s6oICydWAm+HRUGTmI+//xv2hvXYA==",
      "license": "MIT",
      "dependencies": {
        "minimist": "^1.2.0"
      },
      "bin": {
        "json5": "lib/cli.js"
      }
    },
    "node_modules/tsconfig-paths/node_modules/strip-bom": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/strip-bom/-/strip-bom-3.0.0.tgz",
      "integrity": "sha512-vavAMRXOgBVNF6nyEEmL3DBK19iRpDcoIwW+swQ+CbGiu7lju6t+JklA1MHweoWtadgt4ISVUsXLyDq34ddcwA==",
      "license": "MIT",
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/tslib": {
      "version": "2.8.1",
      "resolved": "https://registry.npmjs.org/tslib/-/tslib-2.8.1.tgz",
      "integrity": "sha512-oJFu94HQb+KVduSUQL7wnpmqnfmLsOA/nAh6b6EH0wCEoK0/mPeXU6c3wKDV83MkOuHPRHtSXKKU99IBazS/2w==",
      "license": "0BSD"
    },
    "node_modules/tsutils": {
      "version": "3.21.0",
      "resolved": "https://registry.npmjs.org/tsutils/-/tsutils-3.21.0.tgz",
      "integrity": "sha512-mHKK3iUXL+3UF6xL5k0PEhKRUBKPBCv/+RkEOpjRWxxx27KKRBmmA60A9pgOUvMi8GKhRMPEmjBRPzs2W7O1OA==",
      "license": "MIT",
      "dependencies": {
        "tslib": "^1.8.1"
      },
      "engines": {
        "node": ">= 6"
      },
      "peerDependencies": {
        "typescript": ">=2.8.0 || >= 3.2.0-dev || >= 3.3.0-dev || >= 3.4.0-dev || >= 3.5.0-dev || >= 3.6.0-dev || >= 3.6.0-beta || >= 3.7.0-dev || >= 3.7.0-beta"
      }
    },
    "node_modules/tsutils/node_modules/tslib": {
      "version": "1.14.1",
      "resolved": "https://registry.npmjs.org/tslib/-/tslib-1.14.1.tgz",
      "integrity": "sha512-Xni35NKzjgMrwevysHTCArtLDpPvye8zV/0E4EyYn43P7/7qvQwPh9BGkHewbMulVntbigmcT7rdX3BNo9wRJg==",
      "license": "0BSD"
    },
    "node_modules/type-check": {
      "version": "0.4.0",
      "resolved": "https://registry.npmjs.org/type-check/-/type-check-0.4.0.tgz",
      "integrity": "sha512-XleUoc9uwGXqjWwXaUTZAmzMcFZ5858QA2vvx1Ur5xIcixXIP+8LnFDgRplU30us6teqdlskFfu+ae4K79Ooew==",
      "license": "MIT",
      "dependencies": {
        "prelude-ls": "^1.2.1"
      },
      "engines": {
        "node": ">= 0.8.0"
      }
    },
    "node_modules/type-detect": {
      "version": "4.0.8",
      "resolved": "https://registry.npmjs.org/type-detect/-/type-detect-4.0.8.tgz",
      "integrity": "sha512-0fr/mIH1dlO+x7TlcMy+bIDqKPsw/70tVyeHW787goQjhmqaZe10uwLujubK9q9Lg6Fiho1KUKDYz0Z7k7g5/g==",
      "license": "MIT",
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/type-fest": {
      "version": "0.21.3",
      "resolved": "https://registry.npmjs.org/type-fest/-/type-fest-0.21.3.tgz",
      "integrity": "sha512-t0rzBq87m3fVcduHDUFhKmyyX+9eo6WQjZvf51Ea/M0Q7+T374Jp1aUiyUl0GKxp8M/OETVHSDvmkyPgvX+X2w==",
      "license": "(MIT OR CC0-1.0)",
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/type-is": {
      "version": "1.6.18",
      "resolved": "https://registry.npmjs.org/type-is/-/type-is-1.6.18.tgz",
      "integrity": "sha512-TkRKr9sUTxEH8MdfuCSP7VizJyzRNMjj2J2do2Jr3Kym598JVdEksuzPQCnlFPW4ky9Q+iA+ma9BGm06XQBy8g==",
      "license": "MIT",
      "dependencies": {
        "media-typer": "0.3.0",
        "mime-types": "~2.1.24"
      },
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/typed-array-buffer": {
      "version": "1.0.3",
      "resolved": "https://registry.npmjs.org/typed-array-buffer/-/typed-array-buffer-1.0.3.tgz",
      "integrity": "sha512-nAYYwfY3qnzX30IkA6AQZjVbtK6duGontcQm1WSG1MD94YLqK0515GNApXkoxKOWMusVssAHWLh9SeaoefYFGw==",
      "license": "MIT",
      "dependencies": {
        "call-bound": "^1.0.3",
        "es-errors": "^1.3.0",
        "is-typed-array": "^1.1.14"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/typed-array-byte-length": {
      "version": "1.0.3",
      "resolved": "https://registry.npmjs.org/typed-array-byte-length/-/typed-array-byte-length-1.0.3.tgz",
      "integrity": "sha512-BaXgOuIxz8n8pIq3e7Atg/7s+DpiYrxn4vdot3w9KbnBhcRQq6o3xemQdIfynqSeXeDrF32x+WvfzmOjPiY9lg==",
      "license": "MIT",
      "dependencies": {
        "call-bind": "^1.0.8",
        "for-each": "^0.3.3",
        "gopd": "^1.2.0",
        "has-proto": "^1.2.0",
        "is-typed-array": "^1.1.14"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/typed-array-byte-offset": {
      "version": "1.0.4",
      "resolved": "https://registry.npmjs.org/typed-array-byte-offset/-/typed-array-byte-offset-1.0.4.tgz",
      "integrity": "sha512-bTlAFB/FBYMcuX81gbL4OcpH5PmlFHqlCCpAl8AlEzMz5k53oNDvN8p1PNOWLEmI2x4orp3raOFB51tv9X+MFQ==",
      "license": "MIT",
      "dependencies": {
        "available-typed-arrays": "^1.0.7",
        "call-bind": "^1.0.8",
        "for-each": "^0.3.3",
        "gopd": "^1.2.0",
        "has-proto": "^1.2.0",
        "is-typed-array": "^1.1.15",
        "reflect.getprototypeof": "^1.0.9"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/typed-array-length": {
      "version": "1.0.7",
      "resolved": "https://registry.npmjs.org/typed-array-length/-/typed-array-length-1.0.7.tgz",
      "integrity": "sha512-3KS2b+kL7fsuk/eJZ7EQdnEmQoaho/r6KUef7hxvltNA5DR8NAUM+8wJMbJyZ4G9/7i3v5zPBIMN5aybAh2/Jg==",
      "license": "MIT",
      "dependencies": {
        "call-bind": "^1.0.7",
        "for-each": "^0.3.3",
        "gopd": "^1.0.1",
        "is-typed-array": "^1.1.13",
        "possible-typed-array-names": "^1.0.0",
        "reflect.getprototypeof": "^1.0.6"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/typedarray-to-buffer": {
      "version": "3.1.5",
      "resolved": "https://registry.npmjs.org/typedarray-to-buffer/-/typedarray-to-buffer-3.1.5.tgz",
      "integrity": "sha512-zdu8XMNEDepKKR+XYOXAVPtWui0ly0NtohUscw+UmaHiAWT8hrV1rr//H6V+0DvJ3OQ19S979M0laLfX8rm82Q==",
      "license": "MIT",
      "dependencies": {
        "is-typedarray": "^1.0.0"
      }
    },
    "node_modules/typescript": {
      "version": "4.9.5",
      "resolved": "https://registry.npmjs.org/typescript/-/typescript-4.9.5.tgz",
      "integrity": "sha512-1FXk9E2Hm+QzZQ7z+McJiHL4NW1F2EzMu9Nq9i3zAaGqibafqYwCVU6WyWAuyQRRzOlxou8xZSyXLEN8oKj24g==",
      "license": "Apache-2.0",
      "bin": {
        "tsc": "bin/tsc",
        "tsserver": "bin/tsserver"
      },
      "engines": {
        "node": ">=4.2.0"
      }
    },
    "node_modules/unbox-primitive": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/unbox-primitive/-/unbox-primitive-1.1.0.tgz",
      "integrity": "sha512-nWJ91DjeOkej/TA8pXQ3myruKpKEYgqvpw9lz4OPHj/NWFNluYrjbz9j01CJ8yKQd2g4jFoOkINCTW2I5LEEyw==",
      "license": "MIT",
      "dependencies": {
        "call-bound": "^1.0.3",
        "has-bigints": "^1.0.2",
        "has-symbols": "^1.1.0",
        "which-boxed-primitive": "^1.1.1"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/underscore": {
      "version": "1.12.1",
      "resolved": "https://registry.npmjs.org/underscore/-/underscore-1.12.1.tgz",
      "integrity": "sha512-hEQt0+ZLDVUMhebKxL4x1BTtDY7bavVofhZ9KZ4aI26X9SRaE+Y3m83XUL1UP2jn8ynjndwCCpEHdUG+9pP1Tw==",
      "license": "MIT"
    },
    "node_modules/unicode-canonical-property-names-ecmascript": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/unicode-canonical-property-names-ecmascript/-/unicode-canonical-property-names-ecmascript-2.0.1.tgz",
      "integrity": "sha512-dA8WbNeb2a6oQzAQ55YlT5vQAWGV9WXOsi3SskE3bcCdM0P4SDd+24zS/OCacdRq5BkdsRj9q3Pg6YyQoxIGqg==",
      "license": "MIT",
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/unicode-match-property-ecmascript": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/unicode-match-property-ecmascript/-/unicode-match-property-ecmascript-2.0.0.tgz",
      "integrity": "sha512-5kaZCrbp5mmbz5ulBkDkbY0SsPOjKqVS35VpL9ulMPfSl0J0Xsm+9Evphv9CoIZFwre7aJoa94AY6seMKGVN5Q==",
      "license": "MIT",
      "dependencies": {
        "unicode-canonical-property-names-ecmascript": "^2.0.0",
        "unicode-property-aliases-ecmascript": "^2.0.0"
      },
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/unicode-match-property-value-ecmascript": {
      "version": "2.2.0",
      "resolved": "https://registry.npmjs.org/unicode-match-property-value-ecmascript/-/unicode-match-property-value-ecmascript-2.2.0.tgz",
      "integrity": "sha512-4IehN3V/+kkr5YeSSDDQG8QLqO26XpL2XP3GQtqwlT/QYSECAwFztxVHjlbh0+gjJ3XmNLS0zDsbgs9jWKExLg==",
      "license": "MIT",
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/unicode-property-aliases-ecmascript": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/unicode-property-aliases-ecmascript/-/unicode-property-aliases-ecmascript-2.1.0.tgz",
      "integrity": "sha512-6t3foTQI9qne+OZoVQB/8x8rk2k1eVy1gRXhV3oFQ5T6R1dqQ1xtin3XqSlx3+ATBkliTaR/hHyJBm+LVPNM8w==",
      "license": "MIT",
      "engines": {
        "node": ">=4"
      }
    },
    "node_modules/unique-string": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/unique-string/-/unique-string-2.0.0.tgz",
      "integrity": "sha512-uNaeirEPvpZWSgzwsPGtU2zVSTrn/8L5q/IexZmH0eH6SA73CmAA5U4GwORTxQAZs95TAXLNqeLoPPNO5gZfWg==",
      "license": "MIT",
      "dependencies": {
        "crypto-random-string": "^2.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/universalify": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/universalify/-/universalify-2.0.1.tgz",
      "integrity": "sha512-gptHNQghINnc/vTGIk0SOFGFNXw7JVrlRUtConJRlvaw6DuX0wO5Jeko9sWrMBhh+PsYAZ7oXAiOnf/UKogyiw==",
      "license": "MIT",
      "engines": {
        "node": ">= 10.0.0"
      }
    },
    "node_modules/unpipe": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/unpipe/-/unpipe-1.0.0.tgz",
      "integrity": "sha512-pjy2bYhSsufwWlKwPc+l3cN7+wuJlK6uz0YdJEOlQDbl6jo/YlPi4mb8agUkVC8BF7V8NuzeyPNqRksA3hztKQ==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/unquote": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/unquote/-/unquote-1.1.1.tgz",
      "integrity": "sha512-vRCqFv6UhXpWxZPyGDh/F3ZpNv8/qo7w6iufLpQg9aKnQ71qM4B5KiI7Mia9COcjEhrO9LueHpMYjYzsWH3OIg==",
      "license": "MIT"
    },
    "node_modules/upath": {
      "version": "1.2.0",
      "resolved": "https://registry.npmjs.org/upath/-/upath-1.2.0.tgz",
      "integrity": "sha512-aZwGpamFO61g3OlfT7OQCHqhGnW43ieH9WZeP7QxN/G/jS4jfqUkZxoryvJgVPEcrl5NL/ggHsSmLMHuH64Lhg==",
      "license": "MIT",
      "engines": {
        "node": ">=4",
        "yarn": "*"
      }
    },
    "node_modules/update-browserslist-db": {
      "version": "1.1.3",
      "resolved": "https://registry.npmjs.org/update-browserslist-db/-/update-browserslist-db-1.1.3.tgz",
      "integrity": "sha512-UxhIZQ+QInVdunkDAaiazvvT/+fXL5Osr0JZlJulepYu6Jd7qJtDZjlur0emRlT71EN3ScPoE7gvsuIKKNavKw==",
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/browserslist"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/browserslist"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "escalade": "^3.2.0",
        "picocolors": "^1.1.1"
      },
      "bin": {
        "update-browserslist-db": "cli.js"
      },
      "peerDependencies": {
        "browserslist": ">= 4.21.0"
      }
    },
    "node_modules/uri-js": {
      "version": "4.4.1",
      "resolved": "https://registry.npmjs.org/uri-js/-/uri-js-4.4.1.tgz",
      "integrity": "sha512-7rKUyy33Q1yc98pQ1DAmLtwX109F7TIfWlW1Ydo8Wl1ii1SeHieeh0HHfPeL2fMXK6z0s8ecKs9frCuLJvndBg==",
      "license": "BSD-2-Clause",
      "dependencies": {
        "punycode": "^2.1.0"
      }
    },
    "node_modules/url-parse": {
      "version": "1.5.10",
      "resolved": "https://registry.npmjs.org/url-parse/-/url-parse-1.5.10.tgz",
      "integrity": "sha512-WypcfiRhfeUP9vvF0j6rw0J3hrWrw6iZv3+22h6iRMJ/8z1Tj6XfLP4DsUix5MhMPnXpiHDoKyoZ/bdCkwBCiQ==",
      "license": "MIT",
      "dependencies": {
        "querystringify": "^2.1.1",
        "requires-port": "^1.0.0"
      }
    },
    "node_modules/util-deprecate": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/util-deprecate/-/util-deprecate-1.0.2.tgz",
      "integrity": "sha512-EPD5q1uXyFxJpCrLnCc1nHnq3gOa6DZBocAIiI2TaSCA7VCJ1UJDMagCzIkXNsUYfD1daK//LTEQ8xiIbrHtcw==",
      "license": "MIT"
    },
    "node_modules/util.promisify": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/util.promisify/-/util.promisify-1.0.1.tgz",
      "integrity": "sha512-g9JpC/3He3bm38zsLupWryXHoEcS22YHthuPQSJdMy6KNrzIRzWqcsHzD/WUnqe45whVou4VIsPew37DoXWNrA==",
      "license": "MIT",
      "dependencies": {
        "define-properties": "^1.1.3",
        "es-abstract": "^1.17.2",
        "has-symbols": "^1.0.1",
        "object.getownpropertydescriptors": "^2.1.0"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/utila": {
      "version": "0.4.0",
      "resolved": "https://registry.npmjs.org/utila/-/utila-0.4.0.tgz",
      "integrity": "sha512-Z0DbgELS9/L/75wZbro8xAnT50pBVFQZ+hUEueGDU5FN51YSCYM+jdxsfCiHjwNP/4LCDD0i/graKpeBnOXKRA==",
      "license": "MIT"
    },
    "node_modules/utils-merge": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/utils-merge/-/utils-merge-1.0.1.tgz",
      "integrity": "sha512-pMZTvIkT1d+TFGvDOqodOclx0QWkkgi6Tdoa8gC8ffGAAqz9pzPTZWAybbsHHoED/ztMtkv/VoYTYyShUn81hA==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4.0"
      }
    },
    "node_modules/uuid": {
      "version": "8.3.2",
      "resolved": "https://registry.npmjs.org/uuid/-/uuid-8.3.2.tgz",
      "integrity": "sha512-+NYs2QeMWy+GWFOEm9xnn6HCDp0l7QBD7ml8zLUmJ+93Q5NF0NocErnwkTkXVFNiX3/fpC6afS8Dhb/gz7R7eg==",
      "license": "MIT",
      "bin": {
        "uuid": "dist/bin/uuid"
      }
    },
    "node_modules/v8-to-istanbul": {
      "version": "8.1.1",
      "resolved": "https://registry.npmjs.org/v8-to-istanbul/-/v8-to-istanbul-8.1.1.tgz",
      "integrity": "sha512-FGtKtv3xIpR6BYhvgH8MI/y78oT7d8Au3ww4QIxymrCtZEh5b8gCw2siywE+puhEmuWKDtmfrvF5UlB298ut3w==",
      "license": "ISC",
      "dependencies": {
        "@types/istanbul-lib-coverage": "^2.0.1",
        "convert-source-map": "^1.6.0",
        "source-map": "^0.7.3"
      },
      "engines": {
        "node": ">=10.12.0"
      }
    },
    "node_modules/v8-to-istanbul/node_modules/convert-source-map": {
      "version": "1.9.0",
      "resolved": "https://registry.npmjs.org/convert-source-map/-/convert-source-map-1.9.0.tgz",
      "integrity": "sha512-ASFBup0Mz1uyiIjANan1jzLQami9z1PoYSZCiiYW2FczPbenXc45FZdBZLzOT+r6+iciuEModtmCti+hjaAk0A==",
      "license": "MIT"
    },
    "node_modules/vary": {
      "version": "1.1.2",
      "resolved": "https://registry.npmjs.org/vary/-/vary-1.1.2.tgz",
      "integrity": "sha512-BNGbWLfd0eUPabhkXUVm0j8uuvREyTh5ovRa/dyow/BqAbZJyC+5fU+IzQOzmAKzYqYRAISoRhdQr3eIZ/PXqg==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/w3c-hr-time": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/w3c-hr-time/-/w3c-hr-time-1.0.2.tgz",
      "integrity": "sha512-z8P5DvDNjKDoFIHK7q8r8lackT6l+jo/Ye3HOle7l9nICP9lf1Ci25fy9vHd0JOWewkIFzXIEig3TdKT7JQ5fQ==",
      "deprecated": "Use your platform's native performance.now() and performance.timeOrigin.",
      "license": "MIT",
      "dependencies": {
        "browser-process-hrtime": "^1.0.0"
      }
    },
    "node_modules/w3c-xmlserializer": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/w3c-xmlserializer/-/w3c-xmlserializer-2.0.0.tgz",
      "integrity": "sha512-4tzD0mF8iSiMiNs30BiLO3EpfGLZUT2MSX/G+o7ZywDzliWQ3OPtTZ0PTC3B3ca1UAf4cJMHB+2Bf56EriJuRA==",
      "license": "MIT",
      "dependencies": {
        "xml-name-validator": "^3.0.0"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/walker": {
      "version": "1.0.8",
      "resolved": "https://registry.npmjs.org/walker/-/walker-1.0.8.tgz",
      "integrity": "sha512-ts/8E8l5b7kY0vlWLewOkDXMmPdLcVV4GmOQLyxuSswIJsweeFZtAsMF7k1Nszz+TYBQrlYRmzOnr398y1JemQ==",
      "license": "Apache-2.0",
      "dependencies": {
        "makeerror": "1.0.12"
      }
    },
    "node_modules/watchpack": {
      "version": "2.4.4",
      "resolved": "https://registry.npmjs.org/watchpack/-/watchpack-2.4.4.tgz",
      "integrity": "sha512-c5EGNOiyxxV5qmTtAB7rbiXxi1ooX1pQKMLX/MIabJjRA0SJBQOjKF+KSVfHkr9U1cADPon0mRiVe/riyaiDUA==",
      "license": "MIT",
      "dependencies": {
        "glob-to-regexp": "^0.4.1",
        "graceful-fs": "^4.1.2"
      },
      "engines": {
        "node": ">=10.13.0"
      }
    },
    "node_modules/wbuf": {
      "version": "1.7.3",
      "resolved": "https://registry.npmjs.org/wbuf/-/wbuf-1.7.3.tgz",
      "integrity": "sha512-O84QOnr0icsbFGLS0O3bI5FswxzRr8/gHwWkDlQFskhSPryQXvrTMxjxGP4+iWYoauLoBvfDpkrOauZ+0iZpDA==",
      "license": "MIT",
      "dependencies": {
        "minimalistic-assert": "^1.0.0"
      }
    },
    "node_modules/web-vitals": {
      "version": "2.1.4",
      "resolved": "https://registry.npmjs.org/web-vitals/-/web-vitals-2.1.4.tgz",
      "integrity": "sha512-sVWcwhU5mX6crfI5Vd2dC4qchyTqxV8URinzt25XqVh+bHEPGH4C3NPrNionCP7Obx59wrYEbNlw4Z8sjALzZg==",
      "license": "Apache-2.0"
    },
    "node_modules/webidl-conversions": {
      "version": "6.1.0",
      "resolved": "https://registry.npmjs.org/webidl-conversions/-/webidl-conversions-6.1.0.tgz",
      "integrity": "sha512-qBIvFLGiBpLjfwmYAaHPXsn+ho5xZnGvyGvsarywGNc8VyQJUMHJ8OBKGGrPER0okBeMDaan4mNBlgBROxuI8w==",
      "license": "BSD-2-Clause",
      "engines": {
        "node": ">=10.4"
      }
    },
    "node_modules/webpack": {
      "version": "5.99.9",
      "resolved": "https://registry.npmjs.org/webpack/-/webpack-5.99.9.tgz",
      "integrity": "sha512-brOPwM3JnmOa+7kd3NsmOUOwbDAj8FT9xDsG3IW0MgbN9yZV7Oi/s/+MNQ/EcSMqw7qfoRyXPoeEWT8zLVdVGg==",
      "license": "MIT",
      "dependencies": {
        "@types/eslint-scope": "^3.7.7",
        "@types/estree": "^1.0.6",
        "@types/json-schema": "^7.0.15",
        "@webassemblyjs/ast": "^1.14.1",
        "@webassemblyjs/wasm-edit": "^1.14.1",
        "@webassemblyjs/wasm-parser": "^1.14.1",
        "acorn": "^8.14.0",
        "browserslist": "^4.24.0",
        "chrome-trace-event": "^1.0.2",
        "enhanced-resolve": "^5.17.1",
        "es-module-lexer": "^1.2.1",
        "eslint-scope": "5.1.1",
        "events": "^3.2.0",
        "glob-to-regexp": "^0.4.1",
        "graceful-fs": "^4.2.11",
        "json-parse-even-better-errors": "^2.3.1",
        "loader-runner": "^4.2.0",
        "mime-types": "^2.1.27",
        "neo-async": "^2.6.2",
        "schema-utils": "^4.3.2",
        "tapable": "^2.1.1",
        "terser-webpack-plugin": "^5.3.11",
        "watchpack": "^2.4.1",
        "webpack-sources": "^3.2.3"
      },
      "bin": {
        "webpack": "bin/webpack.js"
      },
      "engines": {
        "node": ">=10.13.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/webpack"
      },
      "peerDependenciesMeta": {
        "webpack-cli": {
          "optional": true
        }
      }
    },
    "node_modules/webpack-dev-middleware": {
      "version": "5.3.4",
      "resolved": "https://registry.npmjs.org/webpack-dev-middleware/-/webpack-dev-middleware-5.3.4.tgz",
      "integrity": "sha512-BVdTqhhs+0IfoeAf7EoH5WE+exCmqGerHfDM0IL096Px60Tq2Mn9MAbnaGUe6HiMa41KMCYF19gyzZmBcq/o4Q==",
      "license": "MIT",
      "dependencies": {
        "colorette": "^2.0.10",
        "memfs": "^3.4.3",
        "mime-types": "^2.1.31",
        "range-parser": "^1.2.1",
        "schema-utils": "^4.0.0"
      },
      "engines": {
        "node": ">= 12.13.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/webpack"
      },
      "peerDependencies": {
        "webpack": "^4.0.0 || ^5.0.0"
      }
    },
    "node_modules/webpack-dev-server": {
      "version": "4.15.2",
      "resolved": "https://registry.npmjs.org/webpack-dev-server/-/webpack-dev-server-4.15.2.tgz",
      "integrity": "sha512-0XavAZbNJ5sDrCbkpWL8mia0o5WPOd2YGtxrEiZkBK9FjLppIUK2TgxK6qGD2P3hUXTJNNPVibrerKcx5WkR1g==",
      "license": "MIT",
      "dependencies": {
        "@types/bonjour": "^3.5.9",
        "@types/connect-history-api-fallback": "^1.3.5",
        "@types/express": "^4.17.13",
        "@types/serve-index": "^1.9.1",
        "@types/serve-static": "^1.13.10",
        "@types/sockjs": "^0.3.33",
        "@types/ws": "^8.5.5",
        "ansi-html-community": "^0.0.8",
        "bonjour-service": "^1.0.11",
        "chokidar": "^3.5.3",
        "colorette": "^2.0.10",
        "compression": "^1.7.4",
        "connect-history-api-fallback": "^2.0.0",
        "default-gateway": "^6.0.3",
        "express": "^4.17.3",
        "graceful-fs": "^4.2.6",
        "html-entities": "^2.3.2",
        "http-proxy-middleware": "^2.0.3",
        "ipaddr.js": "^2.0.1",
        "launch-editor": "^2.6.0",
        "open": "^8.0.9",
        "p-retry": "^4.5.0",
        "rimraf": "^3.0.2",
        "schema-utils": "^4.0.0",
        "selfsigned": "^2.1.1",
        "serve-index": "^1.9.1",
        "sockjs": "^0.3.24",
        "spdy": "^4.0.2",
        "webpack-dev-middleware": "^5.3.4",
        "ws": "^8.13.0"
      },
      "bin": {
        "webpack-dev-server": "bin/webpack-dev-server.js"
      },
      "engines": {
        "node": ">= 12.13.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/webpack"
      },
      "peerDependencies": {
        "webpack": "^4.37.0 || ^5.0.0"
      },
      "peerDependenciesMeta": {
        "webpack": {
          "optional": true
        },
        "webpack-cli": {
          "optional": true
        }
      }
    },
    "node_modules/webpack-dev-server/node_modules/ws": {
      "version": "8.18.2",
      "resolved": "https://registry.npmjs.org/ws/-/ws-8.18.2.tgz",
      "integrity": "sha512-DMricUmwGZUVr++AEAe2uiVM7UoO9MAVZMDu05UQOaUII0lp+zOzLLU4Xqh/JvTqklB1T4uELaaPBKyjE1r4fQ==",
      "license": "MIT",
      "engines": {
        "node": ">=10.0.0"
      },
      "peerDependencies": {
        "bufferutil": "^4.0.1",
        "utf-8-validate": ">=5.0.2"
      },
      "peerDependenciesMeta": {
        "bufferutil": {
          "optional": true
        },
        "utf-8-validate": {
          "optional": true
        }
      }
    },
    "node_modules/webpack-manifest-plugin": {
      "version": "4.1.1",
      "resolved": "https://registry.npmjs.org/webpack-manifest-plugin/-/webpack-manifest-plugin-4.1.1.tgz",
      "integrity": "sha512-YXUAwxtfKIJIKkhg03MKuiFAD72PlrqCiwdwO4VEXdRO5V0ORCNwaOwAZawPZalCbmH9kBDmXnNeQOw+BIEiow==",
      "license": "MIT",
      "dependencies": {
        "tapable": "^2.0.0",
        "webpack-sources": "^2.2.0"
      },
      "engines": {
        "node": ">=12.22.0"
      },
      "peerDependencies": {
        "webpack": "^4.44.2 || ^5.47.0"
      }
    },
    "node_modules/webpack-manifest-plugin/node_modules/source-map": {
      "version": "0.6.1",
      "resolved": "https://registry.npmjs.org/source-map/-/source-map-0.6.1.tgz",
      "integrity": "sha512-UjgapumWlbMhkBgzT7Ykc5YXUT46F0iKu8SGXq0bcwP5dz/h0Plj6enJqjz1Zbq2l5WaqYnrVbwWOWMyF3F47g==",
      "license": "BSD-3-Clause",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/webpack-manifest-plugin/node_modules/webpack-sources": {
      "version": "2.3.1",
      "resolved": "https://registry.npmjs.org/webpack-sources/-/webpack-sources-2.3.1.tgz",
      "integrity": "sha512-y9EI9AO42JjEcrTJFOYmVywVZdKVUfOvDUPsJea5GIr1JOEGFVqwlY2K098fFoIjOkDzHn2AjRvM8dsBZu+gCA==",
      "license": "MIT",
      "dependencies": {
        "source-list-map": "^2.0.1",
        "source-map": "^0.6.1"
      },
      "engines": {
        "node": ">=10.13.0"
      }
    },
    "node_modules/webpack-sources": {
      "version": "3.3.2",
      "resolved": "https://registry.npmjs.org/webpack-sources/-/webpack-sources-3.3.2.tgz",
      "integrity": "sha512-ykKKus8lqlgXX/1WjudpIEjqsafjOTcOJqxnAbMLAu/KCsDCJ6GBtvscewvTkrn24HsnvFwrSCbenFrhtcCsAA==",
      "license": "MIT",
      "engines": {
        "node": ">=10.13.0"
      }
    },
    "node_modules/webpack/node_modules/eslint-scope": {
      "version": "5.1.1",
      "resolved": "https://registry.npmjs.org/eslint-scope/-/eslint-scope-5.1.1.tgz",
      "integrity": "sha512-2NxwbF/hZ0KpepYN0cNbo+FN6XoK7GaHlQhgx/hIZl6Va0bF45RQOOwhLIy8lQDbuCiadSLCBnH2CFYquit5bw==",
      "license": "BSD-2-Clause",
      "dependencies": {
        "esrecurse": "^4.3.0",
        "estraverse": "^4.1.1"
      },
      "engines": {
        "node": ">=8.0.0"
      }
    },
    "node_modules/webpack/node_modules/estraverse": {
      "version": "4.3.0",
      "resolved": "https://registry.npmjs.org/estraverse/-/estraverse-4.3.0.tgz",
      "integrity": "sha512-39nnKffWz8xN1BU/2c79n9nB9HDzo0niYUqx6xyqUnyoAnQyyWpOTdZEeiCch8BBu515t4wp9ZmgVfVhn9EBpw==",
      "license": "BSD-2-Clause",
      "engines": {
        "node": ">=4.0"
      }
    },
    "node_modules/websocket-driver": {
      "version": "0.7.4",
      "resolved": "https://registry.npmjs.org/websocket-driver/-/websocket-driver-0.7.4.tgz",
      "integrity": "sha512-b17KeDIQVjvb0ssuSDF2cYXSg2iztliJ4B9WdsuB6J952qCPKmnVq4DyW5motImXHDC1cBT/1UezrJVsKw5zjg==",
      "license": "Apache-2.0",
      "dependencies": {
        "http-parser-js": ">=0.5.1",
        "safe-buffer": ">=5.1.0",
        "websocket-extensions": ">=0.1.1"
      },
      "engines": {
        "node": ">=0.8.0"
      }
    },
    "node_modules/websocket-extensions": {
      "version": "0.1.4",
      "resolved": "https://registry.npmjs.org/websocket-extensions/-/websocket-extensions-0.1.4.tgz",
      "integrity": "sha512-OqedPIGOfsDlo31UNwYbCFMSaO9m9G/0faIHj5/dZFDMFqPTcx6UwqyOy3COEaEOg/9VsGIpdqn62W5KhoKSpg==",
      "license": "Apache-2.0",
      "engines": {
        "node": ">=0.8.0"
      }
    },
    "node_modules/whatwg-encoding": {
      "version": "1.0.5",
      "resolved": "https://registry.npmjs.org/whatwg-encoding/-/whatwg-encoding-1.0.5.tgz",
      "integrity": "sha512-b5lim54JOPN9HtzvK9HFXvBma/rnfFeqsic0hSpjtDbVxR3dJKLc+KB4V6GgiGOvl7CY/KNh8rxSo9DKQrnUEw==",
      "license": "MIT",
      "dependencies": {
        "iconv-lite": "0.4.24"
      }
    },
    "node_modules/whatwg-encoding/node_modules/iconv-lite": {
      "version": "0.4.24",
      "resolved": "https://registry.npmjs.org/iconv-lite/-/iconv-lite-0.4.24.tgz",
      "integrity": "sha512-v3MXnZAcvnywkTUEZomIActle7RXXeedOR31wwl7VlyoXO4Qi9arvSenNQWne1TcRwhCL1HwLI21bEqdpj8/rA==",
      "license": "MIT",
      "dependencies": {
        "safer-buffer": ">= 2.1.2 < 3"
      },
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/whatwg-fetch": {
      "version": "3.6.20",
      "resolved": "https://registry.npmjs.org/whatwg-fetch/-/whatwg-fetch-3.6.20.tgz",
      "integrity": "sha512-EqhiFU6daOA8kpjOWTL0olhVOF3i7OrFzSYiGsEMB8GcXS+RrzauAERX65xMeNWVqxA6HXH2m69Z9LaKKdisfg==",
      "license": "MIT"
    },
    "node_modules/whatwg-mimetype": {
      "version": "2.3.0",
      "resolved": "https://registry.npmjs.org/whatwg-mimetype/-/whatwg-mimetype-2.3.0.tgz",
      "integrity": "sha512-M4yMwr6mAnQz76TbJm914+gPpB/nCwvZbJU28cUD6dR004SAxDLOOSUaB1JDRqLtaOV/vi0IC5lEAGFgrjGv/g==",
      "license": "MIT"
    },
    "node_modules/whatwg-url": {
      "version": "8.7.0",
      "resolved": "https://registry.npmjs.org/whatwg-url/-/whatwg-url-8.7.0.tgz",
      "integrity": "sha512-gAojqb/m9Q8a5IV96E3fHJM70AzCkgt4uXYX2O7EmuyOnLrViCQlsEBmF9UQIu3/aeAIp2U17rtbpZWNntQqdg==",
      "license": "MIT",
      "dependencies": {
        "lodash": "^4.7.0",
        "tr46": "^2.1.0",
        "webidl-conversions": "^6.1.0"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/which": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/which/-/which-2.0.2.tgz",
      "integrity": "sha512-BLI3Tl1TW3Pvl70l3yq3Y64i+awpwXqsGBYWkkqMtnbXgrMD+yj7rhW0kuEDxzJaYXGjEW5ogapKNMEKNMjibA==",
      "license": "ISC",
      "dependencies": {
        "isexe": "^2.0.0"
      },
      "bin": {
        "node-which": "bin/node-which"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/which-boxed-primitive": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/which-boxed-primitive/-/which-boxed-primitive-1.1.1.tgz",
      "integrity": "sha512-TbX3mj8n0odCBFVlY8AxkqcHASw3L60jIuF8jFP78az3C2YhmGvqbHBpAjTRH2/xqYunrJ9g1jSyjCjpoWzIAA==",
      "license": "MIT",
      "dependencies": {
        "is-bigint": "^1.1.0",
        "is-boolean-object": "^1.2.1",
        "is-number-object": "^1.1.1",
        "is-string": "^1.1.1",
        "is-symbol": "^1.1.1"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/which-builtin-type": {
      "version": "1.2.1",
      "resolved": "https://registry.npmjs.org/which-builtin-type/-/which-builtin-type-1.2.1.tgz",
      "integrity": "sha512-6iBczoX+kDQ7a3+YJBnh3T+KZRxM/iYNPXicqk66/Qfm1b93iu+yOImkg0zHbj5LNOcNv1TEADiZ0xa34B4q6Q==",
      "license": "MIT",
      "dependencies": {
        "call-bound": "^1.0.2",
        "function.prototype.name": "^1.1.6",
        "has-tostringtag": "^1.0.2",
        "is-async-function": "^2.0.0",
        "is-date-object": "^1.1.0",
        "is-finalizationregistry": "^1.1.0",
        "is-generator-function": "^1.0.10",
        "is-regex": "^1.2.1",
        "is-weakref": "^1.0.2",
        "isarray": "^2.0.5",
        "which-boxed-primitive": "^1.1.0",
        "which-collection": "^1.0.2",
        "which-typed-array": "^1.1.16"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/which-collection": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/which-collection/-/which-collection-1.0.2.tgz",
      "integrity": "sha512-K4jVyjnBdgvc86Y6BkaLZEN933SwYOuBFkdmBu9ZfkcAbdVbpITnDmjvZ/aQjRXQrv5EPkTnD1s39GiiqbngCw==",
      "license": "MIT",
      "dependencies": {
        "is-map": "^2.0.3",
        "is-set": "^2.0.3",
        "is-weakmap": "^2.0.2",
        "is-weakset": "^2.0.3"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/which-typed-array": {
      "version": "1.1.19",
      "resolved": "https://registry.npmjs.org/which-typed-array/-/which-typed-array-1.1.19.tgz",
      "integrity": "sha512-rEvr90Bck4WZt9HHFC4DJMsjvu7x+r6bImz0/BrbWb7A2djJ8hnZMrWnHo9F8ssv0OMErasDhftrfROTyqSDrw==",
      "license": "MIT",
      "dependencies": {
        "available-typed-arrays": "^1.0.7",
        "call-bind": "^1.0.8",
        "call-bound": "^1.0.4",
        "for-each": "^0.3.5",
        "get-proto": "^1.0.1",
        "gopd": "^1.2.0",
        "has-tostringtag": "^1.0.2"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/word-wrap": {
      "version": "1.2.5",
      "resolved": "https://registry.npmjs.org/word-wrap/-/word-wrap-1.2.5.tgz",
      "integrity": "sha512-BN22B5eaMMI9UMtjrGd5g5eCYPpCPDUy0FJXbYsaT5zYxjFOckS53SQDE3pWkVoWpHXVb3BrYcEN4Twa55B5cA==",
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/workbox-background-sync": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-background-sync/-/workbox-background-sync-6.6.0.tgz",
      "integrity": "sha512-jkf4ZdgOJxC9u2vztxLuPT/UjlH7m/nWRQ/MgGL0v8BJHoZdVGJd18Kck+a0e55wGXdqyHO+4IQTk0685g4MUw==",
      "license": "MIT",
      "dependencies": {
        "idb": "^7.0.1",
        "workbox-core": "6.6.0"
      }
    },
    "node_modules/workbox-broadcast-update": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-broadcast-update/-/workbox-broadcast-update-6.6.0.tgz",
      "integrity": "sha512-nm+v6QmrIFaB/yokJmQ/93qIJ7n72NICxIwQwe5xsZiV2aI93MGGyEyzOzDPVz5THEr5rC3FJSsO3346cId64Q==",
      "license": "MIT",
      "dependencies": {
        "workbox-core": "6.6.0"
      }
    },
    "node_modules/workbox-build": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-build/-/workbox-build-6.6.0.tgz",
      "integrity": "sha512-Tjf+gBwOTuGyZwMz2Nk/B13Fuyeo0Q84W++bebbVsfr9iLkDSo6j6PST8tET9HYA58mlRXwlMGpyWO8ETJiXdQ==",
      "license": "MIT",
      "dependencies": {
        "@apideck/better-ajv-errors": "^0.3.1",
        "@babel/core": "^7.11.1",
        "@babel/preset-env": "^7.11.0",
        "@babel/runtime": "^7.11.2",
        "@rollup/plugin-babel": "^5.2.0",
        "@rollup/plugin-node-resolve": "^11.2.1",
        "@rollup/plugin-replace": "^2.4.1",
        "@surma/rollup-plugin-off-main-thread": "^2.2.3",
        "ajv": "^8.6.0",
        "common-tags": "^1.8.0",
        "fast-json-stable-stringify": "^2.1.0",
        "fs-extra": "^9.0.1",
        "glob": "^7.1.6",
        "lodash": "^4.17.20",
        "pretty-bytes": "^5.3.0",
        "rollup": "^2.43.1",
        "rollup-plugin-terser": "^7.0.0",
        "source-map": "^0.8.0-beta.0",
        "stringify-object": "^3.3.0",
        "strip-comments": "^2.0.1",
        "tempy": "^0.6.0",
        "upath": "^1.2.0",
        "workbox-background-sync": "6.6.0",
        "workbox-broadcast-update": "6.6.0",
        "workbox-cacheable-response": "6.6.0",
        "workbox-core": "6.6.0",
        "workbox-expiration": "6.6.0",
        "workbox-google-analytics": "6.6.0",
        "workbox-navigation-preload": "6.6.0",
        "workbox-precaching": "6.6.0",
        "workbox-range-requests": "6.6.0",
        "workbox-recipes": "6.6.0",
        "workbox-routing": "6.6.0",
        "workbox-strategies": "6.6.0",
        "workbox-streams": "6.6.0",
        "workbox-sw": "6.6.0",
        "workbox-window": "6.6.0"
      },
      "engines": {
        "node": ">=10.0.0"
      }
    },
    "node_modules/workbox-build/node_modules/@apideck/better-ajv-errors": {
      "version": "0.3.6",
      "resolved": "https://registry.npmjs.org/@apideck/better-ajv-errors/-/better-ajv-errors-0.3.6.tgz",
      "integrity": "sha512-P+ZygBLZtkp0qqOAJJVX4oX/sFo5JR3eBWwwuqHHhK0GIgQOKWrAfiAaWX0aArHkRWHMuggFEgAZNxVPwPZYaA==",
      "license": "MIT",
      "dependencies": {
        "json-schema": "^0.4.0",
        "jsonpointer": "^5.0.0",
        "leven": "^3.1.0"
      },
      "engines": {
        "node": ">=10"
      },
      "peerDependencies": {
        "ajv": ">=8"
      }
    },
    "node_modules/workbox-build/node_modules/ajv": {
      "version": "8.17.1",
      "resolved": "https://registry.npmjs.org/ajv/-/ajv-8.17.1.tgz",
      "integrity": "sha512-B/gBuNg5SiMTrPkC+A2+cW0RszwxYmn6VYxB/inlBStS5nx6xHIt/ehKRhIMhqusl7a8LjQoZnjCs5vhwxOQ1g==",
      "license": "MIT",
      "dependencies": {
        "fast-deep-equal": "^3.1.3",
        "fast-uri": "^3.0.1",
        "json-schema-traverse": "^1.0.0",
        "require-from-string": "^2.0.2"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/epoberezkin"
      }
    },
    "node_modules/workbox-build/node_modules/fs-extra": {
      "version": "9.1.0",
      "resolved": "https://registry.npmjs.org/fs-extra/-/fs-extra-9.1.0.tgz",
      "integrity": "sha512-hcg3ZmepS30/7BSFqRvoo3DOMQu7IjqxO5nCDt+zM9XWjb33Wg7ziNT+Qvqbuc3+gWpzO02JubVyk2G4Zvo1OQ==",
      "license": "MIT",
      "dependencies": {
        "at-least-node": "^1.0.0",
        "graceful-fs": "^4.2.0",
        "jsonfile": "^6.0.1",
        "universalify": "^2.0.0"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/workbox-build/node_modules/json-schema-traverse": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/json-schema-traverse/-/json-schema-traverse-1.0.0.tgz",
      "integrity": "sha512-NM8/P9n3XjXhIZn1lLhkFaACTOURQXjWhV4BA/RnOv8xvgqtqpAX9IO4mRQxSx1Rlo4tqzeqb0sOlruaOy3dug==",
      "license": "MIT"
    },
    "node_modules/workbox-build/node_modules/source-map": {
      "version": "0.8.0-beta.0",
      "resolved": "https://registry.npmjs.org/source-map/-/source-map-0.8.0-beta.0.tgz",
      "integrity": "sha512-2ymg6oRBpebeZi9UUNsgQ89bhx01TcTkmNTGnNO88imTmbSgy4nfujrgVEFKWpMTEGA11EDkTt7mqObTPdigIA==",
      "license": "BSD-3-Clause",
      "dependencies": {
        "whatwg-url": "^7.0.0"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/workbox-build/node_modules/tr46": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/tr46/-/tr46-1.0.1.tgz",
      "integrity": "sha512-dTpowEjclQ7Kgx5SdBkqRzVhERQXov8/l9Ft9dVM9fmg0W0KQSVaXX9T4i6twCPNtYiZM53lpSSUAwJbFPOHxA==",
      "license": "MIT",
      "dependencies": {
        "punycode": "^2.1.0"
      }
    },
    "node_modules/workbox-build/node_modules/webidl-conversions": {
      "version": "4.0.2",
      "resolved": "https://registry.npmjs.org/webidl-conversions/-/webidl-conversions-4.0.2.tgz",
      "integrity": "sha512-YQ+BmxuTgd6UXZW3+ICGfyqRyHXVlD5GtQr5+qjiNW7bF0cqrzX500HVXPBOvgXb5YnzDd+h0zqyv61KUD7+Sg==",
      "license": "BSD-2-Clause"
    },
    "node_modules/workbox-build/node_modules/whatwg-url": {
      "version": "7.1.0",
      "resolved": "https://registry.npmjs.org/whatwg-url/-/whatwg-url-7.1.0.tgz",
      "integrity": "sha512-WUu7Rg1DroM7oQvGWfOiAK21n74Gg+T4elXEQYkOhtyLeWiJFoOGLXPKI/9gzIie9CtwVLm8wtw6YJdKyxSjeg==",
      "license": "MIT",
      "dependencies": {
        "lodash.sortby": "^4.7.0",
        "tr46": "^1.0.1",
        "webidl-conversions": "^4.0.2"
      }
    },
    "node_modules/workbox-cacheable-response": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-cacheable-response/-/workbox-cacheable-response-6.6.0.tgz",
      "integrity": "sha512-JfhJUSQDwsF1Xv3EV1vWzSsCOZn4mQ38bWEBR3LdvOxSPgB65gAM6cS2CX8rkkKHRgiLrN7Wxoyu+TuH67kHrw==",
      "deprecated": "workbox-background-sync@6.6.0",
      "license": "MIT",
      "dependencies": {
        "workbox-core": "6.6.0"
      }
    },
    "node_modules/workbox-core": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-core/-/workbox-core-6.6.0.tgz",
      "integrity": "sha512-GDtFRF7Yg3DD859PMbPAYPeJyg5gJYXuBQAC+wyrWuuXgpfoOrIQIvFRZnQ7+czTIQjIr1DhLEGFzZanAT/3bQ==",
      "license": "MIT"
    },
    "node_modules/workbox-expiration": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-expiration/-/workbox-expiration-6.6.0.tgz",
      "integrity": "sha512-baplYXcDHbe8vAo7GYvyAmlS4f6998Jff513L4XvlzAOxcl8F620O91guoJ5EOf5qeXG4cGdNZHkkVAPouFCpw==",
      "license": "MIT",
      "dependencies": {
        "idb": "^7.0.1",
        "workbox-core": "6.6.0"
      }
    },
    "node_modules/workbox-google-analytics": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-google-analytics/-/workbox-google-analytics-6.6.0.tgz",
      "integrity": "sha512-p4DJa6OldXWd6M9zRl0H6vB9lkrmqYFkRQ2xEiNdBFp9U0LhsGO7hsBscVEyH9H2/3eZZt8c97NB2FD9U2NJ+Q==",
      "deprecated": "It is not compatible with newer versions of GA starting with v4, as long as you are using GAv3 it should be ok, but the package is not longer being maintained",
      "license": "MIT",
      "dependencies": {
        "workbox-background-sync": "6.6.0",
        "workbox-core": "6.6.0",
        "workbox-routing": "6.6.0",
        "workbox-strategies": "6.6.0"
      }
    },
    "node_modules/workbox-navigation-preload": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-navigation-preload/-/workbox-navigation-preload-6.6.0.tgz",
      "integrity": "sha512-utNEWG+uOfXdaZmvhshrh7KzhDu/1iMHyQOV6Aqup8Mm78D286ugu5k9MFD9SzBT5TcwgwSORVvInaXWbvKz9Q==",
      "license": "MIT",
      "dependencies": {
        "workbox-core": "6.6.0"
      }
    },
    "node_modules/workbox-precaching": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-precaching/-/workbox-precaching-6.6.0.tgz",
      "integrity": "sha512-eYu/7MqtRZN1IDttl/UQcSZFkHP7dnvr/X3Vn6Iw6OsPMruQHiVjjomDFCNtd8k2RdjLs0xiz9nq+t3YVBcWPw==",
      "license": "MIT",
      "dependencies": {
        "workbox-core": "6.6.0",
        "workbox-routing": "6.6.0",
        "workbox-strategies": "6.6.0"
      }
    },
    "node_modules/workbox-range-requests": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-range-requests/-/workbox-range-requests-6.6.0.tgz",
      "integrity": "sha512-V3aICz5fLGq5DpSYEU8LxeXvsT//mRWzKrfBOIxzIdQnV/Wj7R+LyJVTczi4CQ4NwKhAaBVaSujI1cEjXW+hTw==",
      "license": "MIT",
      "dependencies": {
        "workbox-core": "6.6.0"
      }
    },
    "node_modules/workbox-recipes": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-recipes/-/workbox-recipes-6.6.0.tgz",
      "integrity": "sha512-TFi3kTgYw73t5tg73yPVqQC8QQjxJSeqjXRO4ouE/CeypmP2O/xqmB/ZFBBQazLTPxILUQ0b8aeh0IuxVn9a6A==",
      "license": "MIT",
      "dependencies": {
        "workbox-cacheable-response": "6.6.0",
        "workbox-core": "6.6.0",
        "workbox-expiration": "6.6.0",
        "workbox-precaching": "6.6.0",
        "workbox-routing": "6.6.0",
        "workbox-strategies": "6.6.0"
      }
    },
    "node_modules/workbox-routing": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-routing/-/workbox-routing-6.6.0.tgz",
      "integrity": "sha512-x8gdN7VDBiLC03izAZRfU+WKUXJnbqt6PG9Uh0XuPRzJPpZGLKce/FkOX95dWHRpOHWLEq8RXzjW0O+POSkKvw==",
      "license": "MIT",
      "dependencies": {
        "workbox-core": "6.6.0"
      }
    },
    "node_modules/workbox-strategies": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-strategies/-/workbox-strategies-6.6.0.tgz",
      "integrity": "sha512-eC07XGuINAKUWDnZeIPdRdVja4JQtTuc35TZ8SwMb1ztjp7Ddq2CJ4yqLvWzFWGlYI7CG/YGqaETntTxBGdKgQ==",
      "license": "MIT",
      "dependencies": {
        "workbox-core": "6.6.0"
      }
    },
    "node_modules/workbox-streams": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-streams/-/workbox-streams-6.6.0.tgz",
      "integrity": "sha512-rfMJLVvwuED09CnH1RnIep7L9+mj4ufkTyDPVaXPKlhi9+0czCu+SJggWCIFbPpJaAZmp2iyVGLqS3RUmY3fxg==",
      "license": "MIT",
      "dependencies": {
        "workbox-core": "6.6.0",
        "workbox-routing": "6.6.0"
      }
    },
    "node_modules/workbox-sw": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-sw/-/workbox-sw-6.6.0.tgz",
      "integrity": "sha512-R2IkwDokbtHUE4Kus8pKO5+VkPHD2oqTgl+XJwh4zbF1HyjAbgNmK/FneZHVU7p03XUt9ICfuGDYISWG9qV/CQ==",
      "license": "MIT"
    },
    "node_modules/workbox-webpack-plugin": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-webpack-plugin/-/workbox-webpack-plugin-6.6.0.tgz",
      "integrity": "sha512-xNZIZHalboZU66Wa7x1YkjIqEy1gTR+zPM+kjrYJzqN7iurYZBctBLISyScjhkJKYuRrZUP0iqViZTh8rS0+3A==",
      "license": "MIT",
      "dependencies": {
        "fast-json-stable-stringify": "^2.1.0",
        "pretty-bytes": "^5.4.1",
        "upath": "^1.2.0",
        "webpack-sources": "^1.4.3",
        "workbox-build": "6.6.0"
      },
      "engines": {
        "node": ">=10.0.0"
      },
      "peerDependencies": {
        "webpack": "^4.4.0 || ^5.9.0"
      }
    },
    "node_modules/workbox-webpack-plugin/node_modules/source-map": {
      "version": "0.6.1",
      "resolved": "https://registry.npmjs.org/source-map/-/source-map-0.6.1.tgz",
      "integrity": "sha512-UjgapumWlbMhkBgzT7Ykc5YXUT46F0iKu8SGXq0bcwP5dz/h0Plj6enJqjz1Zbq2l5WaqYnrVbwWOWMyF3F47g==",
      "license": "BSD-3-Clause",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/workbox-webpack-plugin/node_modules/webpack-sources": {
      "version": "1.4.3",
      "resolved": "https://registry.npmjs.org/webpack-sources/-/webpack-sources-1.4.3.tgz",
      "integrity": "sha512-lgTS3Xhv1lCOKo7SA5TjKXMjpSM4sBjNV5+q2bqesbSPs5FjGmU6jjtBSkX9b4qW87vDIsCIlUPOEhbZrMdjeQ==",
      "license": "MIT",
      "dependencies": {
        "source-list-map": "^2.0.0",
        "source-map": "~0.6.1"
      }
    },
    "node_modules/workbox-window": {
      "version": "6.6.0",
      "resolved": "https://registry.npmjs.org/workbox-window/-/workbox-window-6.6.0.tgz",
      "integrity": "sha512-L4N9+vka17d16geaJXXRjENLFldvkWy7JyGxElRD0JvBxvFEd8LOhr+uXCcar/NzAmIBRv9EZ+M+Qr4mOoBITw==",
      "license": "MIT",
      "dependencies": {
        "@types/trusted-types": "^2.0.2",
        "workbox-core": "6.6.0"
      }
    },
    "node_modules/wrap-ansi": {
      "version": "7.0.0",
      "resolved": "https://registry.npmjs.org/wrap-ansi/-/wrap-ansi-7.0.0.tgz",
      "integrity": "sha512-YVGIj2kamLSTxw6NsZjoBxfSwsn0ycdesmc4p+Q21c5zPuZ1pl+NfxVdxPtdHvmNVOQ6XSYG4AUtyt/Fi7D16Q==",
      "license": "MIT",
      "dependencies": {
        "ansi-styles": "^4.0.0",
        "string-width": "^4.1.0",
        "strip-ansi": "^6.0.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/chalk/wrap-ansi?sponsor=1"
      }
    },
    "node_modules/wrap-ansi-cjs": {
      "name": "wrap-ansi",
      "version": "7.0.0",
      "resolved": "https://registry.npmjs.org/wrap-ansi/-/wrap-ansi-7.0.0.tgz",
      "integrity": "sha512-YVGIj2kamLSTxw6NsZjoBxfSwsn0ycdesmc4p+Q21c5zPuZ1pl+NfxVdxPtdHvmNVOQ6XSYG4AUtyt/Fi7D16Q==",
      "license": "MIT",
      "dependencies": {
        "ansi-styles": "^4.0.0",
        "string-width": "^4.1.0",
        "strip-ansi": "^6.0.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/chalk/wrap-ansi?sponsor=1"
      }
    },
    "node_modules/wrappy": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/wrappy/-/wrappy-1.0.2.tgz",
      "integrity": "sha512-l4Sp/DRseor9wL6EvV2+TuQn63dMkPjZ/sp9XkghTEbV9KlPS1xUsZ3u7/IQO4wxtcFB4bgpQPRcR3QCvezPcQ==",
      "license": "ISC"
    },
    "node_modules/write-file-atomic": {
      "version": "3.0.3",
      "resolved": "https://registry.npmjs.org/write-file-atomic/-/write-file-atomic-3.0.3.tgz",
      "integrity": "sha512-AvHcyZ5JnSfq3ioSyjrBkH9yW4m7Ayk8/9My/DD9onKeu/94fwrMocemO2QAJFAlnnDN+ZDS+ZjAR5ua1/PV/Q==",
      "license": "ISC",
      "dependencies": {
        "imurmurhash": "^0.1.4",
        "is-typedarray": "^1.0.0",
        "signal-exit": "^3.0.2",
        "typedarray-to-buffer": "^3.1.5"
      }
    },
    "node_modules/ws": {
      "version": "7.5.10",
      "resolved": "https://registry.npmjs.org/ws/-/ws-7.5.10.tgz",
      "integrity": "sha512-+dbF1tHwZpXcbOJdVOkzLDxZP1ailvSxM6ZweXTegylPny803bFhA+vqBYw4s31NSAk4S2Qz+AKXK9a4wkdjcQ==",
      "license": "MIT",
      "engines": {
        "node": ">=8.3.0"
      },
      "peerDependencies": {
        "bufferutil": "^4.0.1",
        "utf-8-validate": "^5.0.2"
      },
      "peerDependenciesMeta": {
        "bufferutil": {
          "optional": true
        },
        "utf-8-validate": {
          "optional": true
        }
      }
    },
    "node_modules/xml-name-validator": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/xml-name-validator/-/xml-name-validator-3.0.0.tgz",
      "integrity": "sha512-A5CUptxDsvxKJEU3yO6DuWBSJz/qizqzJKOMIfUJHETbBw/sFaDxgd6fxm1ewUaM0jZ444Fc5vC5ROYurg/4Pw==",
      "license": "Apache-2.0"
    },
    "node_modules/xmlchars": {
      "version": "2.2.0",
      "resolved": "https://registry.npmjs.org/xmlchars/-/xmlchars-2.2.0.tgz",
      "integrity": "sha512-JZnDKK8B0RCDw84FNdDAIpZK+JuJw+s7Lz8nksI7SIuU3UXJJslUthsi+uWBUYOwPFwW7W7PRLRfUKpxjtjFCw==",
      "license": "MIT"
    },
    "node_modules/y18n": {
      "version": "5.0.8",
      "resolved": "https://registry.npmjs.org/y18n/-/y18n-5.0.8.tgz",
      "integrity": "sha512-0pfFzegeDWJHJIAmTLRP2DwHjdF5s7jo9tuztdQxAhINCdvS+3nGINqPd00AphqJR/0LhANUS6/+7SCb98YOfA==",
      "license": "ISC",
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/yallist": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/yallist/-/yallist-3.1.1.tgz",
      "integrity": "sha512-a4UGQaWPH59mOXUYnAG2ewncQS4i4F43Tv3JoAM+s2VDAmS9NsK8GpDMLrCHPksFT7h3K6TOoUNn2pb7RoXx4g==",
      "license": "ISC"
    },
    "node_modules/yaml": {
      "version": "1.10.2",
      "resolved": "https://registry.npmjs.org/yaml/-/yaml-1.10.2.tgz",
      "integrity": "sha512-r3vXyErRCYJ7wg28yvBY5VSoAF8ZvlcW9/BwUzEtUsjvX/DKs24dIkuwjtuprwJJHsbyUbLApepYTR1BN4uHrg==",
      "license": "ISC",
      "engines": {
        "node": ">= 6"
      }
    },
    "node_modules/yargs": {
      "version": "16.2.0",
      "resolved": "https://registry.npmjs.org/yargs/-/yargs-16.2.0.tgz",
      "integrity": "sha512-D1mvvtDG0L5ft/jGWkLpG1+m0eQxOfaBvTNELraWj22wSVUMWxZUvYgJYcKh6jGGIkJFhH4IZPQhR4TKpc8mBw==",
      "license": "MIT",
      "dependencies": {
        "cliui": "^7.0.2",
        "escalade": "^3.1.1",
        "get-caller-file": "^2.0.5",
        "require-directory": "^2.1.1",
        "string-width": "^4.2.0",
        "y18n": "^5.0.5",
        "yargs-parser": "^20.2.2"
      },
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/yargs-parser": {
      "version": "20.2.9",
      "resolved": "https://registry.npmjs.org/yargs-parser/-/yargs-parser-20.2.9.tgz",
      "integrity": "sha512-y11nGElTIV+CT3Zv9t7VKl+Q3hTQoT9a1Qzezhhl6Rp21gJ/IVTW7Z3y9EWXhuUBC2Shnf+DX0antecpAwSP8w==",
      "license": "ISC",
      "engines": {
        "node": ">=10"
      }
    },
    "node_modules/yocto-queue": {
      "version": "0.1.0",
      "resolved": "https://registry.npmjs.org/yocto-queue/-/yocto-queue-0.1.0.tgz",
      "integrity": "sha512-rVksvsnNCdJ/ohGc6xgPwyN8eheCxsiLM8mxuE/t/mOVqJewPuO1miLpTHQiRgTKCLexL4MeAFVagts7HmNZ2Q==",
      "license": "MIT",
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    }
  }
}


============================================================
FILE: frontend/package.json
============================================================
{
  "name": "frontend",
  "version": "0.1.0",
  "private": true,
  "proxy": "http://localhost:8080",
  "dependencies": {
    "@heroicons/react": "^2.1.5",
    "@tailwindcss/forms": "^0.5.9",
    "@tailwindcss/postcss": "^4.1.10",
    "@testing-library/jest-dom": "^5.17.0",
    "@testing-library/react": "^13.4.0",
    "@testing-library/user-event": "^13.5.0",
    "@types/jest": "^27.5.2",
    "@types/node": "^16.18.119",
    "@types/react": "^18.3.15",
    "@types/react-dom": "^18.3.1",
    "autoprefixer": "^10.4.20",
    "clsx": "^2.1.1",
    "lucide-react": "^0.468.0",
    "postcss": "^8.5.4",
    "react": "^18.3.1",
    "react-dom": "^18.3.1",
    "react-scripts": "5.0.1",
    "tailwindcss": "^3.4.17",
    "typescript": "^4.9.5",
    "web-vitals": "^2.1.4"
  },
  "scripts": {
    "start": "react-scripts start",
    "build": "react-scripts build",
    "test": "react-scripts test",
    "eject": "react-scripts eject"
  },
  "eslintConfig": {
    "extends": [
      "react-app",
      "react-app/jest"
    ]
  },
  "browserslist": {
    "production": [
      ">0.2%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 1 chrome version",
      "last 1 firefox version",
      "last 1 safari version"
    ]
  }
}


============================================================
FILE: frontend/tsconfig.json
============================================================
{
  "compilerOptions": {
    "target": "es5",
    "lib": [
      "dom",
      "dom.iterable",
      "esnext"
    ],
    "allowJs": true,
    "skipLibCheck": true,
    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true,
    "noFallthroughCasesInSwitch": true,
    "module": "esnext",
    "moduleResolution": "node",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx"
  },
  "include": [
    "src"
  ]
}

FILE: frontend/postcss.config.js - Standard .js file
FILE: frontend/public/favicon.ico - Standard .ico file
FILE: frontend/public/index.html - Standard .html file
FILE: frontend/public/logo512.png - Standard .png file

============================================================
FILE: frontend/public/manifest.json
============================================================
{
  "short_name": "React App",
  "name": "Create React App Sample",
  "icons": [
    {
      "src": "favicon.ico",
      "sizes": "64x64 32x32 24x24 16x16",
      "type": "image/x-icon"
    },
    {
      "src": "logo192.png",
      "type": "image/png",
      "sizes": "192x192"
    },
    {
      "src": "logo512.png",
      "type": "image/png",
      "sizes": "512x512"
    }
  ],
  "start_url": ".",
  "display": "standalone",
  "theme_color": "#000000",
  "background_color": "#ffffff"
}


============================================================
FILE: frontend/public/robots.txt
============================================================
# https://www.robotstxt.org/robotstxt.html
User-agent: *
Disallow:

FILE: frontend/public/logo192.png - Standard .png file
FILE: frontend/src/index.tsx - Standard .tsx file
FILE: frontend/src/App.tsx - Standard .tsx file
FILE: frontend/src/App.css - Standard .css file
FILE: frontend/src/index.css - Standard .css file
FILE: frontend/src/react-app-env.d.ts - Standard .ts file
FILE: frontend/src/components/SettingsPage.tsx - Standard .tsx file
FILE: frontend/src/components/Knowledge/KnowledgeDrivenDashboard.tsx - Standard .tsx file

============================================================
FILE: archive/AI_Chief_of_Staff_Technical_Specification.txt
============================================================
# AI Chief of Staff - Comprehensive Technical Specification
# =====================================================================
# Version: 2.0 (Knowledge Tree First Architecture)
# Created: December 2024
# Purpose: Complete technical reference for AI training and development

## 1. PROJECT OVERVIEW

### 1.1 Vision and Mission
The AI Chief of Staff is a sophisticated business intelligence platform that transforms Gmail and Calendar data into a proactive AI assistant. Unlike traditional email clients or basic AI tools, this system creates a comprehensive "business brain" that understands relationships, projects, and context to provide actionable intelligence.

### 1.2 Core Innovation: "Knowledge Tree First" Architecture
The fundamental breakthrough is the "Knowledge Tree First" approach:
- Instead of processing emails individually, the system analyzes ALL emails as a corpus
- Creates a master knowledge tree with consistent taxonomy
- Uses this tree as the foundation for all subsequent processing
- Ensures semantic consistency and eliminates categorization drift
- Provides deep business context for every interaction

### 1.3 Target Users
- C-level executives managing complex relationships and projects
- Business development professionals tracking opportunities
- Project managers coordinating multiple stakeholders
- Entrepreneurs managing investor and partner relationships
- Anyone with high-volume, high-value email communications

## 2. TECHNICAL ARCHITECTURE

### 2.1 System Architecture Overview
```
┌─────────────────────────────────────────────────────────────┐
│                    AI Chief of Staff                        │
├─────────────────────────────────────────────────────────────┤
│  Frontend (React/TypeScript)                               │
│  ├── Dashboard & Analytics                                  │
│  ├── People & Relationship Intelligence                     │
│  ├── Task & Project Management                             │
│  ├── Settings & Workflow Configuration                      │
│  └── Real-time Chat Interface                              │
├─────────────────────────────────────────────────────────────┤
│  Backend API Layer (Flask/Python)                          │
│  ├── Authentication & Authorization                         │
│  ├── Email Processing Pipeline                             │
│  ├── Knowledge Tree Management                             │
│  ├── Contact Tier Classification                           │
│  ├── Task Extraction & Prioritization                      │
│  └── Intelligence Chat System                              │
├─────────────────────────────────────────────────────────────┤
│  AI/ML Processing Layer                                     │
│  ├── Claude 4 Sonnet Integration                           │
│  ├── Email Quality Filter                                  │
│  ├── Unified Entity Engine                                 │
│  ├── Prompt Management System                              │
│  └── Context-Aware Intelligence                            │
├─────────────────────────────────────────────────────────────┤
│  Data Layer                                                │
│  ├── PostgreSQL Database                                   │
│  ├── Knowledge Tree JSON Files                             │
│  ├── Email Content Storage                                 │
│  └── User Preferences & Settings                           │
├─────────────────────────────────────────────────────────────┤
│  External Integrations                                     │
│  ├── Gmail API (OAuth 2.0)                                │
│  ├── Google Calendar API                                   │
│  └── Anthropic Claude API                                  │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 Technology Stack

**Frontend:**
- React 18 with TypeScript
- Tailwind CSS for styling
- Real-time WebSocket connections
- Component-based architecture
- State management with React hooks

**Backend:**
- Python 3.9+ with Flask framework
- SQLAlchemy ORM for database operations
- Async processing capabilities
- RESTful API design
- JWT-based authentication

**Database:**
- PostgreSQL for relational data
- JSON fields for flexible schema
- Optimized indexes for performance
- Migration system for schema evolution

**AI/ML:**
- Anthropic Claude 4 Sonnet API
- Custom prompt engineering system
- Semantic analysis and classification
- Natural language processing pipelines

**DevOps:**
- Docker containerization
- Environment-based configuration
- Logging and monitoring systems
- Error tracking and alerting

## 3. CORE SYSTEMS DEEP DIVE

### 3.1 Knowledge Tree System (The Heart of the Platform)

#### 3.1.1 Concept and Philosophy
The Knowledge Tree is the fundamental innovation that sets this system apart. Instead of treating emails as isolated documents, it creates a unified semantic framework:

```
Knowledge Tree Structure:
├── Topics (Business Areas)
│   ├── AI Music Creation Tools
│   ├── Strategic Partnership Exploration
│   ├── Funding and Investment
│   └── Product Development
├── People (Relationship Graph)
│   ├── Michael Shulman (Suno)
│   ├── Investors & VCs
│   ├── Strategic Partners
│   └── Team Members
└── Projects (Active Initiatives)
    ├── HitCraft AI Assistant
    ├── Partnership Negotiations
    └── Fundraising Activities
```

#### 3.1.2 Technical Implementation
The knowledge tree is built through a sophisticated multi-phase process:

**Phase 1: Corpus Analysis**
```python
# Pseudo-code for knowledge tree building
def build_knowledge_tree(all_emails):
    # Combine all emails into a mega-corpus
    corpus = aggregate_email_content(all_emails)
    
    # Send to Claude for comprehensive analysis
    tree_prompt = generate_tree_building_prompt(corpus, user_context)
    
    # Claude returns structured JSON tree
    master_tree = claude_api.analyze(tree_prompt)
    
    # Validate and store tree structure
    validated_tree = validate_tree_structure(master_tree)
    store_knowledge_tree(validated_tree)
    
    return validated_tree
```

**Phase 2: Email Assignment**
Once the tree exists, individual emails are assigned to tree nodes:
```python
def assign_email_to_tree(email, knowledge_tree):
    assignment_prompt = create_assignment_prompt(email, knowledge_tree)
    classification = claude_api.classify(assignment_prompt)
    
    # High-confidence assignments are applied automatically
    if classification.confidence > 0.8:
        assign_email(email, classification.topics, classification.people)
    else:
        queue_for_review(email, classification)
```

#### 3.1.3 Tree Evolution and Refinement
The knowledge tree is not static - it evolves as new information arrives:

- **Incremental Updates**: New emails can trigger tree refinement
- **Confidence Scoring**: Each node has confidence metrics
- **Merge Operations**: Similar topics/people are consolidated
- **Pruning**: Low-confidence or unused nodes are removed

### 3.2 Contact Tier Classification System

#### 3.2.1 Philosophy: Engagement-Based Intelligence
Traditional CRM systems treat all contacts equally. The AI Chief of Staff implements a sophisticated tier system based on actual engagement patterns:

**Tier 1 (Strategic Contacts)**: People you actively respond to
- High response rate (>50%)
- Regular bidirectional communication
- Strategic importance to business

**Tier 2 (Developing Relationships)**: New or occasional contacts
- Limited interaction history
- Moderate engagement
- Potential for growth

**Tier Last (Low Value)**: Contacts you consistently ignore
- Low response rate (<10%)
- One-way communication
- Low strategic value

#### 3.2.2 Technical Implementation
```python
class EmailQualityFilter:
    def _determine_contact_tier(self, stats):
        # Analyze response patterns
        if stats.response_rate >= 0.5:
            return ContactTier.TIER_1, "High engagement"
        
        # Check for consistent non-response
        if (stats.emails_received >= 5 and 
            stats.response_rate <= 0.1):
            return ContactTier.TIER_LAST, "Consistently ignored"
        
        # Default to developing relationship
        return ContactTier.TIER_2, "Developing relationship"
```

#### 3.2.3 Business Impact
This tier system enables:
- **Noise Reduction**: Filters out irrelevant contacts
- **Focus Enhancement**: Highlights truly important relationships
- **Resource Optimization**: Directs attention to high-value interactions
- **Relationship Insights**: Reveals communication patterns

### 3.3 Email Processing Pipeline

#### 3.3.1 Multi-Stage Processing Architecture
```
Email Ingestion → Quality Filter → Normalization → AI Analysis → Entity Extraction → Storage
      ↓              ↓              ↓              ↓               ↓            ↓
   Gmail API    Tier System    Clean Content   Claude API   Unified Engine  Database
```

#### 3.3.2 Detailed Pipeline Stages

**Stage 1: Email Ingestion**
- Gmail API integration with OAuth 2.0
- Batch processing for historical emails
- Real-time processing for new emails
- Rate limiting and error handling

**Stage 2: Quality Filtering**
```python
def process_email_quality(email, user_id):
    sender_tier = get_contact_tier(email.sender, user_id)
    
    # Skip processing for Tier LAST contacts
    if sender_tier == ContactTier.TIER_LAST:
        return EmailQualityResult(should_process=False)
    
    # Enhanced processing for Tier 1
    if sender_tier == ContactTier.TIER_1:
        return EmailQualityResult(should_process=True, priority="high")
    
    return EmailQualityResult(should_process=True, priority="normal")
```

**Stage 3: Content Normalization**
- HTML stripping and text extraction
- Signature removal
- Thread de-duplication
- Content cleaning and standardization

**Stage 4: AI Analysis with Claude**
- Context-aware prompt generation
- Semantic analysis and categorization
- Entity extraction (people, topics, projects)
- Task identification and prioritization

**Stage 5: Unified Entity Engine**
```python
class UnifiedEntityEngine:
    def process_email_entities(self, email, analysis):
        # Extract and link people
        people = self.extract_people(analysis.people, email)
        
        # Extract and categorize topics
        topics = self.extract_topics(analysis.topics, email)
        
        # Identify actionable tasks
        tasks = self.extract_tasks(analysis.tasks, email)
        
        # Create entity relationships
        relationships = self.create_relationships(people, topics, tasks)
        
        return EntityExtractionResult(people, topics, tasks, relationships)
```

### 3.4 Task Extraction and Management

#### 3.4.1 Tactical Task Philosophy
The system implements a "tactical tasks only" approach to prevent task overwhelm:

- **High Confidence Threshold**: Only tasks with >70% confidence
- **Specific Deliverables**: Concrete, actionable items only
- **Business Context Integration**: Tasks linked to knowledge tree
- **Strategic Filtering**: Rejects vague "follow up" tasks

#### 3.4.2 Task Extraction Process
```python
def extract_tactical_tasks(email_content, business_context):
    prompt = create_tactical_task_prompt(email_content, business_context)
    
    task_analysis = claude_api.analyze(prompt)
    
    # Filter by confidence and specificity
    tactical_tasks = []
    for task in task_analysis.tasks:
        if (task.confidence > 0.7 and 
            task.is_specific and 
            not task.is_vague_followup):
            tactical_tasks.append(task)
    
    return tactical_tasks
```

#### 3.4.3 Task Prioritization
Tasks are automatically prioritized based on:
- **Email urgency indicators**
- **Sender importance (contact tier)**
- **Project deadlines**
- **Strategic significance**
- **Temporal context**

### 3.5 Prompt Management System

#### 3.5.1 External Prompt Architecture
All AI prompts are externalized from code into structured text files:

```
prompts/
├── knowledge_tree/
│   ├── build_initial_tree.txt
│   ├── refine_existing_tree.txt
│   └── assign_emails_to_tree.txt
├── email_intelligence/
│   ├── email_analysis_system.txt
│   └── relationship_analysis.txt
├── task_extraction/
│   └── tactical_task_extraction.txt
└── intelligence_chat/
    └── enhanced_chat_system.txt
```

#### 3.5.2 Prompt Loading System
```python
class PromptLoader:
    def load_prompt(self, category, name, **variables):
        prompt_path = f"prompts/{category}/{name}.txt"
        template = self.read_template(prompt_path)
        
        # Variable substitution
        return template.format(**variables)
    
    def get_knowledge_tree_prompt(self, user_email, emails_data):
        return self.load_prompt(
            "knowledge_tree", 
            "build_initial_tree",
            user_email=user_email,
            emails_data=emails_data
        )
```

#### 3.5.3 Benefits of External Prompts
- **Rapid iteration**: Modify prompts without code changes
- **Version control**: Track prompt evolution
- **A/B testing**: Compare prompt effectiveness
- **Domain expertise**: Non-technical users can refine prompts
- **Transparency**: Clear understanding of AI instructions

## 4. DATABASE SCHEMA AND DATA MODELS

### 4.1 Core Entity Models

#### 4.1.1 User Model
```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    google_id VARCHAR(255) UNIQUE NOT NULL,
    name VARCHAR(255) NOT NULL,
    access_token TEXT,
    refresh_token TEXT,
    token_expires_at TIMESTAMP,
    scopes JSON,
    created_at TIMESTAMP DEFAULT NOW(),
    last_login TIMESTAMP DEFAULT NOW(),
    is_active BOOLEAN DEFAULT TRUE,
    email_fetch_limit INTEGER DEFAULT 50,
    email_days_back INTEGER DEFAULT 30,
    auto_process_emails BOOLEAN DEFAULT TRUE
);
```

#### 4.1.2 Email Model (Enhanced)
```sql
CREATE TABLE emails (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id),
    gmail_id VARCHAR(255) UNIQUE,
    thread_id VARCHAR(255),
    sender VARCHAR(500),
    sender_name VARCHAR(255),
    subject TEXT,
    body_text TEXT,
    body_html TEXT,
    body_clean TEXT,  -- Normalized content
    body_preview VARCHAR(500),
    snippet VARCHAR(500),
    recipients TEXT,
    cc TEXT,
    bcc TEXT,
    email_date TIMESTAMP,
    
    -- AI Analysis Results
    ai_summary TEXT,
    ai_category VARCHAR(100),
    sentiment_score FLOAT,
    urgency_score FLOAT,
    key_insights JSON,
    topics JSON,
    mentioned_people JSON,
    
    -- Processing Metadata
    processed_at TIMESTAMP,
    normalizer_version VARCHAR(50),
    processing_version VARCHAR(50),
    primary_topic_id INTEGER REFERENCES topics(id),
    
    -- Storage Optimization
    content_hash VARCHAR(64),
    blob_storage_key VARCHAR(255),
    
    created_at TIMESTAMP DEFAULT NOW()
);
```

#### 4.1.3 Person Model (Relationship Intelligence)
```sql
CREATE TABLE people (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id),
    email_address VARCHAR(255),
    name VARCHAR(255) NOT NULL,
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    
    -- Professional Details
    title VARCHAR(255),
    company VARCHAR(255),
    role VARCHAR(255),
    department VARCHAR(255),
    phone VARCHAR(50),
    linkedin_url VARCHAR(500),
    
    -- Relationship Intelligence
    relationship_type VARCHAR(100),
    communication_frequency VARCHAR(50),
    importance_level FLOAT DEFAULT 0.5,
    engagement_score FLOAT DEFAULT 0.0,
    is_trusted_contact BOOLEAN DEFAULT FALSE,
    
    -- AI-Generated Insights
    bio TEXT,
    professional_story TEXT,
    communication_style TEXT,
    skills JSON,
    interests JSON,
    personality_traits JSON,
    preferences JSON,
    key_topics JSON,
    notes TEXT,
    
    -- Metadata
    first_mentioned TIMESTAMP DEFAULT NOW(),
    last_interaction TIMESTAMP DEFAULT NOW(),
    total_emails INTEGER DEFAULT 0,
    total_interactions INTEGER DEFAULT 0,
    knowledge_confidence FLOAT DEFAULT 0.5,
    last_updated_by_ai TIMESTAMP,
    ai_version VARCHAR(50),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

#### 4.1.4 Topic Model (Knowledge Tree Nodes)
```sql
CREATE TABLE topics (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id),
    name VARCHAR(255) NOT NULL,
    slug VARCHAR(255),
    description TEXT,
    
    -- Tree Structure
    parent_topic_id INTEGER REFERENCES topics(id),
    is_official BOOLEAN DEFAULT FALSE,
    
    -- Intelligence Data
    keywords TEXT,
    email_count INTEGER DEFAULT 0,
    total_mentions INTEGER DEFAULT 0,
    last_mentioned TIMESTAMP,
    intelligence_summary TEXT,
    strategic_importance FLOAT DEFAULT 0.5,
    
    -- Metadata
    confidence_score FLOAT DEFAULT 0.5,
    version INTEGER DEFAULT 1,
    last_used TIMESTAMP,
    usage_frequency INTEGER DEFAULT 0,
    confidence_threshold FLOAT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    ai_version VARCHAR(50)
);
```

#### 4.1.5 Task Model (Tactical Actions)
```sql
CREATE TABLE tasks (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id),
    email_id INTEGER REFERENCES emails(id),
    
    -- Task Details
    description TEXT NOT NULL,
    assignee VARCHAR(255),
    due_date TIMESTAMP,
    due_date_text VARCHAR(100),
    priority VARCHAR(20) DEFAULT 'medium',
    category VARCHAR(100),
    status VARCHAR(20) DEFAULT 'pending',
    
    -- AI Metadata
    confidence FLOAT DEFAULT 0.5,
    source_text TEXT,
    topics JSON,
    extractor_version VARCHAR(50),
    model_used VARCHAR(50),
    
    -- Lifecycle
    completed_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

### 4.2 Advanced Data Models

#### 4.2.1 Contact Context (Rich Relationship Data)
```sql
CREATE TABLE contact_contexts (
    id SERIAL PRIMARY KEY,
    person_id INTEGER REFERENCES people(id),
    user_id INTEGER REFERENCES users(id),
    
    context_type VARCHAR(50) NOT NULL,  -- communication_pattern, project_involvement, etc.
    title VARCHAR(255) NOT NULL,
    description TEXT,
    confidence_score FLOAT DEFAULT 0.5,
    
    -- Evidence
    source_emails JSON,
    supporting_quotes JSON,
    tags JSON,
    
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

#### 4.2.2 Entity Relationships
```sql
CREATE TABLE entity_relationships (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id),
    
    -- Relationship Definition
    source_entity_type VARCHAR(50),  -- person, topic, project
    source_entity_id INTEGER,
    target_entity_type VARCHAR(50),
    target_entity_id INTEGER,
    relationship_type VARCHAR(100),
    
    -- Relationship Strength
    strength FLOAT DEFAULT 0.5,
    confidence FLOAT DEFAULT 0.5,
    evidence_count INTEGER DEFAULT 0,
    
    -- Supporting Data
    evidence_emails JSON,
    relationship_summary TEXT,
    
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

### 4.3 Performance Optimizations

#### 4.3.1 Strategic Indexing
```sql
-- High-performance indexes for common queries
CREATE INDEX idx_emails_user_date ON emails(user_id, email_date DESC);
CREATE INDEX idx_emails_user_sender ON emails(user_id, sender);
CREATE INDEX idx_people_user_email ON people(user_id, email_address);
CREATE INDEX idx_people_user_importance ON people(user_id, importance_level DESC);
CREATE INDEX idx_topics_user_mentions ON topics(user_id, total_mentions DESC);
CREATE INDEX idx_tasks_user_status ON tasks(user_id, status, created_at DESC);
```

#### 4.3.2 Data Archival Strategy
- Hot data: Last 6 months (fast SSD storage)
- Warm data: 6-24 months (standard storage)
- Cold data: >24 months (compressed archive)

## 5. FRONTEND ARCHITECTURE

### 5.1 Component Architecture

#### 5.1.1 Main Application Structure
```
src/
├── App.tsx (Main application shell)
├── components/
│   ├── Dashboard/
│   │   ├── DashboardOverview.tsx
│   │   ├── IntelligenceMetrics.tsx
│   │   └── ProactiveInsights.tsx
│   ├── People/
│   │   ├── PeopleList.tsx
│   │   ├── PersonDetail.tsx
│   │   └── RelationshipAnalytics.tsx
│   ├── Tasks/
│   │   ├── TaskList.tsx
│   │   ├── TaskDetail.tsx
│   │   └── TaskPrioritization.tsx
│   ├── Settings/
│   │   ├── SettingsPage.tsx
│   │   ├── WorkflowConfiguration.tsx
│   │   └── KnowledgeTreeManager.tsx
│   └── Chat/
│       ├── IntelligenceChat.tsx
│       └── ContextualChatPanel.tsx
├── hooks/
│   ├── useAuth.ts
│   ├── useWebSocket.ts
│   └── useIntelligence.ts
├── services/
│   ├── api.ts
│   ├── websocket.ts
│   └── storage.ts
└── types/
    ├── entities.ts
    ├── api.ts
    └── intelligence.ts
```

#### 5.1.2 State Management Philosophy
The application uses a hybrid state management approach:

**Local State (React Hooks)**: Component-specific data
**Context API**: Shared application state
**WebSocket**: Real-time updates
**Local Storage**: User preferences and caching

#### 5.1.3 Key Frontend Features

**Universal Entity Detail Modals**
Every UI element is clickable and shows:
- AI analysis and insights
- Raw source content (emails)
- Related entities and cross-references
- Confidence scores and traceability

**Real-Time Intelligence Updates**
- WebSocket connections for live data
- Progressive enhancement of insights
- Background processing indicators
- Contextual notifications

**Responsive Design**
- Mobile-first approach
- Adaptive layouts for different screen sizes
- Touch-friendly interactions
- Optimized for executive use cases

### 5.2 User Experience Design

#### 5.2.1 Three-Step Workflow
The UI implements a clear, sequential workflow:

**Step 1: Build Contact Base**
1. Fetch Sent Emails
2. Normalize Email Content  
3. Build Contact Rules
4. Fetch Calendar Events

**Step 2: Build Knowledge Tree**
1. Fetch All Emails
2. Build/Refine Tree
3. Assign Emails to Tree

**Step 3: Augment with Knowledge**
1. Create Tactical Tasks
2. Augment People Profiles
3. Augment Meeting Intelligence

#### 5.2.2 Progressive Disclosure
Information is revealed progressively:
- Overview cards show key metrics
- Click for detailed analysis
- Drill down to raw source content
- Cross-reference related entities

## 6. API ARCHITECTURE

### 6.1 RESTful API Design

#### 6.1.1 API Structure Overview
```
/api/
├── auth/
│   ├── /login
│   ├── /callback
│   └── /logout
├── email/
│   ├── /fetch-sent-emails
│   ├── /normalize-emails
│   ├── /process-batch
│   └── /sync-tree-to-database
├── knowledge/
│   ├── /topics/hierarchy
│   ├── /foundation/build-from-bulk-emails
│   └── /reorganize-content
├── people/
│   ├── /people
│   ├── /augment-with-knowledge
│   └── /relationship-analytics
├── tasks/
│   ├── /tasks
│   ├── /create-tactical
│   └── /prioritize
├── calendar/
│   ├── /fetch-events
│   ├── /sync
│   └── /augment-with-knowledge
├── intelligence/
│   ├── /chat
│   ├── /metrics
│   ├── /insights
│   └── /entity-context
└── settings/
    ├── /sync-settings
    ├── /flush-database
    └── /knowledge-tree-status
```

#### 6.1.2 API Response Patterns

**Standard Success Response**
```json
{
  "success": true,
  "data": { ... },
  "metadata": {
    "timestamp": "2024-12-16T01:00:00Z",
    "processing_time_ms": 150,
    "version": "2.0"
  }
}
```

**Error Response**
```json
{
  "success": false,
  "error": "Descriptive error message",
  "error_code": "SPECIFIC_ERROR_CODE",
  "details": { ... },
  "timestamp": "2024-12-16T01:00:00Z"
}
```

#### 6.1.3 Authentication and Security

**OAuth 2.0 Flow**
1. User initiates login
2. Redirect to Google OAuth
3. Receive authorization code
4. Exchange for access/refresh tokens
5. Store encrypted tokens
6. Use JWT for session management

**Security Measures**
- HTTPS enforcement
- Token encryption at rest
- Rate limiting per user
- Input validation and sanitization
- SQL injection prevention
- XSS protection

### 6.2 Advanced API Features

#### 6.2.1 Batch Processing
Large operations are handled via batch processing:

```python
@email_bp.route('/process-batch', methods=['POST'])
def process_email_batch():
    # Queue emails for background processing
    batch_id = queue_batch_processing(email_ids)
    
    # Return batch tracking ID
    return jsonify({
        'success': True,
        'batch_id': batch_id,
        'status_url': f'/api/email/batch-status/{batch_id}'
    })
```

#### 6.2.2 Real-Time Updates
WebSocket integration provides real-time updates:

```python
def notify_processing_progress(user_id, progress_data):
    socketio.emit('processing_update', progress_data, room=f'user_{user_id}')
```

## 7. AI/ML INTEGRATION

### 7.1 Claude 4 Sonnet Integration

#### 7.1.1 API Client Configuration
```python
class ClaudeClient:
    def __init__(self, api_key):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.default_model = "claude-3-5-sonnet-20241022"
        self.max_tokens = 4000
    
    def analyze_with_context(self, prompt, context_data):
        enhanced_prompt = self.build_contextual_prompt(prompt, context_data)
        
        response = self.client.messages.create(
            model=self.default_model,
            max_tokens=self.max_tokens,
            messages=[{"role": "user", "content": enhanced_prompt}]
        )
        
        return self.parse_structured_response(response)
```

#### 7.1.2 Prompt Engineering Strategies

**Context-Aware Prompts**
All prompts include relevant business context:
- User's business domain
- Existing knowledge tree structure
- Historical relationship patterns
- Strategic priorities

**Structured Output Formatting**
Prompts are designed to return structured JSON:
```python
def create_structured_prompt(base_prompt, expected_schema):
    return f"""
    {base_prompt}
    
    Please respond in this exact JSON format:
    {json.dumps(expected_schema, indent=2)}
    
    Ensure all fields are populated with relevant, accurate information.
    """
```

**Chain-of-Thought Reasoning**
Complex analysis uses multi-step reasoning:
1. Analyze individual components
2. Identify relationships and patterns
3. Synthesize insights
4. Generate actionable recommendations

### 7.2 Advanced AI Features

#### 7.2.1 Semantic Consistency Engine
Ensures consistent categorization across the system:

```python
class SemanticConsistencyEngine:
    def __init__(self, knowledge_tree):
        self.tree = knowledge_tree
        self.similarity_threshold = 0.85
    
    def ensure_consistency(self, new_classification):
        # Check against existing categories
        similar_categories = self.find_similar_categories(new_classification)
        
        if similar_categories:
            return self.merge_or_redirect(new_classification, similar_categories)
        
        return self.validate_new_category(new_classification)
```

#### 7.2.2 Confidence Scoring System
All AI outputs include confidence scores:

```python
class ConfidenceScorer:
    def calculate_confidence(self, analysis_result):
        factors = {
            'source_quality': self.assess_source_quality(analysis_result.sources),
            'pattern_strength': self.measure_pattern_strength(analysis_result.patterns),
            'context_completeness': self.evaluate_context(analysis_result.context),
            'consistency': self.check_consistency(analysis_result.classification)
        }
        
        # Weighted confidence calculation
        confidence = sum(score * weight for score, weight in factors.items())
        return min(1.0, max(0.0, confidence))
```

## 8. PERFORMANCE AND SCALABILITY

### 8.1 Performance Optimization Strategies

#### 8.1.1 Database Optimization
- Strategic indexing for common query patterns
- Query optimization and execution plan analysis
- Connection pooling and prepared statements
- Materialized views for complex aggregations

#### 8.1.2 Caching Architecture
```python
class CacheManager:
    def __init__(self):
        self.memory_cache = {}  # In-memory for hot data
        self.redis_cache = redis.Redis()  # Distributed cache
        self.file_cache = {}  # File system cache
    
    def get_with_fallback(self, key, fetch_function):
        # Try memory cache first
        if key in self.memory_cache:
            return self.memory_cache[key]
        
        # Try Redis cache
        redis_value = self.redis_cache.get(key)
        if redis_value:
            self.memory_cache[key] = redis_value
            return redis_value
        
        # Fetch and cache
        value = fetch_function()
        self.cache_value(key, value)
        return value
```

#### 8.1.3 Background Processing
CPU-intensive operations run in background:

```python
@celery.task
def process_email_intelligence(email_id, user_id):
    # Long-running AI analysis
    analysis = perform_deep_analysis(email_id)
    
    # Update database with results
    store_analysis_results(email_id, analysis)
    
    # Notify frontend of completion
    notify_analysis_complete(user_id, email_id)
```

### 8.2 Scalability Considerations

#### 8.2.1 Horizontal Scaling Architecture
- Stateless API servers
- Database read replicas
- Load balancing strategies
- Microservices decomposition potential

#### 8.2.2 Resource Management
- Memory usage optimization
- CPU utilization monitoring
- Storage growth management
- API rate limiting

## 9. SECURITY AND PRIVACY

### 9.1 Data Protection

#### 9.1.1 Encryption Strategy
- Data at rest: AES-256 encryption
- Data in transit: TLS 1.3
- Token storage: Encrypted storage
- PII handling: Minimal data retention

#### 9.1.2 Privacy Compliance
- GDPR compliance measures
- Data minimization principles
- User consent management
- Right to deletion implementation

### 9.2 Security Architecture

#### 9.2.1 Authentication Security
```python
class SecurityManager:
    def validate_token(self, token):
        # JWT validation with expiration checking
        payload = jwt.decode(token, self.secret_key, algorithms=['HS256'])
        
        # Additional security checks
        if self.is_token_revoked(payload['jti']):
            raise SecurityException("Token revoked")
        
        if self.detect_suspicious_activity(payload['user_id']):
            raise SecurityException("Suspicious activity detected")
        
        return payload
```

#### 9.2.2 Input Validation
All user inputs are rigorously validated:
- SQL injection prevention
- XSS attack mitigation
- Command injection protection
- File upload security

## 10. DEPLOYMENT AND OPERATIONS

### 10.1 Deployment Architecture

#### 10.1.1 Containerization Strategy
```dockerfile
# Production-ready Docker configuration
FROM python:3.9-slim

# Security: Non-root user
RUN useradd --create-home --shell /bin/bash app

# Install dependencies
COPY requirements.txt /tmp/
RUN pip install --no-cache-dir -r /tmp/requirements.txt

# Copy application
COPY --chown=app:app . /app
WORKDIR /app

# Switch to non-root user
USER app

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s \
  CMD curl -f http://localhost:5000/health || exit 1

# Start application
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "main:app"]
```

#### 10.1.2 Environment Configuration
```python
class Config:
    # Database
    DATABASE_URL = os.getenv('DATABASE_URL')
    DATABASE_POOL_SIZE = int(os.getenv('DATABASE_POOL_SIZE', '10'))
    
    # AI Services
    ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')
    
    # Google APIs
    GOOGLE_CLIENT_ID = os.getenv('GOOGLE_CLIENT_ID')
    GOOGLE_CLIENT_SECRET = os.getenv('GOOGLE_CLIENT_SECRET')
    
    # Security
    SECRET_KEY = os.getenv('SECRET_KEY')
    JWT_EXPIRATION = int(os.getenv('JWT_EXPIRATION', '3600'))
    
    # Performance
    CACHE_TTL = int(os.getenv('CACHE_TTL', '300'))
    MAX_WORKERS = int(os.getenv('MAX_WORKERS', '4'))
```

### 10.2 Monitoring and Logging

#### 10.2.1 Comprehensive Logging
```python
import structlog

logger = structlog.get_logger()

def process_email_with_logging(email_id, user_id):
    log = logger.bind(email_id=email_id, user_id=user_id)
    
    log.info("Starting email processing")
    
    try:
        result = process_email(email_id)
        log.info("Email processed successfully", 
                processing_time=result.processing_time,
                entities_extracted=len(result.entities))
        return result
    
    except Exception as e:
        log.error("Email processing failed", 
                 error=str(e), 
                 traceback=traceback.format_exc())
        raise
```

#### 10.2.2 Performance Monitoring
- Response time tracking
- Database query performance
- AI API usage monitoring
- Resource utilization alerts

## 11. FUTURE ROADMAP AND EXTENSIBILITY

### 11.1 Planned Enhancements

#### 11.1.1 Advanced AI Features
- **Multi-modal Analysis**: Process attachments, images, documents
- **Predictive Analytics**: Forecast relationship changes, project outcomes
- **Automated Workflows**: AI-driven task automation
- **Voice Integration**: Voice commands and responses

#### 11.1.2 Integration Expansion
- **Slack Integration**: Include team communication analysis
- **CRM Synchronization**: Bidirectional sync with Salesforce, HubSpot
- **Calendar Intelligence**: Advanced meeting preparation and follow-up
- **Document Analysis**: Process contracts, proposals, reports

#### 11.1.3 Business Intelligence
- **Custom Dashboards**: User-configurable analytics views
- **Trend Analysis**: Long-term pattern recognition
- **Competitive Intelligence**: External data integration
- **ROI Measurement**: Quantify relationship value

### 11.2 Technical Evolution

#### 11.2.1 Architecture Improvements
- **Microservices Migration**: Decompose monolith for better scalability
- **Event-Driven Architecture**: Real-time processing improvements
- **Edge Computing**: Reduce latency for global users
- **Blockchain Integration**: Secure, verifiable relationship history

#### 11.2.2 AI Model Evolution
- **Custom Model Training**: User-specific AI optimization
- **Federated Learning**: Privacy-preserving model improvements
- **Multi-Agent Systems**: Specialized AI agents for different domains
- **Continuous Learning**: Models that improve with usage

## 12. BUSINESS VALUE AND METRICS

### 12.1 Key Performance Indicators

#### 12.1.1 User Engagement Metrics
- **Daily Active Users (DAU)**
- **Session Duration and Frequency**
- **Feature Adoption Rates**
- **User Retention Curves**

#### 12.1.2 Intelligence Quality Metrics
- **Classification Accuracy**
- **Task Completion Rates**
- **Relationship Insights Accuracy**
- **User Satisfaction Scores**

#### 12.1.3 Business Impact Metrics
- **Time Saved per User**
- **Relationship Quality Improvement**
- **Project Success Rates**
- **Revenue Attribution**

### 12.2 Competitive Advantages

#### 12.2.1 Technical Differentiators
- **Knowledge Tree First Architecture**: Unique semantic consistency approach
- **Contact Tier Classification**: Sophisticated relationship intelligence
- **Tactical Task Extraction**: High-precision, actionable task identification
- **External Prompt Management**: Rapid AI iteration capabilities

#### 12.2.2 Business Differentiators
- **Executive-Focused Design**: Built for high-level decision makers
- **Privacy-First Approach**: On-premise deployment options
- **Deep Context Understanding**: Goes beyond simple email parsing
- **Proactive Intelligence**: Anticipates needs rather than reactive responses

## 13. CONCLUSION

The AI Chief of Staff represents a paradigm shift in business intelligence tools. By implementing a "Knowledge Tree First" architecture, sophisticated contact tier classification, and tactical task extraction, it transforms raw email and calendar data into actionable business intelligence.

The system's technical architecture balances sophistication with maintainability, leveraging cutting-edge AI while maintaining human oversight and control. The external prompt management system enables rapid iteration and customization, while the unified entity engine ensures consistent, high-quality data extraction.

Key innovations include:

1. **Semantic Consistency**: The knowledge tree ensures all categorization follows a coherent taxonomy
2. **Relationship Intelligence**: Contact tiers filter noise and focus on valuable relationships
3. **Tactical Precision**: High-confidence task extraction prevents information overload
4. **Transparent AI**: Every insight traces back to source material for verification
5. **Executive-Grade UX**: Designed for busy professionals who need quick, accurate insights

The platform scales from individual executives to enterprise teams, with robust security, privacy protection, and performance optimization. Future enhancements will expand AI capabilities, integration breadth, and business intelligence depth.

This technical specification serves as a comprehensive reference for understanding, extending, and optimizing the AI Chief of Staff platform. The architecture supports rapid development, reliable operation, and continuous improvement to meet evolving user needs in the dynamic world of business communication and relationship management.

---

**Document Version**: 2.0
**Last Updated**: December 2024
**Classification**: Technical Specification
**Audience**: AI Training, Development Team, Technical Stakeholders 

============================================================
FILE: archive/ARCHIVE_SUMMARY.md
============================================================
# Archive Summary - AI Chief of Staff Project Cleanup

**Date:** $(date)
**Total Files Archived:** 59 files (~1.5MB)

## Summary
This archive contains files that were identified as unused or obsolete after the AI Chief of Staff application was refactored from a Flask template-based architecture to a React frontend with API-only backend.

## Archived Categories

### 1. HTML Templates (`html_templates/`)
**Why Archived:** The application now uses React exclusively for the frontend. The main.py file redirects all routes to `localhost:3000` (React dev server), making these HTML templates unused.

**Files Include:**
- All Flask Jinja2 templates (dashboard.html, tasks.html, people.html, etc.)
- Base templates and error pages
- Enhanced template variants

### 2. Backup Files (`backup_files/`)
**Why Archived:** These contain old versions of the codebase from previous iterations.

**Files Include:**
- v1_original/ directory with legacy main.py
- Old main.py file from chief_of_staff_ai/

### 3. Old Documentation (`old_docs/`)
**Why Archived:** Planning documents, analysis files, and outdated specifications that are no longer relevant to the current architecture.

**Files Include:**
- MISSING_IMPLEMENTATION_ANALYSIS.md
- FINAL_REFACTOR_ASSESSMENT.md
- MIGRATION_LOG.md
- HOW_IT_WORKS_FOR_KIDS.txt
- Architecture_Reality_v1.txt
- Priority_Fixes_v1.txt
- Various planning and specification documents

### 4. Test Files (`test_files/`)
**Why Archived:** Debug utilities and test files that aren't part of the production application.

**Files Include:**
- debug_models.py
- debug_import.py
- test_models.py
- test_import.py
- fix_model_imports.py
- run.py (standalone utility)

### 5. Static Assets (`static_assets/`)
**Why Archived:** JavaScript files that were part of the old template-based frontend, now replaced by React components.

**Files Include:**
- knowledge.js (52KB of legacy JavaScript)

### 6. Cache Files (`cache_files/`)
**Why Archived:** System-generated files that should not be in version control.

**Files Include:**
- __pycache__ directories
- .DS_Store files (Mac system files)

### 7. Refactor Documentation (`refactor_docs/`)
**Why Archived:** Planning documents used during the refactoring process that are no longer needed.

**Files Include:**
- instructions.txt
- Enhanced database specs
- API integration layer plans
- Frontend enhancement plans
- Analytics engine specifications

## Current Active Architecture

The cleaned-up project now focuses on:

1. **Backend:** Flask API with modular blueprints (`api/routes/`)
2. **Frontend:** React TypeScript application (`frontend/`)
3. **AI Integration:** Claude Sonnet integration for intelligence processing
4. **Data Processing:** Email quality filtering and knowledge tree building
5. **Core Features:** 
   - Contact tier classification
   - Tactical task extraction
   - People augmentation with AI
   - Three-step workflow orchestration

## Restoration Notes

If any archived files are needed in the future:
1. Check this summary to understand why they were archived
2. Consider if the current React-based architecture can achieve the same functionality
3. Only restore if absolutely necessary for specific functionality
4. Update any restored files to work with the current architecture

## Empty Directories Removed

The following empty directories were also cleaned up:
- chief_of_staff_ai/templates
- chief_of_staff_ai/backup
- chief_of_staff_ai/refactor
- test_files/Spec_and_architecture_OLD
- static/js
- chief_of_staff_ai/static/js
- chief_of_staff_ai/static/css 

============================================================
FILE: archive/AI_Chief_of_Staff_How_It_Works.txt
============================================================
=============================================================================
                    AI CHIEF OF STAFF - HOW IT WORKS
                         End-to-End Product Overview
=============================================================================

WHAT IS IT?
-----------
An AI-powered personal assistant that reads your emails, understands your 
business relationships, and helps you make better decisions. Think of it as 
having a smart assistant who knows everyone you work with and can spot 
important patterns in your communications.


HOW IT WORKS (STEP BY STEP):
============================

STEP 1: LOGIN & SETUP
----------------------
• You login with your Google account
• The system gets permission to read your Gmail and Google Calendar
• It creates a secure profile just for you

STEP 2: EMAIL QUALITY FILTERING (THE SECRET SAUCE)
--------------------------------------------------
Before processing ANY emails, the system does something smart:

• It analyzes who you actually respond to vs who you ignore
• It creates 3 tiers of contacts:
  - TIER 1: People you regularly respond to (high quality)
  - TIER 2: New contacts or people you sometimes respond to (medium quality)  
  - TIER LAST: People you consistently ignore (low quality = spam/noise)

• ONLY emails from Tier 1 and Tier 2 contacts get processed by AI
• Tier LAST emails are filtered out completely (no "shitty content" processed)

This means the AI only learns from your REAL business relationships, not spam.

STEP 3: EMAIL SYNC & AI PROCESSING
----------------------------------
• System fetches your recent emails (you control how many and how far back)
• For each QUALITY email (Tier 1/2 only):
  - Claude AI reads and understands the content
  - Extracts important people, topics, and action items
  - Creates tasks if someone asked you to do something
  - Identifies business opportunities and relationships
  - Builds a knowledge base of your professional world

• Emails from people you ignore (Tier LAST) are stored but NOT processed by AI

STEP 4: INTELLIGENCE GENERATION
-------------------------------
The AI continuously analyzes your data to create:

• STRATEGIC INSIGHTS: "You have 3 high-priority decisions pending"
• RELATIONSHIP INTELLIGENCE: "You haven't contacted John from BigCorp in 30 days"
• BUSINESS OPPORTUNITIES: "5 people mentioned the new product launch"
• MEETING PREPARATION: Auto-generates prep materials for upcoming meetings
• TASK EXTRACTION: Finds action items people asked you to do

STEP 5: CALENDAR INTELLIGENCE
-----------------------------
• Syncs with Google Calendar
• Analyzes upcoming meetings
• Creates preparation tasks automatically
• Provides context about who you're meeting with
• Suggests discussion points based on recent emails

STEP 6: SMART DASHBOARD
-----------------------
You get a beautiful web interface that shows:

• DASHBOARD: High-level overview of insights and metrics
• TASKS: Action items extracted from emails with full context
• PEOPLE: Your professional network with engagement scores and relationship intelligence
• CALENDAR: Upcoming meetings with AI-generated preparation notes
• TOPICS: Business themes and projects the AI has identified
• SETTINGS: Control sync frequency, view contact tiers, clean up data


KEY FEATURES THAT MAKE IT SMART:
===============================

1. CONTACT TIER FILTERING
   - No spam or noise in your business intelligence
   - Only processes emails from people you actually engage with
   - Automatically learns your communication patterns

2. RELATIONSHIP INTELLIGENCE
   - Tracks engagement scores for each contact
   - Warns you when important relationships need attention
   - Provides context for every person in your network

3. PROACTIVE INSIGHTS
   - Spots patterns you might miss
   - Alerts you to urgent decisions
   - Identifies business opportunities automatically

4. CONTEXTUAL TASK EXTRACTION
   - Finds action items in emails with full background
   - Explains WHY each task is important
   - Connects tasks to relationships and projects

5. MEETING PREPARATION
   - Auto-generates prep materials
   - Provides attendee intelligence
   - Creates relevant talking points

6. PRIVACY & SECURITY
   - All data stays in your private database
   - Uses Claude AI for processing (no data stored by Anthropic)
   - You control what gets synced and processed


EXAMPLE OF THE SYSTEM IN ACTION:
===============================

1. You receive 50 emails today
2. System filters: 
   - 35 emails from Tier 1/2 contacts → Processed by AI
   - 15 emails from Tier LAST contacts → Filtered out (saved but not analyzed)
3. AI analyzes the 35 quality emails and finds:
   - 3 action items you need to complete
   - 2 people asking for meetings
   - 1 urgent decision that needs your attention
   - 5 mentions of "Project Alpha" across different conversations
4. Dashboard shows:
   - "URGENT: 3 critical decisions pending"
   - "Relationship alert: Haven't responded to Sarah from ABC Corp"
   - "Project Alpha gaining momentum - 5 mentions this week"
5. For tomorrow's meeting with John:
   - AI finds your last 3 conversations with John
   - Generates prep notes about ongoing projects
   - Suggests discussion points based on recent emails
   - Creates task: "Follow up on budget approval John mentioned"


WHY IT'S BETTER THAN OTHER TOOLS:
=================================

• NO SPAM POLLUTION: Other tools process everything. This one is smart about 
  WHAT it processes, ensuring only real business intelligence.

• RELATIONSHIP-CENTRIC: Built around people and relationships, not just tasks.

• CONTEXTUAL INTELLIGENCE: Doesn't just extract tasks - explains WHY they 
  matter and HOW they connect to your business.

• PROACTIVE, NOT REACTIVE: Spots problems and opportunities before you do.

• PRIVACY-FIRST: Your data stays private and secure.


SETTINGS & CONTROL:
==================

You can:
• Control how many emails to sync (5-500)
• Set how far back to look (1-365 days)
• View your contact tiers and see who's filtered out
• Clean up existing data from low-quality contacts
• Flush and restart if needed
• Manual override contact tiers for special cases


THE BOTTOM LINE:
===============
This is like having a brilliant assistant who:
1. Reads all your emails (but ignores the junk)
2. Remembers everyone you work with
3. Spots important patterns and opportunities
4. Prepares you for meetings
5. Makes sure nothing important falls through the cracks
6. Helps you focus on what actually matters in your business

It turns your overwhelming email inbox into actionable business intelligence.

============================================================================= 

============================================================
FILE: archive/end_to_end_architecture.txt
============================================================
=============================================================================
                    AI CHIEF OF STAFF - END-TO-END ARCHITECTURE
                        Complete System Architecture Document
                           UPDATED: TACTICAL KNOWLEDGE TREE APPROACH
=============================================================================

PRODUCT OVERVIEW:
================
AI Chief of Staff is a sophisticated business intelligence platform that transforms 
your Gmail and Calendar into a proactive AI assistant. It reads your emails, 
understands your professional relationships, and provides strategic business insights 
through advanced AI processing powered by Claude 4 Sonnet.

**CRITICAL ARCHITECTURE: "KNOWLEDGE TREE FIRST" APPROACH**
===========================================================

Unlike traditional email processing systems, this platform:
1. **FIRST** builds a comprehensive master knowledge tree from ALL emails
2. **THEN** assigns individual emails to this tree structure  
3. **FINALLY** uses the knowledge tree to extract tactical tasks and augment data

This ensures consistency, context-awareness, and prevents over-generation of tasks.

ARCHITECTURE LAYERS:
===================

1. FRONTEND LAYER (React + TypeScript)
   - **Settings UI**: Reorganized 3-step knowledge-first workflow
   - **Dashboard**: Real-time business intelligence display
   - **Knowledge Tree Modal**: Tree visualization and management
   - **Chat Interface**: Context-aware AI assistant

2. BACKEND LAYER (Flask + Python)
   - **Email Routes**: Knowledge tree building and email assignment
   - **Task Routes**: NEW tactical task creation with confidence thresholds
   - **People Routes**: NEW knowledge-enhanced people augmentation
   - **Calendar Routes**: NEW meeting intelligence with attendee analysis
   - **Intelligence Routes**: Business insights and chat with knowledge context

3. AI PROCESSING LAYER (Claude 4 Sonnet)
   - **External Prompt Management**: All prompts now in prompts/ directory
   - **Knowledge Tree Prompts**: build_initial_tree.txt, refine_existing_tree.txt
   - **Tactical Task Prompts**: 360_task_extraction.txt with high selectivity
   - **Business Intelligence**: Enhanced context-aware processing

4. DATA INTEGRATION LAYER
   - **Gmail API**: Email fetching with quality filtering
   - **Calendar API**: Meeting data with attendee intelligence
   - **Contact Analysis**: Tier-based filtering (Tier 1/2/Last)

5. STORAGE LAYER (SQLite/PostgreSQL)
   - **Knowledge Trees**: JSON storage with user-specific trees
   - **Email Processing**: Strategic importance scoring
   - **Enhanced People**: Knowledge tree augmented profiles
   - **Tactical Tasks**: High-confidence, context-aware tasks

6. INTELLIGENCE LAYER
   - **Master Knowledge Tree**: Central business context repository
   - **Tactical Task Engine**: 0.7+ confidence threshold
   - **People Augmentation**: Relationship and project mapping
   - **Meeting Intelligence**: Attendee analysis and prep tasks

THE NEW WORKFLOW:
================

**STEP 1: BUILD CONTACT BASE**
🔄 Fetch Sent Emails → 🔄 Build Contact Rules → 🔄 Fetch Calendar Events

Files Involved:
- `api/routes/email_routes.py`: extract_sent_contacts endpoint
- `chief_of_staff_ai/engagement_analysis/smart_contact_strategy.py`
- `api/routes/calendar_routes.py`: calendar sync

Story: System analyzes sent emails to identify trusted contacts (Tier 1), 
builds contact tier rules, and fetches calendar events for relationship context.

**STEP 2: BUILD KNOWLEDGE TREE** (CORE INNOVATION)
🔄 Fetch All Emails → 🌳 Build/Refine Tree → 📧 Assign Emails to Tree

Files Involved:
- `api/routes/email_routes.py`: 
  * `build_knowledge_tree()` - Creates master tree
  * `assign_emails_to_tree()` - Categorizes emails
- `prompts/knowledge_tree/build_initial_tree.txt`
- `prompts/knowledge_tree/refine_existing_tree.txt`
- `prompts/knowledge_tree/assign_email_to_tree.txt`
- `knowledge_trees/user_X_master_tree.json`

Story: 
1. System fetches ALL emails as one mega corpus
2. Claude analyzes complete email history and builds MASTER knowledge tree with:
   - Topics & subtopics (business themes)
   - People & relationships (key contacts)
   - Projects & initiatives (ongoing work)
   - Business context (industry, role, focus areas)
3. Master tree saved as JSON structure
4. Individual emails assigned to appropriate tree categories
5. Each email gets: primary_topic, related_people, related_projects, importance_score

**STEP 3: AUGMENT WITH KNOWLEDGE** (TACTICAL ENHANCEMENT)
✅ Create Tactical Tasks → 👥 Augment People → 📅 Augment Meetings

Files Involved:
- `api/routes/task_routes.py`: create_tactical_tasks endpoint
- `api/routes/people_routes.py`: augment_people_with_knowledge endpoint  
- `api/routes/calendar_routes.py`: augment_meetings_with_knowledge endpoint
- `prompts/task_extraction/360_task_extraction.txt` (enhanced for tactical focus)

Story:
1. **Create Tactical Tasks**: Uses knowledge tree to extract ONLY obvious, high-confidence 
   tasks (0.7+ threshold) from categorized emails. Focuses on tactical deliverables, 
   not strategic follow-ups.

2. **Augment People**: Enhances people profiles with knowledge tree data:
   - Role and company information
   - Relationship strength scores
   - Primary business topics
   - Related projects and strategic importance

3. **Augment Meetings**: Enhances calendar events with:
   - Attendee intelligence from knowledge tree
   - Related topics and projects
   - Automatically generated preparation tasks
   - Strategic meeting classification

KEY ARCHITECTURAL IMPROVEMENTS:
==============================

**1. TACTICAL TASK EXTRACTION**
   - Confidence threshold: 0.7+ (vs previous 0.5)
   - Rejects vague "follow up" tasks
   - Focuses on concrete deliverables
   - Uses knowledge tree context for prioritization

**2. PROMPT EXTERNALIZATION**
   - All AI prompts moved to `prompts/` directory
   - Easy editing without code changes
   - Documented goals and context for each prompt
   - Version control for prompt evolution

**3. KNOWLEDGE-FIRST PROCESSING**
   - Master tree built from complete email corpus
   - Consistent categorization across all emails
   - Context-aware task extraction
   - Strategic relationship mapping

**4. ENHANCED DATA QUALITY**
   - Contact tier filtering (removes low-value contacts)
   - Strategic importance scoring
   - Business context augmentation
   - Meeting preparation automation

BUSINESS INTELLIGENCE OUTPUT:
============================

After complete processing, the system provides:

**📊 Dashboard Metrics**
- Real-time intelligence quality scores
- Contact tier statistics
- Knowledge tree coverage
- Task completion analytics

**👥 People Intelligence**
- Relationship strength mapping
- Business context profiles
- Project collaboration analysis
- Strategic contact identification

**📧 Email Intelligence**  
- Topic-based categorization
- Strategic importance scoring
- Automated task extraction
- Business context analysis

**📅 Meeting Intelligence**
- Attendee relationship analysis
- Topic and project connections
- Automated preparation tasks
- Strategic meeting identification

**💬 AI Chat with Context**
- Knowledge tree-aware responses
- Business intelligence integration
- Strategic decision support
- Relationship-informed advice

FILES SUMMARY:
=============

**Core Processing Files:**
- `api/routes/email_routes.py` - Knowledge tree building and management
- `api/routes/task_routes.py` - Tactical task creation
- `api/routes/people_routes.py` - People augmentation
- `api/routes/calendar_routes.py` - Meeting intelligence
- `frontend/src/components/SettingsPage.tsx` - Updated workflow UI

**Prompt Files:**
- `prompts/knowledge_tree/` - Master tree building prompts
- `prompts/task_extraction/360_task_extraction.txt` - Tactical task prompt
- `prompts/email_intelligence/` - Business intelligence prompts
- `prompts/prompt_loader.py` - Centralized prompt management

**Storage:**
- `knowledge_trees/user_X_master_tree.json` - Master knowledge trees
- Database tables: emails, people, tasks, calendar_events (enhanced)

**THE COMPLETE STORY:**
======================

1. **Foundation Phase**: System builds trusted contact base from sent emails and calendar

2. **Knowledge Tree Phase**: Claude analyzes ALL emails together, creates master 
   knowledge structure, then assigns each email to appropriate categories

3. **Tactical Enhancement Phase**: Using knowledge tree context, system creates 
   high-confidence tactical tasks, enhances people profiles, and augments meetings 
   with intelligence

4. **Continuous Intelligence**: AI chat and dashboard provide knowledge-aware 
   insights and strategic support

**RESULT**: A business intelligence system that truly "gets" your business context 
and provides tactical, actionable insights rather than generic email processing.


DETAILED COMPONENT BREAKDOWN:
============================

FRONTEND COMPONENTS (frontend/):
-------------------------------
• frontend/src/App.tsx (102KB, 2,595 lines)
  - Main React application component
  - Dashboard with intelligence metrics and insights
  - Real-time UI updates and state management
  - Component routing and user interface orchestration
  - **KnowledgeTreeModal component** for viewing the master tree structure

• frontend/src/components/ (Directory)
  - Modular React components for each feature
  - Dashboard widgets, task lists, people grids
  - Calendar views, settings panels, chat interface

• frontend/package.json
  - React dependencies and build configuration
  - Scripts for development and production builds

• frontend/tailwind.config.js + postcss.config.js
  - Modern CSS framework configuration
  - Responsive design and theming system


BACKEND CORE FILES:
------------------
• main.py (678 lines)
  - Flask application entry point and configuration
  - Google OAuth authentication flow
  - Session management with user isolation
  - Strategic business insights generation engine
  - Route handlers redirecting to React frontend

• api/ (Directory - Modular API Architecture)
  - api/routes/auth_routes.py - Authentication endpoints
  - **api/routes/email_routes.py** - Email sync, knowledge tree building, and categorization
  - api/routes/intelligence_routes.py - AI insights and analytics
  - api/routes/task_routes.py - Task management and extraction
  - api/routes/people_routes.py - Relationship intelligence
  - api/routes/calendar_routes.py - Meeting preparation and analysis
  - api/routes/settings_routes.py - User preferences and configuration


KNOWLEDGE TREE SYSTEM (THE CORE INNOVATION):
============================================

**Master Knowledge Tree Files (knowledge_trees/):**
• knowledge_trees/user_{id}_master_tree.json
  - Comprehensive topic hierarchy built from ALL emails
  - People relationships and roles mapped to topics
  - Projects and initiatives with associated people/topics
  - Business context (industry, role, company stage, focus areas)
  - Relationship mappings between all entities

**Key Functions in api/routes/email_routes.py:**
• build_initial_knowledge_tree() - Creates master tree from email corpus
• refine_knowledge_tree() - Updates tree when new themes emerge  
• assign_email_to_knowledge_tree() - Categorizes individual emails using tree
• get_master_knowledge_tree() - Retrieves stored tree for user
• save_master_knowledge_tree() - Persists tree structure

**API Endpoints for Knowledge Tree Operations:**
• POST /api/emails/build-knowledge-tree - Build/refine master tree
• POST /api/emails/assign-to-tree - Categorize emails using existing tree
• GET /api/emails/knowledge-tree - Get current master tree structure


AI PROCESSING ENGINES (chief_of_staff_ai/processors/):
-----------------------------------------------------
• email_intelligence.py (100KB, 1,996 lines)
  - **ENHANCED**: Uses master knowledge tree as context for email analysis
  - Strategic importance scoring within established topic framework
  - Task extraction linked to pre-defined projects and topics
  - Relationship intelligence mapped to known entity network

• email_quality_filter.py (32KB, 723 lines)
  - Contact tier classification (Tier 1/2/Last)
  - Response rate analysis and engagement scoring
  - Quality filtering to prevent spam pollution
  - Smart contact relationship management

• knowledge_engine.py (39KB, 914 lines)
  - **CRITICAL**: Builds foundation knowledge hierarchy from email corpus
  - build_topic_hierarchy_from_content() - Creates comprehensive topic tree
  - Topic relationship analysis and hierarchical structuring
  - Integration with tree-based categorization system

• intelligence_engine.py (30KB, 651 lines)
  - Business intelligence generation using knowledge tree context
  - Relationship decay prediction within established network
  - Opportunity detection using topic momentum from tree
  - Cross-entity analysis using pre-mapped relationships

• task_extractor.py (49KB, 1,157 lines)
  - **TREE-CONTEXTUAL**: Task identification using knowledge tree categories
  - Links extracted tasks to pre-defined projects and topics
  - Priority scoring within business context from tree
  - Action item categorization using established taxonomy

• realtime_processor.py (33KB, 755 lines)
  - Continuous processing using established knowledge tree
  - Real-time categorization of new emails against master tree
  - Live updates to topic momentum and relationship scores
  - Event-driven updates within consistent categorization framework


DATABASE MODELS (chief_of_staff_ai/models/):
-------------------------------------------
• database.py (106KB, 2,418 lines)
  - Core SQLAlchemy models and database schema
  - User, Email, Person, Task, Project, Topic entities
  - Complex relationships and foreign key constraints
  - Database manager with user isolation and security

• enhanced_models.py (19KB, 450 lines)
  - Intelligence-enhanced model extensions
  - **TopicHierarchy model** for structured knowledge tree storage
  - Engagement scoring and relationship metrics
  - AI metadata and processing status tracking

• knowledge_models.py (17KB, 436 lines)
  - **TopicHierarchy class** with hierarchical relationships
  - Knowledge graph and entity relationship models
  - Topic clustering and theme analysis structures
  - Tree-based knowledge storage and retrieval


CORRECTED END-TO-END PROCESS FLOW STORY:
========================================

STEP 1: USER AUTHENTICATION & SETUP
------------------------------------
Files Involved:
- main.py (routes: /auth/google, /auth/google/callback)
- chief_of_staff_ai/auth/gmail_auth.py
- frontend/src/App.tsx (login components)

Story:
1. User visits the application at localhost:5000
2. main.py redirects to React frontend at localhost:3000
3. React app detects no authentication, shows login button
4. User clicks "Sign in with Google", triggering /auth/google
5. Gmail auth handler generates OAuth URL with proper scopes
6. User authorizes Gmail and Calendar access on Google
7. Google redirects back to /auth/google/callback with authorization code
8. Backend exchanges code for access tokens
9. User profile is created/retrieved from database.py models
10. Secure session established with user isolation
11. React frontend receives authentication success and loads dashboard

STEP 2: EMAIL FETCHING & QUALITY FILTERING
------------------------------------------
Files Involved:
- chief_of_staff_ai/processors/email_quality_filter.py
- chief_of_staff_ai/ingest/gmail_fetcher.py
- api/routes/email_routes.py

Story:
1. User clicks "Sync Intelligence" button in React frontend
2. Frontend sends POST request to /api/trigger-email-sync
3. Gmail fetcher connects to Gmail API using stored OAuth tokens
4. System fetches ALL emails in bulk (configurable: 5-500 emails)
5. Email quality filter analyzes user's response patterns:
   - Tier 1: People user regularly responds to (high quality)
   - Tier 2: New contacts or occasional responses (medium quality)
   - Tier Last: People user consistently ignores (filtered out)
6. Quality-filtered emails are prepared for master tree building
7. Contact tier data is saved to database for future filtering

**STEP 3: MASTER KNOWLEDGE TREE GENERATION (THE KEY INNOVATION)**
----------------------------------------------------------------
Files Involved:
- api/routes/email_routes.py (build_initial_knowledge_tree function)
- knowledge_trees/user_{id}_master_tree.json
- Anthropic Claude 4 Sonnet API

Story:
1. **MEGA FILE CREATION**: All quality-filtered emails are bundled into one comprehensive dataset
2. **MASTER TREE PROMPT**: System sends the ENTIRE email corpus to Claude with this instruction:
   "Build a MASTER knowledge tree that will be used to categorize ALL future emails consistently"
3. **COMPREHENSIVE ANALYSIS**: Claude analyzes the complete communication history and creates:
   - **Topics & Subtopics**: Hierarchical business themes specific to user's work
   - **People & Relationships**: Key contacts mapped to topics with relationship strength
   - **Projects & Initiatives**: Active work mapped to people and topics
   - **Business Context**: Industry, role, company stage, key focus areas
   - **Entity Relationships**: How all components connect together
4. **TREE STRUCTURE CREATION**: Claude returns a comprehensive JSON structure with:
   ```json
   {
     "topics": [{"name": "Fundraising", "subtopics": [...], "importance": 0.9}],
     "people": [{"email": "investor@vc.com", "primary_topics": ["Fundraising"]}],
     "projects": [{"name": "Series A", "related_topics": ["Fundraising"]}],
     "business_context": {"industry": "Technology", "role": "Founder"}
   }
   ```
5. **TREE PERSISTENCE**: Master tree saved to knowledge_trees/user_{id}_master_tree.json
6. **FOUNDATION ESTABLISHED**: This tree becomes the authoritative categorization framework

**STEP 4: EMAIL CATEGORIZATION USING MASTER TREE (THE CONSISTENCY ENGINE)**
--------------------------------------------------------------------------
Files Involved:
- api/routes/email_routes.py (assign_email_to_knowledge_tree function)
- knowledge_trees/user_{id}_master_tree.json (as context)
- Anthropic Claude 4 Sonnet API

Story:
1. **INDIVIDUAL EMAIL PROCESSING**: For each email, system sends BOTH:
   - The individual email content
   - The COMPLETE master knowledge tree as context
2. **TREE-CONSTRAINED CATEGORIZATION**: Claude prompt specifically instructs:
   "Use ONLY topics, people, and projects that exist in the provided knowledge tree"
3. **CONSISTENT CLASSIFICATION**: Each email gets categorized with:
   - Primary topic (must exist in tree)
   - Secondary topics (must exist in tree)
   - Related people (must exist in tree)
   - Related projects (must exist in tree)
   - Summary contextualizing email within the established knowledge framework
4. **STRUCTURED INSIGHTS**: Every email analysis is consistent because it uses the same taxonomy
5. **CUMULATIVE INTELLIGENCE**: Insights build upon each other within consistent categories

**STEP 5: KNOWLEDGE TREE REFINEMENT & EVOLUTION**
------------------------------------------------
Files Involved:
- api/routes/email_routes.py (refine_knowledge_tree function)
- knowledge_trees/user_{id}_master_tree.json (updated)

Story:
1. **NEW EMAIL BATCHES**: When processing new emails, system checks if new themes emerge
2. **TREE EVALUATION**: System determines if master tree needs updating based on:
   - New people appearing frequently
   - New topics that don't fit existing categories
   - Changes in business focus or priorities
3. **INTELLIGENT REFINEMENT**: If updates needed, Claude receives:
   - Current master tree
   - New emails that suggest changes
   - Instructions to refine (not rebuild) the tree
4. **EVOLUTIONARY UPDATES**: Tree evolves while maintaining consistency:
   - New categories added only when genuinely needed
   - Existing categories preserved for consistency
   - Importance scores updated based on new activity
5. **MAINTAINED COHERENCE**: All previously categorized emails remain valid within updated tree

STEP 6: STRATEGIC BUSINESS INSIGHTS WITH TREE CONTEXT
----------------------------------------------------
Files Involved:
- main.py (get_strategic_business_insights function)
- chief_of_staff_ai/processors/intelligence_engine.py
- knowledge_trees/user_{id}_master_tree.json (as authoritative source)

Story:
1. **TREE-BASED INTELLIGENCE**: All strategic insights generated using established knowledge tree
2. **CONSISTENT CATEGORIES**: Business insights reference known topics, people, and projects
3. **CROSS-TREE ANALYSIS**: System identifies patterns across tree categories:
   - Which topics have high activity but low attention
   - Which relationships are decaying within known network
   - Which projects need attention based on established priorities
4. **CONTEXTUAL RECOMMENDATIONS**: All suggestions use the established business vocabulary
5. **CUMULATIVE LEARNING**: Insights improve because they build on consistent categorization

STEP 7: REAL-TIME PROCESSING WITH ESTABLISHED FRAMEWORK
------------------------------------------------------
Files Involved:
- chief_of_staff_ai/processors/realtime_processor.py
- knowledge_trees/user_{id}_master_tree.json (continuous reference)

Story:
1. **NEW EMAIL ARRIVAL**: When new emails arrive, they're immediately categorized using existing tree
2. **INSTANT CLASSIFICATION**: No need to "learn" categories - they already exist in the tree
3. **CONSISTENT UPDATES**: All metrics and insights updated within established framework
4. **COHERENT INTELLIGENCE**: New insights connect to existing knowledge structure
5. **MAINTAINED CONTEXT**: User sees consistent categories and relationships over time


KEY ARCHITECTURAL ADVANTAGES OF KNOWLEDGE TREE FIRST APPROACH:
=============================================================

1. **CONSISTENT CATEGORIZATION**
   - All emails use the same taxonomy and vocabulary
   - No category drift or inconsistent naming over time
   - Business intelligence builds coherently over time

2. **COMPREHENSIVE UNDERSTANDING**
   - Tree built from complete communication history
   - Captures full business context, not just recent emails
   - Better topic hierarchy and relationship mapping

3. **SCALABLE FRAMEWORK**
   - New emails instantly fit into established categories
   - No need to rebuild understanding with each email
   - Consistent user experience regardless of volume

4. **INTELLIGENT EVOLUTION**
   - Tree can be refined when business changes
   - Maintains historical consistency while adapting
   - Prevents category explosion and confusion

5. **SUPERIOR BUSINESS INTELLIGENCE**
   - Cross-category analysis possible because categories are stable
   - Trend analysis meaningful because taxonomy is consistent
   - Strategic insights based on established business vocabulary


TECHNOLOGY STACK SUMMARY:
=========================

Frontend:
- React 18 with TypeScript
- Tailwind CSS for styling
- PostCSS for CSS processing
- Modern ES6+ JavaScript
- **Knowledge Tree Visualization Components**

Backend:
- Python 3.10+ with Flask
- SQLAlchemy for database ORM
- Flask-Session for session management
- Flask-CORS for cross-origin requests
- **JSON-based Knowledge Tree Storage**

AI & Integration:
- **Anthropic Claude 4 Sonnet API** (Two-stage processing: Tree Building + Email Categorization)
- Google Gmail API
- Google Calendar API
- Google OAuth 2.0

Database & Storage:
- SQLite for development with **TopicHierarchy models**
- **File-based knowledge tree storage** (knowledge_trees/ directory)
- Complex relational schema with tree integration
- User data isolation

DevOps & Deployment:
- Heroku deployment pipeline
- Environment configuration
- Automated dependency management
- Production monitoring and logging

=============================================================================
This architecture creates a sophisticated AI-powered business intelligence 
platform that uses a "Knowledge Tree First" approach to ensure consistent, 
comprehensive, and scalable email intelligence that builds coherently over time.

The key innovation is building the complete knowledge framework FIRST, then 
using it as an authoritative context for all subsequent email processing, 
ensuring consistent categorization and cumulative business intelligence.
============================================================================= 
FILE: archive/html_templates/topics.html - Standard .html file
FILE: archive/html_templates/profile.html - Standard .html file
FILE: archive/html_templates/people.html - Standard .html file
FILE: archive/html_templates/calendar.html - Standard .html file
FILE: archive/html_templates/tasks.html - Standard .html file
FILE: archive/html_templates/base.html - Standard .html file
FILE: archive/html_templates/500.html - Standard .html file
FILE: archive/html_templates/404.html - Standard .html file
FILE: archive/html_templates/login.html - Standard .html file
FILE: archive/html_templates/dashboard.html - Standard .html file
FILE: archive/html_templates/base_broken.html - Standard .html file
FILE: archive/html_templates/batch_processing.html - Standard .html file
FILE: archive/html_templates/analytics.html - Standard .html file
FILE: archive/html_templates/search.html - Standard .html file
FILE: archive/html_templates/realtime.html - Standard .html file
FILE: archive/html_templates/api_testing.html - Test file
FILE: archive/cache_files/.DS_Store - Standard  file
FILE: archive/static_assets/knowledge.js - Standard .js file

============================================================
FILE: archive/test_files/run.py
============================================================
#!/usr/bin/env python3
"""
AI Chief of Staff - Gmail E2E Flow Entry Point

This script demonstrates the complete Gmail processing pipeline:
1. Gmail authentication
2. Message fetching
3. Email normalization  
4. Task extraction
5. Basic reporting

Usage:
    python run.py --email user@example.com
    python run.py --email user@example.com --max-emails 10
    python run.py --test-auth user@example.com
"""

import sys
import argparse
import logging
from datetime import datetime
from typing import Dict, List

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    from config.settings import settings
    from auth.gmail_auth import gmail_auth
    from ingest.gmail_fetcher import gmail_fetcher
    from processors.email_normalizer import email_normalizer
    from processors.task_extractor import task_extractor
except ImportError as e:
    logger.error(f"Failed to import modules: {e}")
    logger.error("Make sure you're running from the chief_of_staff_ai directory")
    sys.exit(1)

def test_authentication(user_email: str) -> bool:
    """Test Gmail authentication for a user"""
    print(f"\n=== Testing Gmail Authentication for {user_email} ===")
    
    try:
        # Check if user is authenticated
        is_authenticated = gmail_auth.is_authenticated(user_email)
        print(f"Authentication status: {'✓ Authenticated' if is_authenticated else '✗ Not authenticated'}")
        
        if is_authenticated:
            # Test Gmail access
            gmail_access = gmail_auth.test_gmail_access(user_email)
            print(f"Gmail access: {'✓ Working' if gmail_access else '✗ Failed'}")
            
            # Get detailed status
            status = gmail_auth.get_authentication_status(user_email)
            print(f"Detailed status: {status}")
            
            return gmail_access
        else:
            print("\nTo authenticate:")
            print("1. Start the Flask app: python main.py")
            print("2. Go to http://localhost:5000")
            print("3. Complete the OAuth flow")
            print("4. Run this script again")
            
        return False
        
    except Exception as e:
        logger.error(f"Authentication test failed: {str(e)}")
        return False

def run_gmail_e2e_flow(user_email: str, max_emails: int = 10, days_back: int = 7) -> Dict:
    """Run the complete Gmail E2E flow"""
    print(f"\n=== Gmail E2E Flow for {user_email} ===")
    print(f"Fetching up to {max_emails} emails from the last {days_back} days")
    
    results = {
        'user_email': user_email,
        'started_at': datetime.now().isoformat(),
        'steps_completed': [],
        'errors': [],
        'summary': {}
    }
    
    try:
        # Step 1: Test authentication
        print("\n1. Testing authentication...")
        if not test_authentication(user_email):
            results['errors'].append('Authentication failed')
            return results
        results['steps_completed'].append('authentication')
        
        # Step 2: Fetch emails
        print("\n2. Fetching Gmail messages...")
        fetch_result = gmail_fetcher.fetch_recent_messages(
            user_email=user_email,
            max_results=max_emails,
            days_back=days_back
        )
        
        if not fetch_result.get('success'):
            error_msg = f"Failed to fetch emails: {fetch_result.get('error')}"
            print(f"✗ {error_msg}")
            results['errors'].append(error_msg)
            return results
            
        raw_messages = fetch_result.get('messages', [])
        print(f"✓ Fetched {len(raw_messages)} messages")
        results['steps_completed'].append('fetch')
        results['summary']['raw_messages_count'] = len(raw_messages)
        
        if not raw_messages:
            print("No messages to process")
            return results
        
        # Step 3: Normalize emails
        print("\n3. Normalizing email messages...")
        normalized_messages = email_normalizer.normalize_batch(raw_messages)
        
        successful_normalizations = [msg for msg in normalized_messages if not msg.get('error')]
        failed_normalizations = [msg for msg in normalized_messages if msg.get('error')]
        
        print(f"✓ Normalized {len(successful_normalizations)} messages")
        if failed_normalizations:
            print(f"✗ Failed to normalize {len(failed_normalizations)} messages")
            
        results['steps_completed'].append('normalization')
        results['summary']['normalized_messages_count'] = len(successful_normalizations)
        results['summary']['normalization_errors'] = len(failed_normalizations)
        
        # Step 4: Extract tasks
        print("\n4. Extracting tasks and action items...")
        task_results = task_extractor.extract_tasks_from_batch(successful_normalizations)
        
        # Analyze task extraction results
        total_tasks = 0
        emails_with_tasks = 0
        skipped_emails = 0
        extraction_errors = 0
        
        for result in task_results:
            if result.get('error'):
                extraction_errors += 1
            elif result.get('skipped'):
                skipped_emails += 1
            else:
                tasks = result.get('tasks', [])
                total_tasks += len(tasks)
                if tasks:
                    emails_with_tasks += 1
        
        print(f"✓ Extracted {total_tasks} tasks from {emails_with_tasks} emails")
        print(f"  - Skipped {skipped_emails} emails (auto-replies, newsletters)")
        if extraction_errors:
            print(f"  - {extraction_errors} extraction errors")
            
        results['steps_completed'].append('task_extraction')
        results['summary']['total_tasks'] = total_tasks
        results['summary']['emails_with_tasks'] = emails_with_tasks
        results['summary']['skipped_emails'] = skipped_emails
        results['summary']['extraction_errors'] = extraction_errors
        
        # Step 5: Generate report
        print("\n5. Generating summary report...")
        generate_summary_report(results, normalized_messages, task_results)
        results['steps_completed'].append('reporting')
        
        results['completed_at'] = datetime.now().isoformat()
        print("\n✓ Gmail E2E flow completed successfully!")
        
        return results
        
    except Exception as e:
        error_msg = f"Unexpected error in E2E flow: {str(e)}"
        logger.error(error_msg)
        results['errors'].append(error_msg)
        return results

def generate_summary_report(results: Dict, normalized_messages: List[Dict], task_results: List[Dict]):
    """Generate a summary report of the processing results"""
    print("\n" + "="*60)
    print("PROCESSING SUMMARY REPORT")
    print("="*60)
    
    # Basic stats
    print(f"User: {results['user_email']}")
    print(f"Processed: {results['summary'].get('normalized_messages_count', 0)} emails")
    print(f"Extracted: {results['summary'].get('total_tasks', 0)} tasks")
    
    # Message type breakdown
    print("\nMessage Type Breakdown:")
    type_counts = {}
    for msg in normalized_messages:
        msg_type = msg.get('message_type', 'unknown')
        type_counts[msg_type] = type_counts.get(msg_type, 0) + 1
    
    for msg_type, count in sorted(type_counts.items()):
        print(f"  {msg_type}: {count}")
    
    # Task breakdown
    print("\nTask Breakdown:")
    priority_counts = {'high': 0, 'medium': 0, 'low': 0}
    category_counts = {}
    
    for task_result in task_results:
        for task in task_result.get('tasks', []):
            priority = task.get('priority', 'medium')
            priority_counts[priority] = priority_counts.get(priority, 0) + 1
            
            category = task.get('category', 'other')
            category_counts[category] = category_counts.get(category, 0) + 1
    
    print("By Priority:")
    for priority, count in priority_counts.items():
        print(f"  {priority}: {count}")
    
    print("By Category:")
    for category, count in sorted(category_counts.items()):
        print(f"  {category}: {count}")
    
    # High priority tasks
    high_priority_tasks = []
    urgent_tasks = []
    
    for task_result in task_results:
        for task in task_result.get('tasks', []):
            if task.get('priority') == 'high':
                high_priority_tasks.append(task)
            if task.get('due_date'):
                urgent_tasks.append(task)
    
    if high_priority_tasks:
        print(f"\nHigh Priority Tasks ({len(high_priority_tasks)}):")
        for task in high_priority_tasks[:5]:  # Show first 5
            print(f"  • {task.get('description', 'No description')}")
            if task.get('due_date'):
                print(f"    Due: {task['due_date']}")
    
    if urgent_tasks:
        print(f"\nTasks with Deadlines ({len(urgent_tasks)}):")
        for task in sorted(urgent_tasks, key=lambda x: x.get('due_date', ''))[:5]:
            print(f"  • {task.get('description', 'No description')}")
            print(f"    Due: {task.get('due_date')}")
    
    print("\n" + "="*60)

def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description='AI Chief of Staff - Gmail E2E Flow')
    parser.add_argument('--email', '-e', required=True, help='User email address')
    parser.add_argument('--max-emails', '-m', type=int, default=10, help='Maximum emails to process (default: 10)')
    parser.add_argument('--days-back', '-d', type=int, default=7, help='Days back to fetch emails (default: 7)')
    parser.add_argument('--test-auth', action='store_true', help='Only test authentication')
    parser.add_argument('--verbose', '-v', action='store_true', help='Enable verbose logging')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Validate configuration
    try:
        settings.validate_required_settings()
    except ValueError as e:
        print(f"Configuration Error: {e}")
        print("Please check your .env file and ensure all required variables are set.")
        sys.exit(1)
    
    # Create necessary directories
    settings.create_directories()
    
    print("AI Chief of Staff - Gmail E2E Flow")
    print("=" * 40)
    
    if args.test_auth:
        # Only test authentication
        success = test_authentication(args.email)
        sys.exit(0 if success else 1)
    else:
        # Run full E2E flow
        results = run_gmail_e2e_flow(
            user_email=args.email,
            max_emails=args.max_emails,
            days_back=args.days_back
        )
        
        if results.get('errors'):
            print(f"\nCompleted with {len(results['errors'])} errors:")
            for error in results['errors']:
                print(f"  - {error}")
            sys.exit(1)
        else:
            print(f"\nCompleted successfully! Processed {results['summary'].get('normalized_messages_count', 0)} emails and extracted {results['summary'].get('total_tasks', 0)} tasks.")
            sys.exit(0)

if __name__ == '__main__':
    main()


============================================================
FILE: archive/test_files/test_import.py
============================================================
import sys
import os

# Add the chief_of_staff_ai directory to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'chief_of_staff_ai'))

print("Step 1: Importing config.settings...")
try:
    from config.settings import settings
    print("✓ Settings imported successfully")
except Exception as e:
    print(f"✗ Settings import failed: {e}")
    exit(1)

print("Step 2: Importing SQLAlchemy modules...")
try:
    from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean, Float, ForeignKey, Index
    from sqlalchemy.ext.declarative import declarative_base
    from sqlalchemy.orm import sessionmaker, relationship, Session
    print("✓ SQLAlchemy modules imported successfully")
except Exception as e:
    print(f"✗ SQLAlchemy import failed: {e}")
    exit(1)

print("Step 3: Testing basic database.py imports...")
try:
    # Try importing just the base and basic classes first
    import models.database
    print("✓ models.database module imported successfully")
except Exception as e:
    print(f"✗ models.database import failed: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

print("Step 4: Testing specific imports...")
try:
    from models.database import Base, User, Email, Task
    print("✓ Base classes imported successfully")
except Exception as e:
    print(f"✗ Base classes import failed: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

print("Step 5: Testing Person and Project imports...")
try:
    from models.database import Person, Project
    print("✓ Person and Project imported successfully")
except Exception as e:
    print(f"✗ Person and Project import failed: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

print("Step 6: Testing get_db_manager import...")
try:
    from models.database import get_db_manager
    print("✓ get_db_manager imported successfully")
except Exception as e:
    print(f"✗ get_db_manager import failed: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

print("All imports successful!") 

============================================================
FILE: archive/test_files/debug_models.py
============================================================
#!/usr/bin/env python3

import sys
import traceback

print("🔍 Debugging database models...")

try:
    print("1. Testing imports...")
    from config.settings import settings
    print("   ✅ Settings imported")
    
    from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean, Float, ForeignKey, Index
    from sqlalchemy.ext.declarative import declarative_base
    from sqlalchemy.orm import sessionmaker, relationship, Session
    print("   ✅ SQLAlchemy imports successful")
    
    print("2. Testing database initialization...")
    exec(open('models/database.py').read())
    print("   ✅ Database file executed")
    
except Exception as e:
    print(f"❌ Error: {e}")
    traceback.print_exc()
    sys.exit(1)

print("✅ All tests passed!") 

============================================================
FILE: archive/test_files/debug_import.py
============================================================
import sys
import os

# Add the chief_of_staff_ai directory to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'chief_of_staff_ai'))

print("Importing models.database...")
import models.database

print("Available attributes in models.database:")
for attr in dir(models.database):
    if not attr.startswith('_'):
        print(f"  - {attr}")

print("\nChecking for specific classes:")
classes_to_check = ['Base', 'User', 'Email', 'Task', 'Person', 'Project', 'DatabaseManager', 'get_db_manager']

for class_name in classes_to_check:
    if hasattr(models.database, class_name):
        print(f"✓ {class_name} is available")
    else:
        print(f"✗ {class_name} is NOT available") 

============================================================
FILE: archive/test_files/test_models.py
============================================================
#!/usr/bin/env python3

import os
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean, Float, ForeignKey, Index
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship, Session
from sqlalchemy.types import TypeDecorator

from config.settings import settings

logger = logging.getLogger(__name__)

# Base class for all models
Base = declarative_base()

# Custom JSON type that works with both SQLite and PostgreSQL
class JSONType(TypeDecorator):
    impl = Text
    
    def process_bind_param(self, value, dialect):
        if value is not None:
            return json.dumps(value)
        return value
    
    def process_result_value(self, value, dialect):
        if value is not None:
            return json.loads(value)
        return value

class Person(Base):
    """Person model for tracking individuals mentioned in emails"""
    __tablename__ = 'people'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, nullable=False, index=True)
    
    # Person identification
    email_address = Column(String(255), index=True)
    name = Column(String(255), nullable=False)
    
    def __repr__(self):
        return f"<Person(name='{self.name}', email='{self.email_address}')>"

print("Person model defined successfully!") 

============================================================
FILE: archive/test_files/fix_model_imports.py
============================================================
#!/usr/bin/env python3
"""
Fix Model Imports - Replace old models with enhanced models in database.py
This script updates the model definitions to use the enhanced models.
"""

import os
import re

def fix_model_imports():
    """Fix the model import and usage issue"""
    
    database_py_path = 'models/database.py'
    
    if not os.path.exists(database_py_path):
        print(f"Error: {database_py_path} not found!")
        return False
    
    print("🔧 Fixing model imports in database.py...")
    
    # Read the current file
    with open(database_py_path, 'r') as f:
        content = f.read()
    
    # Check if we need to fix imports
    if 'from models.enhanced_models import' in content and 'class Email(Base):' in content:
        print("✅ Model conflict detected - will fix by updating imports")
        
        # Strategy: Update the model references to use enhanced models directly
        # Instead of having duplicate model definitions
        
        # Replace the old model class definitions with aliases to enhanced models
        replacements = [
            # Replace class definitions with aliases
            (r'class Email\(Base\):.*?(?=class|\Z)', 'Email = EnhancedEmail\n'),
            (r'class Task\(Base\):.*?(?=class|\Z)', 'Task = EnhancedTask\n'),
            (r'class Person\(Base\):.*?(?=class|\Z)', 'Person = EnhancedPerson\n'),
            (r'class Topic\(Base\):.*?(?=class|\Z)', 'Topic = EnhancedTopic\n'),
            (r'class Project\(Base\):.*?(?=class|\Z)', 'Project = EnhancedProject\n'),
        ]
        
        # Apply replacements
        new_content = content
        for pattern, replacement in replacements:
            # Use DOTALL flag to match across newlines
            new_content = re.sub(pattern, replacement, new_content, flags=re.DOTALL)
        
        # Also update the enhanced model imports to remove the "Enhanced" prefix
        new_content = new_content.replace(
            'Topic as EnhancedTopic, Person as EnhancedPerson, Task as EnhancedTask,',
            'Topic, Person, Task,'
        )
        new_content = new_content.replace(
            'Email as EnhancedEmail, CalendarEvent, Project as EnhancedProject,',
            'Email, CalendarEvent, Project,'
        )
        
        # Create backup
        backup_path = f'{database_py_path}.backup'
        with open(backup_path, 'w') as f:
            f.write(content)
        print(f"📁 Created backup: {backup_path}")
        
        # Write the fixed content
        with open(database_py_path, 'w') as f:
            f.write(new_content)
        
        print("✅ Model imports fixed successfully!")
        print("🔄 The application now uses enhanced models with all the new columns.")
        return True
    
    elif 'from models.enhanced_models import' not in content:
        print("❌ Enhanced model imports not found - database.py needs manual update")
        return False
    
    else:
        print("✅ Model imports appear to be correct already")
        return True

def create_simple_model_fix():
    """Create a simpler fix - just update the Base metadata"""
    
    fix_script = """# Quick fix for SQLAlchemy metadata cache issue
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.database import get_db_manager
from models.enhanced_models import Base as EnhancedBase

def refresh_metadata():
    db_manager = get_db_manager()
    
    # Clear the metadata cache
    db_manager.engine.dispose()
    
    # Reflect the actual database schema
    EnhancedBase.metadata.reflect(bind=db_manager.engine)
    
    print("✅ SQLAlchemy metadata refreshed")

if __name__ == "__main__":
    refresh_metadata()
"""
    
    with open('refresh_metadata.py', 'w') as f:
        f.write(fix_script)
    
    print("📝 Created refresh_metadata.py as a backup solution")

if __name__ == "__main__":
    print("🔧 AI Chief of Staff - Model Import Fix")
    print("="*50)
    
    success = fix_model_imports()
    
    if not success:
        print("\n🔧 Creating alternative solution...")
        create_simple_model_fix()
    
    print("\n📋 NEXT STEPS:")
    print("1. Restart the application to clear SQLAlchemy cache")
    print("2. Test email processing to confirm the fix")
    print("3. If issues persist, run: python refresh_metadata.py") 

============================================================
FILE: archive/backup_files/main.py
============================================================
# Main Flask application for AI Chief of Staff - Enhanced V2.0
# Updated to use the new entity-centric API architecture

import os
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any
from flask import Flask, render_template, request, jsonify, session, redirect, url_for, flash
import anthropic

# Configuration and Auth
from config.settings import settings
from auth.gmail_auth import gmail_auth

# Enhanced API System
from api import enhanced_api_bp

# Import other existing API blueprints
from api.batch_endpoints import batch_api_bp
from api.auth_endpoints import auth_api_bp
from api.docs_endpoints import docs_api_bp

# Enhanced Processors
from ingest.gmail_fetcher import gmail_fetcher
from processors.unified_entity_engine import entity_engine, EntityContext
from processors.enhanced_ai_pipeline import enhanced_ai_processor
from processors.realtime_processing import realtime_processor, EventType
from processors import processor_manager
from models.database import Topic, Person, Task, IntelligenceInsight, EntityRelationship, Email

# Database
from models.database import get_db_manager

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Create Flask application
app = Flask(__name__)
app.config['SECRET_KEY'] = settings.SECRET_KEY
app.config['SESSION_TYPE'] = 'filesystem'

# Initialize Claude client for chat
claude_client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)

# Version constant
CURRENT_VERSION = 'v2.0-enhanced'

# Start enhanced processor system when app starts
if not realtime_processor.running:
    processor_manager.start_all_processors()

# =====================================================================
# API BLUEPRINT REGISTRATION
# =====================================================================

# Register enhanced API blueprint
app.register_blueprint(enhanced_api_bp)

# Register batch processing blueprint
app.register_blueprint(batch_api_bp)

# Register authentication blueprint  
app.register_blueprint(auth_api_bp)

# Register documentation blueprint
app.register_blueprint(docs_api_bp)

logger.info("Registered API blueprints:")
logger.info("  - Enhanced API: /api/enhanced/*")
logger.info("  - Batch API: /api/batch/*")
logger.info("  - Auth API: /api/auth/*")
logger.info("  - Docs API: /api/docs/*")

# =====================================================================
# FRONTEND ROUTES (Updated to use enhanced backend)
# =====================================================================

@app.route('/')
def index():
    """Main dashboard route - enhanced with new entity data"""
    user_email = session.get('user_email')
    
    if not user_email:
        return render_template('login.html')
    
    try:
        # Get user information
        user_info = gmail_auth.get_user_by_email(user_email)
        if not user_info:
            session.clear()
            return render_template('login.html')
        
        # Get enhanced user statistics using new models
        user = get_db_manager().get_user_by_email(user_email)
        user_stats = {
            'total_emails': 0,
            'total_tasks': 0,
            'pending_tasks': 0,
            'completed_tasks': 0,
            'total_people': 0,
            'total_topics': 0,
            'recent_insights': 0
        }
        
        if user:
            with get_db_manager().get_session() as db_session:
                # Email statistics
                user_stats['total_emails'] = db_session.query(Email).filter(
                    Email.user_id == user.id
                ).count()
                
                # Task statistics  
                user_stats['total_tasks'] = db_session.query(Task).filter(
                    Task.user_id == user.id
                ).count()
                
                user_stats['pending_tasks'] = db_session.query(Task).filter(
                    Task.user_id == user.id,
                    Task.status.in_(['pending', 'open'])
                ).count()
                
                user_stats['completed_tasks'] = db_session.query(Task).filter(
                    Task.user_id == user.id,
                    Task.status == 'completed'
                ).count()
                
                # Entity statistics (new)
                user_stats['total_people'] = db_session.query(Person).filter(
                    Person.user_id == user.id
                ).count()
                
                user_stats['total_topics'] = db_session.query(Topic).filter(
                    Topic.user_id == user.id
                ).count()
        
        return render_template('dashboard.html', 
                             user_info=user_info, 
                             user_stats=user_stats)
    
    except Exception as e:
        logger.error(f"Dashboard error for {user_email}: {str(e)}")
        flash('An error occurred loading your dashboard. Please try again.', 'error')
        return render_template('dashboard.html', 
                             user_info={'email': user_email}, 
                             user_stats={'total_emails': 0, 'total_tasks': 0})

@app.route('/login')
def login():
    """Login page"""
    return render_template('login.html')

@app.route('/auth/google')
def auth_google():
    """Initiate Google OAuth flow"""
    try:
        auth_url, state = gmail_auth.get_authorization_url('user_session')
        session['oauth_state'] = state
        return redirect(auth_url)
    except Exception as e:
        logger.error(f"Google auth initiation error: {str(e)}")
        flash('Failed to initiate Google authentication. Please try again.', 'error')
        return redirect(url_for('login'))

@app.route('/auth/callback')
def auth_callback():
    """Handle OAuth callback"""
    try:
        authorization_code = request.args.get('code')
        state = request.args.get('state')
        
        if not authorization_code:
            flash('Authorization failed. Please try again.', 'error')
            return redirect(url_for('login'))
        
        # Handle OAuth callback
        result = gmail_auth.handle_oauth_callback(authorization_code, state)
        
        if result['success']:
            session['user_email'] = result['user_email']
            session['authenticated'] = True
            session['db_user_id'] = result.get('db_user_id')  # For enhanced API compatibility
            flash(f'Successfully authenticated as {result["user_email"]}!', 'success')
            return redirect(url_for('index'))
        else:
            flash(f'Authentication failed: {result["error"]}', 'error')
            return redirect(url_for('login'))
    
    except Exception as e:
        logger.error(f"OAuth callback error: {str(e)}")
        flash('Authentication error occurred. Please try again.', 'error')
        return redirect(url_for('login'))

@app.route('/logout')
def logout():
    """Logout user"""
    user_email = session.get('user_email')
    session.clear()
    
    if user_email:
        flash(f'Successfully logged out from {user_email}', 'success')
    
    return redirect(url_for('login'))

# =====================================================================
# ENHANCED ENTITY PAGES
# =====================================================================

@app.route('/people')
def people_page():
    """People management page"""
    user_email = session.get('user_email')
    if not user_email:
        return redirect(url_for('login'))
    
    return render_template('people.html', user_email=user_email)

@app.route('/topics')
def topics_page():
    """Topics management page"""
    user_email = session.get('user_email')
    if not user_email:
        return redirect(url_for('login'))
    
    return render_template('topics.html', user_email=user_email)

@app.route('/analytics')
def analytics_page():
    """Analytics dashboard page"""
    user_email = session.get('user_email')
    if not user_email:
        return redirect(url_for('login'))
    
    return render_template('analytics.html', 
                         user_email=user_email)

@app.route('/real-time')
def realtime_page():
    """Real-time processing dashboard"""
    user_email = session.get('user_email')
    if not user_email:
        return redirect(url_for('login'))
    
    return render_template('realtime.html', 
                         user_email=user_email)

@app.route('/api-testing')
def api_testing_page():
    """API testing and documentation interface"""
    user_email = session.get('user_email')
    if not user_email:
        return redirect(url_for('login'))
    
    return render_template('api_testing.html', 
                         user_email=user_email)

@app.route('/batch-processing')
def batch_processing_page():
    """Batch processing management page"""
    user_email = session.get('user_email')
    if not user_email:
        return redirect(url_for('login'))
    
    return render_template('batch_processing.html', 
                         user_email=user_email)

@app.route('/tasks')
def tasks_page():
    """Enhanced task management page"""
    user_email = session.get('user_email')
    if not user_email:
        return redirect(url_for('login'))
    
    return render_template('tasks.html', user_email=user_email)

@app.route('/profile')
def profile_page():
    """User profile and settings page"""
    user_email = session.get('user_email')
    if not user_email:
        return redirect(url_for('login'))
    
    return render_template('profile.html', 
                         user_email=user_email)

@app.route('/search')
def search_page():
    """Universal search and discovery page"""
    user_email = session.get('user_email')
    if not user_email:
        return redirect(url_for('login'))
    
    return render_template('search.html', user_email=user_email)

# =====================================================================
# LEGACY API ENDPOINTS (Maintained for backward compatibility)
# =====================================================================

@app.route('/api/process-emails', methods=['POST'])
def api_process_emails():
    """Legacy API endpoint - now uses enhanced processing"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        data = request.get_json() or {}
        days_back = data.get('days_back', 7)
        limit = data.get('limit', 50)
        force_refresh = data.get('force_refresh', False)
        
        logger.info(f"Legacy email processing for {user_email} (using enhanced backend)")
        
        # Get user database ID for enhanced processing
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        # Fetch and process emails using enhanced Gmail fetcher
        fetch_result = gmail_fetcher.fetch_recent_emails(
            user_email, 
            days_back=days_back, 
            limit=limit,
            force_refresh=force_refresh
        )
        
        if not fetch_result['success']:
            return jsonify({
                'success': False, 
                'error': f"Failed to fetch emails: {fetch_result.get('error')}"
            }), 400
        
        # Return enhanced processing result
        response = {
            'success': True,
            'fetch_result': fetch_result,
            'enhanced_processing': True,
            'processed_at': datetime.utcnow().isoformat()
        }
        
        return jsonify(response), 200
    
    except Exception as e:
        logger.error(f"Legacy email processing error for {user_email}: {str(e)}")
        return jsonify({
            'success': False, 
            'error': f"Processing failed: {str(e)}"
        }), 500

@app.route('/api/emails')
def api_get_emails():
    """Legacy API endpoint for getting emails"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        limit = request.args.get('limit', 50, type=int)
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        emails = get_db_manager().get_user_emails(user.id, limit)
        
        response = {
            'success': True,
            'emails': [email.to_dict() for email in emails],
            'count': len(emails)
        }
        
        return jsonify(response), 200
    
    except Exception as e:
        logger.error(f"Get emails error for {user_email}: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/tasks')
def api_get_tasks():
    """Legacy API endpoint for getting tasks"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        status = request.args.get('status')
        limit = request.args.get('limit', 100, type=int)
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        # Get tasks directly from database
        tasks = get_db_manager().get_user_tasks(user.id, status=status, limit=limit)
        
        response = {
            'success': True,
            'tasks': [task.to_dict() for task in tasks],
            'count': len(tasks)
        }
        
        return jsonify(response), 200
    
    except Exception as e:
        logger.error(f"Get tasks error for {user_email}: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/tasks/<int:task_id>/status', methods=['PUT'])
def api_update_task_status(task_id):
    """Legacy API endpoint for updating task status"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        data = request.get_json()
        new_status = data.get('status')
        
        if new_status not in ['pending', 'open', 'in_progress', 'completed', 'cancelled']:
            return jsonify({'success': False, 'error': 'Invalid status'}), 400
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        # Update task status directly
        task = get_db_manager().update_task_status(task_id, new_status, user.id)
        
        if task:
            response = {
                'success': True,
                'task': task.to_dict()
            }
        else:
            response = {
                'success': False,
                'error': 'Task not found or access denied'
            }
        
        return jsonify(response), 200
    
    except Exception as e:
        logger.error(f"Update task status error for {user_email}: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/chat', methods=['POST'])
def api_chat():
    """Enhanced API endpoint for Claude chat with entity context"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        data = request.get_json()
        message = data.get('message', '').strip()
        
        if not message:
            return jsonify({'success': False, 'error': 'Message is required'}), 400
        
        # Get enhanced user context
        user = get_db_manager().get_user_by_email(user_email)
        context_info = ""
        
        if user:
            # Generate comprehensive insights for context
            insights_result = processor_manager.generate_user_insights(user.id, 'comprehensive')
            
            if insights_result['success']:
                insights = insights_result['result']
                
                # Build rich context from insights
                context_parts = []
                
                if insights.get('proactive_insights'):
                    recent_insights = insights['proactive_insights'][:3]
                    insights_list = "\n".join([f"- {insight['title']}" for insight in recent_insights])
                    context_parts.append(f"Recent insights:\n{insights_list}")
                
                # Add predictive analytics context if available
                if insights.get('predictive_analytics'):
                    pred_analytics = insights['predictive_analytics']
                    if pred_analytics.get('upcoming_needs'):
                        needs_list = "\n".join([f"- {need['title']}" for need in pred_analytics['upcoming_needs'][:3]])
                        context_parts.append(f"Upcoming needs:\n{needs_list}")
                
                if context_parts:
                    context_info = f"\n\nCurrent context:\n" + "\n\n".join(context_parts)
        
        # Build enhanced system prompt
        system_prompt = f"""You are an AI Chief of Staff assistant helping {user_email}. 
You have access to their comprehensive work data including tasks, contacts, and business topics.

Be helpful, professional, and concise. Focus on actionable advice related to their work, relationships, and strategic priorities. Use the context provided to give personalized recommendations.{context_info}"""
        
        # Call Claude with enhanced context
        response = claude_client.messages.create(
            model=settings.CLAUDE_MODEL,
            max_tokens=1000,
            temperature=0.3,
            system=system_prompt,
            messages=[{
                "role": "user",
                "content": message
            }]
        )
        
        reply = response.content[0].text
        
        return jsonify({
            'success': True,
            'message': message,
            'reply': reply,
            'enhanced_context': bool(context_info),
            'timestamp': datetime.utcnow().isoformat()
        })
    
    except Exception as e:
        logger.error(f"Enhanced chat error for {user_email}: {str(e)}")
        return jsonify({
            'success': False, 
            'error': 'Failed to process chat message'
        }), 500

@app.route('/api/status')
def api_status():
    """Enhanced API endpoint to get system status"""
    user_email = session.get('user_email')
    
    status = {
        'authenticated': bool(user_email),
        'user_email': user_email,
        'timestamp': datetime.utcnow().isoformat(),
        'api_version': CURRENT_VERSION,
        'enhanced_features': True,
        'database_connected': True,
        'gmail_auth_available': bool(settings.GOOGLE_CLIENT_ID and settings.GOOGLE_CLIENT_SECRET),
        'claude_available': bool(settings.ANTHROPIC_API_KEY),
        'processor_manager': True,
        'real_time_processing': True
    }
    
    # Test database connection
    try:
        get_db_manager().get_session().close()
    except Exception as e:
        status['database_connected'] = False
        status['database_error'] = str(e)
    
    # Test processor manager
    try:
        stats_result = processor_manager.get_processing_statistics()
        status['processor_manager_stats'] = stats_result['result'] if stats_result['success'] else None
    except Exception as e:
        status['processor_manager'] = False
        status['processor_manager_error'] = str(e)
    
    # Test Gmail auth if user is authenticated
    if user_email:
        try:
            auth_status = gmail_auth.get_authentication_status(user_email)
            status['gmail_auth_status'] = auth_status
        except Exception as e:
            status['gmail_auth_error'] = str(e)
    
    resp = jsonify(status)
    return add_version_headers(resp, CURRENT_VERSION), 200

# =====================================================================
# BUSINESS INTELLIGENCE ENDPOINT (NEW)
# =====================================================================

def get_strategic_business_insights(user_email: str) -> List[Dict]:
    """Generate strategic business insights for a user"""
    try:
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return []
        
        # Use processor manager to generate comprehensive insights
        insights_result = processor_manager.generate_user_insights(user.id, 'strategic')
        
        if insights_result['success']:
            return insights_result['result'].get('insights', [])
        else:
            logger.error(f"Failed to generate insights: {insights_result['error']}")
            return []
    
    except Exception as e:
        logger.error(f"Error generating strategic insights: {str(e)}")
        return []

# =====================================================================
# ENHANCED UNIFIED PROCESSING ENDPOINT
# =====================================================================

@app.route('/api/enhanced-unified-sync', methods=['POST'])
def enhanced_unified_intelligence_sync():
    """
    Enhanced unified processing using entity-centric architecture.
    Demonstrates the full power of the integrated intelligence platform.
    """
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        data = request.get_json() or {}
        max_emails = data.get('max_emails', 10)
        days_back = data.get('days_back', 3)
        enable_real_time = data.get('enable_real_time', True)
        
        processing_summary = {
            'success': True,
            'enhanced_architecture': True,
            'entity_intelligence': {},
            'processing_stages': {},
            'proactive_insights': [],
            'real_time_processing': enable_real_time,
            'entity_relationships': {},
            'intelligence_quality': {}
        }
        
        # Stage 1: Enhanced Email Processing with Entity Intelligence
        logger.info(f"Starting enhanced unified sync for {user_email}")
        
        email_result = gmail_fetcher.fetch_recent_emails(
            user_email, max_emails=max_emails, days_back=days_back
        )
        
        processing_summary['processing_stages']['emails_fetched'] = email_result.get('emails_fetched', 0)
        
        if email_result.get('success') and email_result.get('emails'):
            if enable_real_time:
                # Send to real-time processor
                for email_data in email_result['emails']:
                    realtime_processor.process_new_email(email_data, user.id, priority=2)
            else:
                # Process directly through enhanced AI pipeline
                for email_data in email_result['emails']:
                    result = enhanced_ai_processor.process_email_with_context(email_data, user.id)
                    if result.success:
                        processing_summary['processing_stages']['emails_processed'] = processing_summary['processing_stages'].get('emails_processed', 0) + 1
        
        # Stage 2: Generate Entity Intelligence Summary
        entity_intelligence = generate_entity_intelligence_summary(user.id)
        processing_summary['entity_intelligence'] = entity_intelligence
        
        # Stage 3: Generate Proactive Insights
        proactive_insights = entity_engine.generate_proactive_insights(user.id)
        processing_summary['proactive_insights'] = [
            {
                'id': insight.id,
                'type': insight.insight_type,
                'title': insight.title,
                'description': insight.description,
                'priority': insight.priority,
                'confidence': insight.confidence,
                'created_at': insight.created_at.isoformat()
            }
            for insight in proactive_insights
        ]
        
        # Stage 4: Entity Relationship Analysis
        relationship_analysis = analyze_entity_relationships(user.id)
        processing_summary['entity_relationships'] = relationship_analysis
        
        # Stage 5: Intelligence Quality Metrics
        quality_metrics = calculate_intelligence_quality_metrics(user.id)
        processing_summary['intelligence_quality'] = quality_metrics
        
        # Stage 6: Real-time Processing Statistics
        if enable_real_time:
            rt_stats = realtime_processor.get_stats()
            processing_summary['real_time_stats'] = {
                'queue_size': rt_stats['queue_size'],
                'events_processed': rt_stats['events_processed'],
                'avg_processing_time': rt_stats['avg_processing_time'],
                'workers_active': rt_stats['workers_active']
            }
        
        logger.info(f"Enhanced unified sync complete: {len(proactive_insights)} insights generated")
        
        return jsonify(processing_summary)
        
    except Exception as e:
        logger.error(f"Enhanced unified sync failed: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'enhanced_architecture': True,
            'real_time_processing': False
        }), 500

# =====================================================================
# ENTITY-CENTRIC API ENDPOINTS
# =====================================================================

@app.route('/api/entities/topics', methods=['GET'])
def get_topics_with_intelligence():
    """Get topics with accumulated intelligence and relationship data"""
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        with get_db_manager().get_session() as session:
            topics = session.query(Topic).filter(Topic.user_id == user.id).all()
            
            topics_data = []
            for topic in topics:
                topic_data = {
                    'id': topic.id,
                    'name': topic.name,
                    'description': topic.description,
                    'keywords': topic.keywords.split(',') if topic.keywords else [],
                    'is_official': topic.is_official,
                    'confidence_score': topic.confidence_score,
                    'total_mentions': topic.total_mentions,
                    'last_mentioned': topic.last_mentioned.isoformat() if topic.last_mentioned else None,
                    'intelligence_summary': topic.intelligence_summary,
                    'strategic_importance': topic.strategic_importance,
                    'created_at': topic.created_at.isoformat(),
                    'updated_at': topic.updated_at.isoformat(),
                    'version': topic.version,
                    
                    # Relationship data
                    'connected_people': len(topic.people),
                    'related_tasks': len(topic.tasks),
                    'connected_events': len(topic.events),
                    'total_connections': len(topic.people) + len(topic.tasks) + len(topic.events)
                }
                topics_data.append(topic_data)
            
            # Sort by strategic importance and recent activity
            topics_data.sort(key=lambda x: (x['strategic_importance'], x['total_mentions']), reverse=True)
            
            return jsonify({
                'success': True,
                'topics': topics_data,
                'summary': {
                    'total_topics': len(topics_data),
                    'official_topics': len([t for t in topics_data if t['is_official']]),
                    'high_importance': len([t for t in topics_data if t['strategic_importance'] > 0.7]),
                    'recently_active': len([t for t in topics_data if t['last_mentioned'] and 
                                          datetime.fromisoformat(t['last_mentioned']) > datetime.utcnow() - timedelta(days=7)]),
                    'highly_connected': len([t for t in topics_data if t['total_connections'] > 3])
                }
            })
            
    except Exception as e:
        logger.error(f"Failed to get topics with intelligence: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/entities/people', methods=['GET'])
def get_people_with_relationship_intelligence():
    """Get people with comprehensive relationship intelligence"""
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        with get_db_manager().get_session() as session:
            people = session.query(Person).filter(Person.user_id == user.id).all()
            
            people_data = []
            for person in people:
                person_data = {
                    'id': person.id,
                    'name': person.name,
                    'email_address': person.email_address,
                    'phone': person.phone,
                    'company': person.company,
                    'title': person.title,
                    'relationship_type': person.relationship_type,
                    'importance_level': person.importance_level,
                    'communication_frequency': person.communication_frequency,
                    'last_contact': person.last_contact.isoformat() if person.last_contact else None,
                    'total_interactions': person.total_interactions,
                    'linkedin_url': person.linkedin_url,
                    'professional_story': person.professional_story,
                    'created_at': person.created_at.isoformat(),
                    'updated_at': person.updated_at.isoformat(),
                    
                    # Relationship intelligence
                    'connected_topics': [{'name': topic.name, 'strategic_importance': topic.strategic_importance} for topic in person.topics],
                    'assigned_tasks': len(person.tasks_assigned),
                    'mentioned_in_tasks': len(person.tasks_mentioned),
                    'topic_connections': len(person.topics),
                    'engagement_score': calculate_person_engagement_score(person)
                }
                people_data.append(person_data)
            
            # Sort by importance and recent activity
            people_data.sort(key=lambda x: (x['importance_level'] or 0, x['total_interactions']), reverse=True)
            
            return jsonify({
                'success': True,
                'people': people_data,
                'summary': {
                    'total_people': len(people_data),
                    'high_importance': len([p for p in people_data if (p['importance_level'] or 0) > 0.7]),
                    'recent_contacts': len([p for p in people_data if p['last_contact'] and 
                                          datetime.fromisoformat(p['last_contact']) > datetime.utcnow() - timedelta(days=30)]),
                    'highly_connected': len([p for p in people_data if p['topic_connections'] > 2]),
                    'with_professional_story': len([p for p in people_data if p['professional_story']])
                }
            })
            
    except Exception as e:
        logger.error(f"Failed to get people with relationship intelligence: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/intelligence/insights', methods=['GET'])
def get_proactive_intelligence_insights():
    """Get proactive intelligence insights with filtering"""
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        # Get query parameters
        status_filter = request.args.get('status', 'new')
        insight_type = request.args.get('type', None)
        limit = int(request.args.get('limit', 20))
        
        with get_db_manager().get_session() as session:
            query = session.query(IntelligenceInsight).filter(
                IntelligenceInsight.user_id == user.id
            )
            
            if status_filter:
                query = query.filter(IntelligenceInsight.status == status_filter)
            
            if insight_type:
                query = query.filter(IntelligenceInsight.insight_type == insight_type)
            
            # Filter out expired insights
            query = query.filter(
                (IntelligenceInsight.expires_at.is_(None)) | 
                (IntelligenceInsight.expires_at > datetime.utcnow())
            )
            
            insights = query.order_by(
                IntelligenceInsight.priority.desc(),
                IntelligenceInsight.created_at.desc()
            ).limit(limit).all()
            
            insights_data = []
            for insight in insights:
                insight_data = {
                    'id': insight.id,
                    'insight_type': insight.insight_type,
                    'title': insight.title,
                    'description': insight.description,
                    'priority': insight.priority,
                    'confidence': insight.confidence,
                    'status': insight.status,
                    'user_feedback': insight.user_feedback,
                    'created_at': insight.created_at.isoformat(),
                    'expires_at': insight.expires_at.isoformat() if insight.expires_at else None,
                    'related_entity': {
                        'type': insight.related_entity_type,
                        'id': insight.related_entity_id
                    } if insight.related_entity_type else None
                }
                insights_data.append(insight_data)
            
            return jsonify({
                'success': True,
                'insights': insights_data,
                'summary': {
                    'total_insights': len(insights_data),
                    'by_type': count_insights_by_type(insights_data),
                    'by_priority': count_insights_by_priority(insights_data),
                    'actionable': len([i for i in insights_data if i['status'] == 'new']),
                    'high_confidence': len([i for i in insights_data if i['confidence'] > 0.8])
                }
            })
            
    except Exception as e:
        logger.error(f"Failed to get intelligence insights: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/intelligence/generate-insights', methods=['POST'])
def generate_proactive_insights():
    """Generate proactive insights manually (for testing)"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    user = get_db_manager().get_user_by_email(user_email)
    if not user:
        return jsonify({'success': False, 'error': 'User not found'}), 404
    
    try:
        # Generate insights using entity engine
        insights = entity_engine.generate_proactive_insights(user.id)
        
        return jsonify({
            'success': True,
            'insights_generated': len(insights),
            'insights': [
                {
                    'type': insight.insight_type,
                    'title': insight.title,
                    'description': insight.description,
                    'priority': insight.priority,
                    'confidence': insight.confidence,
                    'created_at': insight.created_at.isoformat()
                }
                for insight in insights
            ]
        })
        
    except Exception as e:
        logger.error(f"Failed to generate proactive insights: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/real-time/status', methods=['GET'])
def get_realtime_processing_status():
    """Get real-time processing status and statistics"""
    try:
        stats = realtime_processor.get_stats()
        queue_status = realtime_processor.get_queue_status()
        
        return jsonify({
            'success': True,
            'real_time_processing': {
                'is_running': stats['is_running'],
                'queue_size': stats['queue_size'],
                'workers_active': stats['workers_active'],
                'events_processed': stats['events_processed'],
                'events_failed': stats['events_failed'],
                'avg_processing_time': stats['avg_processing_time'],
                'last_processed': stats['last_processed'].isoformat() if stats['last_processed'] else None
            },
            'performance_metrics': {
                'processing_rate': stats['events_processed'] / max(1, (datetime.utcnow() - realtime_processor.stats.get('start_time', datetime.utcnow())).total_seconds() / 60),  # events per minute
                'error_rate': stats['events_failed'] / max(1, stats['events_processed'] + stats['events_failed']),
                'queue_utilization': stats['queue_size'] / 1000  # Assume max queue size of 1000
            }
        })
        
    except Exception as e:
        logger.error(f"Failed to get real-time status: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

# =====================================================================
# ENHANCED METRICS AND FEEDBACK ENDPOINTS (MISSING FROM DASHBOARD)
# =====================================================================

@app.route('/api/entities/metrics', methods=['GET'])
def get_entity_metrics():
    """Get comprehensive entity metrics for dashboard"""
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        with get_db_manager().get_session() as session:
            # Entity counts
            topics_count = session.query(Topic).filter(Topic.user_id == user.id).count()
            people_count = session.query(Person).filter(Person.user_id == user.id).count()
            tasks_count = session.query(Task).filter(Task.user_id == user.id).count()
            insights_count = session.query(IntelligenceInsight).filter(
                IntelligenceInsight.user_id == user.id,
                IntelligenceInsight.status == 'new'
            ).count()
            
            # Active relationships
            relationships_count = session.query(EntityRelationship).filter(
                EntityRelationship.user_id == user.id
            ).count()
            
            # Calculate intelligence quality score
            high_conf_topics = session.query(Topic).filter(
                Topic.user_id == user.id,
                Topic.confidence_score > 0.8
            ).count()
            
            topics_with_summary = session.query(Topic).filter(
                Topic.user_id == user.id,
                Topic.intelligence_summary.isnot(None)
            ).count()
            
            intelligence_quality = 0.0
            if topics_count > 0:
                intelligence_quality = (high_conf_topics + topics_with_summary) / (topics_count * 2)
            
            # Topic momentum (topics active in last 7 days)
            week_ago = datetime.utcnow() - timedelta(days=7)
            active_topics = session.query(Topic).filter(
                Topic.user_id == user.id,
                Topic.last_mentioned > week_ago
            ).count()
            
            topic_momentum = 0.0
            if topics_count > 0:
                topic_momentum = active_topics / topics_count
            
            # Relationship density
            relationship_density = 0.0
            total_entities = topics_count + people_count
            if total_entities > 0:
                relationship_density = relationships_count / total_entities
            
            metrics = {
                'total_entities': topics_count + people_count + tasks_count,
                'topics': topics_count,
                'people': people_count,
                'tasks': tasks_count,
                'active_insights': insights_count,
                'entity_relationships': relationships_count,
                'intelligence_quality': intelligence_quality,
                'topic_momentum': topic_momentum,
                'relationship_density': relationship_density
            }
            
            return jsonify({'success': True, 'metrics': metrics})
            
    except Exception as e:
        logger.error(f"Failed to get entity metrics: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/intelligence/feedback', methods=['POST'])
def record_insight_feedback():
    """Record user feedback on intelligence insights"""
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        data = request.get_json()
        insight_id = data.get('insight_id')
        feedback = data.get('feedback')
        
        if not insight_id or not feedback:
            return jsonify({'success': False, 'error': 'Missing insight_id or feedback'}), 400
        
        with get_db_manager().get_session() as session:
            insight = session.query(IntelligenceInsight).filter(
                IntelligenceInsight.id == insight_id,
                IntelligenceInsight.user_id == user.id
            ).first()
            
            if not insight:
                return jsonify({'success': False, 'error': 'Insight not found'}), 404
            
            insight.user_feedback = feedback
            insight.updated_at = datetime.utcnow()
            
            # Mark as reviewed if feedback provided
            if insight.status == 'new':
                insight.status = 'reviewed'
            
            session.commit()
            
            return jsonify({'success': True, 'message': 'Feedback recorded'})
            
    except Exception as e:
        logger.error(f"Failed to record insight feedback: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/tasks', methods=['GET'])
def get_enhanced_tasks():
    """Enhanced task endpoint with context information"""
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        limit = request.args.get('limit', 20, type=int)
        with_context = request.args.get('with_context', 'false').lower() == 'true'
        status = request.args.get('status')
        
        with get_db_manager().get_session() as session:
            query = session.query(Task).filter(Task.user_id == user.id)
            
            if with_context:
                query = query.filter(Task.context_story.isnot(None))
            
            if status:
                query = query.filter(Task.status == status)
            
            tasks = query.order_by(Task.created_at.desc()).limit(limit).all()
            
            tasks_data = []
            for task in tasks:
                task_data = {
                    'id': task.id,
                    'description': task.description,
                    'status': task.status,
                    'priority': task.priority,
                    'confidence': task.confidence,
                    'context_story': task.context_story,
                    'due_date': task.due_date.isoformat() if task.due_date else None,
                    'created_at': task.created_at.isoformat(),
                    'assignee': {
                        'name': task.assignee.name if task.assignee else None,
                        'email': task.assignee.email_address if task.assignee else None
                    } if task.assignee else None
                }
                tasks_data.append(task_data)
            
            return jsonify({'success': True, 'tasks': tasks_data})
            
    except Exception as e:
        logger.error(f"Failed to get enhanced tasks: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

# =====================================================================
# UNIFIED INTELLIGENCE SYNC ENDPOINT (MISSING FROM REFACTOR)
# =====================================================================

@app.route('/api/unified-intelligence-sync', methods=['POST'])
def unified_intelligence_sync():
    """
    Enhanced unified processing that integrates email, calendar, and generates
    real-time intelligence with entity-centric architecture.
    """
    try:
        user_email = session.get('user_email')
        if not user_email:
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        # Get processing parameters
        data = request.get_json() or {}
        max_emails = data.get('max_emails', 20)
        days_back = data.get('days_back', 7)
        days_forward = data.get('days_forward', 14)
        force_refresh = data.get('force_refresh', False)
        
        processing_summary = {
            'success': True,
            'processing_stages': {},
            'entity_intelligence': {},
            'insights_generated': [],
            'real_time_processing': True,
            'next_steps': []
        }
        
        # Stage 1: Fetch and process emails in real-time
        logger.info(f"Starting unified intelligence sync for {user_email}")
        
        # Fetch emails
        email_result = gmail_fetcher.fetch_recent_emails(
            user_email, max_emails=max_emails, days_back=days_back, force_refresh=force_refresh
        )
        
        processing_summary['processing_stages']['emails_fetched'] = email_result.get('emails_fetched', 0)
        
        if email_result.get('success') and email_result.get('emails'):
            # Process each email through real-time pipeline
            for email_data in email_result['emails']:
                realtime_processor.process_new_email(email_data, user.id, priority=3)
        
        # Stage 2: Fetch and enhance calendar events (if calendar fetcher available)
        try:
            from ingest.calendar_fetcher import calendar_fetcher
            calendar_result = calendar_fetcher.fetch_calendar_events(
                user_email, days_back=3, days_forward=days_forward, create_prep_tasks=True
            )
            
            processing_summary['processing_stages']['calendar_events_fetched'] = calendar_result.get('events_fetched', 0)
            
            if calendar_result.get('success') and calendar_result.get('events'):
                # Process each calendar event through real-time pipeline
                for event_data in calendar_result['events']:
                    realtime_processor.process_new_calendar_event(event_data, user.id, priority=4)
        except ImportError:
            logger.info("Calendar fetcher not available, skipping calendar processing")
            processing_summary['processing_stages']['calendar_events_fetched'] = 0
        
        # Stage 3: Generate comprehensive business intelligence
        intelligence_summary = generate_360_business_intelligence(user.id)
        processing_summary['entity_intelligence'] = intelligence_summary
        
        # Stage 4: Generate proactive insights
        proactive_insights = entity_engine.generate_proactive_insights(user.id)
        processing_summary['insights_generated'] = [
            {
                'type': insight.insight_type if hasattr(insight, 'insight_type') else 'general',
                'title': insight.title if hasattr(insight, 'title') else 'Insight',
                'description': insight.description if hasattr(insight, 'description') else 'No description',
                'priority': insight.priority if hasattr(insight, 'priority') else 'medium',
                'confidence': insight.confidence if hasattr(insight, 'confidence') else 0.5
            }
            for insight in proactive_insights
        ]
        
        # Generate next steps based on intelligence
        processing_summary['next_steps'] = generate_intelligent_next_steps(intelligence_summary, proactive_insights)
        
        logger.info(f"Completed unified intelligence sync for {user_email}: "
                   f"{processing_summary['processing_stages']['emails_fetched']} emails, "
                   f"{processing_summary['processing_stages']['calendar_events_fetched']} events, "
                   f"{len(proactive_insights)} insights")
        
        return jsonify(processing_summary)
        
    except Exception as e:
        logger.error(f"Failed unified intelligence sync: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'processing_stages': {},
            'real_time_processing': False
        }), 500

# =====================================================================
# BUSINESS INTELLIGENCE GENERATION (MISSING FROM REFACTOR)
# =====================================================================

def generate_360_business_intelligence(user_id: int) -> Dict:
    """Generate comprehensive 360-degree business intelligence"""
    try:
        intelligence = {
            'entity_summary': {},
            'relationship_intelligence': {},
            'strategic_insights': {},
            'activity_patterns': {},
            'intelligence_quality': {}
        }
        
        with get_db_manager().get_session() as session:
            # Entity summary
            topics_count = session.query(Topic).filter(Topic.user_id == user_id).count()
            people_count = session.query(Person).filter(Person.user_id == user_id).count()
            tasks_count = session.query(Task).filter(Task.user_id == user_id).count()
            
            from models.database import CalendarEvent
            events_count = session.query(CalendarEvent).filter(CalendarEvent.user_id == user_id).count()
            
            intelligence['entity_summary'] = {
                'topics': topics_count,
                'people': people_count,
                'tasks': tasks_count,
                'calendar_events': events_count,
                'total_entities': topics_count + people_count + tasks_count + events_count
            }
            
            # Relationship intelligence
            from models.database import EntityRelationship
            relationships_count = session.query(EntityRelationship).filter(
                EntityRelationship.user_id == user_id
            ).count()
            
            # Active topics (mentioned in last 30 days)
            active_topics = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.last_mentioned > datetime.utcnow() - timedelta(days=30)
            ).count()
            
            # Recent contacts
            recent_contacts = session.query(Person).filter(
                Person.user_id == user_id,
                Person.last_contact > datetime.utcnow() - timedelta(days=30)
            ).count()
            
            intelligence['relationship_intelligence'] = {
                'total_relationships': relationships_count,
                'active_topics': active_topics,
                'recent_contacts': recent_contacts,
                'relationship_density': relationships_count / max(1, people_count + topics_count)
            }
            
            # Activity patterns
            recent_tasks = session.query(Task).filter(
                Task.user_id == user_id,
                Task.created_at > datetime.utcnow() - timedelta(days=7)
            ).count()
            
            intelligence['activity_patterns'] = {
                'tasks_this_week': recent_tasks,
                'average_daily_tasks': recent_tasks / 7,
                'topic_momentum': active_topics / max(1, topics_count)
            }
            
            # Intelligence quality metrics
            high_confidence_tasks = session.query(Task).filter(
                Task.user_id == user_id,
                Task.confidence > 0.8
            ).count()
            
            tasks_with_context = session.query(Task).filter(
                Task.user_id == user_id,
                Task.context_story.isnot(None)
            ).count()
            
            intelligence['intelligence_quality'] = {
                'high_confidence_extractions': high_confidence_tasks / max(1, tasks_count),
                'contextualized_tasks': tasks_with_context / max(1, tasks_count),
                'entity_interconnection': relationships_count / max(1, intelligence['entity_summary']['total_entities'])
            }
        
        return intelligence
        
    except Exception as e:
        logger.error(f"Failed to generate 360 business intelligence: {str(e)}")
        return {}

def generate_intelligent_next_steps(intelligence_summary: Dict, insights: List) -> List[str]:
    """Generate intelligent next steps based on business intelligence"""
    next_steps = []
    
    try:
        entity_summary = intelligence_summary.get('entity_summary', {})
        relationship_intel = intelligence_summary.get('relationship_intelligence', {})
        activity_patterns = intelligence_summary.get('activity_patterns', {})
        
        # Suggest next steps based on data
        if entity_summary.get('total_entities', 0) < 10:
            next_steps.append("Process more email data to build comprehensive business intelligence")
        
        if relationship_intel.get('relationship_density', 0) < 0.3:
            next_steps.append("Focus on building relationship connections between contacts and topics")
        
        if activity_patterns.get('tasks_this_week', 0) > 10:
            next_steps.append("Consider prioritizing and organizing your task backlog")
        
        if len(insights) > 5:
            next_steps.append("Review and act on high-priority insights")
        elif len(insights) < 2:
            next_steps.append("Continue processing communications to generate more insights")
        
        # Always suggest at least one action
        if not next_steps:
            next_steps.append("Continue using the system to build your business intelligence")
        
    except Exception as e:
        logger.error(f"Failed to generate intelligent next steps: {str(e)}")
        next_steps = ["Continue building your business intelligence"]
    
    return next_steps

def calculate_relationship_strength(person: Person) -> float:
    """Calculate relationship strength for a person"""
    score = 0.0
    
    # Interaction frequency
    if person.total_interactions > 10:
        score += 0.3
    elif person.total_interactions > 5:
        score += 0.2
    elif person.total_interactions > 0:
        score += 0.1
    
    # Recent contact
    if person.last_contact and person.last_contact > datetime.utcnow() - timedelta(days=7):
        score += 0.3
    elif person.last_contact and person.last_contact > datetime.utcnow() - timedelta(days=30):
        score += 0.2
    
    # Importance level
    if person.importance_level:
        score += person.importance_level * 0.4
    
    return min(1.0, score)

def calculate_communication_frequency(person: Person) -> str:
    """Calculate communication frequency description"""
    if not person.last_contact:
        return "No recent contact"
    
    days_since = (datetime.utcnow() - person.last_contact).days
    
    if days_since <= 7:
        return "Weekly"
    elif days_since <= 30:
        return "Monthly"
    elif days_since <= 90:
        return "Quarterly"
    else:
        return "Infrequent"

def calculate_engagement_score(person: Person) -> float:
    """Calculate engagement score for a person"""
    score = 0.0
    
    # Interaction frequency
    if person.total_interactions > 10:
        score += 0.3
    elif person.total_interactions > 5:
        score += 0.2
    elif person.total_interactions > 0:
        score += 0.1
    
    # Recent contact
    if person.last_contact and person.last_contact > datetime.utcnow() - timedelta(days=7):
        score += 0.3
    elif person.last_contact and person.last_contact > datetime.utcnow() - timedelta(days=30):
        score += 0.2
    
    # Professional context
    if person.professional_story:
        score += 0.2
    
    # Topic connections
    topic_count = len(person.topics) if person.topics else 0
    if topic_count > 3:
        score += 0.2
    elif topic_count > 0:
        score += 0.1
    
    return min(1.0, score)

def get_person_topic_affinity(person_id: int, topic_id: int) -> float:
    """Get affinity score between person and topic"""
    try:
        from models.database import get_db_manager
        from models.database import person_topic_association
        
        with get_db_manager().get_session() as session:
            # Query the association table for affinity score
            result = session.execute(
                person_topic_association.select().where(
                    (person_topic_association.c.person_id == person_id) &
                    (person_topic_association.c.topic_id == topic_id)
                )
            ).first()
            
            return result.affinity_score if result else 0.5
            
    except Exception as e:
        logger.error(f"Failed to get person-topic affinity: {str(e)}")
        return 0.5

def calculate_task_strategic_importance(task: Task) -> float:
    """Calculate strategic importance of a task"""
    importance = 0.0
    
    # High priority tasks are more strategic
    if task.priority == 'high':
        importance += 0.4
    elif task.priority == 'medium':
        importance += 0.2
    
    # Tasks with context are more strategic
    if task.context_story:
        importance += 0.3
    
    # Tasks with high confidence are more strategic
    if task.confidence > 0.8:
        importance += 0.2
    
    # Tasks connected to multiple topics are more strategic
    topic_count = len(task.topics) if task.topics else 0
    if topic_count > 2:
        importance += 0.1
    
    return min(1.0, importance)

def count_by_field(data_list: List[Dict], field: str) -> Dict:
    """Count items by a specific field"""
    counts = {}
    for item in data_list:
        value = item.get(field, 'unknown')
        counts[value] = counts.get(value, 0) + 1
    return counts

# =====================================================================
# HELPER FUNCTIONS FOR ENHANCED PROCESSING
# =====================================================================

def add_version_headers(response, version: str):
    """Add version headers to API responses"""
    response.headers['X-API-Version'] = version
    response.headers['X-Enhanced-Features'] = 'true'
    return response

def generate_entity_intelligence_summary(user_id: int) -> Dict:
    """Generate comprehensive entity intelligence summary"""
    try:
        with get_db_manager().get_session() as session:
            # Count entities
            topics_count = session.query(Topic).filter(Topic.user_id == user_id).count()
            people_count = session.query(Person).filter(Person.user_id == user_id).count()
            tasks_count = session.query(Task).filter(Task.user_id == user_id).count()
            insights_count = session.query(IntelligenceInsight).filter(IntelligenceInsight.user_id == user_id).count()
            
            # Active entities (recently updated)
            week_ago = datetime.utcnow() - timedelta(days=7)
            active_topics = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.last_mentioned > week_ago
            ).count()
            
            recent_contacts = session.query(Person).filter(
                Person.user_id == user_id,
                Person.last_contact > week_ago
            ).count()
            
            return {
                'entity_counts': {
                    'topics': topics_count,
                    'people': people_count,
                    'tasks': tasks_count,
                    'insights': insights_count,
                    'total_entities': topics_count + people_count + tasks_count
                },
                'activity_metrics': {
                    'active_topics': active_topics,
                    'recent_contacts': recent_contacts,
                    'activity_rate': (active_topics + recent_contacts) / max(1, topics_count + people_count)
                },
                'intelligence_density': {
                    'topics_per_person': topics_count / max(1, people_count),
                    'tasks_per_topic': tasks_count / max(1, topics_count),
                    'insights_per_entity': insights_count / max(1, topics_count + people_count)
                }
            }
            
    except Exception as e:
        logger.error(f"Failed to generate entity intelligence summary: {str(e)}")
        return {}

def analyze_entity_relationships(user_id: int) -> Dict:
    """Analyze entity relationships and connection patterns"""
    try:
        with get_db_manager().get_session() as session:
            relationships = session.query(EntityRelationship).filter(
                EntityRelationship.user_id == user_id
            ).all()
            
            relationship_types = {}
            strong_relationships = 0
            
            for rel in relationships:
                rel_type = rel.relationship_type
                relationship_types[rel_type] = relationship_types.get(rel_type, 0) + 1
                
                if rel.strength > 0.7:
                    strong_relationships += 1
            
            return {
                'total_relationships': len(relationships),
                'relationship_types': relationship_types,
                'strong_relationships': strong_relationships,
                'relationship_density': len(relationships) / max(1, session.query(Topic).filter(Topic.user_id == user_id).count() + session.query(Person).filter(Person.user_id == user_id).count()),
                'avg_relationship_strength': sum(rel.strength for rel in relationships) / max(1, len(relationships))
            }
            
    except Exception as e:
        logger.error(f"Failed to analyze entity relationships: {str(e)}")
        return {}

def calculate_intelligence_quality_metrics(user_id: int) -> Dict:
    """Calculate intelligence quality metrics"""
    try:
        with get_db_manager().get_session() as session:
            # High confidence entities
            high_conf_topics = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.confidence_score > 0.8
            ).count()
            
            high_conf_tasks = session.query(Task).filter(
                Task.user_id == user_id,
                Task.confidence > 0.8
            ).count()
            
            # Entities with context
            topics_with_summary = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.intelligence_summary.isnot(None)
            ).count()
            
            tasks_with_context = session.query(Task).filter(
                Task.user_id == user_id,
                Task.context_story.isnot(None)
            ).count()
            
            total_topics = session.query(Topic).filter(Topic.user_id == user_id).count()
            total_tasks = session.query(Task).filter(Task.user_id == user_id).count()
            
            return {
                'confidence_metrics': {
                    'high_confidence_topics': high_conf_topics / max(1, total_topics),
                    'high_confidence_tasks': high_conf_tasks / max(1, total_tasks)
                },
                'context_richness': {
                    'topics_with_intelligence': topics_with_summary / max(1, total_topics),
                    'tasks_with_context': tasks_with_context / max(1, total_tasks)
                },
                'overall_quality_score': (
                    (high_conf_topics / max(1, total_topics)) +
                    (high_conf_tasks / max(1, total_tasks)) +
                    (topics_with_summary / max(1, total_topics)) +
                    (tasks_with_context / max(1, total_tasks))
                ) / 4
            }
            
    except Exception as e:
        logger.error(f"Failed to calculate intelligence quality metrics: {str(e)}")
        return {}

def calculate_person_engagement_score(person: Person) -> float:
    """Calculate engagement score for a person"""
    score = 0.0
    
    # Interaction frequency
    if person.total_interactions > 10:
        score += 0.3
    elif person.total_interactions > 5:
        score += 0.2
    elif person.total_interactions > 0:
        score += 0.1
    
    # Recent contact
    if person.last_contact and person.last_contact > datetime.utcnow() - timedelta(days=7):
        score += 0.3
    elif person.last_contact and person.last_contact > datetime.utcnow() - timedelta(days=30):
        score += 0.2
    
    # Professional context
    if person.professional_story:
        score += 0.2
    
    # Topic connections
    topic_count = len(person.topics) if person.topics else 0
    if topic_count > 3:
        score += 0.2
    elif topic_count > 0:
        score += 0.1
    
    return min(1.0, score)

def count_insights_by_type(insights_data: List[Dict]) -> Dict:
    """Count insights by type"""
    counts = {}
    for insight in insights_data:
        insight_type = insight['insight_type']
        counts[insight_type] = counts.get(insight_type, 0) + 1
    return counts

def count_insights_by_priority(insights_data: List[Dict]) -> Dict:
    """Count insights by priority"""
    counts = {}
    for insight in insights_data:
        priority = insight['priority']
        counts[priority] = counts.get(priority, 0) + 1
    return counts

# =====================================================================
# ERROR HANDLERS
# =====================================================================

@app.errorhandler(404)
def not_found_error(error):
    return render_template('404.html', api_version=CURRENT_VERSION), 404

@app.errorhandler(500)
def internal_error(error):
    logger.error(f"Internal server error: {str(error)}")
    return render_template('500.html', api_version=CURRENT_VERSION), 500

# =====================================================================
# APPLICATION STARTUP
# =====================================================================

if __name__ == '__main__':
    # Validate configuration
    config_errors = settings.validate_config()
    if config_errors:
        logger.error("Configuration errors:")
        for error in config_errors:
            logger.error(f"  - {error}")
        exit(1)
    
    logger.info("Starting AI Chief of Staff Enhanced Application v2.0...")
    logger.info(f"Database URL: {settings.DATABASE_URL}")
    logger.info(f"Environment: {'Production' if settings.is_production() else 'Development'}")
    logger.info(f"API Version: {CURRENT_VERSION}")
    
    # Initialize database with enhanced models
    try:
        get_db_manager().initialize_database()
        logger.info("Enhanced database initialized successfully")
    except Exception as e:
        logger.error(f"Database initialization failed: {str(e)}")
        exit(1)
    
    # Initialize processor manager
    try:
        # Test processor manager connection
        stats_result = processor_manager.get_processing_statistics()
        if stats_result['success']:
            logger.info("Processor manager initialized successfully")
        else:
            logger.warning(f"Processor manager warning: {stats_result['error']}")
    except Exception as e:
        logger.error(f"Processor manager initialization failed: {str(e)}")
        # Don't exit - application can still run with reduced functionality
    
    # Start real-time processing engine
    try:
        from processors.realtime_processing import realtime_processor
        realtime_processor.start(num_workers=3)
        logger.info("Real-time processing engine started successfully")
    except Exception as e:
        logger.error(f"Real-time processor startup failed: {str(e)}")
        # Don't exit - application can still run with batch processing
    
    logger.info("Enhanced API endpoints registered:")
    logger.info("  - Legacy compatibility maintained")
    logger.info("  - New v2 APIs available")
    logger.info("  - Real-time processing enabled")
    logger.info("  - Entity management active")
    logger.info("  - Analytics engine ready")
    
    # Start the application
    app.run(
        host='0.0.0.0',
        port=settings.PORT,
        debug=settings.DEBUG
    ) 

============================================================
FILE: archive/backup_files/v1_original/PROCESSOR_ARCHIVAL_README.md
============================================================
# Processor Archival Documentation

## Overview
This document describes the processors that were archived during the entity-centric transformation of the AI Chief of Staff system. The old processors have been replaced with enhanced, unified processors that provide superior functionality.

## Archived Processors

### 1. task_extractor.py (49KB, 1157 lines)
**Original Purpose**: Extract tasks from emails and text
**Archived Location**: `backup/v1_original/processors_archived/task_extractor.py`
**Replacement**: `processors/enhanced_processors/enhanced_task_processor.py`

**Key Improvements in Replacement**:
- Uses unified entity engine for context-aware task creation
- Creates full entity relationships (assignees, topics, projects)  
- Provides task analytics and pattern analysis
- Supports real-time processing
- Better context stories explaining WHY tasks exist

**Migration Notes**: 
- Legacy interface available via `processors/adapter_layer.py`
- Import `from processors.adapter_layer import task_extractor` for compatibility

### 2. email_intelligence.py (74KB, 1477 lines)
**Original Purpose**: AI-powered email analysis and intelligence extraction
**Archived Location**: `backup/v1_original/processors_archived/email_intelligence.py`
**Replacement**: `processors/enhanced_processors/enhanced_email_processor.py`

**Key Improvements in Replacement**:
- Entity-centric processing with relationship building
- Real-time processing capabilities
- Comprehensive business context extraction
- Historical communication analysis
- Batch processing optimizations
- Integration with unified AI pipeline

**Migration Notes**:
- Legacy interface available via `processors/adapter_layer.py`
- Import `from processors.adapter_layer import email_intelligence` for compatibility

### 3. email_normalizer.py (17KB, 446 lines)
**Original Purpose**: Clean and normalize email data from Gmail API
**Archived Location**: `backup/v1_original/processors_archived/email_normalizer.py`
**Replacement**: `processors/enhanced_processors/enhanced_data_normalizer.py`

**Key Improvements in Replacement**:
- Multi-source normalization (email, calendar, contacts)
- Quality assessment and issue detection
- Structured data extraction (emails, phones, URLs, dates)
- Content signatures for deduplication
- Comprehensive metadata extraction
- Enhanced content cleaning algorithms

**Migration Notes**:
- Legacy interface available via `processors/adapter_layer.py`
- Import `from processors.adapter_layer import email_normalizer` for compatibility

## New Processor Architecture

### Enhanced Processors
1. **Enhanced Task Processor**: Context-aware task management with entity integration
2. **Enhanced Email Processor**: Comprehensive email intelligence with relationship building
3. **Enhanced Data Normalizer**: Multi-source data cleaning and preparation

### Unified Processors
1. **Unified Entity Engine**: Central hub for all entity operations and relationships
2. **Enhanced AI Pipeline**: Context-aware AI processing with single-pass efficiency
3. **Real-time Processor**: Continuous intelligence with proactive insights

### Integration Layer
1. **Adapter Layer**: Backward compatibility for existing code
2. **Integration Manager**: Unified interface coordinating all processors

## Using the New Architecture

### For New Development
Use the integration manager for all processor interactions:

```python
from processors.integration_manager import integration_manager

# Process email with full entity creation
result = integration_manager.process_email_complete(email_data, user_id)

# Create manual task with relationships
task_result = integration_manager.create_manual_task_complete(
    task_description="Review proposal", 
    user_id=user_id,
    topic_names=["business development"],
    priority="high"
)

# Generate comprehensive insights
insights = integration_manager.generate_user_insights(user_id)
```

### For Legacy Compatibility
Use adapter layer for existing code:

```python
# These imports provide the same interface as before
from processors.adapter_layer import task_extractor, email_intelligence, email_normalizer

# Existing code continues to work unchanged
tasks = task_extractor.extract_tasks_from_email(email_data, user_id)
intelligence = email_intelligence.process_email(email_data, user_id)
normalized = email_normalizer.normalize_gmail_email(raw_email)
```

## Performance Improvements

### Processing Speed
- **Single-pass AI processing**: One AI call handles multiple entity types
- **Batch processing**: Optimized for processing multiple emails
- **Real-time queuing**: Background processing for improved responsiveness

### Intelligence Quality  
- **Entity relationships**: Deep connections between people, topics, tasks
- **Historical context**: Leverages past interactions for better insights
- **Business intelligence**: Strategic importance assessment and recommendations

### Scalability
- **Unified entity management**: Consistent handling across all data types
- **Real-time processing**: Scales to handle continuous data streams
- **Caching and optimization**: Reduced redundant processing

## Rollback Instructions

If rollback to original processors is needed:

1. **Copy archived processors back**:
   ```bash
   cp backup/v1_original/processors_archived/*.py processors/
   ```

2. **Update imports in application code**:
   - Remove imports from `processors.integration_manager`
   - Remove imports from `processors.adapter_layer`
   - Restore direct imports to `processors.task_extractor`, etc.

3. **Database considerations**:
   - Enhanced models will still exist but won't be populated
   - Original models remain functional
   - Run `data/migrations/001_entity_centric_migration.py rollback_migration()` if needed

## Benefits of New Architecture

### For Developers
- **Unified interface**: Single integration manager for all processor needs
- **Better error handling**: Comprehensive result objects with quality scores
- **Testing support**: Enhanced testing capabilities with adapter layer
- **Documentation**: Clearer interfaces and better type hints

### For Users
- **Faster processing**: Optimized algorithms and caching
- **Better insights**: Context-aware AI with entity relationships
- **Proactive intelligence**: Real-time insights and recommendations
- **Improved accuracy**: Quality assessment and confidence scoring

### For Operations
- **Monitoring**: Built-in processing statistics and performance metrics
- **Debugging**: Comprehensive logging and error tracking
- **Scalability**: Real-time processing and batch optimizations
- **Maintenance**: Modular architecture with clear separation of concerns

## Support and Migration Assistance

For questions about the new processor architecture or migration assistance:

1. **Check adapter layer compatibility**: Most existing code should work unchanged
2. **Review integration manager documentation**: For new development patterns
3. **Test thoroughly**: Validate existing functionality with new processors
4. **Performance monitoring**: Compare processing times and accuracy

## Version Information

- **Original processors archived**: Entity-centric migration
- **New architecture version**: Enhanced v2.0
- **Backward compatibility**: Available via adapter layer
- **Migration date**: $(date)
- **Migration completed**: Steps 14-17 of 55-step transformation

---

This archival maintains full backward compatibility while providing a foundation for advanced entity-centric intelligence capabilities. 

============================================================
FILE: archive/backup_files/v1_original/main.py
============================================================
# Main Flask application for AI Chief of Staff

import os
import logging
from datetime import datetime
from flask import Flask, render_template, request, jsonify, session, redirect, url_for, flash
import anthropic

from config.settings import settings
from auth.gmail_auth import gmail_auth
from ingest.gmail_fetcher import gmail_fetcher
from processors.email_normalizer import email_normalizer
from processors.task_extractor import task_extractor
from models.database import get_db_manager, Email, Task

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Create Flask application
app = Flask(__name__)
app.config['SECRET_KEY'] = settings.SECRET_KEY
app.config['SESSION_TYPE'] = 'filesystem'

# Initialize Claude client for chat
claude_client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)

@app.route('/')
def index():
    """Main dashboard route"""
    user_email = session.get('user_email')
    
    if not user_email:
        return render_template('login.html')
    
    try:
        # Get user information
        user_info = gmail_auth.get_user_by_email(user_email)
        if not user_info:
            session.clear()
            return render_template('login.html')
        
        # Get user statistics
        user = get_db_manager().get_user_by_email(user_email)
        user_stats = {
            'total_emails': 0,
            'total_tasks': 0,
            'pending_tasks': 0,
            'completed_tasks': 0
        }
        
        if user:
            with get_db_manager().get_session() as db_session:
                user_stats['total_emails'] = db_session.query(Email).filter(
                    Email.user_id == user.id
                ).count()
                
                user_stats['total_tasks'] = db_session.query(Task).filter(
                    Task.user_id == user.id
                ).count()
                
                user_stats['pending_tasks'] = db_session.query(Task).filter(
                    Task.user_id == user.id,
                    Task.status == 'pending'
                ).count()
                
                user_stats['completed_tasks'] = db_session.query(Task).filter(
                    Task.user_id == user.id,
                    Task.status == 'completed'
                ).count()
        
        return render_template('dashboard.html', 
                             user_info=user_info, 
                             user_stats=user_stats)
    
    except Exception as e:
        logger.error(f"Dashboard error for {user_email}: {str(e)}")
        flash('An error occurred loading your dashboard. Please try again.', 'error')
        return render_template('dashboard.html', 
                             user_info={'email': user_email}, 
                             user_stats={'total_emails': 0, 'total_tasks': 0})

@app.route('/login')
def login():
    """Login page"""
    return render_template('login.html')

@app.route('/auth/google')
def auth_google():
    """Initiate Google OAuth flow"""
    try:
        auth_url, state = gmail_auth.get_authorization_url('user_session')
        session['oauth_state'] = state
        return redirect(auth_url)
    except Exception as e:
        logger.error(f"Google auth initiation error: {str(e)}")
        flash('Failed to initiate Google authentication. Please try again.', 'error')
        return redirect(url_for('login'))

@app.route('/auth/callback')
def auth_callback():
    """Handle OAuth callback"""
    try:
        authorization_code = request.args.get('code')
        state = request.args.get('state')
        
        if not authorization_code:
            flash('Authorization failed. Please try again.', 'error')
            return redirect(url_for('login'))
        
        # Handle OAuth callback
        result = gmail_auth.handle_oauth_callback(authorization_code, state)
        
        if result['success']:
            session['user_email'] = result['user_email']
            session['authenticated'] = True
            flash(f'Successfully authenticated as {result["user_email"]}!', 'success')
            return redirect(url_for('index'))
        else:
            flash(f'Authentication failed: {result["error"]}', 'error')
            return redirect(url_for('login'))
    
    except Exception as e:
        logger.error(f"OAuth callback error: {str(e)}")
        flash('Authentication error occurred. Please try again.', 'error')
        return redirect(url_for('login'))

@app.route('/logout')
def logout():
    """Logout user"""
    user_email = session.get('user_email')
    session.clear()
    
    if user_email:
        flash(f'Successfully logged out from {user_email}', 'success')
    
    return redirect(url_for('login'))

@app.route('/api/process-emails', methods=['POST'])
def api_process_emails():
    """API endpoint to fetch and process emails"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        data = request.get_json() or {}
        days_back = data.get('days_back', 7)
        limit = data.get('limit', 50)
        force_refresh = data.get('force_refresh', False)
        
        # Step 1: Fetch emails
        logger.info(f"Fetching emails for {user_email}")
        fetch_result = gmail_fetcher.fetch_recent_emails(
            user_email, 
            days_back=days_back, 
            limit=limit,
            force_refresh=force_refresh
        )
        
        if not fetch_result['success']:
            return jsonify({
                'success': False, 
                'error': f"Failed to fetch emails: {fetch_result.get('error')}"
            }), 400
        
        # Step 2: Normalize emails
        logger.info(f"Normalizing emails for {user_email}")
        normalize_result = email_normalizer.normalize_user_emails(user_email, limit)
        
        # Step 3: Extract tasks
        logger.info(f"Extracting tasks for {user_email}")
        task_result = task_extractor.extract_tasks_for_user(user_email, limit)
        
        return jsonify({
            'success': True,
            'fetch_result': fetch_result,
            'normalize_result': normalize_result,
            'task_result': task_result,
            'processed_at': datetime.utcnow().isoformat()
        })
    
    except Exception as e:
        logger.error(f"Email processing error for {user_email}: {str(e)}")
        return jsonify({
            'success': False, 
            'error': f"Processing failed: {str(e)}"
        }), 500

@app.route('/api/emails')
def api_get_emails():
    """API endpoint to get user emails"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        limit = request.args.get('limit', 50, type=int)
        
        user = get_db_manager().get_user_by_email(user_email)
        if not user:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        emails = get_db_manager().get_user_emails(user.id, limit)
        
        return jsonify({
            'success': True,
            'emails': [email.to_dict() for email in emails],
            'count': len(emails)
        })
    
    except Exception as e:
        logger.error(f"Get emails error for {user_email}: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/tasks')
def api_get_tasks():
    """API endpoint to get user tasks"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        status = request.args.get('status')
        limit = request.args.get('limit', 100, type=int)
        
        result = task_extractor.get_user_tasks(user_email, status, limit)
        return jsonify(result)
    
    except Exception as e:
        logger.error(f"Get tasks error for {user_email}: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/tasks/<int:task_id>/status', methods=['PUT'])
def api_update_task_status(task_id):
    """API endpoint to update task status"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        data = request.get_json()
        new_status = data.get('status')
        
        if new_status not in ['pending', 'in_progress', 'completed', 'cancelled']:
            return jsonify({'success': False, 'error': 'Invalid status'}), 400
        
        result = task_extractor.update_task_status(user_email, task_id, new_status)
        return jsonify(result)
    
    except Exception as e:
        logger.error(f"Update task status error for {user_email}: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/chat', methods=['POST'])
def api_chat():
    """API endpoint for Claude chat"""
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'success': False, 'error': 'Not authenticated'}), 401
    
    try:
        data = request.get_json()
        message = data.get('message', '').strip()
        
        if not message:
            return jsonify({'success': False, 'error': 'Message is required'}), 400
        
        # Get user context for better responses
        user = get_db_manager().get_user_by_email(user_email)
        context_info = ""
        
        if user:
            with get_db_manager().get_session() as db_session:
                recent_tasks = db_session.query(Task).filter(
                    Task.user_id == user.id,
                    Task.status == 'pending'
                ).order_by(Task.created_at.desc()).limit(5).all()
                
                if recent_tasks:
                    task_list = "\n".join([f"- {task.description}" for task in recent_tasks])
                    context_info = f"\n\nYour recent pending tasks:\n{task_list}"
        
        # Build system prompt with context
        system_prompt = f"""You are an AI Chief of Staff assistant helping {user_email}. 
You have access to their email-derived tasks and can help with work organization, prioritization, and productivity.

Be helpful, professional, and concise. Focus on actionable advice related to their work and tasks.{context_info}"""
        
        # Call Claude
        response = claude_client.messages.create(
            model=settings.CLAUDE_MODEL,
            max_tokens=1000,
            temperature=0.3,
            system=system_prompt,
            messages=[{
                "role": "user",
                "content": message
            }]
        )
        
        reply = response.content[0].text
        
        return jsonify({
            'success': True,
            'message': message,
            'reply': reply,
            'timestamp': datetime.utcnow().isoformat()
        })
    
    except Exception as e:
        logger.error(f"Chat error for {user_email}: {str(e)}")
        return jsonify({
            'success': False, 
            'error': 'Failed to process chat message'
        }), 500

@app.route('/api/status')
def api_status():
    """API endpoint to get system status"""
    user_email = session.get('user_email')
    
    status = {
        'authenticated': bool(user_email),
        'user_email': user_email,
        'timestamp': datetime.utcnow().isoformat(),
        'database_connected': True,
        'gmail_auth_available': bool(settings.GOOGLE_CLIENT_ID and settings.GOOGLE_CLIENT_SECRET),
        'claude_available': bool(settings.ANTHROPIC_API_KEY)
    }
    
    # Test database connection
    try:
        get_db_manager().get_session().close()
    except Exception as e:
        status['database_connected'] = False
        status['database_error'] = str(e)
    
    # Test Gmail auth if user is authenticated
    if user_email:
        try:
            auth_status = gmail_auth.get_authentication_status(user_email)
            status['gmail_auth_status'] = auth_status
        except Exception as e:
            status['gmail_auth_error'] = str(e)
    
    return jsonify(status)

@app.errorhandler(404)
def not_found_error(error):
    return render_template('404.html'), 404

@app.errorhandler(500)
def internal_error(error):
    logger.error(f"Internal server error: {str(error)}")
    return render_template('500.html'), 500

if __name__ == '__main__':
    # Validate configuration
    config_errors = settings.validate_config()
    if config_errors:
        logger.error("Configuration errors:")
        for error in config_errors:
            logger.error(f"  - {error}")
        exit(1)
    
    logger.info("Starting AI Chief of Staff web application...")
    logger.info(f"Database URL: {settings.DATABASE_URL}")
    logger.info(f"Environment: {'Production' if settings.is_production() else 'Development'}")
    
    # Initialize database
    try:
        get_db_manager().initialize_database()
        logger.info("Database initialized successfully")
    except Exception as e:
        logger.error(f"Database initialization failed: {str(e)}")
        exit(1)
    
    app.run(
        host='0.0.0.0',
        port=settings.PORT,
        debug=settings.DEBUG
    ) 

============================================================
FILE: archive/backup_files/v1_original/processors_archived/email_normalizer.py
============================================================
# Normalizes raw Gmail data into clean format

import re
import logging
from datetime import datetime
from typing import Dict, List, Optional
from html import unescape
from bs4 import BeautifulSoup

from models.database import get_db_manager, Email

logger = logging.getLogger(__name__)

class EmailNormalizer:
    """Normalizes emails into clean, standardized format with entity extraction"""
    
    def __init__(self):
        self.version = "1.0"
        
    def normalize_user_emails(self, user_email: str, limit: int = None) -> Dict:
        """
        Normalize all emails for a user that haven't been normalized yet
        
        Args:
            user_email: Email of the user
            limit: Maximum number of emails to process
            
        Returns:
            Dictionary with normalization results
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            # Get emails that need normalization
            with get_db_manager().get_session() as session:
                emails = session.query(Email).filter(
                    Email.user_id == user.id,
                    Email.body_clean.is_(None)  # Not normalized yet
                ).limit(limit or 100).all()
            
            if not emails:
                logger.info(f"No emails to normalize for {user_email}")
                return {
                    'success': True,
                    'user_email': user_email,
                    'processed': 0,
                    'message': 'No emails need normalization'
                }
            
            processed_count = 0
            error_count = 0
            
            for email in emails:
                try:
                    # Convert database email to dict for processing
                    email_dict = {
                        'id': email.gmail_id,
                        'subject': email.subject,
                        'body_text': email.body_text,
                        'body_html': email.body_html,
                        'sender': email.sender,
                        'sender_name': email.sender_name,
                        'snippet': email.snippet,
                        'timestamp': email.email_date
                    }
                    
                    # Normalize the email
                    normalized = self.normalize_email(email_dict)
                    
                    # Update the database record
                    with get_db_manager().get_session() as session:
                        email_record = session.query(Email).filter(
                            Email.user_id == user.id,
                            Email.gmail_id == email.gmail_id
                        ).first()
                        
                        if email_record:
                            email_record.body_clean = normalized.get('body_clean')
                            email_record.body_preview = normalized.get('body_preview')
                            email_record.entities = normalized.get('entities', {})
                            email_record.message_type = normalized.get('message_type')
                            email_record.priority_score = normalized.get('priority_score')
                            email_record.normalizer_version = self.version
                            
                            session.commit()
                            processed_count += 1
                    
                except Exception as e:
                    logger.error(f"Failed to normalize email {email.gmail_id}: {str(e)}")
                    error_count += 1
                    continue
            
            logger.info(f"Normalized {processed_count} emails for {user_email} ({error_count} errors)")
            
            return {
                'success': True,
                'user_email': user_email,
                'processed': processed_count,
                'errors': error_count,
                'normalizer_version': self.version
            }
            
        except Exception as e:
            logger.error(f"Failed to normalize emails for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def normalize_email(self, email_data: Dict) -> Dict:
        """
        Normalize a single email into clean format
        
        Args:
            email_data: Raw email data dictionary
            
        Returns:
            Normalized email data
        """
        try:
            # Start with original data
            normalized = email_data.copy()
            
            # Clean and extract body content
            body_clean = self._extract_clean_body(email_data)
            normalized['body_clean'] = body_clean
            
            # Create preview (first 300 chars)
            normalized['body_preview'] = self._create_preview(body_clean)
            
            # Extract entities
            normalized['entities'] = self._extract_entities(email_data, body_clean)
            
            # Determine message type
            normalized['message_type'] = self._classify_message_type(email_data, body_clean)
            
            # Calculate priority score
            normalized['priority_score'] = self._calculate_priority_score(email_data, body_clean)
            
            # Add processing metadata
            normalized['processing_metadata'] = {
                'normalizer_version': self.version,
                'normalized_at': datetime.utcnow().isoformat(),
                'body_length': len(body_clean) if body_clean else 0
            }
            
            return normalized
            
        except Exception as e:
            logger.error(f"Failed to normalize email {email_data.get('id', 'unknown')}: {str(e)}")
            return {
                **email_data,
                'normalization_error': str(e),
                'processing_metadata': {
                    'normalizer_version': self.version,
                    'normalized_at': datetime.utcnow().isoformat(),
                    'error': True
                }
            }
    
    def _extract_clean_body(self, email_data: Dict) -> str:
        """
        Extract clean text from email body
        
        Args:
            email_data: Email data dictionary
            
        Returns:
            Clean body text
        """
        try:
            body_text = email_data.get('body_text', '')
            body_html = email_data.get('body_html', '')
            
            # Prefer HTML if available, fallback to text
            if body_html:
                # Parse HTML and extract text
                soup = BeautifulSoup(body_html, 'html.parser')
                
                # Remove script and style elements
                for script in soup(['script', 'style']):
                    script.decompose()
                
                # Get text and clean it
                text = soup.get_text()
                
                # Break into lines and remove leading/trailing spaces
                lines = (line.strip() for line in text.splitlines())
                
                # Break multi-headlines into a line each
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                
                # Drop blank lines
                clean_text = '\n'.join(chunk for chunk in chunks if chunk)
                
            elif body_text:
                clean_text = body_text
                
            else:
                # Fallback to snippet
                clean_text = email_data.get('snippet', '')
            
            if not clean_text:
                return ''
                
            # Remove quoted text (replies/forwards)
            clean_text = self._remove_quoted_text(clean_text)
            
            # Remove excessive whitespace
            clean_text = re.sub(r'\n\s*\n', '\n\n', clean_text)
            clean_text = re.sub(r' +', ' ', clean_text)
            
            # Decode HTML entities
            clean_text = unescape(clean_text)
            
            return clean_text.strip()
            
        except Exception as e:
            logger.error(f"Failed to extract clean body: {str(e)}")
            return email_data.get('snippet', '')
    
    def _remove_quoted_text(self, text: str) -> str:
        """
        Remove quoted text from emails (replies/forwards)
        
        Args:
            text: Email body text
            
        Returns:
            Text with quoted sections removed
        """
        try:
            # Common quote patterns
            quote_patterns = [
                r'On .* wrote:.*',
                r'From:.*\nSent:.*\nTo:.*\nSubject:.*',
                r'-----Original Message-----.*',
                r'> .*',  # Lines starting with >
                r'________________________________.*',  # Outlook separator
                r'From: .*<.*>.*',
                r'Sent from my .*',
                r'\n\n.*On.*\d{4}.*at.*\d{1,2}:\d{2}.*wrote:'
            ]
            
            cleaned_text = text
            
            for pattern in quote_patterns:
                cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.DOTALL | re.IGNORECASE)
            
            # Remove excessive newlines created by quote removal
            cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text)
            
            return cleaned_text.strip()
            
        except Exception as e:
            logger.error(f"Failed to remove quoted text: {str(e)}")
            return text
    
    def _create_preview(self, body_text: str) -> str:
        """
        Create a preview of the email body
        
        Args:
            body_text: Clean email body text
            
        Returns:
            Preview text (first 300 characters)
        """
        if not body_text:
            return ''
        
        # Take first 300 characters
        preview = body_text[:300]
        
        # If we cut in the middle of a word, cut to last complete word
        if len(body_text) > 300:
            last_space = preview.rfind(' ')
            if last_space > 250:  # Only if we have a reasonable amount of text
                preview = preview[:last_space] + '...'
            else:
                preview += '...'
        
        return preview
    
    def _extract_entities(self, email_data: Dict, body_text: str) -> Dict:
        """
        Extract entities from email content
        
        Args:
            email_data: Email data dictionary
            body_text: Clean email body text
            
        Returns:
            Dictionary of extracted entities
        """
        try:
            entities = {
                'people': [],
                'companies': [],
                'dates': [],
                'times': [],
                'urls': [],
                'emails': [],
                'phone_numbers': [],
                'amounts': []
            }
            
            # Extract email addresses
            email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
            entities['emails'] = list(set(re.findall(email_pattern, body_text)))
            
            # Extract URLs
            url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
            entities['urls'] = list(set(re.findall(url_pattern, body_text)))
            
            # Extract phone numbers (US format)
            phone_pattern = r'\b(?:\+?1[-.\s]?)?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})\b'
            phone_matches = re.findall(phone_pattern, body_text)
            entities['phone_numbers'] = ['-'.join(match) for match in phone_matches]
            
            # Extract dates (simple patterns)
            date_patterns = [
                r'\b\d{1,2}/\d{1,2}/\d{4}\b',
                r'\b\d{1,2}-\d{1,2}-\d{4}\b',
                r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2},? \d{4}\b'
            ]
            for pattern in date_patterns:
                entities['dates'].extend(re.findall(pattern, body_text, re.IGNORECASE))
            
            # Extract times
            time_pattern = r'\b\d{1,2}:\d{2}(?:\s?[AP]M)?\b'
            entities['times'] = list(set(re.findall(time_pattern, body_text, re.IGNORECASE)))
            
            # Extract monetary amounts
            amount_pattern = r'\$\d{1,3}(?:,\d{3})*(?:\.\d{2})?'
            entities['amounts'] = list(set(re.findall(amount_pattern, body_text)))
            
            # Remove empty lists and duplicates
            for key in entities:
                entities[key] = list(set(entities[key])) if entities[key] else []
            
            return entities
            
        except Exception as e:
            logger.error(f"Failed to extract entities: {str(e)}")
            return {}
    
    def _classify_message_type(self, email_data: Dict, body_text: str) -> str:
        """
        Classify the type of email message
        
        Args:
            email_data: Email data dictionary
            body_text: Clean email body text
            
        Returns:
            Message type classification
        """
        try:
            subject = email_data.get('subject', '').lower()
            body_lower = body_text.lower() if body_text else ''
            sender = email_data.get('sender', '').lower()
            
            # Meeting/Calendar invites
            meeting_keywords = ['meeting', 'call', 'zoom', 'teams', 'webex', 'conference', 'invite', 'calendar']
            if any(keyword in subject for keyword in meeting_keywords):
                return 'meeting'
            
            # Automated/System emails
            system_domains = ['noreply', 'no-reply', 'donotreply', 'mailer-daemon', 'bounce']
            if any(domain in sender for domain in system_domains):
                return 'automated'
            
            # Newsletters/Marketing
            newsletter_keywords = ['unsubscribe', 'newsletter', 'marketing', 'promotional']
            if any(keyword in body_lower for keyword in newsletter_keywords):
                return 'newsletter'
            
            # Action required
            action_keywords = ['urgent', 'asap', 'deadline', 'required', 'please review', 'action needed']
            if any(keyword in subject for keyword in action_keywords):
                return 'action_required'
            
            # FYI/Information
            fyi_keywords = ['fyi', 'for your information', 'heads up', 'update', 'status']
            if any(keyword in subject for keyword in fyi_keywords):
                return 'informational'
            
            # Default to regular
            return 'regular'
            
        except Exception as e:
            logger.error(f"Failed to classify message type: {str(e)}")
            return 'regular'
    
    def _calculate_priority_score(self, email_data: Dict, body_text: str) -> float:
        """
        Calculate priority score for email (0.0 to 1.0)
        
        Args:
            email_data: Email data dictionary
            body_text: Clean email body text
            
        Returns:
            Priority score between 0.0 and 1.0
        """
        try:
            score = 0.5  # Base score
            
            subject = email_data.get('subject', '').lower()
            body_lower = body_text.lower() if body_text else ''
            
            # High priority keywords
            urgent_keywords = ['urgent', 'asap', 'emergency', 'critical', 'deadline']
            for keyword in urgent_keywords:
                if keyword in subject or keyword in body_lower:
                    score += 0.2
            
            # Medium priority keywords
            important_keywords = ['important', 'priority', 'please review', 'action needed']
            for keyword in important_keywords:
                if keyword in subject or keyword in body_lower:
                    score += 0.1
            
            # Questions increase priority slightly
            if '?' in subject or '?' in body_text:
                score += 0.05
            
            # Direct communication (personal emails)
            if '@' in email_data.get('sender', '') and 'noreply' not in email_data.get('sender', ''):
                score += 0.1
            
            # Reduce score for automated emails
            automated_keywords = ['unsubscribe', 'automated', 'noreply', 'notification']
            for keyword in automated_keywords:
                if keyword in email_data.get('sender', '').lower():
                    score -= 0.2
            
            # Ensure score is between 0.0 and 1.0
            return max(0.0, min(1.0, score))
            
        except Exception as e:
            logger.error(f"Failed to calculate priority score: {str(e)}")
            return 0.5

# Create global instance
email_normalizer = EmailNormalizer()

============================================================
FILE: archive/backup_files/v1_original/processors_archived/task_extractor.py
============================================================
# Extract actionable tasks from emails using Claude 4 Sonnet

import json
import logging
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional
import re
from dateutil import parser
import anthropic

from config.settings import settings
from models.database import get_db_manager, Email, Task

logger = logging.getLogger(__name__)

class TaskExtractor:
    """Extracts actionable tasks from emails using Claude 4 Sonnet"""
    
    def __init__(self):
        self.client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        self.model = "claude-3-5-sonnet-20241022"
        self.version = "1.0"
        
    def extract_tasks_for_user(self, user_email: str, limit: int = None, force_refresh: bool = False) -> Dict:
        """
        ENHANCED 360-CONTEXT TASK EXTRACTION
        
        Extract tasks with comprehensive business intelligence by cross-referencing:
        - Email communications & AI analysis
        - People relationships & interaction patterns
        - Project context & status
        - Calendar events & meeting intelligence
        - Topic analysis & business themes
        - Strategic decisions & opportunities
        
        Creates super relevant and actionable tasks with full business context
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            # COMPREHENSIVE BUSINESS CONTEXT COLLECTION
            business_context = self._get_360_business_context(user.id)
            
            # Get normalized emails that need task extraction
            with get_db_manager().get_session() as session:
                query = session.query(Email).filter(
                    Email.user_id == user.id,
                    Email.body_clean.isnot(None)  # Already normalized
                )
                
                if not force_refresh:
                    # Only process emails that don't have tasks yet
                    query = query.filter(~session.query(Task).filter(
                        Task.email_id == Email.id
                    ).exists())
                
                emails = query.limit(limit or 50).all()
            
            if not emails:
                logger.info(f"No emails to process for 360-context task extraction for {user_email}")
                return {
                    'success': True,
                    'user_email': user_email,
                    'processed_emails': 0,
                    'extracted_tasks': 0,
                    'message': 'No emails need 360-context task extraction'
                }
            
            processed_emails = 0
            total_tasks = 0
            error_count = 0
            context_enhanced_tasks = 0
            
            for email in emails:
                try:
                    # Convert database email to dict for processing
                    email_dict = {
                        'id': email.gmail_id,
                        'subject': email.subject,
                        'sender': email.sender,
                        'sender_name': email.sender_name,
                        'body_clean': email.body_clean,
                        'body_preview': email.body_preview,
                        'timestamp': email.email_date,
                        'message_type': email.message_type,
                        'priority_score': email.priority_score,
                        'ai_summary': email.ai_summary,
                        'key_insights': email.key_insights,
                        'topics': email.topics
                    }
                    
                    # ENHANCED EXTRACTION with 360-context
                    extraction_result = self.extract_tasks_with_360_context(email_dict, business_context)
                    
                    if extraction_result['success'] and extraction_result['tasks']:
                        # Save tasks to database with enhanced context
                        for task_data in extraction_result['tasks']:
                            task_data['email_id'] = email.id
                            task_data['extractor_version'] = f"{self.version}_360_context"
                            task_data['model_used'] = self.model
                            
                            # Check if this task was context-enhanced
                            if task_data.get('context_enhanced'):
                                context_enhanced_tasks += 1
                            
                            get_db_manager().save_task(user.id, email.id, task_data)
                            total_tasks += 1
                    
                    processed_emails += 1
                    
                except Exception as e:
                    logger.error(f"Failed to extract 360-context tasks from email {email.gmail_id}: {str(e)}")
                    error_count += 1
                    continue
            
            logger.info(f"Extracted {total_tasks} tasks ({context_enhanced_tasks} context-enhanced) from {processed_emails} emails for {user_email} ({error_count} errors)")
            
            return {
                'success': True,
                'user_email': user_email,
                'processed_emails': processed_emails,
                'extracted_tasks': total_tasks,
                'context_enhanced_tasks': context_enhanced_tasks,
                'errors': error_count,
                'extractor_version': f"{self.version}_360_context"
            }
            
        except Exception as e:
            logger.error(f"Failed to extract 360-context tasks for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _get_360_business_context(self, user_id: int) -> Dict:
        """
        Collect comprehensive business intelligence context for task extraction
        """
        try:
            context = {
                'people': [],
                'projects': [],
                'topics': [],
                'calendar_events': [],
                'recent_decisions': [],
                'opportunities': [],
                'relationship_map': {},
                'project_map': {},
                'topic_keywords': {}
            }
            
            # Get business data
            people = get_db_manager().get_user_people(user_id, limit=100)
            projects = get_db_manager().get_user_projects(user_id, limit=50)
            topics = get_db_manager().get_user_topics(user_id, limit=50)
            calendar_events = get_db_manager().get_user_calendar_events(user_id, limit=50)
            emails = get_db_manager().get_user_emails(user_id, limit=100)
            
            # Process people for relationship context
            for person in people:
                if person.name and person.email_address:
                    person_info = {
                        'name': person.name,
                        'email': person.email_address,
                        'company': person.company,
                        'title': person.title,
                        'relationship': person.relationship_type,
                        'total_emails': person.total_emails or 0,
                        'importance': person.importance_level or 0.5
                    }
                    context['people'].append(person_info)
                    context['relationship_map'][person.email_address.lower()] = person_info
            
            # Process projects for context linking
            for project in projects:
                if project.name and project.status == 'active':
                    project_info = {
                        'name': project.name,
                        'description': project.description,
                        'status': project.status,
                        'priority': project.priority,
                        'stakeholders': project.stakeholders or []
                    }
                    context['projects'].append(project_info)
                    context['project_map'][project.name.lower()] = project_info
            
            # Process topics for keyword matching
            for topic in topics:
                if topic.name:
                    topic_info = {
                        'name': topic.name,
                        'description': topic.description,
                        'keywords': json.loads(topic.keywords) if topic.keywords else [],
                        'is_official': topic.is_official
                    }
                    context['topics'].append(topic_info)
                    # Build keyword map for topic detection
                    all_keywords = [topic.name.lower()] + [kw.lower() for kw in topic_info['keywords']]
                    for keyword in all_keywords:
                        if keyword not in context['topic_keywords']:
                            context['topic_keywords'][keyword] = []
                        context['topic_keywords'][keyword].append(topic_info)
            
            # Process calendar events for meeting context
            now = datetime.now(timezone.utc)
            upcoming_meetings = [e for e in calendar_events if e.start_time and e.start_time > now]
            for meeting in upcoming_meetings[:20]:  # Next 20 meetings
                meeting_info = {
                    'title': meeting.title,
                    'start_time': meeting.start_time,
                    'attendees': meeting.attendees or [],
                    'description': meeting.description
                }
                context['calendar_events'].append(meeting_info)
            
            # Extract recent decisions and opportunities from emails
            for email in emails[-30:]:  # Recent 30 emails
                if email.key_insights and isinstance(email.key_insights, dict):
                    decisions = email.key_insights.get('key_decisions', [])
                    context['recent_decisions'].extend(decisions[:2])  # Top 2 per email
                    
                    opportunities = email.key_insights.get('strategic_opportunities', [])
                    context['opportunities'].extend(opportunities[:2])  # Top 2 per email
            
            return context
            
        except Exception as e:
            logger.error(f"Failed to get 360-context for task extraction: {str(e)}")
            return {}
    
    def extract_tasks_with_360_context(self, email_data: Dict, business_context: Dict) -> Dict:
        """
        Extract actionable tasks with comprehensive 360-context intelligence
        
        Args:
            email_data: Normalized email data dictionary with AI analysis
            business_context: Comprehensive business intelligence context
            
        Returns:
            Dictionary containing extracted tasks with enhanced context
        """
        try:
            # Check if email has enough content for task extraction
            body_clean = email_data.get('body_clean', '')
            if not body_clean or len(body_clean.strip()) < 20:
                return {
                    'success': True,
                    'email_id': email_data.get('id'),
                    'tasks': [],
                    'reason': 'Email content too short for task extraction'
                }
            
            # Skip certain message types that unlikely contain tasks
            message_type = email_data.get('message_type', 'regular')
            if message_type in ['newsletter', 'automated']:
                return {
                    'success': True,
                    'email_id': email_data.get('id'),
                    'tasks': [],
                    'reason': f'Message type "{message_type}" skipped for task extraction'
                }
            
            # ANALYZE BUSINESS CONTEXT CONNECTIONS
            email_context = self._analyze_email_business_connections(email_data, business_context)
            
            # Prepare enhanced email context for Claude
            enhanced_email_context = self._prepare_360_email_context(email_data, email_context, business_context)
            
            # Call Claude for 360-context task extraction
            claude_response = self._call_claude_for_360_tasks(enhanced_email_context, email_context)
            
            if not claude_response:
                return {
                    'success': False,
                    'email_id': email_data.get('id'),
                    'error': 'Failed to get response from Claude for 360-context extraction'
                }
            
            # Parse Claude's response with context enhancement
            tasks = self._parse_claude_360_response(claude_response, email_data, email_context)
            
            # Enhance tasks with 360-context metadata
            enhanced_tasks = []
            for task in tasks:
                enhanced_task = self._enhance_task_with_360_context(task, email_data, email_context, business_context)
                enhanced_tasks.append(enhanced_task)
            
            return {
                'success': True,
                'email_id': email_data.get('id'),
                'tasks': enhanced_tasks,
                'extraction_metadata': {
                    'extracted_at': datetime.utcnow().isoformat(),
                    'extractor_version': f"{self.version}_360_context",
                    'model_used': self.model,
                    'email_priority': email_data.get('priority_score', 0.5),
                    'context_connections': email_context.get('connection_count', 0),
                    'business_intelligence_used': True
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to extract 360-context tasks from email {email_data.get('id', 'unknown')}: {str(e)}")
            return {
                'success': False,
                'email_id': email_data.get('id'),
                'error': str(e)
            }
    
    def _prepare_email_context(self, email_data: Dict) -> str:
        """
        Prepare email context for Claude task extraction
        
        Args:
            email_data: Email data dictionary
            
        Returns:
            Formatted email context string
        """
        sender = email_data.get('sender_name') or email_data.get('sender', '')
        subject = email_data.get('subject', '')
        body = email_data.get('body_clean', '')
        timestamp = email_data.get('timestamp')
        
        # Format timestamp
        if timestamp:
            try:
                if isinstance(timestamp, str):
                    timestamp = parser.parse(timestamp)
                date_str = timestamp.strftime('%Y-%m-%d %H:%M')
            except:
                date_str = 'Unknown date'
        else:
            date_str = 'Unknown date'
        
        context = f"""Email Details:
From: {sender}
Date: {date_str}
Subject: {subject}

Email Content:
{body}
"""
        return context
    
    def _call_claude_for_tasks(self, email_context: str) -> Optional[str]:
        """
        Call Claude 4 Sonnet to extract tasks from email
        
        Args:
            email_context: Formatted email context
            
        Returns:
            Claude's response or None if failed
        """
        try:
            system_prompt = """You are an expert AI assistant that extracts actionable tasks from emails. Your job is to identify specific tasks, action items, deadlines, and follow-ups from email content.

Please analyze the email and extract actionable tasks following these guidelines:

1. **Task Identification**: Look for:
   - Direct requests or assignments
   - Deadlines and due dates
   - Follow-up actions needed
   - Meetings to schedule or attend
   - Documents to review or create
   - Decisions to make
   - Items requiring response

2. **Task Details**: For each task, identify:
   - Clear description of what needs to be done
   - Who is responsible (assignee)
   - When it needs to be done (due date/deadline)
   - Priority level (high, medium, low)
   - Category (follow-up, deadline, meeting, review, etc.)

3. **Response Format**: Return a JSON array of tasks. Each task should have:
   - "description": Clear, actionable description
   - "assignee": Who should do this (if mentioned)
   - "due_date": Specific date if mentioned (YYYY-MM-DD format)
   - "due_date_text": Original due date text from email
   - "priority": high/medium/low based on urgency and importance
   - "category": type of task (follow-up, deadline, meeting, review, etc.)
   - "confidence": 0.0-1.0 confidence score
   - "source_text": Original text from email that led to this task

Return ONLY the JSON array. If no actionable tasks are found, return an empty array []."""

            user_prompt = f"""Please analyze this email and extract actionable tasks:

{email_context}

Remember to return only a JSON array of tasks, or an empty array [] if no actionable tasks are found."""

            message = self.client.messages.create(
                model=self.model,
                max_tokens=2000,
                temperature=0.1,
                system=system_prompt,
                messages=[
                    {
                        "role": "user",
                        "content": user_prompt
                    }
                ]
            )
            
            response_text = message.content[0].text.strip()
            logger.debug(f"Claude response: {response_text}")
            
            return response_text
            
        except Exception as e:
            logger.error(f"Failed to call Claude for task extraction: {str(e)}")
            return None
    
    def _parse_claude_response(self, response: str, email_data: Dict) -> List[Dict]:
        """
        Parse Claude's JSON response into task dictionaries
        
        Args:
            response: Claude's response text
            email_data: Original email data
            
        Returns:
            List of task dictionaries
        """
        try:
            # Try to find JSON in the response
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            
            if json_start == -1 or json_end == 0:
                logger.warning("No JSON array found in Claude response")
                return []
            
            json_text = response[json_start:json_end]
            
            # Parse JSON
            tasks_data = json.loads(json_text)
            
            if not isinstance(tasks_data, list):
                logger.warning("Claude response is not a JSON array")
                return []
            
            tasks = []
            for task_data in tasks_data:
                if isinstance(task_data, dict) and task_data.get('description'):
                    tasks.append(task_data)
            
            logger.info(f"Parsed {len(tasks)} tasks from Claude response")
            return tasks
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Claude JSON response: {str(e)}")
            logger.error(f"Response was: {response}")
            return []
        except Exception as e:
            logger.error(f"Failed to parse Claude response: {str(e)}")
            return []
    
    def _enhance_task(self, task: Dict, email_data: Dict) -> Dict:
        """
        Enhance task with additional metadata and processing
        
        Args:
            task: Task dictionary from Claude
            email_data: Original email data
            
        Returns:
            Enhanced task dictionary
        """
        try:
            # Start with Claude's task data
            enhanced_task = task.copy()
            
            # Parse due date if provided
            if task.get('due_date'):
                try:
                    # Try to parse various date formats
                    due_date = parser.parse(task['due_date'])
                    enhanced_task['due_date'] = due_date
                except:
                    # If parsing fails, try to extract from due_date_text
                    enhanced_task['due_date'] = self._extract_date_from_text(
                        task.get('due_date_text', '')
                    )
            elif task.get('due_date_text'):
                enhanced_task['due_date'] = self._extract_date_from_text(task['due_date_text'])
            
            # Set default values
            enhanced_task['priority'] = task.get('priority', 'medium').lower()
            enhanced_task['category'] = task.get('category', 'action_item')
            enhanced_task['confidence'] = min(1.0, max(0.0, task.get('confidence', 0.8)))
            enhanced_task['status'] = 'pending'
            
            # Determine assignee context
            if not enhanced_task.get('assignee'):
                # If no specific assignee mentioned, assume it's for the email recipient
                enhanced_task['assignee'] = 'me'
            
            # Enhance priority based on email priority and urgency
            email_priority = email_data.get('priority_score', 0.5)
            if email_priority > 0.8:
                if enhanced_task['priority'] == 'medium':
                    enhanced_task['priority'] = 'high'
            
            # Add contextual category if not specified
            if enhanced_task['category'] == 'action_item':
                enhanced_task['category'] = self._determine_category(
                    enhanced_task['description'], 
                    email_data
                )
            
            return enhanced_task
            
        except Exception as e:
            logger.error(f"Failed to enhance task: {str(e)}")
            return task
    
    def _extract_date_from_text(self, text: str) -> Optional[datetime]:
        """
        Extract date from text using various patterns
        
        Args:
            text: Text that might contain a date
            
        Returns:
            Parsed datetime or None
        """
        if not text:
            return None
        
        try:
            # Try direct parsing first
            return parser.parse(text, fuzzy=True)
        except:
            pass
        
        # Try common patterns
        patterns = [
            r'(\d{1,2}/\d{1,2}/\d{4})',
            r'(\d{1,2}-\d{1,2}-\d{4})',
            r'(\w+\s+\d{1,2},?\s+\d{4})',
            r'(next\s+\w+)',
            r'(tomorrow)',
            r'(today)',
            r'(this\s+week)',
            r'(next\s+week)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text.lower())
            if match:
                try:
                    return parser.parse(match.group(1), fuzzy=True)
                except:
                    continue
        
        return None
    
    def _determine_category(self, description: str, email_data: Dict) -> str:
        """
        Determine task category based on description and email context
        
        Args:
            description: Task description
            email_data: Email context
            
        Returns:
            Task category
        """
        description_lower = description.lower()
        subject = email_data.get('subject', '').lower()
        
        # Meeting-related tasks
        if any(keyword in description_lower for keyword in ['meeting', 'call', 'schedule', 'zoom', 'teams']):
            return 'meeting'
        
        # Review tasks
        if any(keyword in description_lower for keyword in ['review', 'check', 'look at', 'examine']):
            return 'review'
        
        # Response tasks
        if any(keyword in description_lower for keyword in ['reply', 'respond', 'answer', 'get back']):
            return 'follow-up'
        
        # Document tasks
        if any(keyword in description_lower for keyword in ['document', 'report', 'write', 'create', 'draft']):
            return 'document'
        
        # Decision tasks
        if any(keyword in description_lower for keyword in ['decide', 'choose', 'approve', 'confirm']):
            return 'decision'
        
        # Deadline tasks
        if any(keyword in description_lower for keyword in ['deadline', 'due', 'submit', 'deliver']):
            return 'deadline'
        
        return 'action_item'
    
    def get_user_tasks(self, user_email: str, status: str = None, limit: int = None) -> Dict:
        """
        Get extracted tasks for a user
        
        Args:
            user_email: Email of the user
            status: Filter by task status (pending, in_progress, completed)
            limit: Maximum number of tasks to return
            
        Returns:
            Dictionary with user tasks
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            tasks = get_db_manager().get_user_tasks(user.id, status)
            
            if limit:
                tasks = tasks[:limit]
            
            return {
                'success': True,
                'user_email': user_email,
                'tasks': [task.to_dict() for task in tasks],
                'count': len(tasks),
                'status_filter': status
            }
            
        except Exception as e:
            logger.error(f"Failed to get tasks for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def update_task_status(self, user_email: str, task_id: int, status: str) -> Dict:
        """
        Update task status
        
        Args:
            user_email: Email of the user
            task_id: ID of the task to update
            status: New status (pending, in_progress, completed, cancelled)
            
        Returns:
            Dictionary with update result
        """
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            with get_db_manager().get_session() as session:
                task = session.query(Task).filter(
                    Task.id == task_id,
                    Task.user_id == user.id
                ).first()
                
                if not task:
                    return {'success': False, 'error': 'Task not found'}
                
                task.status = status
                task.updated_at = datetime.utcnow()
                
                if status == 'completed':
                    task.completed_at = datetime.utcnow()
                
                session.commit()
                
                return {
                    'success': True,
                    'task_id': task_id,
                    'new_status': status,
                    'updated_at': task.updated_at.isoformat()
                }
            
        except Exception as e:
            logger.error(f"Failed to update task {task_id} for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _analyze_email_business_connections(self, email_data: Dict, business_context: Dict) -> Dict:
        """
        Analyze connections between email and business intelligence context
        """
        try:
            connections = {
                'related_people': [],
                'related_projects': [],
                'related_topics': [],
                'related_meetings': [],
                'connection_count': 0,
                'context_strength': 0.0
            }
            
            sender_email = email_data.get('sender', '').lower()
            subject = (email_data.get('subject') or '').lower()
            body = (email_data.get('body_clean') or '').lower()
            ai_summary = (email_data.get('ai_summary') or '').lower()
            email_topics = email_data.get('topics') or []
            
            # Find related people
            if sender_email in business_context.get('relationship_map', {}):
                person_info = business_context['relationship_map'][sender_email]
                connections['related_people'].append(person_info)
                connections['connection_count'] += 1
                connections['context_strength'] += person_info.get('importance', 0.5)
            
            # Find related projects
            for project_name, project_info in business_context.get('project_map', {}).items():
                if (project_name in subject or project_name in body or project_name in ai_summary):
                    connections['related_projects'].append(project_info)
                    connections['connection_count'] += 1
                    connections['context_strength'] += 0.8  # High value for project connection
            
            # Find related topics
            for topic in email_topics:
                topic_lower = topic.lower()
                if topic_lower in business_context.get('topic_keywords', {}):
                    topic_infos = business_context['topic_keywords'][topic_lower]
                    connections['related_topics'].extend(topic_infos)
                    connections['connection_count'] += len(topic_infos)
                    connections['context_strength'] += 0.6 * len(topic_infos)
            
            # Find related upcoming meetings
            for meeting in business_context.get('calendar_events', []):
                meeting_attendees = meeting.get('attendees', [])
                meeting_title = meeting.get('title', '').lower()
                
                # Check if sender is in meeting attendees
                if any(att.get('email', '').lower() == sender_email for att in meeting_attendees):
                    connections['related_meetings'].append(meeting)
                    connections['connection_count'] += 1
                    connections['context_strength'] += 0.7
                
                # Check if meeting title relates to email subject/content
                if any(keyword in meeting_title for keyword in subject.split() + body.split()[:20] if len(keyword) > 3):
                    if meeting not in connections['related_meetings']:
                        connections['related_meetings'].append(meeting)
                        connections['connection_count'] += 1
                        connections['context_strength'] += 0.5
            
            # Normalize context strength
            connections['context_strength'] = min(1.0, connections['context_strength'] / max(1, connections['connection_count']))
            
            return connections
            
        except Exception as e:
            logger.error(f"Failed to analyze email business connections: {str(e)}")
            return {'related_people': [], 'related_projects': [], 'related_topics': [], 'related_meetings': [], 'connection_count': 0, 'context_strength': 0.0}
    
    def _prepare_360_email_context(self, email_data: Dict, email_context: Dict, business_context: Dict) -> str:
        """
        Prepare comprehensive email context with business intelligence for Claude
        """
        try:
            sender = email_data.get('sender_name') or email_data.get('sender', '')
            subject = email_data.get('subject', '')
            body = email_data.get('body_clean', '')
            ai_summary = email_data.get('ai_summary', '')
            timestamp = email_data.get('timestamp')
            
            # Format timestamp
            if timestamp:
                try:
                    if isinstance(timestamp, str):
                        timestamp = parser.parse(timestamp)
                    date_str = timestamp.strftime('%Y-%m-%d %H:%M')
                except:
                    date_str = 'Unknown date'
            else:
                date_str = 'Unknown date'
            
            # Build business context summary
            context_elements = []
            
            # Add people context
            if email_context['related_people']:
                people_info = []
                for person in email_context['related_people']:
                    people_info.append(f"{person['name']} ({person.get('company', 'Unknown company')}) - {person.get('total_emails', 0)} previous interactions")
                context_elements.append(f"RELATED PEOPLE: {'; '.join(people_info)}")
            
            # Add project context
            if email_context['related_projects']:
                project_info = []
                for project in email_context['related_projects']:
                    project_info.append(f"{project['name']} (Status: {project.get('status', 'Unknown')}, Priority: {project.get('priority', 'Unknown')})")
                context_elements.append(f"RELATED PROJECTS: {'; '.join(project_info)}")
            
            # Add topic context
            if email_context['related_topics']:
                topic_names = [topic['name'] for topic in email_context['related_topics'] if topic.get('is_official')]
                if topic_names:
                    context_elements.append(f"RELATED BUSINESS TOPICS: {', '.join(topic_names)}")
            
            # Add meeting context
            if email_context['related_meetings']:
                meeting_info = []
                for meeting in email_context['related_meetings']:
                    meeting_date = meeting['start_time'].strftime('%Y-%m-%d %H:%M') if meeting.get('start_time') else 'TBD'
                    meeting_info.append(f"{meeting['title']} ({meeting_date})")
                context_elements.append(f"RELATED UPCOMING MEETINGS: {'; '.join(meeting_info)}")
            
            # Add strategic insights
            if business_context.get('recent_decisions'):
                recent_decisions = business_context['recent_decisions'][:3]
                context_elements.append(f"RECENT BUSINESS DECISIONS: {'; '.join(recent_decisions)}")
            
            if business_context.get('opportunities'):
                opportunities = business_context['opportunities'][:3]
                context_elements.append(f"STRATEGIC OPPORTUNITIES: {'; '.join(opportunities)}")
            
            business_intelligence = '\n'.join(context_elements) if context_elements else "No specific business context identified."
            
            enhanced_context = f"""Email Details:
From: {sender}
Date: {date_str}
Subject: {subject}

AI Summary: {ai_summary}

Email Content:
{body}

BUSINESS INTELLIGENCE CONTEXT:
{business_intelligence}

Context Strength: {email_context.get('context_strength', 0.0):.2f} (0.0 = no context, 1.0 = highly connected)
"""
            return enhanced_context
            
        except Exception as e:
            logger.error(f"Failed to prepare 360-context email: {str(e)}")
            return self._prepare_email_context(email_data)
    
    def _call_claude_for_360_tasks(self, enhanced_email_context: str, email_context: Dict) -> Optional[str]:
        """
        Call Claude 4 Sonnet for 360-context task extraction with business intelligence
        """
        try:
            context_strength = email_context.get('context_strength', 0.0)
            connection_count = email_context.get('connection_count', 0)
            
            system_prompt = f"""You are an expert AI Chief of Staff that extracts actionable tasks from emails using comprehensive business intelligence context. You have access to the user's complete business ecosystem including relationships, projects, topics, and strategic insights.

BUSINESS INTELLIGENCE CAPABILITIES:
- Cross-reference people relationships and interaction history
- Connect tasks to active projects and strategic initiatives  
- Leverage topic analysis and business themes
- Consider upcoming meetings and calendar context
- Incorporate recent business decisions and opportunities

ENHANCED TASK EXTRACTION GUIDELINES:

1. **360-Context Task Identification**: Look for tasks that:
   - Connect to the business relationships and projects mentioned
   - Align with strategic opportunities and recent decisions
   - Prepare for upcoming meetings with related attendees
   - Advance active projects and business initiatives
   - Leverage the full business context for maximum relevance

2. **Business-Aware Task Details**: For each task, provide:
   - Clear, actionable description with business context
   - Connect to specific people, projects, or meetings when relevant
   - Priority based on business importance and relationships
   - Category that reflects business context (project_work, relationship_management, strategic_planning, etc.)
   - Due dates that consider business timing and meeting schedules

3. **Context Enhancement Indicators**: 
   - Mark tasks as "context_enhanced": true if they leverage business intelligence
   - Include "business_context" field explaining the connection
   - Add "stakeholders" field if specific people are involved
   - Include "project_connection" if tied to active projects

Current Email Context Strength: {context_strength:.2f} ({connection_count} business connections identified)

RESPONSE FORMAT: Return a JSON array of tasks. Each task should have:
- "description": Clear, actionable description with business context
- "assignee": Who should do this (considering business relationships)
- "due_date": Specific date if mentioned (YYYY-MM-DD format)
- "due_date_text": Original due date text from email
- "priority": high/medium/low (elevated if high business context)
- "category": business-aware category (project_work, relationship_management, meeting_prep, strategic_planning, etc.)
- "confidence": 0.0-1.0 confidence score (higher with business context)
- "source_text": Original text from email that led to this task
- "context_enhanced": true/false (true if business intelligence was used)
- "business_context": Explanation of business connections (if context_enhanced)
- "stakeholders": List of relevant people from business context
- "project_connection": Name of related project if applicable

Return ONLY the JSON array. If no actionable tasks are found, return an empty array []."""

            user_prompt = f"""Please analyze this email with full business intelligence context and extract actionable tasks:

{enhanced_email_context}

Focus on tasks that leverage the business context for maximum relevance and strategic value. Consider the relationships, projects, meetings, and strategic insights provided."""

            message = self.client.messages.create(
                model=self.model,
                max_tokens=3000,  # More tokens for detailed context
                temperature=0.1,
                system=system_prompt,
                messages=[
                    {
                        "role": "user",
                        "content": user_prompt
                    }
                ]
            )
            
            response_text = message.content[0].text.strip()
            logger.debug(f"Claude 360-context response: {response_text}")
            
            return response_text
            
        except Exception as e:
            logger.error(f"Failed to call Claude for 360-context task extraction: {str(e)}")
            return None
    
    def _parse_claude_360_response(self, response: str, email_data: Dict, email_context: Dict) -> List[Dict]:
        """
        Parse Claude's 360-context JSON response into enhanced task dictionaries
        """
        try:
            # Try to find JSON in the response
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            
            if json_start == -1 or json_end == 0:
                logger.warning("No JSON array found in Claude 360-context response")
                return []
            
            json_text = response[json_start:json_end]
            
            # Parse JSON
            tasks_data = json.loads(json_text)
            
            if not isinstance(tasks_data, list):
                logger.warning("Claude 360-context response is not a JSON array")
                return []
            
            tasks = []
            for task_data in tasks_data:
                if isinstance(task_data, dict) and task_data.get('description'):
                    # Validate 360-context fields
                    if task_data.get('context_enhanced') and not task_data.get('business_context'):
                        task_data['business_context'] = f"Connected to {email_context.get('connection_count', 0)} business elements"
                    
                    tasks.append(task_data)
            
            logger.info(f"Parsed {len(tasks)} 360-context tasks from Claude response")
            return tasks
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Claude 360-context JSON response: {str(e)}")
            logger.error(f"Response was: {response}")
            return []
        except Exception as e:
            logger.error(f"Failed to parse Claude 360-context response: {str(e)}")
            return []
    
    def _enhance_task_with_360_context(self, task: Dict, email_data: Dict, email_context: Dict, business_context: Dict) -> Dict:
        """
        Enhance task with comprehensive 360-context metadata and business intelligence
        """
        try:
            # Start with Claude's task data
            enhanced_task = task.copy()
            
            # Parse due date if provided
            if task.get('due_date'):
                try:
                    due_date = parser.parse(task['due_date'])
                    enhanced_task['due_date'] = due_date
                except:
                    enhanced_task['due_date'] = self._extract_date_from_text(
                        task.get('due_date_text', '')
                    )
            elif task.get('due_date_text'):
                enhanced_task['due_date'] = self._extract_date_from_text(task['due_date_text'])
            
            # Set default values with 360-context awareness
            enhanced_task['priority'] = task.get('priority', 'medium').lower()
            enhanced_task['category'] = task.get('category', 'action_item')
            enhanced_task['confidence'] = min(1.0, max(0.0, task.get('confidence', 0.8)))
            enhanced_task['status'] = 'pending'
            
            # Enhance based on business context strength
            context_strength = email_context.get('context_strength', 0.0)
            if context_strength > 0.7:  # High context strength
                if enhanced_task['priority'] == 'medium':
                    enhanced_task['priority'] = 'high'
                enhanced_task['confidence'] = min(1.0, enhanced_task['confidence'] + 0.1)
            
            # Determine assignee with business context
            if not enhanced_task.get('assignee'):
                # Check if specific people are mentioned in business context
                related_people = email_context.get('related_people', [])
                if related_people and len(related_people) == 1:
                    enhanced_task['assignee'] = related_people[0]['name']
                else:
                    enhanced_task['assignee'] = 'me'
            
            # Enhance category with business context
            if enhanced_task['category'] == 'action_item':
                enhanced_task['category'] = self._determine_360_category(
                    enhanced_task['description'], 
                    email_data,
                    email_context
                )
            
            # Add 360-context specific fields
            if task.get('context_enhanced'):
                enhanced_task['context_enhanced'] = True
                enhanced_task['business_context'] = task.get('business_context', 'Business intelligence context applied')
                enhanced_task['context_strength'] = context_strength
                enhanced_task['connection_count'] = email_context.get('connection_count', 0)
            
            # Add stakeholder information
            stakeholders = task.get('stakeholders', [])
            if not stakeholders and email_context.get('related_people'):
                stakeholders = [person['name'] for person in email_context['related_people']]
            enhanced_task['stakeholders'] = stakeholders
            
            # Add project connection
            if task.get('project_connection'):
                enhanced_task['project_connection'] = task['project_connection']
            elif email_context.get('related_projects'):
                enhanced_task['project_connection'] = email_context['related_projects'][0]['name']
            
            return enhanced_task
            
        except Exception as e:
            logger.error(f"Failed to enhance task with 360-context: {str(e)}")
            return task
    
    def _determine_360_category(self, description: str, email_data: Dict, email_context: Dict) -> str:
        """
        Determine task category with 360-context business intelligence
        """
        try:
            description_lower = description.lower()
            subject = email_data.get('subject', '').lower()
            
            # Business context-aware categorization
            if email_context.get('related_projects'):
                return 'project_work'
            
            if email_context.get('related_meetings'):
                return 'meeting_prep'
            
            if email_context.get('related_people') and len(email_context['related_people']) > 0:
                person = email_context['related_people'][0]
                if person.get('importance', 0) > 0.7:
                    return 'relationship_management'
            
            # Strategic context
            if any(keyword in description_lower for keyword in ['strategy', 'strategic', 'decision', 'opportunity']):
                return 'strategic_planning'
            
            # Default categorization with business awareness
            if any(keyword in description_lower for keyword in ['meeting', 'call', 'schedule', 'zoom', 'teams']):
                return 'meeting'
            
            if any(keyword in description_lower for keyword in ['review', 'check', 'look at', 'examine']):
                return 'review'
            
            if any(keyword in description_lower for keyword in ['reply', 'respond', 'answer', 'get back']):
                return 'follow-up'
            
            if any(keyword in description_lower for keyword in ['document', 'report', 'write', 'create', 'draft']):
                return 'document'
            
            if any(keyword in description_lower for keyword in ['decide', 'choose', 'approve', 'confirm']):
                return 'decision'
            
            if any(keyword in description_lower for keyword in ['deadline', 'due', 'submit', 'deliver']):
                return 'deadline'
            
            return 'action_item'
            
        except Exception as e:
            logger.error(f"Failed to determine 360-context category: {str(e)}")
            return 'action_item'
    
    def extract_tasks_from_email(self, email_data: Dict) -> Dict:
        """
        LEGACY METHOD: Extract actionable tasks from a single email using Claude 4 Sonnet
        This method is kept for backward compatibility but users should use extract_tasks_with_360_context
        """
        try:
            logger.warning("Using legacy task extraction - consider upgrading to 360-context extraction")
            
            # Check if email has enough content for task extraction
            body_clean = email_data.get('body_clean', '')
            if not body_clean or len(body_clean.strip()) < 20:
                return {
                    'success': True,
                    'email_id': email_data.get('id'),
                    'tasks': [],
                    'reason': 'Email content too short for task extraction'
                }
            
            # Skip certain message types that unlikely contain tasks
            message_type = email_data.get('message_type', 'regular')
            if message_type in ['newsletter', 'automated']:
                return {
                    'success': True,
                    'email_id': email_data.get('id'),
                    'tasks': [],
                    'reason': f'Message type "{message_type}" skipped for task extraction'
                }
            
            # Prepare email context for Claude
            email_context = self._prepare_email_context(email_data)
            
            # Call Claude for task extraction
            claude_response = self._call_claude_for_tasks(email_context)
            
            if not claude_response:
                return {
                    'success': False,
                    'email_id': email_data.get('id'),
                    'error': 'Failed to get response from Claude'
                }
            
            # Parse Claude's response
            tasks = self._parse_claude_response(claude_response, email_data)
            
            # Enhance tasks with additional metadata
            enhanced_tasks = []
            for task in tasks:
                enhanced_task = self._enhance_task(task, email_data)
                enhanced_tasks.append(enhanced_task)
            
            return {
                'success': True,
                'email_id': email_data.get('id'),
                'tasks': enhanced_tasks,
                'extraction_metadata': {
                    'extracted_at': datetime.utcnow().isoformat(),
                    'extractor_version': self.version,
                    'model_used': self.model,
                    'email_priority': email_data.get('priority_score', 0.5)
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to extract tasks from email {email_data.get('id', 'unknown')}: {str(e)}")
            return {
                'success': False,
                'email_id': email_data.get('id'),
                'error': str(e)
            }

# Create global instance
task_extractor = TaskExtractor()

============================================================
FILE: archive/backup_files/v1_original/processors_archived/email_intelligence.py
============================================================
# Enhanced Email Intelligence Processor using Claude 4 Sonnet

import json
import logging
import re
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple
import anthropic
import time

from config.settings import settings
from models.database import get_db_manager, Email, Person, Project, Task, User

logger = logging.getLogger(__name__)

class EmailIntelligenceProcessor:
    """Advanced email intelligence using Claude 4 Sonnet for comprehensive understanding"""
    
    def __init__(self):
        self.client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        self.model = "claude-3-5-sonnet-20241022"
        self.version = "2.2"  # Debug version with relaxed filters
        
        # Quality filtering patterns (RELAXED FOR DEBUGGING)
        self.non_human_patterns = [
            'noreply', 'no-reply', 'donotreply', 'automated', 'newsletter',
            'unsubscribe', 'notification', 'system', 'support', 'help',
            'admin', 'contact', 'info', 'sales', 'marketing', 'hello',
            'team', 'notifications', 'alerts', 'updates', 'reports'
        ]
        
        # RELAXED quality thresholds to capture more content
        self.min_insight_length = 10  # Reduced from 15
        self.min_confidence_score = 0.4  # Reduced from 0.6 - be more inclusive
        
    def process_user_emails_intelligently(self, user_email: str, limit: int = None, force_refresh: bool = False) -> Dict:
        """
        Process user emails with Claude 4 Sonnet for high-quality business intelligence
        Enhanced with quality filtering and strategic insights
        """
        try:
            logger.info(f"Starting quality-focused email processing for {user_email}")
            
            # Get user and validate
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
                
            # Get business context for enhanced AI analysis
            user_context = self._get_user_business_context(user.id)
            
            # Get emails needing processing with quality pre-filtering
            emails_to_process = self._get_emails_needing_processing(user.id, limit or 100, force_refresh)
            
            # RELAXED: Filter for quality emails but be more inclusive
            quality_filtered_emails = self._filter_quality_emails_debug(emails_to_process, user_email)
            
            logger.info(f"Found {len(emails_to_process)} emails to process, {len(quality_filtered_emails)} passed quality filters")
            
            if not quality_filtered_emails:
                logger.warning(f"No emails passed quality filters for {user_email}")
                return {
                    'success': True,
                    'user_email': user_email,
                    'processed_emails': 0,
                    'high_quality_insights': 0,
                    'human_contacts_identified': 0,
                    'meaningful_projects': 0,
                    'actionable_tasks': 0,
                    'processor_version': self.version,
                    'debug_info': f"No emails passed filters out of {len(emails_to_process)} total emails"
                }
            
            # Limit to top quality emails for processing
            emails_to_process = quality_filtered_emails[:limit or 50]
            
            processed_count = 0
            insights_extracted = 0
            people_identified = 0
            projects_identified = 0
            tasks_created = 0
            
            for idx, email in enumerate(emails_to_process):
                try:
                    logger.info(f"Processing email {idx + 1}/{len(emails_to_process)} for {user_email}")
                    logger.debug(f"Email from: {email.sender}, subject: {email.subject}")
                    
                    # Skip if email has issues
                    if not email.body_clean and not email.snippet:
                        logger.warning(f"Skipping email {email.gmail_id} - no content")
                        continue
                    
                    # Get comprehensive email analysis from Claude with enhanced prompts
                    analysis = self._get_quality_focused_email_analysis(email, user, user_context)
                    
                    if analysis:
                        logger.debug(f"AI Analysis received for email {email.gmail_id}")
                        logger.debug(f"Strategic value score: {analysis.get('strategic_value_score', 'N/A')}")
                        logger.debug(f"Sender analysis: {analysis.get('sender_analysis', {})}")
                        logger.debug(f"People found: {len(analysis.get('people', []))}")
                        
                        if self._validate_analysis_quality_debug(analysis):
                            # Update email with insights
                            self._update_email_with_insights(email, analysis)
                            
                            # Extract and update people information (with human filtering)
                            if analysis.get('people') or analysis.get('sender_analysis'):
                                people_count = self._process_human_contacts_only_debug(user.id, analysis, email)
                                people_identified += people_count
                                logger.info(f"Extracted {people_count} people from email {email.gmail_id}")
                            
                            # Extract and update project information
                            if analysis.get('project') and self._validate_project_quality(analysis['project']):
                                project = self._process_project_insights(user.id, analysis['project'], email)
                                if project:
                                    projects_identified += 1
                                    email.project_id = project.id
                            
                            # Extract high-confidence tasks only
                            if analysis.get('tasks'):
                                tasks_count = self._process_high_quality_tasks(user.id, email.id, analysis['tasks'])
                                tasks_created += tasks_count
                            
                            insights_extracted += 1
                        else:
                            logger.info(f"Analysis for email {email.gmail_id} didn't meet quality thresholds")
                    else:
                        logger.warning(f"No analysis returned for email {email.gmail_id}")
                    
                    processed_count += 1
                    
                    # Add a small delay to prevent overwhelming the system
                    time.sleep(0.1)
                    
                except Exception as e:
                    logger.error(f"Failed to intelligently process email {email.gmail_id}: {str(e)}")
                    continue
            
            logger.info(f"Quality-focused processing: {processed_count} emails, {people_identified} people identified for {user_email}")
            
            return {
                'success': True,
                'user_email': user_email,
                'processed_emails': processed_count,
                'high_quality_insights': insights_extracted,
                'human_contacts_identified': people_identified,
                'meaningful_projects': projects_identified,
                'actionable_tasks': tasks_created,
                'processor_version': self.version,
                'debug_info': f"Processed {processed_count} emails, passed quality filters: {len(quality_filtered_emails)}"
            }
            
        except Exception as e:
            logger.error(f"Failed intelligent email processing for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _get_emails_needing_processing(self, user_id: int, limit: int, force_refresh: bool) -> List[Email]:
        """Get emails that need Claude analysis (generic filter)"""
        with get_db_manager().get_session() as session:
            query = session.query(Email).filter(
                Email.user_id == user_id,
                Email.body_clean.isnot(None)
            )
            
            if not force_refresh:
                query = query.filter(Email.ai_summary.is_(None))
            
            # Detach from session before returning to avoid issues
            emails = query.order_by(Email.email_date.desc()).limit(limit).all()
            session.expunge_all()
            return emails

    def _filter_unreplied_emails(self, emails: List[Email], user_email: str) -> List[Email]:
        """Filter a list of emails to find ones that are likely unreplied"""
        unreplied = []
        for email in emails:
            # If email is from the user themselves, skip
            if email.sender and user_email.lower() in email.sender.lower():
                continue

            # If email contains certain patterns suggesting it's automated, skip
            automated_patterns = [
                'noreply', 'no-reply', 'donotreply', 'automated', 'newsletter',
                'unsubscribe', 'notification only', 'system generated'
            ]
            sender_lower = (email.sender or '').lower()
            subject_lower = (email.subject or '').lower()
            if any(pattern in sender_lower or pattern in subject_lower for pattern in automated_patterns):
                continue

            # Default to including emails that seem personal/business oriented
            unreplied.append(email)
        return unreplied
    
    def _is_unreplied_email(self, email: Email, user_email: str) -> bool:
        """Determine if an email is unreplied using heuristics"""
        # If email is from the user themselves, skip
        if email.sender and user_email.lower() in email.sender.lower():
            return False
        
        # If email contains certain patterns suggesting it's automated, skip
        automated_patterns = [
            'noreply', 'no-reply', 'donotreply', 'automated', 'newsletter',
            'unsubscribe', 'notification only', 'system generated'
        ]
        
        sender_lower = (email.sender or '').lower()
        subject_lower = (email.subject or '').lower()
        
        if any(pattern in sender_lower or pattern in subject_lower for pattern in automated_patterns):
            return False
        
        # If email is marked as important or has action-oriented subject, include it
        action_words = ['review', 'approve', 'sign', 'confirm', 'urgent', 'asap', 'deadline', 'meeting']
        if any(word in subject_lower for word in action_words):
            return True
        
        # Default to including emails that seem personal/business oriented
        return True
    
    def _get_quality_focused_email_analysis(self, email: Email, user, user_context: Dict) -> Optional[Dict]:
        """Get quality-focused email analysis from Claude with enhanced business context"""
        try:
            # Safety check to prevent processing very large emails
            email_content = email.body_clean or email.snippet or ""
            if len(email_content) > 10000:  # Limit email content size
                logger.warning(f"Email {email.gmail_id} too large ({len(email_content)} chars), truncating")
                email_content = email_content[:10000] + "... [truncated]"
            
            if len(email_content) < 10:  # Skip very short emails
                logger.warning(f"Email {email.gmail_id} too short, skipping AI analysis")
                return None
            
            email_context = self._prepare_enhanced_email_context(email, user)
            
            # Limit context size to prevent API issues
            if len(email_context) > 15000:
                logger.warning(f"Email context too large for {email.gmail_id}, truncating")
                email_context = email_context[:15000] + "... [truncated]"
            
            # Enhanced system prompt with business context and quality requirements
            business_context_str = self._format_business_context(user_context)
            
            system_prompt = f"""You are an expert AI Chief of Staff that provides comprehensive email analysis for business intelligence and productivity. Be INCLUSIVE and extract valuable insights from business communications.

**YOUR MISSION:**
- Extract ALL valuable business intelligence, contacts, tasks, and insights
- Be inclusive rather than restrictive - capture business value wherever it exists
- Focus on building comprehensive knowledge about professional relationships and work

**BUSINESS CONTEXT FOR {user.email}:**
{business_context_str}

**ANALYSIS REQUIREMENTS:**

1. **EMAIL SUMMARY**: Clear description of the email's business purpose and content
2. **PEOPLE EXTRACTION**: Extract ALL human contacts with professional relevance (be generous!)
   - ALWAYS extract the sender if they're a real person
   - Extract anyone mentioned by name with business context
   - Include names even with limited contact information
3. **TASK IDENTIFICATION**: Find ANY actionable items or commitments mentioned
4. **BUSINESS INSIGHTS**: Extract any strategic value, opportunities, or challenges
5. **PROJECT CONTEXT**: Identify any work initiatives or business activities
6. **TOPIC EXTRACTION**: Identify business topics, project names, company names, technologies

**INCLUSIVE EXTRACTION GUIDELINES:**
- Extract people even if limited info is available (name + context is enough)
- Include tasks with clear actionable language, even if informal
- Capture business insights at any level (strategic, operational, or tactical)
- Process emails from colleagues, clients, partners, vendors - anyone professional
- Include follow-ups, scheduling, decisions, updates, and work discussions
- Extract topics like project names, company names, technologies, business areas
- Be generous with topic extraction - include any business-relevant subjects

Return a JSON object with this structure:
{{
    "summary": "Clear description of the email's business purpose and key content",
    "strategic_value_score": 0.7,  // Be generous - most business emails have value
    "sender_analysis": {{
        "name": "Sender's actual name (extract from signature or display name)",
        "role": "Their role/title if mentioned",
        "company": "Their company if identifiable",
        "relationship": "Professional relationship context",
        "is_human_contact": true,  // Default to true for most senders
        "business_relevance": "Why this person is professionally relevant"
    }},
    "people": [
        {{
            "name": "Full name of any person mentioned",
            "email": "their_email@example.com",
            "role": "Their role if mentioned",
            "company": "Company if mentioned", 
            "relationship": "Professional context",
            "business_relevance": "Why they're mentioned/relevant",
            "mentioned_context": "How they were mentioned in the email"
        }}
    ],
    "project": {{
        "name": "Project or initiative name",
        "description": "Description of the work or project",
        "category": "business/client_work/internal/operational",
        "priority": "high/medium/low",
        "status": "active/planning/discussed",
        "business_impact": "Potential impact or value",
        "key_stakeholders": ["person1", "person2"]
    }},
    "business_insights": {{
        "key_decisions": ["Any decisions mentioned or needed"],
        "strategic_opportunities": ["Opportunities or potential business value"],
        "business_challenges": ["Challenges or issues discussed"],
        "actionable_metrics": ["Any numbers or metrics mentioned"],
        "competitive_intelligence": ["Market or competitor information"],
        "partnership_opportunities": ["Collaboration potential"]
    }},
    "tasks": [
        {{
            "description": "Clear description of the actionable item",
            "assignee": "{user.email}",
            "due_date": "2025-02-15",
            "due_date_text": "deadline mentioned in email",
            "priority": "high/medium/low",
            "category": "action_item/follow_up/meeting/review",
            "confidence": 0.8,  // Be generous with confidence scores
            "business_context": "Why this task matters",
            "success_criteria": "What completion looks like"
        }}
    ],
    "topics": ["HitCraft", "board meeting", "fundraising", "AI in music", "certification", "business development"],  // Extract: project names, company names, technologies, business areas, meeting types
    "ai_category": "business_communication/client_work/project_coordination/operational"
}}

**IMPORTANT**: Extract value from most business emails. Only skip obvious spam or completely irrelevant content. Be generous with people extraction and task identification.
"""

            user_prompt = f"""Analyze this email comprehensively for business intelligence. Extract ALL valuable people, tasks, and insights:

{email_context}

Focus on building comprehensive business knowledge. Extract people and tasks generously - capture business value wherever it exists."""

            # Add timeout and retry protection
            max_retries = 2
            for attempt in range(max_retries):
                try:
                    logger.info(f"Calling Claude API for comprehensive analysis of email {email.gmail_id}, attempt {attempt + 1}")
                    
                    message = self.client.messages.create(
                        model=self.model,
                        max_tokens=3000,
                        temperature=0.1,
                        system=system_prompt,
                        messages=[{"role": "user", "content": user_prompt}]
                    )
                    
                    response_text = message.content[0].text.strip()
                    
                    # Handle null responses (low-quality emails)
                    if response_text.lower().strip() in ['null', 'none', '{}', '']:
                        logger.info(f"Claude rejected email {email.gmail_id} as low-quality")
                        return None
                    
                    # Parse JSON response with better error handling
                    json_start = response_text.find('{')
                    json_end = response_text.rfind('}') + 1
                    
                    if json_start != -1 and json_end > json_start:
                        json_text = response_text[json_start:json_end]
                        try:
                            analysis = json.loads(json_text)
                            logger.info(f"Successfully analyzed email {email.gmail_id}")
                            return analysis
                        except json.JSONDecodeError as json_error:
                            logger.error(f"JSON parsing error for email {email.gmail_id}: {str(json_error)}")
                            if attempt < max_retries - 1:
                                time.sleep(1)  # Wait before retry
                                continue
                            return None
                    else:
                        logger.warning(f"No valid JSON found in Claude response for email {email.gmail_id}")
                        if attempt < max_retries - 1:
                            time.sleep(1)  # Wait before retry
                            continue
                        return None
                        
                except Exception as api_error:
                    logger.error(f"Claude API error for email {email.gmail_id}, attempt {attempt + 1}: {str(api_error)}")
                    if attempt < max_retries - 1:
                        time.sleep(2)  # Wait longer before retry
                        continue
                    return None
            
            logger.warning(f"Failed to analyze email {email.gmail_id} after {max_retries} attempts")
            return None
            
        except Exception as e:
            logger.error(f"Failed to get email analysis from Claude for {email.gmail_id}: {str(e)}")
            return None
    
    def _format_business_context(self, user_context: Dict) -> str:
        """Format business context for AI prompt"""
        context_parts = []
        
        if user_context.get('existing_projects'):
            context_parts.append(f"Current Projects: {', '.join(user_context['existing_projects'])}")
        
        if user_context.get('key_contacts'):
            context_parts.append(f"Key Business Contacts: {', '.join(user_context['key_contacts'][:5])}")  # Top 5
        
        if user_context.get('official_topics'):
            context_parts.append(f"Business Focus Areas: {', '.join(user_context['official_topics'])}")
        
        return '\n'.join(context_parts) if context_parts else "No existing business context available"
    
    def _validate_analysis_quality(self, analysis: Dict) -> bool:
        """Validate that the analysis meets quality standards - RELAXED VERSION"""
        try:
            # RELAXED: Check strategic value score - lowered threshold
            strategic_value = analysis.get('strategic_value_score', 0)
            if strategic_value < 0.5:  # Reduced from 0.6 to 0.5
                logger.info(f"Analysis rejected - low strategic value: {strategic_value}")
                return False
            
            # RELAXED: Check summary quality - reduced minimum length
            summary = analysis.get('summary', '')
            if len(summary) < self.min_insight_length:
                logger.info(f"Analysis rejected - summary too short: {len(summary)} chars")
                return False
            
            # RELAXED: More lenient trivial content detection
            trivial_phrases = [
                'thanks', 'thank you', 'got it', 'received', 'noted', 'okay', 'ok',
                'sounds good', 'will do', 'understood', 'acknowledged'
            ]
            
            # Only reject if it's VERY short AND contains only trivial phrases
            if any(phrase in summary.lower() for phrase in trivial_phrases) and len(summary) < 30:  # Reduced from 50
                logger.info(f"Analysis rejected - trivial content detected")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"Error validating analysis quality: {str(e)}")
            return False
    
    def _validate_project_quality(self, project_data: Dict) -> bool:
        """Validate that project data meets quality standards"""
        if not project_data or not project_data.get('name'):
            return False
        
        # Check project name is substantial
        if len(project_data['name']) < 5:
            return False
        
        # Check for meaningful description
        description = project_data.get('description', '')
        if len(description) < self.min_insight_length:
            return False
        
        return True
    
    def _process_human_contacts_only(self, user_id: int, analysis: Dict, email: Email) -> int:
        """Process people information with human contact filtering and KNOWLEDGE ACCUMULATION"""
        people_count = 0
        
        # Process sender first (with human validation and knowledge accumulation)
        sender_analysis = analysis.get('sender_analysis')
        if (sender_analysis and email.sender and 
            sender_analysis.get('is_human_contact') and
            not self._is_non_human_contact(email.sender)):
            
            # Get existing person to accumulate knowledge
            existing_person = get_db_manager().find_person_by_email(user_id, email.sender)
            
            # Accumulate notes and context over time
            existing_notes = existing_person.notes if existing_person else ""
            new_relevance = sender_analysis.get('business_relevance', '')
            
            # Combine old and new notes intelligently
            accumulated_notes = existing_notes
            if new_relevance and new_relevance not in accumulated_notes:
                if accumulated_notes:
                    accumulated_notes += f"\n\nRecent Context: {new_relevance}"
                else:
                    accumulated_notes = new_relevance
            
            person_data = {
                'email_address': email.sender,
                'name': sender_analysis.get('name', email.sender_name or email.sender),
                'role': sender_analysis.get('role') or (existing_person.role if existing_person else None),
                'company': sender_analysis.get('company') or (existing_person.company if existing_person else None),
                'relationship_type': sender_analysis.get('relationship') or (existing_person.relationship_type if existing_person else None),
                'notes': accumulated_notes,  # Accumulated knowledge
                'importance_level': max(0.8, existing_person.importance_level if existing_person else 0.8),  # Increment importance
                'ai_version': self.version,
                'total_emails': (existing_person.total_emails if existing_person else 0) + 1  # Increment email count
            }
            get_db_manager().create_or_update_person(user_id, person_data)
            people_count += 1
        
        # Process mentioned people (with human validation and accumulation)
        people_data = analysis.get('people', [])
        if isinstance(people_data, list):
            for person_info in people_data:
                if (person_info.get('email') or person_info.get('name')) and person_info.get('business_relevance'):
                    # Additional human validation
                    email_addr = person_info.get('email', '')
                    if email_addr and self._is_non_human_contact(email_addr):
                        continue
                    
                    # Get existing person to accumulate knowledge
                    existing_person = None
                    if email_addr:
                        existing_person = get_db_manager().find_person_by_email(user_id, email_addr)
                    
                    # Accumulate knowledge
                    existing_notes = existing_person.notes if existing_person else ""
                    new_relevance = person_info.get('business_relevance', '')
                    
                    accumulated_notes = existing_notes
                    if new_relevance and new_relevance not in accumulated_notes:
                        if accumulated_notes:
                            accumulated_notes += f"\n\nMentioned Context: {new_relevance}"
                        else:
                            accumulated_notes = new_relevance
                    
                    person_data = {
                        'email_address': person_info.get('email'),
                        'name': person_info['name'],
                        'role': person_info.get('role') or (existing_person.role if existing_person else None),
                        'company': person_info.get('company') or (existing_person.company if existing_person else None),
                        'relationship_type': person_info.get('relationship') or (existing_person.relationship_type if existing_person else None),
                        'notes': accumulated_notes,  # Accumulated knowledge
                        'ai_version': self.version
                    }
                    get_db_manager().create_or_update_person(user_id, person_data)
                    people_count += 1
        
        return people_count
    
    def _process_high_quality_tasks(self, user_id: int, email_id: int, tasks_data: List[Dict]) -> int:
        """Process and save actionable tasks - VERY INCLUSIVE VERSION"""
        tasks_count = 0
        
        for task_info in tasks_data:
            # Validate task quality - very permissive
            if not task_info.get('description'):
                continue
            
            # VERY INCLUSIVE: Check confidence threshold - very low threshold
            confidence = task_info.get('confidence', 0.8)  # Default to 0.8 if missing
            if confidence < 0.3:  # Very low threshold
                continue
            
            # VERY INCLUSIVE: Check description length - very permissive
            description = task_info['description']
            if len(description) < 5:  # Very short minimum
                continue
            
            task_data = {
                'description': description,
                'assignee': task_info.get('assignee'),
                'due_date': self._parse_due_date(task_info.get('due_date')),
                'due_date_text': task_info.get('due_date_text'),
                'priority': task_info.get('priority', 'medium'),
                'category': task_info.get('category', 'action_item'),
                'confidence': confidence,
                'source_text': task_info.get('success_criteria', ''),
                'context': task_info.get('business_context', ''),
                'status': 'pending',
                'extractor_version': self.version,
                'model_used': self.model
            }
            
            get_db_manager().save_task(user_id, email_id, task_data)
            tasks_count += 1
            logger.info(f"Created task: {description[:50]}...")
        
        return tasks_count
    
    def _prepare_enhanced_email_context(self, email: Email, user) -> str:
        """Prepare comprehensive email context for Claude analysis"""
        timestamp = email.email_date.strftime('%Y-%m-%d %H:%M') if email.email_date else 'Unknown'
        
        context = f"""EMAIL ANALYSIS REQUEST

Recipient: {user.email} ({user.name})
From: {email.sender_name or 'Unknown'} <{email.sender}>
Date: {timestamp}
Subject: {email.subject}

Email Content:
{email.body_clean or email.snippet}

Additional Context:
- Recipients: {', '.join(email.recipients) if email.recipients else 'Not specified'}
- Thread ID: {email.thread_id}
- Email Labels: {', '.join(email.labels) if email.labels else 'None'}
- Message Type: {email.message_type or 'Unknown'}
- Priority Score: {email.priority_score or 'Not calculated'}
"""
        return context
    
    def _update_email_with_insights(self, email: Email, analysis: Dict):
        """Update email record with Claude insights"""
        with get_db_manager().get_session() as session:
            email_record = session.query(Email).filter(Email.id == email.id).first()
            if email_record:
                email_record.ai_summary = analysis.get('summary')
                email_record.ai_category = analysis.get('ai_category')
                email_record.sentiment_score = analysis.get('sentiment_score')
                email_record.urgency_score = analysis.get('urgency_score')
                email_record.key_insights = analysis.get('business_insights')
                email_record.topics = analysis.get('topics')
                email_record.action_required = analysis.get('action_required', False)
                email_record.follow_up_required = analysis.get('follow_up_required', False)
                
                session.commit()
    
    def _process_project_insights(self, user_id: int, project_data: Dict, email: Email) -> Optional[Project]:
        """Process and update project information - SAFE VERSION"""
        if not project_data or not project_data.get('name'):
            return None
        
        try:
            project_info = {
                'name': project_data['name'],
                'slug': self._create_slug(project_data['name']),
                'description': project_data.get('description'),
                'category': project_data.get('category'),
                'priority': project_data.get('priority', 'medium'),
                'status': project_data.get('status', 'active'),
                'key_topics': project_data.get('key_topics', []),
                'stakeholders': project_data.get('stakeholders', []),
                'ai_version': self.version
            }
            
            return get_db_manager().create_or_update_project(user_id, project_info)
            
        except Exception as e:
            logger.error(f"Error processing project insights: {str(e)}")
            return None
    
    def _create_slug(self, name: str) -> str:
        """Create URL-friendly slug from name"""
        return re.sub(r'[^a-zA-Z0-9]+', '-', name.lower()).strip('-')
    
    def _parse_due_date(self, date_str: str) -> Optional[datetime]:
        """Parse due date string into datetime"""
        if not date_str:
            return None
        
        try:
            return datetime.strptime(date_str, '%Y-%m-%d')
        except:
            return None
    
    def get_business_knowledge_summary(self, user_email: str) -> Dict:
        """Get comprehensive business knowledge summary with quality synthesis"""
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            # Get all processed emails with quality filtering
            emails = get_db_manager().get_user_emails(user.id, limit=1000)
            projects = get_db_manager().get_user_projects(user.id, limit=200)
            people = get_db_manager().get_user_people(user.id, limit=500)
            
            # Filter for high-quality insights only
            quality_emails = [e for e in emails if e.ai_summary and len(e.ai_summary) > self.min_insight_length]
            human_contacts = [p for p in people if not self._is_non_human_contact(p.email_address or '')]
            substantial_projects = [p for p in projects if p.description and len(p.description) > self.min_insight_length]
            
            # Synthesize high-quality business insights
            strategic_decisions = []
            business_opportunities = []
            key_challenges = []
            competitive_insights = []
            
            for email in quality_emails:
                if email.key_insights and isinstance(email.key_insights, dict):
                    insights = email.key_insights
                    
                    # Extract strategic-level insights only
                    decisions = insights.get('key_decisions', [])
                    strategic_decisions.extend([d for d in decisions if len(d) > self.min_insight_length])
                    
                    opportunities = insights.get('strategic_opportunities', insights.get('opportunities', []))
                    business_opportunities.extend([o for o in opportunities if len(o) > self.min_insight_length])
                    
                    challenges = insights.get('business_challenges', insights.get('challenges', []))
                    key_challenges.extend([c for c in challenges if len(c) > self.min_insight_length])
                    
                    competitive = insights.get('competitive_intelligence', [])
                    competitive_insights.extend([ci for ci in competitive if len(ci) > self.min_insight_length])
            
            # Get meaningful topics
            topics = get_db_manager().get_user_topics(user.id, limit=1000)
            business_topics = [topic.name for topic in topics if topic.is_official or 
                              (topic.description and len(topic.description) > 10)]
            
            return {
                'success': True,
                'user_email': user_email,
                'business_knowledge': {
                    'summary_stats': {
                        'quality_emails_analyzed': len(quality_emails),
                        'human_contacts': len(human_contacts),
                        'substantial_projects': len(substantial_projects),
                        'strategic_insights': len(strategic_decisions) + len(business_opportunities) + len(key_challenges)
                    },
                    'strategic_intelligence': {
                        'key_decisions': self._deduplicate_and_rank(strategic_decisions)[:8],  # Top 8 strategic decisions
                        'business_opportunities': self._deduplicate_and_rank(business_opportunities)[:8],
                        'key_challenges': self._deduplicate_and_rank(key_challenges)[:8],
                        'competitive_intelligence': self._deduplicate_and_rank(competitive_insights)[:5]
                    },
                    'business_topics': business_topics[:15],  # Top 15 business topics
                    'network_intelligence': {
                        'total_human_contacts': len(human_contacts),
                        'active_projects': len([p for p in substantial_projects if p.status == 'active']),
                        'project_categories': list(set([p.category for p in substantial_projects if p.category]))
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to get business knowledge for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}

    def get_chat_knowledge_summary(self, user_email: str) -> Dict:
        """Get comprehensive knowledge summary for chat interface with enhanced context"""
        try:
            user = get_db_manager().get_user_by_email(user_email)
            if not user:
                return {'success': False, 'error': 'User not found'}
            
            # Get all processed data with quality filters
            emails = get_db_manager().get_user_emails(user.id, limit=1000)
            projects = get_db_manager().get_user_projects(user.id, limit=200)
            people = get_db_manager().get_user_people(user.id, limit=500)
            topics = get_db_manager().get_user_topics(user.id, limit=1000)
            
            # GET CALENDAR EVENTS FOR KNOWLEDGE BASE
            now = datetime.now(timezone.utc)
            calendar_events = get_db_manager().get_user_calendar_events(
                user.id, 
                start_date=now - timedelta(days=30),  # Past 30 days for context
                end_date=now + timedelta(days=60),    # Next 60 days for planning
                limit=200
            )
            
            # Filter for high-quality content
            quality_emails = [e for e in emails if e.ai_summary and len(e.ai_summary) > self.min_insight_length]
            human_contacts = [p for p in people if not self._is_non_human_contact(p.email_address or '') and p.name]
            
            # Compile rich contacts with enhanced professional context
            rich_contacts = []
            for person in human_contacts[:15]:  # Top 15 human contacts
                # Create rich professional story
                professional_story = self._create_professional_story(person, quality_emails)
                
                contact_info = {
                    'name': person.name,
                    'email': person.email_address,
                    'title': person.title or person.role,
                    'company': person.company,
                    'relationship': person.relationship_type,
                    'story': professional_story,
                    'total_emails': person.total_emails or 0,
                    'last_interaction': person.last_interaction.isoformat() if person.last_interaction else None,
                    'importance_score': person.importance_level or 0.5
                }
                rich_contacts.append(contact_info)
            
            # Enhanced business intelligence compilation
            business_decisions = []
            opportunities = []
            challenges = []
            
            for email in quality_emails:
                if email.key_insights and isinstance(email.key_insights, dict):
                    insights = email.key_insights
                    
                    # Enhanced insight extraction with context
                    decisions = insights.get('key_decisions', [])
                    for decision in decisions:
                        if len(decision) > self.min_insight_length:
                            business_decisions.append({
                                'decision': decision,
                                'context': email.ai_summary,
                                'sender': email.sender_name or email.sender,
                                'date': email.email_date.isoformat() if email.email_date else None
                            })
                    
                    opps = insights.get('strategic_opportunities', insights.get('opportunities', []))
                    for opp in opps:
                        if len(opp) > self.min_insight_length:
                            opportunities.append({
                                'opportunity': opp,
                                'context': email.ai_summary,
                                'source': email.sender_name or email.sender,
                                'date': email.email_date.isoformat() if email.email_date else None
                            })
                    
                    chals = insights.get('business_challenges', insights.get('challenges', []))
                    for chal in chals:
                        if len(chal) > self.min_insight_length:
                            challenges.append({
                                'challenge': chal,
                                'context': email.ai_summary,
                                'source': email.sender_name or email.sender,
                                'date': email.email_date.isoformat() if email.email_date else None
                            })
            
            # Enhanced topic knowledge with rich contexts
            topic_knowledge = {
                'all_topics': [topic.name for topic in topics if topic.is_official or 
                              (topic.description and len(topic.description) > 10)],
                'official_topics': [topic.name for topic in topics if topic.is_official],
                'topic_contexts': {}
            }
            
            for topic in topics:
                if topic.is_official or (topic.description and len(topic.description) > 10):
                    topic_emails = [email for email in quality_emails if email.topics and topic.name in email.topics]
                    contexts = []
                    for email in topic_emails[:3]:  # Top 3 emails per topic
                        if email.ai_summary:
                            contexts.append({
                                'summary': email.ai_summary,
                                'sender': email.sender_name or email.sender,
                                'date': email.email_date.isoformat() if email.email_date else None,
                                'email_subject': email.subject
                            })
                    topic_knowledge['topic_contexts'][topic.name] = contexts
            
            # Enhanced statistics
            summary_stats = {
                'total_emails_analyzed': len(quality_emails),
                'rich_contacts': len(rich_contacts),
                'business_decisions': len(business_decisions),
                'opportunities_identified': len(opportunities),
                'challenges_tracked': len(challenges),
                'active_projects': len([p for p in projects if p.status == 'active']),
                'official_topics': len([t for t in topics if t.is_official]),
                'calendar_events': len(calendar_events),
                'upcoming_meetings': len([e for e in calendar_events if e.start_time and e.start_time > now]),
                'recent_meetings': len([e for e in calendar_events if e.start_time and e.start_time < now])
            }
            
            # Process calendar intelligence for knowledge base
            calendar_intelligence = self._extract_calendar_intelligence(calendar_events, people, now)
            
            return {
                'success': True,
                'user_email': user_email,
                'knowledge_base': {
                    'summary_stats': summary_stats,
                    'rich_contacts': rich_contacts,
                    'business_intelligence': {
                        'recent_decisions': sorted(business_decisions, 
                                                 key=lambda x: x['date'] or '', reverse=True)[:8],
                        'top_opportunities': sorted(opportunities,
                                                  key=lambda x: x['date'] or '', reverse=True)[:8],
                        'current_challenges': sorted(challenges,
                                                   key=lambda x: x['date'] or '', reverse=True)[:8]
                    },
                    'topic_knowledge': topic_knowledge,
                    'projects_summary': [
                        {
                            'name': project.name,
                            'description': project.description,
                            'status': project.status,
                            'priority': project.priority,
                            'stakeholders': project.stakeholders or [],
                            'key_topics': project.key_topics or []
                        }
                        for project in projects if project.description and len(project.description) > self.min_insight_length
                    ][:10],  # Top 10 substantial projects
                    'calendar_events': calendar_events,
                    'calendar_intelligence': calendar_intelligence
                }
            }
        
        except Exception as e:
            logger.error(f"Failed to get chat knowledge for {user_email}: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _create_professional_story(self, person: Person, emails: List[Email]) -> str:
        """Create a rich professional story for a contact based on email interactions"""
        try:
            # Find emails from this person
            person_emails = [e for e in emails if e.sender and person.email_address and 
                           e.sender.lower() == person.email_address.lower()]
            
            if not person_emails:
                return f"Professional contact with {person.relationship_type or 'business'} relationship."
            
            # Analyze communication patterns and content
            total_emails = len(person_emails)
            recent_emails = sorted(person_emails, key=lambda x: x.email_date or datetime.min, reverse=True)[:3]
            
            # Extract key themes from their communication
            themes = []
            for email in recent_emails:
                if email.ai_summary and len(email.ai_summary) > 20:
                    themes.append(email.ai_summary)
            
            # Create professional narrative
            story_parts = []
            
            if person.company and person.title:
                story_parts.append(f"{person.title} at {person.company}")
            elif person.company:
                story_parts.append(f"Works at {person.company}")
            elif person.title:
                story_parts.append(f"{person.title}")
            
            if total_emails > 1:
                story_parts.append(f"Active correspondence with {total_emails} substantive emails")
            
            if themes:
                story_parts.append(f"Recent discussions: {'; '.join(themes[:2])}")
            
            if person.relationship_type:
                story_parts.append(f"Relationship: {person.relationship_type}")
            
            return '. '.join(story_parts) if story_parts else "Professional business contact"
            
        except Exception as e:
            logger.error(f"Error creating professional story: {str(e)}")
            return "Professional business contact"
    
    def _deduplicate_and_rank(self, items: List[str]) -> List[str]:
        """Deduplicate similar items and rank by relevance"""
        if not items:
            return []
        
        # Simple deduplication by similarity (basic approach)
        unique_items = []
        for item in items:
            # Check if this item is too similar to existing ones
            is_duplicate = False
            for existing in unique_items:
                # Simple similarity check - if 70% of words overlap, consider duplicate
                item_words = set(item.lower().split())
                existing_words = set(existing.lower().split())
                
                if len(item_words) > 0 and len(existing_words) > 0:
                    overlap = len(item_words & existing_words)
                    similarity = overlap / min(len(item_words), len(existing_words))
                    if similarity > 0.7:
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                unique_items.append(item)
        
        # Rank by length and specificity (longer, more specific items are often better)
        unique_items.sort(key=lambda x: (len(x), len(x.split())), reverse=True)
        
        return unique_items

    def _get_user_business_context(self, user_id: int) -> Dict:
        """Get existing business context to enhance AI analysis"""
        try:
            # Get existing high-quality projects
            projects = get_db_manager().get_user_projects(user_id, limit=50)
            project_context = [p.name for p in projects if p.description and len(p.description) > 20]
            
            # Get existing high-quality people
            people = get_db_manager().get_user_people(user_id, limit=100)
            people_context = [f"{p.name} ({p.role or 'Unknown role'}) at {p.company or 'Unknown company'}" 
                             for p in people if p.name and not self._is_non_human_contact(p.email_address or '')]
            
            # Get existing topics
            topics = get_db_manager().get_user_topics(user_id, limit=100)
            topic_context = [t.name for t in topics if t.is_official]
            
            return {
                'existing_projects': project_context[:10],  # Top 10 projects
                'key_contacts': people_context[:20],  # Top 20 human contacts
                'official_topics': topic_context[:15]  # Top 15 official topics
            }
        except Exception as e:
            logger.error(f"Failed to get user business context: {str(e)}")
            return {'existing_projects': [], 'key_contacts': [], 'official_topics': []}
    
    def _filter_quality_emails_debug(self, emails: List[Email], user_email: str) -> List[Email]:
        """Enhanced filtering for quality-focused email processing - DEBUG VERSION WITH RELAXED FILTERS"""
        quality_emails = []
        
        for email in emails:
            logger.debug(f"Evaluating email from {email.sender} with subject: {email.subject}")
            
            # Skip emails from the user themselves - check both email and name
            if email.sender and user_email.lower() in email.sender.lower():
                logger.debug(f"Skipping email from user themselves: {email.sender}")
                continue
            
            # Also check sender name to catch cases where user's name appears as sender
            user_name_parts = user_email.split('@')[0].lower()  # Get username part
            sender_name = (email.sender_name or '').lower()
            if (sender_name and len(user_name_parts) > 3 and 
                user_name_parts in sender_name.replace('.', '').replace('_', '')):
                logger.debug(f"Skipping email from user by name: {sender_name}")
                continue

            # RELAXED: Only skip the most obvious non-human senders
            if self._is_obviously_non_human_contact(email.sender or ''):
                logger.debug(f"Skipping obviously non-human sender: {email.sender}")
                continue
                
            # RELAXED: Skip only obvious newsletters and promotional content
            if self._is_obvious_newsletter_or_promotional(email):
                logger.debug(f"Skipping obvious newsletter/promotional content")
                continue
                
            # RELAXED: Very permissive content length - just need some content
            content = email.body_clean or email.snippet or ''
            if len(content.strip()) < 10:  # Very permissive
                logger.debug(f"Skipping email with minimal content: {len(content)} chars")
                continue
                
            # RELAXED: Only skip very obvious automated emails
            subject_lower = (email.subject or '').lower()
            automated_subjects = ['automated', 'automatic reply', 'out of office']
            if any(pattern in subject_lower for pattern in automated_subjects) and len(content) < 50:
                logger.debug(f"Skipping automated email with subject: {email.subject}")
                continue
                
            logger.debug(f"Email passed quality filters: {email.sender}")
            quality_emails.append(email)
        
        logger.info(f"Quality filtering: {len(quality_emails)} emails passed out of {len(emails)} total")
        return quality_emails

    def _is_obviously_non_human_contact(self, email_address: str) -> bool:
        """RELAXED: Only filter obviously non-human contacts - for debugging"""
        if not email_address:
            return True
            
        email_lower = email_address.lower()
        
        # Only the most obvious non-human patterns
        obvious_non_human_patterns = [
            'noreply', 'no-reply', 'donotreply', 'mailer-daemon',
            'postmaster@', 'daemon@', 'bounce@', 'automated@',
            'robot@', 'bot@'
        ]
        
        # Check against obvious non-human patterns only
        for pattern in obvious_non_human_patterns:
            if pattern in email_lower:
                logger.debug(f"Obvious non-human pattern detected: {pattern} in {email_address}")
                return True
                
        return False

    def _is_obvious_newsletter_or_promotional(self, email: Email) -> bool:
        """RELAXED: Only filter obvious newsletters - for debugging"""
        if not email:
            return True
            
        sender = (email.sender or '').lower()
        subject = (email.subject or '').lower()
        content = (email.body_clean or email.snippet or '').lower()
        
        # Only check for very obvious newsletter patterns
        obvious_newsletter_patterns = [
            'substack.com', 'mailchimp.com', 'beehiiv.com',
            'unsubscribe', 'view in browser', 'manage preferences'
        ]
        
        # Check domain patterns
        for pattern in obvious_newsletter_patterns:
            if pattern in sender or pattern in content:
                logger.debug(f"Obvious newsletter pattern detected: {pattern}")
                return True
                
        return False

    def _validate_analysis_quality_debug(self, analysis: Dict) -> bool:
        """Validate that the analysis meets quality standards - VERY INCLUSIVE VERSION"""
        try:
            # VERY INCLUSIVE: Check strategic value score - very low threshold
            strategic_value = analysis.get('strategic_value_score', 0.7)  # Default to 0.7 if missing
            if strategic_value < 0.2:  # Only reject very low value
                logger.debug(f"Analysis rejected - very low strategic value: {strategic_value}")
                return False
            
            # VERY INCLUSIVE: Check summary quality - very reduced minimum length
            summary = analysis.get('summary', '')
            if len(summary) < 3:  # Almost any summary is acceptable
                logger.debug(f"Analysis rejected - summary too short: {len(summary)} chars")
                return False
            
            logger.debug(f"Analysis passed quality validation - strategic value: {strategic_value}")
            return True
            
        except Exception as e:
            logger.error(f"Error validating analysis quality: {str(e)}")
            return True  # Default to accepting if validation fails

    def _process_human_contacts_only_debug(self, user_id: int, analysis: Dict, email: Email) -> int:
        """Process people information with RELAXED human contact filtering - DEBUG VERSION"""
        people_count = 0
        
        logger.debug(f"Processing contacts for email from {email.sender}")
        logger.debug(f"Analysis contains sender_analysis: {'sender_analysis' in analysis}")
        logger.debug(f"Analysis contains people: {'people' in analysis}")
        
        # Process sender first (with relaxed human validation)
        sender_analysis = analysis.get('sender_analysis')
        if sender_analysis and email.sender:
            logger.debug(f"Sender analysis: {sender_analysis}")
            
            # RELAXED: Don't require is_human_contact flag, infer from context
            is_human = (sender_analysis.get('is_human_contact', True) and  # Default to True
                       not self._is_obviously_non_human_contact(email.sender))
            
            if is_human:
                logger.debug(f"Processing sender as human contact: {email.sender}")
                
                # Get existing person to accumulate knowledge
                existing_person = get_db_manager().find_person_by_email(user_id, email.sender)
                
                # Accumulate notes and context over time
                existing_notes = existing_person.notes if existing_person else ""
                new_relevance = sender_analysis.get('business_relevance', '')
                
                # Combine old and new notes intelligently
                accumulated_notes = existing_notes
                if new_relevance and new_relevance not in accumulated_notes:
                    if accumulated_notes:
                        accumulated_notes += f"\n\nRecent Context: {new_relevance}"
                    else:
                        accumulated_notes = new_relevance
                
                person_data = {
                    'email_address': email.sender,
                    'name': sender_analysis.get('name', email.sender_name or email.sender),
                    'role': sender_analysis.get('role') or (existing_person.role if existing_person else None),
                    'company': sender_analysis.get('company') or (existing_person.company if existing_person else None),
                    'relationship_type': sender_analysis.get('relationship') or (existing_person.relationship_type if existing_person else None),
                    'notes': accumulated_notes,  # Accumulated knowledge
                    'importance_level': max(0.8, existing_person.importance_level if existing_person else 0.8),  # Increment importance
                    'ai_version': self.version,
                    'total_emails': (existing_person.total_emails if existing_person else 0) + 1  # Increment email count
                }
                get_db_manager().create_or_update_person(user_id, person_data)
                people_count += 1
                logger.info(f"Created/updated person: {person_data['name']} ({person_data['email_address']})")
            else:
                logger.debug(f"Sender not processed as human contact: {email.sender}")
        
        # Process mentioned people (with relaxed validation)
        people_data = analysis.get('people', [])
        if isinstance(people_data, list):
            logger.debug(f"Processing {len(people_data)} mentioned people")
            for person_info in people_data:
                if person_info.get('email') or person_info.get('name'):
                    # RELAXED: Additional human validation but more permissive
                    email_addr = person_info.get('email', '')
                    if email_addr and self._is_obviously_non_human_contact(email_addr):
                        logger.debug(f"Skipping obviously non-human mentioned person: {email_addr}")
                        continue
                    
                    # Get existing person to accumulate knowledge
                    existing_person = None
                    if email_addr:
                        existing_person = get_db_manager().find_person_by_email(user_id, email_addr)
                    
                    # Accumulate knowledge
                    existing_notes = existing_person.notes if existing_person else ""
                    new_relevance = person_info.get('business_relevance', '')
                    
                    accumulated_notes = existing_notes
                    if new_relevance and new_relevance not in accumulated_notes:
                        if accumulated_notes:
                            accumulated_notes += f"\n\nMentioned Context: {new_relevance}"
                        else:
                            accumulated_notes = new_relevance
                    
                    person_data = {
                        'email_address': person_info.get('email'),
                        'name': person_info['name'],
                        'role': person_info.get('role') or (existing_person.role if existing_person else None),
                        'company': person_info.get('company') or (existing_person.company if existing_person else None),
                        'relationship_type': person_info.get('relationship') or (existing_person.relationship_type if existing_person else None),
                        'notes': accumulated_notes,  # Accumulated knowledge
                        'ai_version': self.version
                    }
                    get_db_manager().create_or_update_person(user_id, person_data)
                    people_count += 1
                    logger.info(f"Created/updated mentioned person: {person_data['name']}")
        
        logger.info(f"Total people processed for this email: {people_count}")
        return people_count

    def _filter_quality_emails(self, emails: List[Email], user_email: str) -> List[Email]:
        """Enhanced filtering for quality-focused email processing - BUSINESS FOCUSED"""
        quality_emails = []
        
        for email in emails:
            # ENHANCED: Skip emails from the user themselves - check both email and name
            if email.sender and user_email.lower() in email.sender.lower():
                continue
            
            # ENHANCED: Also check sender name to catch cases where user's name appears as sender
            user_name_parts = user_email.split('@')[0].lower()  # Get username part
            sender_name = (email.sender_name or '').lower()
            if (sender_name and len(user_name_parts) > 3 and 
                user_name_parts in sender_name.replace('.', '').replace('_', '')):
                continue

            # Skip non-human senders
            if self._is_non_human_contact(email.sender or ''):
                continue
                
            # ENHANCED: Skip newsletters and promotional content
            if self._is_newsletter_or_promotional(email):
                continue
                
            # RELAXED: Reduced minimum content length from 50 to 25
            content = email.body_clean or email.snippet or ''
            if len(content.strip()) < 25:  # Much more permissive
                continue
                
            # RELAXED: More permissive automated subject filtering
            subject_lower = (email.subject or '').lower()
            automated_subjects = ['automatic', 'notification', 'alert']  # Removed 're:', 'fwd:', 'update', 'reminder'
            # Only skip if BOTH automated subject AND very short content
            if any(pattern in subject_lower for pattern in automated_subjects) and len(content) < 100:  # More lenient
                continue
                
            # EXPANDED: More business indicators to catch valuable emails
            business_indicators = [
                'meeting', 'project', 'proposal', 'contract', 'agreement',
                'decision', 'feedback', 'review', 'discussion', 'strategy',
                'client', 'customer', 'partner', 'collaboration', 'opportunity',
                'budget', 'funding', 'investment', 'deal', 'business',
                'follow up', 'followup', 'call', 'schedule', 'deadline',
                'urgent', 'important', 'action', 'update', 'progress',
                'team', 'work', 'development', 'launch', 'release'
            ]
            
            has_business_content = any(indicator in content.lower() or indicator in subject_lower 
                                     for indicator in business_indicators)
            
            # MORE INCLUSIVE: Accept if business content OR longer than 150 chars (reduced from 300)
            if has_business_content or len(content) > 150:
                quality_emails.append(email)
            # ADDED: Also include emails with meaningful sender names (not just email addresses)
            elif email.sender_name and len(email.sender_name) > 3 and len(content) > 50:
                quality_emails.append(email)
        
        # Sort by email date (newest first) and content length (longer = potentially more substantial)
        quality_emails.sort(key=lambda e: (e.email_date or datetime.min, len(e.body_clean or e.snippet or '')), reverse=True)
        
        return quality_emails

    def _is_newsletter_or_promotional(self, email: Email) -> bool:
        """Detect and filter out newsletters, promotional emails, and automated content"""
        if not email:
            return True
            
        sender = (email.sender or '').lower()
        subject = (email.subject or '').lower()
        content = (email.body_clean or email.snippet or '').lower()
        sender_name = (email.sender_name or '').lower()
        
        # Newsletter domains and patterns
        newsletter_domains = [
            'substack.com', 'mailchimp.com', 'constantcontact.com', 'campaign-archive.com',
            'beehiiv.com', 'ghost.org', 'medium.com', 'linkedin.com/pulse',
            'newsletter', 'mail.', 'noreply', 'no-reply', 'donotreply', 'marketing',
            'promotions', 'offers', 'deals', 'sales', 'campaigns'
        ]
        
        # Newsletter subject patterns
        newsletter_subjects = [
            'newsletter', 'weekly digest', 'daily digest', 'roundup', 'briefing',
            'this week in', 'weekly update', 'monthly update', 'startup digest',
            'tech digest', 'vc corner', 'venture capital', 'investment newsletter',
            'industry news', 'market update', 'funding round', 'startup funding'
        ]
        
        # Newsletter content patterns
        newsletter_content = [
            'unsubscribe', 'view in browser', 'manage preferences', 'update subscription',
            'forward to a friend', 'share this newsletter', 'subscriber', 'mailing list',
            'this email was sent to', 'you are receiving this', 'promotional email'
        ]
        
        # Newsletter sender name patterns
        newsletter_names = [
            'newsletter', 'digest', 'briefing', 'update', 'news', 'weekly',
            'daily', 'monthly', 'roundup', 'vc corner', 'startup', 'lenny',
            'substack', 'medium', 'ghost'
        ]
        
        # Check domain patterns
        for domain in newsletter_domains:
            if domain in sender:
                return True
        
        # Check subject patterns
        for pattern in newsletter_subjects:
            if pattern in subject:
                return True
            
        # Check content patterns
        for pattern in newsletter_content:
            if pattern in content:
                return True
        
        # Check sender name patterns
        for pattern in newsletter_names:
            if pattern in sender_name:
                return True
        
        # Additional heuristics for promotional content
        promotional_indicators = [
            'special offer', 'limited time', 'exclusive deal', 'discount',
            'sale ends', 'act now', 'don\'t miss', 'free trial', 'premium upgrade',
            'webinar invitation', 'event invitation', 'conference', 'summit'
        ]
        
        promotional_count = sum(1 for indicator in promotional_indicators 
                               if indicator in content or indicator in subject)
        
        # If multiple promotional indicators, likely promotional
        if promotional_count >= 2:
            return True
        
        # Check for mass email patterns
        mass_email_patterns = [
            'dear valued', 'dear customer', 'dear subscriber', 'dear member',
            'greetings', 'hello there', 'hi everyone', 'dear all'
        ]
        
        for pattern in mass_email_patterns:
            if pattern in content[:200]:  # Check first 200 chars
                return True
        
        return False

    def _is_non_human_contact(self, email_address: str) -> bool:
        """Determine if an email address belongs to a non-human sender - BALANCED VERSION"""
        if not email_address:
            return True
            
        email_lower = email_address.lower()
        
        # FOCUSED: Only filter obvious automation, preserve business contacts
        definite_non_human_patterns = [
            'noreply', 'no-reply', 'donotreply', 'do-not-reply', 
            'mailer-daemon', 'postmaster@', 'daemon@', 'bounce@',
            'robot@', 'bot@', 'automated@', 'system@notification',
            'newsletter@', 'digest@', 'updates@notifications'
        ]
        
        # Check against definite non-human patterns only
        for pattern in definite_non_human_patterns:
            if pattern in email_lower:
                return True
        
        # SPECIFIC: Only filter major newsletter/automation services
        automation_domains = [
            'substack.com', 'beehiiv.com', 'mailchimp.com', 'constantcontact.com',
            'campaign-archive.com', 'sendgrid.net', 'mailgun.org', 'mandrill.com'
        ]
        
        for domain in automation_domains:
            if domain in email_lower:
                return True
        
        # PRESERVE: Keep business contacts that might use standard business email patterns
        # Removed: 'admin@', 'info@', 'contact@', 'help@', 'service@', 'team@', 'hello@', 'hi@'
        # Removed: 'linkedin.com', 'facebook.com', etc. - people use these for business
                
        return False

    def _extract_calendar_intelligence(self, calendar_events: List, people: List, now: datetime) -> Dict:
        """Extract calendar intelligence from events for knowledge base"""
        try:
            upcoming_events = []
            recent_events = []
            meeting_patterns = {}
            attendee_frequency = {}
            
            # Process calendar events
            for event in calendar_events:
                if not hasattr(event, 'start_time') or not event.start_time:
                    continue
                    
                # Categorize by time
                if event.start_time > now:
                    # Upcoming events
                    upcoming_events.append({
                        'title': event.title or 'Untitled Meeting',
                        'start_time': event.start_time.isoformat() if event.start_time else None,
                        'attendees': len(event.attendee_emails or []),
                        'meeting_type': getattr(event, 'meeting_type', 'unknown')
                    })
                else:
                    # Recent events for context
                    recent_events.append({
                        'title': event.title or 'Untitled Meeting',
                        'start_time': event.start_time.isoformat() if event.start_time else None,
                        'attendees': len(event.attendee_emails or [])
                    })
                
                # Track meeting patterns
                if hasattr(event, 'meeting_type') and event.meeting_type:
                    meeting_patterns[event.meeting_type] = meeting_patterns.get(event.meeting_type, 0) + 1
                
                # Track attendee frequency for relationship intelligence
                if hasattr(event, 'attendee_emails') and event.attendee_emails:
                    for attendee_email in event.attendee_emails:
                        attendee_frequency[attendee_email] = attendee_frequency.get(attendee_email, 0) + 1
            
            # Get top attendees for relationship context
            top_attendees = sorted(attendee_frequency.items(), key=lambda x: x[1], reverse=True)[:10]
            
            # Map attendees to known people
            attendee_insights = []
            for attendee_email, meeting_count in top_attendees:
                # Find matching person in people database
                matching_person = None
                for person in people:
                    if hasattr(person, 'email_address') and person.email_address == attendee_email:
                        matching_person = person
                        break
                
                if matching_person:
                    attendee_insights.append({
                        'name': matching_person.name,
                        'email': attendee_email,
                        'meeting_frequency': meeting_count,
                        'company': getattr(matching_person, 'company', None),
                        'relationship': getattr(matching_person, 'relationship_type', None)
                    })
                else:
                    attendee_insights.append({
                        'name': attendee_email.split('@')[0],  # Use email username as fallback
                        'email': attendee_email,
                        'meeting_frequency': meeting_count,
                        'company': None,
                        'relationship': 'unknown'
                    })
            
            return {
                'upcoming_events': sorted(upcoming_events, key=lambda x: x['start_time'] or '')[:5],
                'recent_events': sorted(recent_events, key=lambda x: x['start_time'] or '', reverse=True)[:5],
                'meeting_patterns': meeting_patterns,
                'frequent_attendees': attendee_insights,
                'total_upcoming': len(upcoming_events),
                'total_recent': len(recent_events)
            }
            
        except Exception as e:
            logger.error(f"Failed to extract calendar intelligence: {str(e)}")
            return {
                'upcoming_events': [],
                'recent_events': [],
                'meeting_patterns': {},
                'frequent_attendees': [],
                'total_upcoming': 0,
                'total_recent': 0
            }

# Global instance
email_intelligence = EmailIntelligenceProcessor() 

============================================================
FILE: archive/backup_files/v1_original/models/database.py
============================================================
import os
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean, Float, ForeignKey, Index, text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship, Session
from sqlalchemy.dialects.postgresql import JSON
from sqlalchemy.types import TypeDecorator

from config.settings import settings

logger = logging.getLogger(__name__)

# Base class for all models
Base = declarative_base()

# Custom JSON type that works with both SQLite and PostgreSQL
class JSONType(TypeDecorator):
    impl = Text
    
    def process_bind_param(self, value, dialect):
        if value is not None:
            return json.dumps(value)
        return value
    
    def process_result_value(self, value, dialect):
        if value is not None:
            return json.loads(value)
        return value

class User(Base):
    """User model for multi-tenant authentication"""
    __tablename__ = 'users'
    
    id = Column(Integer, primary_key=True)
    email = Column(String(255), unique=True, nullable=False, index=True)
    google_id = Column(String(255), unique=True, nullable=False)
    name = Column(String(255), nullable=False)
    
    # OAuth credentials (encrypted in production)
    access_token = Column(Text)
    refresh_token = Column(Text)
    token_expires_at = Column(DateTime)
    scopes = Column(JSONType)
    
    # Account metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    last_login = Column(DateTime, default=datetime.utcnow)
    is_active = Column(Boolean, default=True)
    
    # Processing preferences
    email_fetch_limit = Column(Integer, default=50)
    email_days_back = Column(Integer, default=30)
    auto_process_emails = Column(Boolean, default=True)
    
    # Relationships
    emails = relationship("Email", back_populates="user", cascade="all, delete-orphan")
    tasks = relationship("Task", back_populates="user", cascade="all, delete-orphan")
    people = relationship("Person", back_populates="user", cascade="all, delete-orphan")
    projects = relationship("Project", back_populates="user", cascade="all, delete-orphan")
    topics = relationship("Topic", back_populates="user", cascade="all, delete-orphan")
    
    def __repr__(self):
        return f"<User(email='{self.email}', name='{self.name}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'email': self.email,
            'name': self.name,
            'google_id': self.google_id,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'last_login': self.last_login.isoformat() if self.last_login else None,
            'is_active': self.is_active,
            'email_fetch_limit': self.email_fetch_limit,
            'email_days_back': self.email_days_back,
            'auto_process_emails': self.auto_process_emails
        }

class Email(Base):
    """Email model for storing processed emails per user"""
    __tablename__ = 'emails'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Gmail identifiers
    gmail_id = Column(String(255), nullable=False, index=True)
    thread_id = Column(String(255), index=True)
    
    # Email content
    sender = Column(String(255), index=True)
    sender_name = Column(String(255))
    subject = Column(Text)
    body_text = Column(Text)
    body_html = Column(Text)
    body_clean = Column(Text)
    body_preview = Column(Text)
    snippet = Column(Text)
    
    # Email metadata
    recipients = Column(JSONType)  # List of recipient emails
    cc = Column(JSONType)  # List of CC emails
    bcc = Column(JSONType)  # List of BCC emails
    labels = Column(JSONType)  # Gmail labels
    attachments = Column(JSONType)  # Attachment metadata
    entities = Column(JSONType)  # Extracted entities
    
    # Email properties
    email_date = Column(DateTime, index=True)
    size_estimate = Column(Integer)
    message_type = Column(String(50), index=True)  # regular, meeting, newsletter, etc.
    priority_score = Column(Float, index=True)
    
    # Email status
    is_read = Column(Boolean, default=False)
    is_important = Column(Boolean, default=False)
    is_starred = Column(Boolean, default=False)
    has_attachments = Column(Boolean, default=False)
    
    # Email classification and AI insights
    project_id = Column(Integer, ForeignKey('projects.id'), index=True)
    mentioned_people = Column(JSONType)  # List of person IDs mentioned in email
    ai_summary = Column(Text)  # Claude-generated summary
    ai_category = Column(String(100))  # AI-determined category
    sentiment_score = Column(Float)  # Sentiment analysis score
    urgency_score = Column(Float)  # AI-determined urgency
    key_insights = Column(JSONType)  # Key insights extracted by Claude
    topics = Column(JSONType)  # Main topics/themes identified
    action_required = Column(Boolean, default=False)  # Whether action is needed
    follow_up_required = Column(Boolean, default=False)  # Whether follow-up needed
    
    # Processing metadata
    processed_at = Column(DateTime, default=datetime.utcnow)
    normalizer_version = Column(String(50))
    has_errors = Column(Boolean, default=False)
    error_message = Column(Text)
    
    # Relationships
    user = relationship("User", back_populates="emails")
    tasks = relationship("Task", back_populates="email", cascade="all, delete-orphan")
    
    # Indexes for performance - Fixed naming to avoid conflicts
    __table_args__ = (
        Index('idx_email_user_gmail', 'user_id', 'gmail_id'),
        Index('idx_email_user_date', 'user_id', 'email_date'),
        Index('idx_email_user_type', 'user_id', 'message_type'),
        Index('idx_email_user_priority', 'user_id', 'priority_score'),
    )
    
    def __repr__(self):
        return f"<Email(gmail_id='{self.gmail_id}', subject='{self.subject[:50]}...')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'gmail_id': self.gmail_id,
            'thread_id': self.thread_id,
            'sender': self.sender,
            'sender_name': self.sender_name,
            'subject': self.subject,
            'body_preview': self.body_preview,
            'snippet': self.snippet,
            'recipients': self.recipients,
            'email_date': self.email_date.isoformat() if self.email_date else None,
            'message_type': self.message_type,
            'priority_score': self.priority_score,
            'is_read': self.is_read,
            'is_important': self.is_important,
            'is_starred': self.is_starred,
            'has_attachments': self.has_attachments,
            'processed_at': self.processed_at.isoformat() if self.processed_at else None,
            'project_id': self.project_id,
            'mentioned_people': self.mentioned_people,
            'ai_summary': self.ai_summary,
            'ai_category': self.ai_category,
            'sentiment_score': self.sentiment_score,
            'urgency_score': self.urgency_score,
            'key_insights': self.key_insights,
            'topics': self.topics,
            'action_required': self.action_required,
            'follow_up_required': self.follow_up_required
        }

class Task(Base):
    """Task model for storing extracted tasks per user"""
    __tablename__ = 'tasks'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    email_id = Column(Integer, ForeignKey('emails.id'), nullable=True, index=True)
    
    # Task content
    description = Column(Text, nullable=False)
    assignee = Column(String(255))
    due_date = Column(DateTime, index=True)
    due_date_text = Column(String(255))
    
    # Task metadata
    priority = Column(String(20), default='medium', index=True)  # high, medium, low
    category = Column(String(50), index=True)  # follow-up, deadline, meeting, etc.
    confidence = Column(Float)  # AI confidence score
    source_text = Column(Text)  # Original text from email
    
    # Task status
    status = Column(String(20), default='pending', index=True)  # pending, in_progress, completed, cancelled
    completed_at = Column(DateTime)
    
    # Extraction metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    extractor_version = Column(String(50))
    model_used = Column(String(100))
    
    # Relationships
    user = relationship("User", back_populates="tasks")
    email = relationship("Email", back_populates="tasks")
    
    # Indexes for performance - Fixed naming to avoid conflicts
    __table_args__ = (
        Index('idx_task_user_status', 'user_id', 'status'),
        Index('idx_task_user_priority_unique', 'user_id', 'priority'),
        Index('idx_task_user_due_date', 'user_id', 'due_date'),
        Index('idx_task_user_category', 'user_id', 'category'),
    )
    
    def __repr__(self):
        return f"<Task(description='{self.description[:50]}...', priority='{self.priority}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'email_id': self.email_id,
            'description': self.description,
            'assignee': self.assignee,
            'due_date': self.due_date.isoformat() if self.due_date else None,
            'due_date_text': self.due_date_text,
            'priority': self.priority,
            'category': self.category,
            'confidence': self.confidence,
            'source_text': self.source_text,
            'status': self.status,
            'completed_at': self.completed_at.isoformat() if self.completed_at else None,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'extractor_version': self.extractor_version,
            'model_used': self.model_used
        }

class Person(Base):
    """Person model for tracking individuals mentioned in emails"""
    __tablename__ = 'people'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Person identification
    email_address = Column(String(255), index=True)
    name = Column(String(255), nullable=False)
    first_name = Column(String(100))
    last_name = Column(String(100))
    
    # Person details (extracted and augmented by Claude)
    title = Column(String(255))
    company = Column(String(255))
    role = Column(String(255))
    department = Column(String(255))
    
    # Relationship and context
    relationship_type = Column(String(100))  # colleague, client, vendor, etc.
    communication_frequency = Column(String(50))  # high, medium, low
    importance_level = Column(Float)  # 0.0 to 1.0
    
    # Knowledge base (JSON fields for flexible data)
    skills = Column(JSONType)  # List of skills/expertise
    interests = Column(JSONType)  # Personal/professional interests
    projects_involved = Column(JSONType)  # List of project IDs
    communication_style = Column(Text)  # Claude's analysis of communication style
    key_topics = Column(JSONType)  # Main topics discussed with this person
    
    # Extracted insights
    personality_traits = Column(JSONType)  # Claude-extracted personality insights
    preferences = Column(JSONType)  # Communication preferences, etc.
    notes = Column(Text)  # Accumulated notes about this person
    
    # Metadata
    first_mentioned = Column(DateTime, default=datetime.utcnow)
    last_interaction = Column(DateTime, default=datetime.utcnow)
    total_emails = Column(Integer, default=0)
    
    # AI processing metadata
    knowledge_confidence = Column(Float, default=0.5)  # Confidence in extracted data
    last_updated_by_ai = Column(DateTime)
    ai_version = Column(String(50))
    
    # NEW: Smart Contact Strategy fields
    is_trusted_contact = Column(Boolean, default=False, index=True)
    engagement_score = Column(Float, default=0.0)
    bidirectional_topics = Column(JSONType)  # Topics with back-and-forth discussion
    
    # Relationships
    user = relationship("User", back_populates="people")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_person_user_email', 'user_id', 'email_address'),
        Index('idx_person_user_name', 'user_id', 'name'),
        Index('idx_person_company', 'user_id', 'company'),
    )
    
    def __repr__(self):
        return f"<Person(name='{self.name}', email='{self.email_address}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'email_address': self.email_address,
            'name': self.name,
            'first_name': self.first_name,
            'last_name': self.last_name,
            'title': self.title,
            'company': self.company,
            'role': self.role,
            'department': self.department,
            'relationship_type': self.relationship_type,
            'communication_frequency': self.communication_frequency,
            'importance_level': self.importance_level,
            'skills': self.skills,
            'interests': self.interests,
            'projects_involved': self.projects_involved,
            'communication_style': self.communication_style,
            'key_topics': self.key_topics,
            'personality_traits': self.personality_traits,
            'preferences': self.preferences,
            'notes': self.notes,
            'first_mentioned': self.first_mentioned.isoformat() if self.first_mentioned else None,
            'last_interaction': self.last_interaction.isoformat() if self.last_interaction else None,
            'total_emails': self.total_emails,
            'knowledge_confidence': self.knowledge_confidence,
            'last_updated_by_ai': self.last_updated_by_ai.isoformat() if self.last_updated_by_ai else None,
            'ai_version': self.ai_version,
            'is_trusted_contact': self.is_trusted_contact,
            'engagement_score': self.engagement_score,
            'bidirectional_topics': self.bidirectional_topics
        }

class Project(Base):
    """Project model for categorizing emails and tracking project-related information"""
    __tablename__ = 'projects'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Project identification
    name = Column(String(255), nullable=False)
    slug = Column(String(255), index=True)  # URL-friendly name
    description = Column(Text)
    
    # Project details
    status = Column(String(50), default='active')  # active, completed, paused, cancelled
    priority = Column(String(20), default='medium')  # high, medium, low
    category = Column(String(100))  # business, personal, client work, etc.
    
    # Timeline
    start_date = Column(DateTime)
    end_date = Column(DateTime)
    deadline = Column(DateTime)
    
    # People and relationships
    stakeholders = Column(JSONType)  # List of person IDs involved
    team_members = Column(JSONType)  # List of person IDs
    
    # Project insights (extracted by Claude)
    key_topics = Column(JSONType)  # Main topics/themes
    objectives = Column(JSONType)  # Project goals and objectives
    challenges = Column(JSONType)  # Identified challenges
    progress_indicators = Column(JSONType)  # Metrics and milestones
    
    # Communication patterns
    communication_frequency = Column(String(50))
    last_activity = Column(DateTime)
    total_emails = Column(Integer, default=0)
    
    # AI analysis
    sentiment_trend = Column(Float)  # Overall sentiment about project
    urgency_level = Column(Float)  # How urgent this project appears
    confidence_score = Column(Float)  # AI confidence in project categorization
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    ai_version = Column(String(50))
    
    # Relationships
    user = relationship("User", back_populates="projects")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_project_user_status', 'user_id', 'status'),
        Index('idx_project_user_priority', 'user_id', 'priority'),
        Index('idx_project_user_category', 'user_id', 'category'),
    )
    
    def __repr__(self):
        return f"<Project(name='{self.name}', status='{self.status}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'name': self.name,
            'slug': self.slug,
            'description': self.description,
            'status': self.status,
            'priority': self.priority,
            'category': self.category,
            'start_date': self.start_date.isoformat() if self.start_date else None,
            'end_date': self.end_date.isoformat() if self.end_date else None,
            'deadline': self.deadline.isoformat() if self.deadline else None,
            'stakeholders': self.stakeholders,
            'team_members': self.team_members,
            'key_topics': self.key_topics,
            'objectives': self.objectives,
            'challenges': self.challenges,
            'progress_indicators': self.progress_indicators,
            'communication_frequency': self.communication_frequency,
            'last_activity': self.last_activity.isoformat() if self.last_activity else None,
            'total_emails': self.total_emails,
            'sentiment_trend': self.sentiment_trend,
            'urgency_level': self.urgency_level,
            'confidence_score': self.confidence_score,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'ai_version': self.ai_version
        }

class Topic(Base):
    """Topic model for organizing and categorizing content"""
    __tablename__ = 'topics'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Topic identification
    name = Column(String(255), nullable=False)
    slug = Column(String(255), index=True)  # URL-friendly name
    description = Column(Text)
    
    # Topic properties
    is_official = Column(Boolean, default=False, index=True)  # Official vs AI-discovered
    parent_topic_id = Column(Integer, ForeignKey('topics.id'), index=True)  # For hierarchical topics
    merged_topics = Column(Text)  # JSON string of merged topic names
    keywords = Column(Text)  # JSON string of keywords for matching (changed from JSONType for compatibility)
    email_count = Column(Integer, default=0)  # Number of emails with this topic
    
    # Usage tracking
    last_used = Column(DateTime)
    usage_frequency = Column(Float)
    confidence_threshold = Column(Float)
    
    # AI analysis
    confidence_score = Column(Float, default=0.5)  # AI confidence in topic classification
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    ai_version = Column(String(50))
    
    # Relationships
    user = relationship("User", back_populates="topics")
    parent_topic = relationship("Topic", remote_side=[id], backref="child_topics")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_topic_user_official', 'user_id', 'is_official'),
        Index('idx_topic_user_name', 'user_id', 'name'),
        Index('idx_topic_slug', 'user_id', 'slug'),
        Index('idx_topic_parent', 'parent_topic_id'),
    )
    
    def __repr__(self):
        return f"<Topic(name='{self.name}', is_official={self.is_official})>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'name': self.name,
            'slug': self.slug,
            'description': self.description,
            'is_official': self.is_official,
            'keywords': json.loads(self.keywords) if self.keywords else [],
            'email_count': self.email_count,
            'confidence_score': self.confidence_score,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'ai_version': self.ai_version,
            'parent_topic_id': self.parent_topic_id,
            'last_used': self.last_used.isoformat() if self.last_used else None
        }

class TrustedContact(Base):
    """Trusted Contact model for engagement-based contact database"""
    __tablename__ = 'trusted_contacts'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Contact identification
    email_address = Column(String(255), nullable=False, index=True)
    name = Column(String(255))
    
    # Engagement metrics
    engagement_score = Column(Float, default=0.0, index=True)
    first_sent_date = Column(DateTime)
    last_sent_date = Column(DateTime, index=True)
    total_sent_emails = Column(Integer, default=0)
    total_received_emails = Column(Integer, default=0)
    bidirectional_threads = Column(Integer, default=0)
    
    # Topic analysis
    topics_discussed = Column(JSONType)  # List of topics from sent/received emails
    bidirectional_topics = Column(JSONType)  # Topics with back-and-forth discussion
    
    # Relationship assessment
    relationship_strength = Column(String(20), default='low', index=True)  # high, medium, low
    communication_frequency = Column(String(20))  # daily, weekly, monthly, occasional
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    last_analyzed = Column(DateTime)
    
    # Relationships
    user = relationship("User", backref="trusted_contacts")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_trusted_contact_user_email', 'user_id', 'email_address'),
        Index('idx_trusted_contact_engagement', 'user_id', 'engagement_score'),
        Index('idx_trusted_contact_strength', 'user_id', 'relationship_strength'),
    )
    
    def __repr__(self):
        return f"<TrustedContact(email='{self.email_address}', strength='{self.relationship_strength}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'email_address': self.email_address,
            'name': self.name,
            'engagement_score': self.engagement_score,
            'first_sent_date': self.first_sent_date.isoformat() if self.first_sent_date else None,
            'last_sent_date': self.last_sent_date.isoformat() if self.last_sent_date else None,
            'total_sent_emails': self.total_sent_emails,
            'total_received_emails': self.total_received_emails,
            'bidirectional_threads': self.bidirectional_threads,
            'topics_discussed': self.topics_discussed,
            'bidirectional_topics': self.bidirectional_topics,
            'relationship_strength': self.relationship_strength,
            'communication_frequency': self.communication_frequency,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None,
            'last_analyzed': self.last_analyzed.isoformat() if self.last_analyzed else None
        }

class ContactContext(Base):
    """Rich context information for contacts"""
    __tablename__ = 'contact_contexts'
    
    id = Column(Integer, primary_key=True)
    person_id = Column(Integer, ForeignKey('people.id'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Context details
    context_type = Column(String(50), nullable=False, index=True)  # communication_pattern, project_involvement, topic_expertise, relationship_notes
    title = Column(String(255), nullable=False)
    description = Column(Text)
    confidence_score = Column(Float, default=0.5)
    
    # Supporting evidence
    source_emails = Column(JSONType)  # List of email IDs that contributed to this context
    supporting_quotes = Column(JSONType)  # Relevant excerpts from emails
    tags = Column(JSONType)  # Flexible tagging system
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    person = relationship("Person", backref="contexts")
    user = relationship("User", backref="contact_contexts")
    
    # Indexes
    __table_args__ = (
        Index('idx_contact_context_person', 'person_id', 'context_type'),
        Index('idx_contact_context_user', 'user_id', 'context_type'),
    )
    
    def __repr__(self):
        return f"<ContactContext(type='{self.context_type}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'person_id': self.person_id,
            'user_id': self.user_id,
            'context_type': self.context_type,
            'title': self.title,
            'description': self.description,
            'confidence_score': self.confidence_score,
            'source_emails': self.source_emails,
            'supporting_quotes': self.supporting_quotes,
            'tags': self.tags,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

class TaskContext(Base):
    """Rich context information for tasks"""
    __tablename__ = 'task_contexts'
    
    id = Column(Integer, primary_key=True)
    task_id = Column(Integer, ForeignKey('tasks.id'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Context details
    context_type = Column(String(50), nullable=False, index=True)  # background, stakeholders, timeline, business_impact
    title = Column(String(255), nullable=False)
    description = Column(Text)
    
    # Related entities
    related_people = Column(JSONType)  # List of person IDs
    related_projects = Column(JSONType)  # List of project IDs
    related_topics = Column(JSONType)  # List of relevant topics
    
    # Source information
    source_email_id = Column(Integer, ForeignKey('emails.id'))
    source_thread_id = Column(String(255))
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    task = relationship("Task", backref="contexts")
    user = relationship("User", backref="task_contexts")
    source_email = relationship("Email")
    
    # Indexes
    __table_args__ = (
        Index('idx_task_context_task', 'task_id', 'context_type'),
        Index('idx_task_context_user', 'user_id', 'context_type'),
    )
    
    def __repr__(self):
        return f"<TaskContext(type='{self.context_type}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'task_id': self.task_id,
            'user_id': self.user_id,
            'context_type': self.context_type,
            'title': self.title,
            'description': self.description,
            'related_people': self.related_people,
            'related_projects': self.related_projects,
            'related_topics': self.related_topics,
            'source_email_id': self.source_email_id,
            'source_thread_id': self.source_thread_id,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

class TopicKnowledgeBase(Base):
    """Comprehensive knowledge base for topics"""
    __tablename__ = 'topic_knowledge_base'
    
    id = Column(Integer, primary_key=True)
    topic_id = Column(Integer, ForeignKey('topics.id'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Knowledge details
    knowledge_type = Column(String(50), nullable=False, index=True)  # methodology, key_people, challenges, success_patterns, tools, decisions
    title = Column(String(255), nullable=False)
    content = Column(Text)
    confidence_score = Column(Float, default=0.5)
    
    # Supporting evidence
    supporting_evidence = Column(JSONType)  # Email excerpts, patterns observed
    source_emails = Column(JSONType)  # List of email IDs that contributed
    patterns = Column(JSONType)  # Observed patterns and trends
    
    # Knowledge metadata
    relevance_score = Column(Float, default=0.5)  # How relevant this knowledge is
    engagement_weight = Column(Float, default=0.5)  # Weight based on user engagement
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    last_updated = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    topic = relationship("Topic", backref="knowledge_base")
    user = relationship("User", backref="topic_knowledge")
    
    # Indexes
    __table_args__ = (
        Index('idx_topic_knowledge_topic', 'topic_id', 'knowledge_type'),
        Index('idx_topic_knowledge_user', 'user_id', 'knowledge_type'),
        Index('idx_topic_knowledge_relevance', 'user_id', 'relevance_score'),
    )
    
    def __repr__(self):
        return f"<TopicKnowledgeBase(type='{self.knowledge_type}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'topic_id': self.topic_id,
            'user_id': self.user_id,
            'knowledge_type': self.knowledge_type,
            'title': self.title,
            'content': self.content,
            'confidence_score': self.confidence_score,
            'supporting_evidence': self.supporting_evidence,
            'source_emails': self.source_emails,
            'patterns': self.patterns,
            'relevance_score': self.relevance_score,
            'engagement_weight': self.engagement_weight,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'last_updated': self.last_updated.isoformat() if self.last_updated else None
        }

class Calendar(Base):
    """Calendar model for storing Google Calendar events per user"""
    __tablename__ = 'calendar_events'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)
    
    # Google Calendar identifiers
    event_id = Column(String(255), nullable=False, index=True)
    calendar_id = Column(String(255), nullable=False, index=True)
    recurring_event_id = Column(String(255), index=True)
    
    # Event content
    title = Column(Text)
    description = Column(Text)
    location = Column(Text)
    status = Column(String(50))  # confirmed, tentative, cancelled
    
    # Event timing
    start_time = Column(DateTime, index=True)
    end_time = Column(DateTime, index=True)
    timezone = Column(String(100))
    is_all_day = Column(Boolean, default=False)
    
    # Attendees and relationships
    organizer_email = Column(String(255), index=True)
    organizer_name = Column(String(255))
    attendees = Column(JSONType)  # List of attendee objects with email, name, status
    attendee_emails = Column(JSONType)  # List of attendee emails for quick lookup
    
    # Meeting metadata
    meeting_type = Column(String(100))  # in-person, video_call, phone, etc.
    conference_data = Column(JSONType)  # Google Meet, Zoom links, etc.
    visibility = Column(String(50))  # default, public, private
    
    # Event properties
    is_recurring = Column(Boolean, default=False)
    recurrence_rules = Column(JSONType)  # RRULE data
    is_busy = Column(Boolean, default=True)
    transparency = Column(String(20))  # opaque, transparent
    
    # AI analysis and insights
    ai_summary = Column(Text)  # Claude-generated meeting summary/purpose
    ai_category = Column(String(100))  # AI-determined category (business, personal, etc.)
    importance_score = Column(Float)  # AI-determined importance
    preparation_needed = Column(Boolean, default=False)
    follow_up_required = Column(Boolean, default=False)
    
    # Contact intelligence integration
    known_attendees = Column(JSONType)  # List of person IDs from People table
    unknown_attendees = Column(JSONType)  # Attendees not in contact database
    business_context = Column(Text)  # AI-generated business context based on attendees
    
    # Free time analysis
    is_free_time = Column(Boolean, default=False, index=True)  # For free time slot identification
    potential_duration = Column(Integer)  # Duration in minutes for free slots
    
    # Processing metadata
    fetched_at = Column(DateTime, default=datetime.utcnow)
    last_updated = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    ai_processed_at = Column(DateTime)
    ai_version = Column(String(50))
    
    # Google Calendar metadata
    html_link = Column(Text)  # Link to event in Google Calendar
    hangout_link = Column(Text)  # Google Meet link
    ical_uid = Column(String(255))
    sequence = Column(Integer)  # For tracking updates
    
    # Relationships
    user = relationship("User", backref="calendar_events")
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_calendar_user_event', 'user_id', 'event_id'),
        Index('idx_calendar_user_time', 'user_id', 'start_time'),
        Index('idx_calendar_user_organizer', 'user_id', 'organizer_email'),
        Index('idx_calendar_free_time', 'user_id', 'is_free_time'),
        Index('idx_calendar_status', 'user_id', 'status'),
    )
    
    def __repr__(self):
        return f"<Calendar(event_id='{self.event_id}', title='{self.title}')>"
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'event_id': self.event_id,
            'calendar_id': self.calendar_id,
            'recurring_event_id': self.recurring_event_id,
            'title': self.title,
            'description': self.description,
            'location': self.location,
            'status': self.status,
            'start_time': self.start_time.isoformat() if self.start_time else None,
            'end_time': self.end_time.isoformat() if self.end_time else None,
            'timezone': self.timezone,
            'is_all_day': self.is_all_day,
            'organizer_email': self.organizer_email,
            'organizer_name': self.organizer_name,
            'attendees': self.attendees,
            'attendee_emails': self.attendee_emails,
            'meeting_type': self.meeting_type,
            'conference_data': self.conference_data,
            'visibility': self.visibility,
            'is_recurring': self.is_recurring,
            'recurrence_rules': self.recurrence_rules,
            'is_busy': self.is_busy,
            'transparency': self.transparency,
            'ai_summary': self.ai_summary,
            'ai_category': self.ai_category,
            'importance_score': self.importance_score,
            'preparation_needed': self.preparation_needed,
            'follow_up_required': self.follow_up_required,
            'known_attendees': self.known_attendees,
            'unknown_attendees': self.unknown_attendees,
            'business_context': self.business_context,
            'is_free_time': self.is_free_time,
            'potential_duration': self.potential_duration,
            'fetched_at': self.fetched_at.isoformat() if self.fetched_at else None,
            'last_updated': self.last_updated.isoformat() if self.last_updated else None,
            'ai_processed_at': self.ai_processed_at.isoformat() if self.ai_processed_at else None,
            'ai_version': self.ai_version,
            'html_link': self.html_link,
            'hangout_link': self.hangout_link,
            'ical_uid': self.ical_uid,
            'sequence': self.sequence
        }

class DatabaseManager:
    """Database manager for handling connections and sessions"""
    
    def __init__(self):
        self.engine = None
        self.SessionLocal = None
        self.initialize_database()
    
    def initialize_database(self):
        """Initialize database connection and create tables"""
        try:
            # Use DATABASE_URL from environment or default to SQLite
            database_url = settings.DATABASE_URL
            
            # Handle PostgreSQL URL for Heroku
            if database_url and database_url.startswith('postgres://'):
                database_url = database_url.replace('postgres://', 'postgresql://', 1)
            
            # Create engine with appropriate settings
            if database_url.startswith('postgresql://'):
                # PostgreSQL settings for Heroku
                self.engine = create_engine(
                    database_url,
                    echo=settings.DEBUG,
                    pool_pre_ping=True,
                    pool_recycle=300
                )
            else:
                # SQLite settings for local development
                self.engine = create_engine(
                    database_url,
                    echo=settings.DEBUG,
                    connect_args={"check_same_thread": False}
                )
            
            # Create session factory
            self.SessionLocal = sessionmaker(bind=self.engine)
            
            # Create all tables
            Base.metadata.create_all(bind=self.engine)
            
            logger.info("Database initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize database: {str(e)}")
            raise
    
    def get_session(self) -> Session:
        """Get a new database session"""
        return self.SessionLocal()
    
    def get_user_by_email(self, email: str) -> Optional[User]:
        """Get user by email address"""
        with self.get_session() as session:
            return session.query(User).filter(User.email == email).first()
    
    def create_or_update_user(self, user_info: Dict, credentials: Dict) -> User:
        """Create or update user with OAuth info"""
        with self.get_session() as session:
            user = session.query(User).filter(User.email == user_info['email']).first()
            
            if user:
                # Update existing user
                user.name = user_info.get('name', user.name)
                user.last_login = datetime.utcnow()
                user.access_token = credentials.get('access_token')
                user.refresh_token = credentials.get('refresh_token')
                user.token_expires_at = credentials.get('expires_at')
                user.scopes = credentials.get('scopes', [])
            else:
                # Create new user
                user = User(
                    email=user_info['email'],
                    google_id=user_info['id'],
                    name=user_info.get('name', ''),
                    access_token=credentials.get('access_token'),
                    refresh_token=credentials.get('refresh_token'),
                    token_expires_at=credentials.get('expires_at'),
                    scopes=credentials.get('scopes', [])
                )
                session.add(user)
            
            session.commit()
            session.refresh(user)
            return user
    
    def save_email(self, user_id: int, email_data: Dict) -> Email:
        """Save processed email to database"""
        with self.get_session() as session:
            # Check if email already exists
            existing = session.query(Email).filter(
                Email.user_id == user_id,
                Email.gmail_id == email_data['id']
            ).first()
            
            if existing:
                return existing
            
            # Create new email record
            email = Email(
                user_id=user_id,
                gmail_id=email_data['id'],
                thread_id=email_data.get('thread_id'),
                sender=email_data.get('sender'),
                sender_name=email_data.get('sender_name'),
                subject=email_data.get('subject'),
                body_text=email_data.get('body_text'),
                body_html=email_data.get('body_html'),
                body_clean=email_data.get('body_clean'),
                body_preview=email_data.get('body_preview'),
                snippet=email_data.get('snippet'),
                recipients=email_data.get('recipients', []),
                cc=email_data.get('cc', []),
                bcc=email_data.get('bcc', []),
                labels=email_data.get('labels', []),
                attachments=email_data.get('attachments', []),
                entities=email_data.get('entities', {}),
                email_date=email_data.get('timestamp'),
                size_estimate=email_data.get('size_estimate'),
                message_type=email_data.get('message_type'),
                priority_score=email_data.get('priority_score'),
                is_read=email_data.get('is_read', False),
                is_important=email_data.get('is_important', False),
                is_starred=email_data.get('is_starred', False),
                has_attachments=email_data.get('has_attachments', False),
                normalizer_version=email_data.get('processing_metadata', {}).get('normalizer_version'),
                has_errors=email_data.get('error', False),
                error_message=email_data.get('error_message')
            )
            
            session.add(email)
            session.commit()
            session.refresh(email)
            return email
    
    def save_task(self, user_id: int, email_id: Optional[int], task_data: Dict) -> Task:
        """Save extracted task to database"""
        try:
            with self.get_session() as session:
                task = Task(
                    user_id=user_id,
                    email_id=email_id,
                    description=task_data['description'],
                    assignee=task_data.get('assignee'),
                    due_date=task_data.get('due_date'),
                    due_date_text=task_data.get('due_date_text'),
                    priority=task_data.get('priority', 'medium'),
                    category=task_data.get('category'),
                    confidence=task_data.get('confidence'),
                    source_text=task_data.get('source_text'),
                    status=task_data.get('status', 'pending'),
                    extractor_version=task_data.get('extractor_version'),
                    model_used=task_data.get('model_used')
                )
                
                session.add(task)
                session.commit()
                session.refresh(task)
                
                # Verify the task object is valid before returning
                if not task or not hasattr(task, 'id') or task.id is None:
                    raise ValueError("Failed to create task - invalid task object returned")
                
                return task
                
        except Exception as e:
            logger.error(f"Failed to save task to database: {str(e)}")
            logger.error(f"Task data: {task_data}")
            raise  # Re-raise the exception instead of returning a dict
    
    def get_user_emails(self, user_id: int, limit: int = 50) -> List[Email]:
        """Get emails for a user"""
        with self.get_session() as session:
            return session.query(Email).filter(
                Email.user_id == user_id
            ).order_by(Email.email_date.desc()).limit(limit).all()
    
    def get_user_tasks(self, user_id: int, status: str = None, limit: int = 500) -> List[Task]:
        """Get tasks for a user"""
        with self.get_session() as session:
            query = session.query(Task).filter(Task.user_id == user_id)
            if status:
                query = query.filter(Task.status == status)
            return query.order_by(Task.created_at.desc()).limit(limit).all()

    def create_or_update_person(self, user_id: int, person_data: Dict) -> Person:
        """Create or update a person record"""
        with self.get_session() as session:
            # Try to find existing person by email or name
            person = session.query(Person).filter(
                Person.user_id == user_id,
                Person.email_address == person_data.get('email_address')
            ).first()
            
            if not person and person_data.get('name'):
                # Try by name if email not found
                person = session.query(Person).filter(
                    Person.user_id == user_id,
                    Person.name == person_data.get('name')
                ).first()
            
            if person:
                # Update existing person
                for key, value in person_data.items():
                    if hasattr(person, key) and value is not None:
                        setattr(person, key, value)
                person.last_interaction = datetime.utcnow()
                person.total_emails += 1
                person.last_updated_by_ai = datetime.utcnow()
            else:
                # Create new person - remove conflicting fields from person_data
                person_data_clean = person_data.copy()
                person_data_clean.pop('total_emails', None)  # Remove if present
                person_data_clean.pop('last_updated_by_ai', None)  # Remove if present
                
                person = Person(
                    user_id=user_id,
                    **person_data_clean,
                    total_emails=1,
                    last_updated_by_ai=datetime.utcnow()
                )
                session.add(person)
            
            session.commit()
            session.refresh(person)
            return person
    
    def create_or_update_project(self, user_id: int, project_data: Dict) -> Project:
        """Create or update a project record"""
        with self.get_session() as session:
            # Try to find existing project by name or slug
            project = session.query(Project).filter(
                Project.user_id == user_id,
                Project.name == project_data.get('name')
            ).first()
            
            if project:
                # Update existing project
                for key, value in project_data.items():
                    if hasattr(project, key) and value is not None:
                        setattr(project, key, value)
                project.last_activity = datetime.utcnow()
                project.total_emails += 1
                project.updated_at = datetime.utcnow()
            else:
                # Create new project
                project = Project(
                    user_id=user_id,
                    **project_data,
                    total_emails=1,
                    updated_at=datetime.utcnow()
                )
                session.add(project)
            
            session.commit()
            session.refresh(project)
            return project
    
    def get_user_people(self, user_id: int, limit: int = 500) -> List[Person]:
        """Get people for a user"""
        with self.get_session() as session:
            query = session.query(Person).filter(Person.user_id == user_id)
            query = query.order_by(Person.last_interaction.desc())
            if limit:
                query = query.limit(limit)
            return query.all()
    
    def get_user_projects(self, user_id: int, status: str = None, limit: int = 200) -> List[Project]:
        """Get projects for a user"""
        with self.get_session() as session:
            query = session.query(Project).filter(Project.user_id == user_id)
            if status:
                query = query.filter(Project.status == status)
            query = query.order_by(Project.last_activity.desc())
            if limit:
                query = query.limit(limit)
            return query.all()
    
    def find_person_by_email(self, user_id: int, email: str) -> Optional[Person]:
        """Find person by email address"""
        with self.get_session() as session:
            return session.query(Person).filter(
                Person.user_id == user_id,
                Person.email_address == email
            ).first()
    
    def find_project_by_keywords(self, user_id: int, keywords: List[str]) -> Optional[Project]:
        """Find project by matching keywords against name, description, or topics - FIXED to prevent memory issues"""
        with self.get_session() as session:
            # CRITICAL FIX: Add limit to prevent loading too many projects
            projects = session.query(Project).filter(Project.user_id == user_id).limit(50).all()
            
            for project in projects:
                # Check name and description
                if any(keyword.lower() in (project.name or '').lower() for keyword in keywords):
                    return project
                if any(keyword.lower() in (project.description or '').lower() for keyword in keywords):
                    return project
                
                # Check key topics
                if project.key_topics:
                    project_topics = [topic.lower() for topic in project.key_topics]
                    if any(keyword.lower() in project_topics for keyword in keywords):
                        return project
            
            return None

    def get_user_topics(self, user_id: int, limit: int = 1000) -> List[Topic]:
        """Get all topics for a user"""
        with self.get_session() as session:
            return session.query(Topic).filter(
                Topic.user_id == user_id
            ).order_by(Topic.is_official.desc(), Topic.name.asc()).limit(limit).all()
    
    def create_or_update_topic(self, user_id: int, topic_data: Dict) -> Topic:
        """Create or update a topic record"""
        with self.get_session() as session:
            # Try to find existing topic by name
            topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.name == topic_data.get('name')
            ).first()
            
            # Handle keywords conversion to JSON string
            topic_data_copy = topic_data.copy()
            if 'keywords' in topic_data_copy and isinstance(topic_data_copy['keywords'], list):
                topic_data_copy['keywords'] = json.dumps(topic_data_copy['keywords'])
            
            if topic:
                # Update existing topic
                for key, value in topic_data_copy.items():
                    if hasattr(topic, key) and key != 'id':
                        setattr(topic, key, value)
                topic.updated_at = datetime.now()
            else:
                # Create new topic
                topic_data_copy['user_id'] = user_id
                topic_data_copy['created_at'] = datetime.now()
                topic_data_copy['updated_at'] = datetime.now()
                
                # Set default values for optional fields
                if 'slug' not in topic_data_copy:
                    topic_data_copy['slug'] = topic_data_copy['name'].lower().replace(' ', '-').replace('_', '-')
                
                if 'is_official' not in topic_data_copy:
                    topic_data_copy['is_official'] = False
                    
                if 'confidence_score' not in topic_data_copy:
                    topic_data_copy['confidence_score'] = 0.5
                    
                if 'email_count' not in topic_data_copy:
                    topic_data_copy['email_count'] = 0
                
                topic = Topic(**topic_data_copy)
                session.add(topic)
            
            session.commit()
            session.refresh(topic)
            return topic

    def update_topic(self, user_id: int, topic_id: int, topic_data: Dict) -> bool:
        """Update a specific topic by ID"""
        with self.get_session() as session:
            topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == topic_id
            ).first()
            
            if not topic:
                return False
            
            # Handle keywords conversion to JSON string
            for key, value in topic_data.items():
                if hasattr(topic, key) and value is not None:
                    if key == 'keywords' and isinstance(value, list):
                        setattr(topic, key, json.dumps(value))
                    else:
                        setattr(topic, key, value)
            
            topic.updated_at = datetime.utcnow()
            session.commit()
            return True

    def mark_topic_official(self, user_id: int, topic_id: int) -> bool:
        """Mark a topic as official"""
        with self.get_session() as session:
            topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == topic_id
            ).first()
            
            if not topic:
                return False
            
            topic.is_official = True
            topic.updated_at = datetime.utcnow()
            session.commit()
            return True

    def merge_topics(self, user_id: int, source_topic_id: int, target_topic_id: int) -> bool:
        """Merge one topic into another"""
        with self.get_session() as session:
            source_topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == source_topic_id
            ).first()
            
            target_topic = session.query(Topic).filter(
                Topic.user_id == user_id,
                Topic.id == target_topic_id
            ).first()
            
            if not source_topic or not target_topic:
                return False
            
            try:
                # Update all emails that reference the source topic
                # This is a simplified version - in practice, you'd need to update
                # the topics JSON array in emails to replace source with target
                
                # For now, we'll merge the email counts and keywords
                target_topic.email_count = (target_topic.email_count or 0) + (source_topic.email_count or 0)
                
                # Merge keywords
                source_keywords = json.loads(source_topic.keywords) if source_topic.keywords else []
                target_keywords = json.loads(target_topic.keywords) if target_topic.keywords else []
                merged_keywords = list(set(source_keywords + target_keywords))
                target_topic.keywords = json.dumps(merged_keywords)
                
                # Update merge tracking
                merged_topics = json.loads(target_topic.merged_topics) if target_topic.merged_topics else []
                merged_topics.append(source_topic.name)
                target_topic.merged_topics = json.dumps(merged_topics)
                
                target_topic.updated_at = datetime.utcnow()
                
                # Delete the source topic
                session.delete(source_topic)
                session.commit()
                return True
                
            except Exception as e:
                session.rollback()
                logger.error(f"Failed to merge topics: {str(e)}")
                return False

    # ===== SMART CONTACT STRATEGY METHODS =====
    
    def create_or_update_trusted_contact(self, user_id: int, contact_data: Dict) -> TrustedContact:
        """Create or update a trusted contact record"""
        with self.get_session() as session:
            contact = session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id,
                TrustedContact.email_address == contact_data['email_address']
            ).first()
            
            if contact:
                # Update existing contact
                for key, value in contact_data.items():
                    if hasattr(contact, key) and value is not None:
                        setattr(contact, key, value)
                contact.updated_at = datetime.utcnow()
            else:
                # Create new trusted contact
                contact = TrustedContact(
                    user_id=user_id,
                    **contact_data,
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                session.add(contact)
            
            session.commit()
            session.refresh(contact)
            return contact
    
    def get_trusted_contacts(self, user_id: int, limit: int = 500) -> List[TrustedContact]:
        """Get trusted contacts for a user"""
        with self.get_session() as session:
            return session.query(TrustedContact).filter(
                TrustedContact.user_id == user_id
            ).order_by(TrustedContact.engagement_score.desc()).limit(limit).all()
    
    def find_trusted_contact_by_email(self, user_id: int, email_address: str) -> Optional[TrustedContact]:
